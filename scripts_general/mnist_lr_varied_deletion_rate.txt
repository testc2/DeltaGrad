period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression MNIST5 16384 120 5
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  MNIST5 1
torch.Size([60000, 785])
tensor([59248, 44444, 42701])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9374, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6658, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1859, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1028, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8775, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8515, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8250, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7461, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7179, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6901, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6639, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6197, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.279228925704956
training time full:: 3.2792975902557373
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5525906085968018
overhead:: 0
overhead2:: 0.10843420028686523
overhead3:: 0
time_baseline:: 2.552623748779297
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.003976583480834961
overhead3:: 0.02494525909423828
overhead4:: 0.47200798988342285
overhead5:: 0
memory usage:: 3030560768
time_provenance:: 0.7360808849334717
curr_diff: 0 tensor(7.2348e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2348e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005535125732421875
overhead3:: 0.0314328670501709
overhead4:: 0.6358954906463623
overhead5:: 0
memory usage:: 3011063808
time_provenance:: 0.9158856868743896
curr_diff: 0 tensor(6.7132e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7132e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006882667541503906
overhead3:: 0.037944793701171875
overhead4:: 0.801729679107666
overhead5:: 0
memory usage:: 3037478912
time_provenance:: 1.1006817817687988
curr_diff: 0 tensor(6.8173e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8173e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0059642791748046875
overhead3:: 0.03447723388671875
overhead4:: 0.7315857410430908
overhead5:: 0
memory usage:: 3047550976
time_provenance:: 1.048525094985962
curr_diff: 0 tensor(1.0355e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0355e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007405281066894531
overhead3:: 0.04015994071960449
overhead4:: 0.8381109237670898
overhead5:: 0
memory usage:: 3031748608
time_provenance:: 1.168583869934082
curr_diff: 0 tensor(7.3232e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3232e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009356498718261719
overhead3:: 0.04483747482299805
overhead4:: 0.9891715049743652
overhead5:: 0
memory usage:: 3031486464
time_provenance:: 1.3297016620635986
curr_diff: 0 tensor(6.4889e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4889e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012458324432373047
overhead3:: 0.060739755630493164
overhead4:: 1.4829273223876953
overhead5:: 0
memory usage:: 3038826496
time_provenance:: 1.999084711074829
curr_diff: 0 tensor(3.4021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01292872428894043
overhead3:: 0.06335020065307617
overhead4:: 1.5869503021240234
overhead5:: 0
memory usage:: 3024162816
time_provenance:: 2.0958003997802734
curr_diff: 0 tensor(3.1754e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1754e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.015572547912597656
overhead3:: 0.06741690635681152
overhead4:: 1.7061257362365723
overhead5:: 0
memory usage:: 3010932736
time_provenance:: 2.216676950454712
curr_diff: 0 tensor(3.0961e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0961e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.023820877075195312
overhead3:: 0.1061546802520752
overhead4:: 2.4723219871520996
overhead5:: 0
memory usage:: 3009703936
time_provenance:: 2.9441068172454834
curr_diff: 0 tensor(1.6170e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6170e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9260, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4573, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1858, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9251, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8142, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7667, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6761, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6752, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6546, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6316, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6129, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5717, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.270998477935791
training time full:: 3.271063804626465
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([26770, 18012, 37909, 33147, 28233, 13327, 11108,  2712, 11819, 44357,
         5269,  4991, 21096, 43114, 24201, 13413, 29525, 53075, 49804, 51335,
         7365, 50552, 16433, 57023, 37985, 48821, 35843, 24298, 46940, 40326,
        45801, 49624, 24173, 41747, 35429, 47384,  5811, 37869, 18342, 31822,
        47905, 42285, 34379,  1411, 33429, 39755, 24120, 40946, 15717, 59289])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.554396390914917
overhead:: 0
overhead2:: 0.1152489185333252
overhead3:: 0
time_baseline:: 2.5544283390045166
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005221843719482422
overhead3:: 0.025786638259887695
overhead4:: 0.4885427951812744
overhead5:: 0
memory usage:: 3047997440
time_provenance:: 0.7617988586425781
curr_diff: 0 tensor(7.5429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006582736968994141
overhead3:: 0.0317227840423584
overhead4:: 0.6359241008758545
overhead5:: 0
memory usage:: 3013033984
time_provenance:: 0.926764726638794
curr_diff: 0 tensor(7.2799e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2799e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0075893402099609375
overhead3:: 0.040114641189575195
overhead4:: 0.8235423564910889
overhead5:: 0
memory usage:: 3014385664
time_provenance:: 1.1349124908447266
curr_diff: 0 tensor(6.6874e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6874e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007692813873291016
overhead3:: 0.03454017639160156
overhead4:: 0.7009284496307373
overhead5:: 0
memory usage:: 3024420864
time_provenance:: 1.0282561779022217
curr_diff: 0 tensor(7.6766e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6766e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009032487869262695
overhead3:: 0.04052472114562988
overhead4:: 0.8467545509338379
overhead5:: 0
memory usage:: 3029557248
time_provenance:: 1.1818041801452637
curr_diff: 0 tensor(7.2943e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2943e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009803533554077148
overhead3:: 0.04751157760620117
overhead4:: 0.9895098209381104
overhead5:: 0
memory usage:: 3011182592
time_provenance:: 1.332472324371338
curr_diff: 0 tensor(7.0553e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0553e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.013490915298461914
overhead3:: 0.06024289131164551
overhead4:: 1.5047922134399414
overhead5:: 0
memory usage:: 3032113152
time_provenance:: 2.030866861343384
curr_diff: 0 tensor(3.4896e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4896e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014186382293701172
overhead3:: 0.06490492820739746
overhead4:: 1.626688003540039
overhead5:: 0
memory usage:: 3039895552
time_provenance:: 2.1476399898529053
curr_diff: 0 tensor(3.3383e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3383e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01582026481628418
overhead3:: 0.06701445579528809
overhead4:: 1.6563928127288818
overhead5:: 0
memory usage:: 3011674112
time_provenance:: 2.1670196056365967
curr_diff: 0 tensor(3.2041e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2041e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.02350449562072754
overhead3:: 0.10032963752746582
overhead4:: 2.411746025085449
overhead5:: 0
memory usage:: 3031408640
time_provenance:: 2.8994743824005127
curr_diff: 0 tensor(1.6170e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6170e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3338, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9362, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6590, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4590, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7734, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7530, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7103, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6829, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6721, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6439, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6230, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5731, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2552402019500732
training time full:: 3.2553117275238037
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5573742389678955
overhead:: 0
overhead2:: 0.11239027976989746
overhead3:: 0
time_baseline:: 2.5574069023132324
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004916667938232422
overhead3:: 0.026066303253173828
overhead4:: 0.4900541305541992
overhead5:: 0
memory usage:: 3044933632
time_provenance:: 0.747251033782959
curr_diff: 0 tensor(7.4882e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4882e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0067005157470703125
overhead3:: 0.031011581420898438
overhead4:: 0.6525804996490479
overhead5:: 0
memory usage:: 3030630400
time_provenance:: 0.9366638660430908
curr_diff: 0 tensor(7.0253e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0253e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007662296295166016
overhead3:: 0.038315773010253906
overhead4:: 0.8133137226104736
overhead5:: 0
memory usage:: 3022131200
time_provenance:: 1.1132276058197021
curr_diff: 0 tensor(6.8015e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8015e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007103443145751953
overhead3:: 0.03272676467895508
overhead4:: 0.7087500095367432
overhead5:: 0
memory usage:: 3016949760
time_provenance:: 1.016587734222412
curr_diff: 0 tensor(9.7557e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7557e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00817108154296875
overhead3:: 0.04119443893432617
overhead4:: 0.8521993160247803
overhead5:: 0
memory usage:: 3037462528
time_provenance:: 1.174103021621704
curr_diff: 0 tensor(8.5155e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5155e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009766817092895508
overhead3:: 0.04480385780334473
overhead4:: 0.9790420532226562
overhead5:: 0
memory usage:: 3048222720
time_provenance:: 1.3209526538848877
curr_diff: 0 tensor(8.1390e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1390e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011598825454711914
overhead3:: 0.0596156120300293
overhead4:: 1.5071110725402832
overhead5:: 0
memory usage:: 3016740864
time_provenance:: 2.0256636142730713
curr_diff: 0 tensor(4.0660e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0660e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01202392578125
overhead3:: 0.06366682052612305
overhead4:: 1.5740022659301758
overhead5:: 0
memory usage:: 3030855680
time_provenance:: 2.082890033721924
curr_diff: 0 tensor(3.6627e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6627e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.013639450073242188
overhead3:: 0.06705284118652344
overhead4:: 1.6909914016723633
overhead5:: 0
memory usage:: 3036688384
time_provenance:: 2.196251392364502
curr_diff: 0 tensor(3.5298e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5298e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.022646665573120117
overhead3:: 0.09966635704040527
overhead4:: 2.384965419769287
overhead5:: 0
memory usage:: 3028709376
time_provenance:: 2.881495237350464
curr_diff: 0 tensor(1.6109e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6109e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3417, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9579, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6817, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4714, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1059, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0354, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8172, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7875, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7676, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7413, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6971, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6797, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6573, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6284, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5890, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5880, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2443525791168213
training time full:: 3.2444241046905518
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.571453809738159
overhead:: 0
overhead2:: 0.11797118186950684
overhead3:: 0
time_baseline:: 2.571484088897705
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0046308040618896484
overhead3:: 0.025536298751831055
overhead4:: 0.5015711784362793
overhead5:: 0
memory usage:: 3043438592
time_provenance:: 0.7607989311218262
curr_diff: 0 tensor(7.8692e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8692e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005815029144287109
overhead3:: 0.031011104583740234
overhead4:: 0.6342170238494873
overhead5:: 0
memory usage:: 3016577024
time_provenance:: 0.9260272979736328
curr_diff: 0 tensor(7.1555e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1555e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008111715316772461
overhead3:: 0.03817605972290039
overhead4:: 0.8320701122283936
overhead5:: 0
memory usage:: 3047903232
time_provenance:: 1.1403465270996094
curr_diff: 0 tensor(6.8939e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8939e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0067746639251708984
overhead3:: 0.03368091583251953
overhead4:: 0.7045211791992188
overhead5:: 0
memory usage:: 3052068864
time_provenance:: 1.0250728130340576
curr_diff: 0 tensor(9.5758e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5758e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007809638977050781
overhead3:: 0.03991532325744629
overhead4:: 0.8512930870056152
overhead5:: 0
memory usage:: 3038523392
time_provenance:: 1.1864228248596191
curr_diff: 0 tensor(7.2522e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2522e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009410858154296875
overhead3:: 0.0478365421295166
overhead4:: 1.030052661895752
overhead5:: 0
memory usage:: 3014275072
time_provenance:: 1.3877193927764893
curr_diff: 0 tensor(7.1006e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1006e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01385498046875
overhead3:: 0.05999469757080078
overhead4:: 1.521263599395752
overhead5:: 0
memory usage:: 3013853184
time_provenance:: 2.0475873947143555
curr_diff: 0 tensor(4.5351e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5351e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012901782989501953
overhead3:: 0.0623476505279541
overhead4:: 1.6052522659301758
overhead5:: 0
memory usage:: 3023781888
time_provenance:: 2.121537446975708
curr_diff: 0 tensor(3.8437e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8437e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.013482093811035156
overhead3:: 0.06886863708496094
overhead4:: 1.6904091835021973
overhead5:: 0
memory usage:: 3038605312
time_provenance:: 2.2099502086639404
curr_diff: 0 tensor(3.7932e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7932e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.023661136627197266
overhead3:: 0.10135269165039062
overhead4:: 2.3990366458892822
overhead5:: 0
memory usage:: 3011239936
time_provenance:: 2.903458595275879
curr_diff: 0 tensor(1.6162e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6162e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3245, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9497, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6668, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4646, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3154, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1104, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0370, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9819, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8515, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8212, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7895, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7474, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.7005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6702, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6613, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6416, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6209, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6107, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5990, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5991, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.280691146850586
training time full:: 3.280759334564209
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([48968, 47377, 17672, 24730, 24136,  7146,  6649, 32061, 20276, 18883,
        57475, 12616, 54664, 35350, 56637, 11036,  4033, 39182, 55859,  6883,
         5467, 57160, 10029, 28646, 58346,  7344, 23429, 52804,  3362, 24386,
        10770, 29557,  5270, 59934, 21536, 16575,  8128,   551, 22721,  3832,
        20688, 11679, 13518, 51947, 46458, 36242, 23089,  5387, 53218, 14888])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5682859420776367
overhead:: 0
overhead2:: 0.11488723754882812
overhead3:: 0
time_baseline:: 2.568317413330078
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0032677650451660156
overhead3:: 0.02499866485595703
overhead4:: 0.4750096797943115
overhead5:: 0
memory usage:: 3029954560
time_provenance:: 0.735360860824585
curr_diff: 0 tensor(7.9373e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9373e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005318164825439453
overhead3:: 0.0312502384185791
overhead4:: 0.630995512008667
overhead5:: 0
memory usage:: 3029782528
time_provenance:: 0.9188272953033447
curr_diff: 0 tensor(6.8136e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8136e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007292747497558594
overhead3:: 0.038674354553222656
overhead4:: 0.8113009929656982
overhead5:: 0
memory usage:: 3030577152
time_provenance:: 1.1251740455627441
curr_diff: 0 tensor(7.7971e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7971e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006639242172241211
overhead3:: 0.03612399101257324
overhead4:: 0.7252302169799805
overhead5:: 0
memory usage:: 3023486976
time_provenance:: 1.05741286277771
curr_diff: 0 tensor(9.0703e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0703e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008103370666503906
overhead3:: 0.03805851936340332
overhead4:: 0.8396487236022949
overhead5:: 0
memory usage:: 3040161792
time_provenance:: 1.1719577312469482
curr_diff: 0 tensor(8.3974e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3974e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009228229522705078
overhead3:: 0.046608686447143555
overhead4:: 1.0108695030212402
overhead5:: 0
memory usage:: 3012984832
time_provenance:: 1.3726274967193604
curr_diff: 0 tensor(8.0088e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0088e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012451410293579102
overhead3:: 0.05902457237243652
overhead4:: 1.5200073719024658
overhead5:: 0
memory usage:: 3027554304
time_provenance:: 2.0384583473205566
curr_diff: 0 tensor(4.0798e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0798e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014521360397338867
overhead3:: 0.06302714347839355
overhead4:: 1.5760538578033447
overhead5:: 0
memory usage:: 3018838016
time_provenance:: 2.095243453979492
curr_diff: 0 tensor(3.9130e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9130e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014870643615722656
overhead3:: 0.06768035888671875
overhead4:: 1.6956324577331543
overhead5:: 0
memory usage:: 3023372288
time_provenance:: 2.2194581031799316
curr_diff: 0 tensor(3.9443e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9443e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.02303624153137207
overhead3:: 0.10428428649902344
overhead4:: 2.410510540008545
overhead5:: 0
memory usage:: 3013128192
time_provenance:: 2.9166183471679688
curr_diff: 0 tensor(1.6138e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6138e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873900
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2826, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0836, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7690, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.2966, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1821, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0566, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0241, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9679, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9435, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.8977, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8779, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8433, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8258, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8122, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8012, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7629, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7287, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7039, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6567, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6514, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6345, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6297, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6151, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6055, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5892, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5884, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.42226243019104
training time full:: 6.422327995300293
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.948370456695557
overhead:: 0
overhead2:: 0.3340730667114258
overhead3:: 0
time_baseline:: 4.948402404785156
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.005215167999267578
overhead3:: 0.029181957244873047
overhead4:: 0.9480588436126709
overhead5:: 0
memory usage:: 3012927488
time_provenance:: 1.3214137554168701
curr_diff: 0 tensor(8.8383e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8383e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.007344961166381836
overhead3:: 0.03664255142211914
overhead4:: 1.301527500152588
overhead5:: 0
memory usage:: 3010965504
time_provenance:: 1.7351315021514893
curr_diff: 0 tensor(8.5660e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5660e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.010099649429321289
overhead3:: 0.04343819618225098
overhead4:: 1.6572167873382568
overhead5:: 0
memory usage:: 3042648064
time_provenance:: 2.161754846572876
curr_diff: 0 tensor(7.9033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9033e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.008413314819335938
overhead3:: 0.03801441192626953
overhead4:: 1.40966796875
overhead5:: 0
memory usage:: 3012628480
time_provenance:: 1.8891692161560059
curr_diff: 0 tensor(8.1431e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1431e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.010549306869506836
overhead3:: 0.04409337043762207
overhead4:: 1.7279319763183594
overhead5:: 0
memory usage:: 3031502848
time_provenance:: 2.2510368824005127
curr_diff: 0 tensor(7.3748e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3748e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.012954950332641602
overhead3:: 0.05046439170837402
overhead4:: 2.0820424556732178
overhead5:: 0
memory usage:: 3019124736
time_provenance:: 2.6740424633026123
curr_diff: 0 tensor(7.2414e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2414e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.017277002334594727
overhead3:: 0.06446123123168945
overhead4:: 2.9259228706359863
overhead5:: 0
memory usage:: 3041595392
time_provenance:: 3.729022741317749
curr_diff: 0 tensor(2.8837e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8837e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.018569469451904297
overhead3:: 0.06796860694885254
overhead4:: 3.0509824752807617
overhead5:: 0
memory usage:: 3018506240
time_provenance:: 3.8718454837799072
curr_diff: 0 tensor(2.7986e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7986e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.020416259765625
overhead3:: 0.07072091102600098
overhead4:: 3.2441933155059814
overhead5:: 0
memory usage:: 3039608832
time_provenance:: 4.095757007598877
curr_diff: 0 tensor(2.7914e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7914e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.032704830169677734
overhead3:: 0.10869002342224121
overhead4:: 5.068163156509399
overhead5:: 0
memory usage:: 3057373184
time_provenance:: 6.195317268371582
curr_diff: 0 tensor(1.5195e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5195e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3352, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8028, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6756, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4687, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2539, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1967, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1498, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1081, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0698, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0049, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9767, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8895, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8667, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8543, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8332, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8048, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.8001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7713, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7371, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7059, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6860, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6803, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6661, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6614, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6360, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6229, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6105, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6062, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5908, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.421900272369385
training time full:: 6.421969413757324
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([26770, 18012, 37909, 33147, 28233, 13327, 11108,  2712, 11819, 44357,
         5269,  4991, 21096, 43114, 24201, 13413, 29525, 53075, 49804, 51335,
         7365, 50552, 16433, 57023, 37985, 48821, 35843, 24298, 46940, 40326,
        45801, 49624, 24173, 41747, 35429, 47384,  5811, 37869, 18342, 31822,
        47905, 42285, 34379,  1411, 33429, 39755, 24120, 40946, 15717, 59289])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.847275495529175
overhead:: 0
overhead2:: 0.3375828266143799
overhead3:: 0
time_baseline:: 4.847307443618774
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.0059587955474853516
overhead3:: 0.029804229736328125
overhead4:: 0.9396748542785645
overhead5:: 0
memory usage:: 3025371136
time_provenance:: 1.337329387664795
curr_diff: 0 tensor(9.2067e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2067e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.008852481842041016
overhead3:: 0.036751747131347656
overhead4:: 1.35829496383667
overhead5:: 0
memory usage:: 3035148288
time_provenance:: 1.8344874382019043
curr_diff: 0 tensor(9.2035e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2035e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011469364166259766
overhead3:: 0.04433941841125488
overhead4:: 1.738523244857788
overhead5:: 0
memory usage:: 3020935168
time_provenance:: 2.2913334369659424
curr_diff: 0 tensor(8.5124e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5124e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.009693145751953125
overhead3:: 0.03860807418823242
overhead4:: 1.4257659912109375
overhead5:: 0
memory usage:: 3015618560
time_provenance:: 1.9216773509979248
curr_diff: 0 tensor(8.9148e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9148e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011530637741088867
overhead3:: 0.04480433464050293
overhead4:: 1.7941746711730957
overhead5:: 0
memory usage:: 3026116608
time_provenance:: 2.3529775142669678
curr_diff: 0 tensor(8.6320e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6320e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.013818740844726562
overhead3:: 0.050765037536621094
overhead4:: 2.10229229927063
overhead5:: 0
memory usage:: 3012808704
time_provenance:: 2.7332828044891357
curr_diff: 0 tensor(8.5172e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5172e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.017569541931152344
overhead3:: 0.06199812889099121
overhead4:: 2.9203901290893555
overhead5:: 0
memory usage:: 3016441856
time_provenance:: 3.7409207820892334
curr_diff: 0 tensor(2.5257e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5257e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.018531084060668945
overhead3:: 0.06542110443115234
overhead4:: 3.118180513381958
overhead5:: 0
memory usage:: 3033530368
time_provenance:: 3.9757678508758545
curr_diff: 0 tensor(2.2141e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2141e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.020289897918701172
overhead3:: 0.07109570503234863
overhead4:: 3.268524646759033
overhead5:: 0
memory usage:: 3017617408
time_provenance:: 4.166869640350342
curr_diff: 0 tensor(2.1476e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1476e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.03359413146972656
overhead3:: 0.10841894149780273
overhead4:: 4.990647554397583
overhead5:: 0
memory usage:: 3012059136
time_provenance:: 6.149689197540283
curr_diff: 0 tensor(1.5055e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5055e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6706, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5587, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3795, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3105, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1931, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1466, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0341, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8631, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8279, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8057, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7790, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7639, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7428, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7256, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6795, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6747, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6649, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6624, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6520, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6553, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6450, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6383, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6289, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6223, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6197, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5904, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5855, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.377685308456421
training time full:: 6.377751350402832
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.885694742202759
overhead:: 0
overhead2:: 0.32534360885620117
overhead3:: 0
time_baseline:: 4.885725975036621
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.006351947784423828
overhead3:: 0.02979302406311035
overhead4:: 0.9476947784423828
overhead5:: 0
memory usage:: 3011256320
time_provenance:: 1.333059549331665
curr_diff: 0 tensor(9.1415e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1415e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.008221864700317383
overhead3:: 0.035568952560424805
overhead4:: 1.3083748817443848
overhead5:: 0
memory usage:: 3010822144
time_provenance:: 1.7642254829406738
curr_diff: 0 tensor(7.7767e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7767e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011156082153320312
overhead3:: 0.044614315032958984
overhead4:: 1.6503384113311768
overhead5:: 0
memory usage:: 3046440960
time_provenance:: 2.1776511669158936
curr_diff: 0 tensor(7.6536e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6536e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.009804010391235352
overhead3:: 0.03855180740356445
overhead4:: 1.4537205696105957
overhead5:: 0
memory usage:: 3015409664
time_provenance:: 1.9381661415100098
curr_diff: 0 tensor(7.1580e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1580e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.01182103157043457
overhead3:: 0.044832468032836914
overhead4:: 1.7663052082061768
overhead5:: 0
memory usage:: 3012665344
time_provenance:: 2.3085827827453613
curr_diff: 0 tensor(6.6393e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6393e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.013863325119018555
overhead3:: 0.05091571807861328
overhead4:: 2.0829899311065674
overhead5:: 0
memory usage:: 3011305472
time_provenance:: 2.688467025756836
curr_diff: 0 tensor(6.4050e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4050e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.0169675350189209
overhead3:: 0.06331586837768555
overhead4:: 2.9416582584381104
overhead5:: 0
memory usage:: 3013808128
time_provenance:: 3.7517056465148926
curr_diff: 0 tensor(2.0718e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0718e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.01849651336669922
overhead3:: 0.06645488739013672
overhead4:: 3.0500049591064453
overhead5:: 0
memory usage:: 3014852608
time_provenance:: 3.8914318084716797
curr_diff: 0 tensor(2.0063e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0063e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.02155303955078125
overhead3:: 0.06841421127319336
overhead4:: 3.289346694946289
overhead5:: 0
memory usage:: 3014221824
time_provenance:: 4.158339738845825
curr_diff: 0 tensor(1.9469e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9469e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.03193163871765137
overhead3:: 0.10935735702514648
overhead4:: 4.986664772033691
overhead5:: 0
memory usage:: 3015315456
time_provenance:: 6.101294994354248
curr_diff: 0 tensor(1.5054e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5054e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3250, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1184, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7975, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5593, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4636, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3815, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3121, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2480, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0332, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9485, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8790, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8376, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8134, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7820, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7670, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7558, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7403, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7273, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7173, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7101, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6884, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6629, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6511, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6520, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6359, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6265, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6225, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6214, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6116, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6134, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6090, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5915, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.418203353881836
training time full:: 6.418269157409668
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.858027935028076
overhead:: 0
overhead2:: 0.34639859199523926
overhead3:: 0
time_baseline:: 4.858060121536255
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.006685018539428711
overhead3:: 0.02977776527404785
overhead4:: 0.9575157165527344
overhead5:: 0
memory usage:: 3015147520
time_provenance:: 1.3399293422698975
curr_diff: 0 tensor(9.2488e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2488e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.008843660354614258
overhead3:: 0.03745269775390625
overhead4:: 1.3178162574768066
overhead5:: 0
memory usage:: 3065102336
time_provenance:: 1.7758638858795166
curr_diff: 0 tensor(7.9254e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9254e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011513710021972656
overhead3:: 0.04275226593017578
overhead4:: 1.732665777206421
overhead5:: 0
memory usage:: 3026276352
time_provenance:: 2.2683522701263428
curr_diff: 0 tensor(7.7337e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7337e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.010015487670898438
overhead3:: 0.037247419357299805
overhead4:: 1.4026477336883545
overhead5:: 0
memory usage:: 3018481664
time_provenance:: 1.8871517181396484
curr_diff: 0 tensor(8.5066e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5066e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011916160583496094
overhead3:: 0.04372835159301758
overhead4:: 1.8077583312988281
overhead5:: 0
memory usage:: 3018620928
time_provenance:: 2.3592731952667236
curr_diff: 0 tensor(7.9744e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9744e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.014767885208129883
overhead3:: 0.05130481719970703
overhead4:: 2.0971240997314453
overhead5:: 0
memory usage:: 3035103232
time_provenance:: 2.714752674102783
curr_diff: 0 tensor(7.8617e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8617e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.01904010772705078
overhead3:: 0.06269288063049316
overhead4:: 2.855018138885498
overhead5:: 0
memory usage:: 3013722112
time_provenance:: 3.6671066284179688
curr_diff: 0 tensor(2.7518e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7518e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.020424365997314453
overhead3:: 0.06856894493103027
overhead4:: 3.1647377014160156
overhead5:: 0
memory usage:: 3024347136
time_provenance:: 4.015722990036011
curr_diff: 0 tensor(2.4316e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4316e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.021812915802001953
overhead3:: 0.07081770896911621
overhead4:: 3.341068983078003
overhead5:: 0
memory usage:: 3013459968
time_provenance:: 4.219745874404907
curr_diff: 0 tensor(2.2685e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2685e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.03277015686035156
overhead3:: 0.10913324356079102
overhead4:: 5.038444757461548
overhead5:: 0
memory usage:: 3038048256
time_provenance:: 6.208987712860107
curr_diff: 0 tensor(1.5071e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5071e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3175, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9461, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5624, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4672, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2537, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1082, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0726, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0036, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9771, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9528, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8514, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8379, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8182, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7821, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7666, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7607, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7473, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7408, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7198, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7038, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6977, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6846, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6800, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6645, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6561, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6516, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6339, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6357, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6178, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6130, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5971, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5976, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.6002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5935, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5865, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.413407325744629
training time full:: 6.413473129272461
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.860807657241821
overhead:: 0
overhead2:: 0.3513188362121582
overhead3:: 0
time_baseline:: 4.86083984375
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.0059201717376708984
overhead3:: 0.030475854873657227
overhead4:: 0.9458045959472656
overhead5:: 0
memory usage:: 3018559488
time_provenance:: 1.3165063858032227
curr_diff: 0 tensor(8.2972e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2972e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.008291244506835938
overhead3:: 0.036054134368896484
overhead4:: 1.3365745544433594
overhead5:: 0
memory usage:: 3020095488
time_provenance:: 1.773956537246704
curr_diff: 0 tensor(8.0233e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0233e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.010869264602661133
overhead3:: 0.043853044509887695
overhead4:: 1.6910862922668457
overhead5:: 0
memory usage:: 3019034624
time_provenance:: 2.205868721008301
curr_diff: 0 tensor(7.2281e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2281e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.009634017944335938
overhead3:: 0.03838658332824707
overhead4:: 1.4501025676727295
overhead5:: 0
memory usage:: 3039801344
time_provenance:: 1.9223456382751465
curr_diff: 0 tensor(7.0471e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0471e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.011981010437011719
overhead3:: 0.04614853858947754
overhead4:: 1.7489352226257324
overhead5:: 0
memory usage:: 3012030464
time_provenance:: 2.292793035507202
curr_diff: 0 tensor(6.1948e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1948e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.013855218887329102
overhead3:: 0.05090951919555664
overhead4:: 2.0798094272613525
overhead5:: 0
memory usage:: 3031236608
time_provenance:: 2.679443597793579
curr_diff: 0 tensor(6.2141e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2141e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.01858353614807129
overhead3:: 0.06385397911071777
overhead4:: 2.9631922245025635
overhead5:: 0
memory usage:: 3018629120
time_provenance:: 3.7616469860076904
curr_diff: 0 tensor(2.7868e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7868e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.019878625869750977
overhead3:: 0.06781721115112305
overhead4:: 3.037710666656494
overhead5:: 0
memory usage:: 3015294976
time_provenance:: 3.8703224658966064
curr_diff: 0 tensor(2.3493e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3493e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.020460844039916992
overhead3:: 0.07137894630432129
overhead4:: 3.2974488735198975
overhead5:: 0
memory usage:: 3012055040
time_provenance:: 4.167290210723877
curr_diff: 0 tensor(2.2843e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2843e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
overhead:: 0
overhead2:: 0.03423595428466797
overhead3:: 0.10770630836486816
overhead4:: 5.0074498653411865
overhead5:: 0
memory usage:: 3054637056
time_provenance:: 6.186307191848755
curr_diff: 0 tensor(1.5090e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5090e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  MNIST5 0
tensor([59248, 31313, 56738, 44444, 42701, 10606])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6631, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8763, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7857, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6866, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6595, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6312, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6191, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5840, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2698516845703125
training time full:: 3.2699220180511475
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.613004207611084
overhead:: 0
overhead2:: 0.1654043197631836
overhead3:: 0
time_baseline:: 2.61303973197937
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.005468130111694336
overhead3:: 0.025706052780151367
overhead4:: 0.47724032402038574
overhead5:: 0
memory usage:: 3064078336
time_provenance:: 0.7877860069274902
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008999109268188477
overhead3:: 0.03262162208557129
overhead4:: 0.6684637069702148
overhead5:: 0
memory usage:: 3032313856
time_provenance:: 1.008314847946167
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011240720748901367
overhead3:: 0.038229942321777344
overhead4:: 0.8235414028167725
overhead5:: 0
memory usage:: 3010543616
time_provenance:: 1.1971778869628906
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007833242416381836
overhead3:: 0.03368401527404785
overhead4:: 0.6771430969238281
overhead5:: 0
memory usage:: 3013902336
time_provenance:: 1.0313310623168945
curr_diff: 0 tensor(2.3378e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3378e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010606527328491211
overhead3:: 0.039310455322265625
overhead4:: 0.84326171875
overhead5:: 0
memory usage:: 3046924288
time_provenance:: 1.2345576286315918
curr_diff: 0 tensor(2.0773e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0773e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012907028198242188
overhead3:: 0.0452573299407959
overhead4:: 1.0049614906311035
overhead5:: 0
memory usage:: 3014971392
time_provenance:: 1.4161689281463623
curr_diff: 0 tensor(1.9815e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9815e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01822066307067871
overhead3:: 0.059662580490112305
overhead4:: 1.5492167472839355
overhead5:: 0
memory usage:: 3048845312
time_provenance:: 2.0996205806732178
curr_diff: 0 tensor(8.7728e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7728e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02254319190979004
overhead3:: 0.06156468391418457
overhead4:: 1.6140122413635254
overhead5:: 0
memory usage:: 3047641088
time_provenance:: 2.179847240447998
curr_diff: 0 tensor(8.1156e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1156e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.022960186004638672
overhead3:: 0.06766772270202637
overhead4:: 1.7449696063995361
overhead5:: 0
memory usage:: 3030577152
time_provenance:: 2.31923770904541
curr_diff: 0 tensor(7.6201e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6201e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.033432960510253906
overhead3:: 0.10477781295776367
overhead4:: 2.447974443435669
overhead5:: 0
memory usage:: 3038408704
time_provenance:: 3.0929501056671143
curr_diff: 0 tensor(1.6156e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6156e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9408, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6668, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4666, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3090, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0287, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9781, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9289, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8475, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8169, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7475, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7062, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6781, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6561, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6330, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6277, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6186, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6059, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5720, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.251560926437378
training time full:: 3.251626491546631
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([26770, 18012, 37909, 33147, 28233, 13327, 11108,  2712, 11819, 44357,
         5269,  4991, 21096, 43114, 24201, 13413, 29525, 53075, 49804, 51335,
         7365, 50552, 16433, 57023, 37985, 48821, 35843, 24298, 46940, 40326,
        45801, 49624, 24173, 41747, 35429, 47384,  5811, 37869, 18342, 31822,
        47905, 42285, 34379,  1411, 33429, 39755, 24120, 40946, 15717, 59289])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6127700805664062
overhead:: 0
overhead2:: 0.15602993965148926
overhead3:: 0
time_baseline:: 2.6128041744232178
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0073337554931640625
overhead3:: 0.02843928337097168
overhead4:: 0.4996528625488281
overhead5:: 0
memory usage:: 3047620608
time_provenance:: 0.805107593536377
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008339881896972656
overhead3:: 0.03155064582824707
overhead4:: 0.6510012149810791
overhead5:: 0
memory usage:: 3024961536
time_provenance:: 0.9789667129516602
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010524511337280273
overhead3:: 0.04206347465515137
overhead4:: 0.8307700157165527
overhead5:: 0
memory usage:: 3048833024
time_provenance:: 1.1798350811004639
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010135650634765625
overhead3:: 0.03439497947692871
overhead4:: 0.6945362091064453
overhead5:: 0
memory usage:: 3012317184
time_provenance:: 1.0502171516418457
curr_diff: 0 tensor(3.1720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013119697570800781
overhead3:: 0.043500423431396484
overhead4:: 0.8704743385314941
overhead5:: 0
memory usage:: 3018604544
time_provenance:: 1.248990774154663
curr_diff: 0 tensor(3.1170e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1170e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01242518424987793
overhead3:: 0.04564476013183594
overhead4:: 0.9884872436523438
overhead5:: 0
memory usage:: 3024703488
time_provenance:: 1.37453293800354
curr_diff: 0 tensor(3.0536e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0536e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01822209358215332
overhead3:: 0.06340289115905762
overhead4:: 1.557088851928711
overhead5:: 0
memory usage:: 3039764480
time_provenance:: 2.108218193054199
curr_diff: 0 tensor(5.9909e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9909e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.021831274032592773
overhead3:: 0.06057929992675781
overhead4:: 1.6052048206329346
overhead5:: 0
memory usage:: 3012509696
time_provenance:: 2.155205488204956
curr_diff: 0 tensor(5.7660e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7660e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.021485567092895508
overhead3:: 0.06848692893981934
overhead4:: 1.6835203170776367
overhead5:: 0
memory usage:: 3050397696
time_provenance:: 2.2328708171844482
curr_diff: 0 tensor(5.5250e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5250e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.031111717224121094
overhead3:: 0.10110735893249512
overhead4:: 2.4385743141174316
overhead5:: 0
memory usage:: 3003617280
time_provenance:: 3.056213855743408
curr_diff: 0 tensor(1.6188e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6188e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3129, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9229, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6535, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4542, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1963, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0235, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9716, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9253, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8800, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8195, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7093, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6709, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6662, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6429, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6316, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6258, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6062, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5930, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5721, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2704122066497803
training time full:: 3.270479917526245
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6197915077209473
overhead:: 0
overhead2:: 0.15465188026428223
overhead3:: 0
time_baseline:: 2.6198244094848633
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006086111068725586
overhead3:: 0.025855064392089844
overhead4:: 0.47784972190856934
overhead5:: 0
memory usage:: 3040100352
time_provenance:: 0.7728450298309326
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008305549621582031
overhead3:: 0.032196998596191406
overhead4:: 0.6484403610229492
overhead5:: 0
memory usage:: 3107135488
time_provenance:: 0.967158317565918
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010891199111938477
overhead3:: 0.038187503814697266
overhead4:: 0.8230545520782471
overhead5:: 0
memory usage:: 3038236672
time_provenance:: 1.1687726974487305
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009453296661376953
overhead3:: 0.032602548599243164
overhead4:: 0.6854860782623291
overhead5:: 0
memory usage:: 3018878976
time_provenance:: 1.0219414234161377
curr_diff: 0 tensor(2.4387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4387e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009900569915771484
overhead3:: 0.04047107696533203
overhead4:: 0.8541092872619629
overhead5:: 0
memory usage:: 3039629312
time_provenance:: 1.2220594882965088
curr_diff: 0 tensor(2.0258e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0258e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012464761734008789
overhead3:: 0.04570293426513672
overhead4:: 0.9942586421966553
overhead5:: 0
memory usage:: 3030593536
time_provenance:: 1.3806736469268799
curr_diff: 0 tensor(1.9646e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9646e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01893782615661621
overhead3:: 0.06681275367736816
overhead4:: 1.5133373737335205
overhead5:: 0
memory usage:: 3014533120
time_provenance:: 2.0587668418884277
curr_diff: 0 tensor(6.8535e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8535e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.017293930053710938
overhead3:: 0.06514668464660645
overhead4:: 1.6110029220581055
overhead5:: 0
memory usage:: 3029954560
time_provenance:: 2.157520294189453
curr_diff: 0 tensor(6.2175e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2175e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01883983612060547
overhead3:: 0.0686345100402832
overhead4:: 1.6934869289398193
overhead5:: 0
memory usage:: 3041849344
time_provenance:: 2.247180461883545
curr_diff: 0 tensor(6.1773e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1773e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03072667121887207
overhead3:: 0.10130953788757324
overhead4:: 2.4248197078704834
overhead5:: 0
memory usage:: 3020836864
time_provenance:: 3.0435070991516113
curr_diff: 0 tensor(1.6187e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6187e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2615, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6383, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4389, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.2978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1817, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0253, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9196, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7299, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6797, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6570, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6492, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6423, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6146, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5889, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.27471923828125
training time full:: 3.274784803390503
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.613396167755127
overhead:: 0
overhead2:: 0.16421890258789062
overhead3:: 0
time_baseline:: 2.6134285926818848
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006231546401977539
overhead3:: 0.024555206298828125
overhead4:: 0.4854552745819092
overhead5:: 0
memory usage:: 3031724032
time_provenance:: 0.7841145992279053
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009017467498779297
overhead3:: 0.031844139099121094
overhead4:: 0.6751499176025391
overhead5:: 0
memory usage:: 3047874560
time_provenance:: 1.0134449005126953
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010606527328491211
overhead3:: 0.03809547424316406
overhead4:: 0.8176288604736328
overhead5:: 0
memory usage:: 3014246400
time_provenance:: 1.181049108505249
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0093536376953125
overhead3:: 0.034186363220214844
overhead4:: 0.7006795406341553
overhead5:: 0
memory usage:: 3012513792
time_provenance:: 1.0450472831726074
curr_diff: 0 tensor(3.1538e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1538e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011083126068115234
overhead3:: 0.041133880615234375
overhead4:: 0.8612358570098877
overhead5:: 0
memory usage:: 3040276480
time_provenance:: 1.2464570999145508
curr_diff: 0 tensor(3.0942e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0942e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013364553451538086
overhead3:: 0.04399418830871582
overhead4:: 0.9972982406616211
overhead5:: 0
memory usage:: 3013013504
time_provenance:: 1.3996236324310303
curr_diff: 0 tensor(3.0104e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0104e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01750969886779785
overhead3:: 0.059784889221191406
overhead4:: 1.517575740814209
overhead5:: 0
memory usage:: 3024560128
time_provenance:: 2.0647051334381104
curr_diff: 0 tensor(9.4088e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4088e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01879596710205078
overhead3:: 0.06450676918029785
overhead4:: 1.6299681663513184
overhead5:: 0
memory usage:: 3011796992
time_provenance:: 2.192455291748047
curr_diff: 0 tensor(8.6843e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6843e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.019951820373535156
overhead3:: 0.06805825233459473
overhead4:: 1.6623766422271729
overhead5:: 0
memory usage:: 3022471168
time_provenance:: 2.229992389678955
curr_diff: 0 tensor(8.4849e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4849e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03841233253479004
overhead3:: 0.10869479179382324
overhead4:: 2.5361592769622803
overhead5:: 0
memory usage:: 3015163904
time_provenance:: 3.2056210041046143
curr_diff: 0 tensor(1.6124e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6124e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9323, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6550, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4545, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1877, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9243, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8474, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8178, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7155, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6852, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6304, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6097, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5876, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5734, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.266434907913208
training time full:: 3.2665019035339355
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 177, 180, 188, 190])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6048614978790283
overhead:: 0
overhead2:: 0.1589527130126953
overhead3:: 0
time_baseline:: 2.6048951148986816
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.005343914031982422
overhead3:: 0.02548694610595703
overhead4:: 0.49584293365478516
overhead5:: 0
memory usage:: 3012558848
time_provenance:: 0.7826037406921387
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008169889450073242
overhead3:: 0.03214216232299805
overhead4:: 0.6478605270385742
overhead5:: 0
memory usage:: 3018895360
time_provenance:: 0.9732050895690918
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009987354278564453
overhead3:: 0.0381016731262207
overhead4:: 0.8126676082611084
overhead5:: 0
memory usage:: 3047305216
time_provenance:: 1.1636488437652588
curr_diff: 0 tensor(9.9642e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9642e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007892847061157227
overhead3:: 0.034348487854003906
overhead4:: 0.7163982391357422
overhead5:: 0
memory usage:: 3014696960
time_provenance:: 1.0485386848449707
curr_diff: 0 tensor(2.5421e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5421e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009918928146362305
overhead3:: 0.040167808532714844
overhead4:: 0.8485033512115479
overhead5:: 0
memory usage:: 3052244992
time_provenance:: 1.2158706188201904
curr_diff: 0 tensor(2.4501e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4501e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012536287307739258
overhead3:: 0.04549241065979004
overhead4:: 0.9897339344024658
overhead5:: 0
memory usage:: 3047706624
time_provenance:: 1.3842127323150635
curr_diff: 0 tensor(2.3433e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3433e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018573760986328125
overhead3:: 0.05924630165100098
overhead4:: 1.5307481288909912
overhead5:: 0
memory usage:: 3011342336
time_provenance:: 2.0606491565704346
curr_diff: 0 tensor(6.1371e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1371e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02045297622680664
overhead3:: 0.06333255767822266
overhead4:: 1.6117100715637207
overhead5:: 0
memory usage:: 3030974464
time_provenance:: 2.161543607711792
curr_diff: 0 tensor(5.8192e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8192e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.020723819732666016
overhead3:: 0.06942272186279297
overhead4:: 1.7191338539123535
overhead5:: 0
memory usage:: 3019350016
time_provenance:: 2.2719709873199463
curr_diff: 0 tensor(5.5935e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5935e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.032286643981933594
overhead3:: 0.10611510276794434
overhead4:: 2.485830545425415
overhead5:: 0
memory usage:: 3002417152
time_provenance:: 3.1164932250976562
curr_diff: 0 tensor(1.6168e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6168e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3338, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9483, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5581, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4647, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3818, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3129, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1954, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1488, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0659, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0331, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9751, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8832, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8702, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8480, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7935, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7803, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7697, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7317, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7120, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6813, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6698, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6581, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6528, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6380, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6305, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6151, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5906, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.414142370223999
training time full:: 6.414207935333252
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.875424146652222
overhead:: 0
overhead2:: 0.38053417205810547
overhead3:: 0
time_baseline:: 4.875455379486084
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.0066585540771484375
overhead3:: 0.029670238494873047
overhead4:: 0.9781172275543213
overhead5:: 0
memory usage:: 3048128512
time_provenance:: 1.382840633392334
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.009328842163085938
overhead3:: 0.03726840019226074
overhead4:: 1.325944185256958
overhead5:: 0
memory usage:: 3048271872
time_provenance:: 1.808140516281128
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.012515783309936523
overhead3:: 0.04387354850769043
overhead4:: 1.7109746932983398
overhead5:: 0
memory usage:: 3038367744
time_provenance:: 2.275773048400879
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.00970768928527832
overhead3:: 0.0378270149230957
overhead4:: 1.4453487396240234
overhead5:: 0
memory usage:: 3013976064
time_provenance:: 1.9348227977752686
curr_diff: 0 tensor(1.3412e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3412e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.012688398361206055
overhead3:: 0.04529714584350586
overhead4:: 1.785836935043335
overhead5:: 0
memory usage:: 3019472896
time_provenance:: 2.355267286300659
curr_diff: 0 tensor(1.2991e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2991e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.015230178833007812
overhead3:: 0.05142092704772949
overhead4:: 2.1302595138549805
overhead5:: 0
memory usage:: 3011121152
time_provenance:: 2.7714014053344727
curr_diff: 0 tensor(1.2532e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2532e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.02017951011657715
overhead3:: 0.06197786331176758
overhead4:: 2.842585802078247
overhead5:: 0
memory usage:: 3014426624
time_provenance:: 3.6636998653411865
curr_diff: 0 tensor(4.2341e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2341e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.022958755493164062
overhead3:: 0.06814336776733398
overhead4:: 3.1117241382598877
overhead5:: 0
memory usage:: 3019075584
time_provenance:: 3.9726316928863525
curr_diff: 0 tensor(3.9991e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9991e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.024427413940429688
overhead3:: 0.07031893730163574
overhead4:: 3.374631404876709
overhead5:: 0
memory usage:: 3060703232
time_provenance:: 4.2785069942474365
curr_diff: 0 tensor(3.9385e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9385e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.03796648979187012
overhead3:: 0.10550260543823242
overhead4:: 5.098821640014648
overhead5:: 0
memory usage:: 3023130624
time_provenance:: 6.3734400272369385
curr_diff: 0 tensor(1.4937e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4937e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9393, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4582, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3765, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1421, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0625, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9703, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9212, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9020, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8615, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7791, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7666, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7597, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7475, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6895, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6818, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6578, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6438, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6424, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6379, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6198, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6096, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6006, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5907, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.324608325958252
training time full:: 6.3246910572052
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([26770, 18012, 37909, 33147, 28233, 13327, 11108,  2712, 11819, 44357,
         5269,  4991, 21096, 43114, 24201, 13413, 29525, 53075, 49804, 51335,
         7365, 50552, 16433, 57023, 37985, 48821, 35843, 24298, 46940, 40326,
        45801, 49624, 24173, 41747, 35429, 47384,  5811, 37869, 18342, 31822,
        47905, 42285, 34379,  1411, 33429, 39755, 24120, 40946, 15717, 59289])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.858766317367554
overhead:: 0
overhead2:: 0.3796665668487549
overhead3:: 0
time_baseline:: 4.858800172805786
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.006012439727783203
overhead3:: 0.029231786727905273
overhead4:: 0.9330666065216064
overhead5:: 0
memory usage:: 3015839744
time_provenance:: 1.3343732357025146
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.008494377136230469
overhead3:: 0.03613901138305664
overhead4:: 1.3167879581451416
overhead5:: 0
memory usage:: 3013177344
time_provenance:: 1.7910113334655762
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.011325359344482422
overhead3:: 0.04193615913391113
overhead4:: 1.7034883499145508
overhead5:: 0
memory usage:: 3051532288
time_provenance:: 2.2651662826538086
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.009488105773925781
overhead3:: 0.03823518753051758
overhead4:: 1.4443042278289795
overhead5:: 0
memory usage:: 3011108864
time_provenance:: 1.9460675716400146
curr_diff: 0 tensor(2.1542e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1542e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.011139631271362305
overhead3:: 0.04438662528991699
overhead4:: 1.7341773509979248
overhead5:: 0
memory usage:: 3020070912
time_provenance:: 2.3011269569396973
curr_diff: 0 tensor(2.1112e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1112e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.01452183723449707
overhead3:: 0.05183887481689453
overhead4:: 2.094820499420166
overhead5:: 0
memory usage:: 3012427776
time_provenance:: 2.739766836166382
curr_diff: 0 tensor(2.0600e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0600e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.019786357879638672
overhead3:: 0.06073951721191406
overhead4:: 2.7789108753204346
overhead5:: 0
memory usage:: 3034869760
time_provenance:: 3.603166341781616
curr_diff: 0 tensor(3.6516e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6516e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.021158218383789062
overhead3:: 0.06814336776733398
overhead4:: 3.123361825942993
overhead5:: 0
memory usage:: 3039518720
time_provenance:: 3.9930403232574463
curr_diff: 0 tensor(3.5585e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5585e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.022951364517211914
overhead3:: 0.07000923156738281
overhead4:: 3.2533202171325684
overhead5:: 0
memory usage:: 3015606272
time_provenance:: 4.163368463516235
curr_diff: 0 tensor(3.4013e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4013e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.03679990768432617
overhead3:: 0.10857367515563965
overhead4:: 5.068175315856934
overhead5:: 0
memory usage:: 3031674880
time_provenance:: 6.345174551010132
curr_diff: 0 tensor(1.4889e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4889e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0858, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7751, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4532, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1423, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1020, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0623, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9435, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9223, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8487, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7799, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7658, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7594, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7382, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6901, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6813, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6535, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6574, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6381, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6241, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6158, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6066, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5910, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5869, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.426393985748291
training time full:: 6.426469326019287
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.909798860549927
overhead:: 0
overhead2:: 0.3806896209716797
overhead3:: 0
time_baseline:: 4.909833908081055
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.007526397705078125
overhead3:: 0.03080010414123535
overhead4:: 0.9668185710906982
overhead5:: 0
memory usage:: 3038674944
time_provenance:: 1.376915693283081
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.009886503219604492
overhead3:: 0.03703927993774414
overhead4:: 1.3568754196166992
overhead5:: 0
memory usage:: 3010179072
time_provenance:: 1.851891279220581
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.012636661529541016
overhead3:: 0.044303178787231445
overhead4:: 1.7192347049713135
overhead5:: 0
memory usage:: 3028115456
time_provenance:: 2.292055606842041
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.010833501815795898
overhead3:: 0.03832817077636719
overhead4:: 1.4604458808898926
overhead5:: 0
memory usage:: 3039547392
time_provenance:: 1.9668633937835693
curr_diff: 0 tensor(9.9622e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9622e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.012806892395019531
overhead3:: 0.0444798469543457
overhead4:: 1.8027524948120117
overhead5:: 0
memory usage:: 3055067136
time_provenance:: 2.381052255630493
curr_diff: 0 tensor(8.8334e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8334e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.016093730926513672
overhead3:: 0.05142331123352051
overhead4:: 2.1263012886047363
overhead5:: 0
memory usage:: 3038855168
time_provenance:: 2.7864880561828613
curr_diff: 0 tensor(8.6864e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6864e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.019903182983398438
overhead3:: 0.06291413307189941
overhead4:: 2.907249927520752
overhead5:: 0
memory usage:: 3018883072
time_provenance:: 3.736640453338623
curr_diff: 0 tensor(3.6673e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6673e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.023476362228393555
overhead3:: 0.06721162796020508
overhead4:: 3.1207034587860107
overhead5:: 0
memory usage:: 3014930432
time_provenance:: 3.9887173175811768
curr_diff: 0 tensor(3.6378e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6378e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.02452397346496582
overhead3:: 0.07082939147949219
overhead4:: 3.2792060375213623
overhead5:: 0
memory usage:: 3026886656
time_provenance:: 4.194643974304199
curr_diff: 0 tensor(3.5234e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5234e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.03849220275878906
overhead3:: 0.10724639892578125
overhead4:: 5.075051546096802
overhead5:: 0
memory usage:: 3007488000
time_provenance:: 6.354194402694702
curr_diff: 0 tensor(1.4966e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4966e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0694, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5320, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4403, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3615, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.2971, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2346, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0596, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0280, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9217, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.8996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8764, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8611, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8122, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7554, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7396, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7099, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6775, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6685, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6673, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6629, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6516, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6525, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6411, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6358, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6358, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6266, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6122, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6094, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.390106916427612
training time full:: 6.390174865722656
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.904877424240112
overhead:: 0
overhead2:: 0.3912241458892822
overhead3:: 0
time_baseline:: 4.904911279678345
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.007309436798095703
overhead3:: 0.030097484588623047
overhead4:: 0.9871160984039307
overhead5:: 0
memory usage:: 3024392192
time_provenance:: 1.3893635272979736
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.01007080078125
overhead3:: 0.03699970245361328
overhead4:: 1.363114595413208
overhead5:: 0
memory usage:: 3036631040
time_provenance:: 1.849625825881958
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.011937379837036133
overhead3:: 0.04448890686035156
overhead4:: 1.762244701385498
overhead5:: 0
memory usage:: 3031216128
time_provenance:: 2.322763204574585
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.010633230209350586
overhead3:: 0.03883028030395508
overhead4:: 1.413459300994873
overhead5:: 0
memory usage:: 3011555328
time_provenance:: 1.9197237491607666
curr_diff: 0 tensor(1.1295e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1295e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.012597799301147461
overhead3:: 0.04425954818725586
overhead4:: 1.7979764938354492
overhead5:: 0
memory usage:: 3038625792
time_provenance:: 2.3674380779266357
curr_diff: 0 tensor(1.1552e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1552e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.014909982681274414
overhead3:: 0.05046558380126953
overhead4:: 2.1369240283966064
overhead5:: 0
memory usage:: 3013414912
time_provenance:: 2.785646915435791
curr_diff: 0 tensor(8.9457e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9457e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.019937753677368164
overhead3:: 0.0630648136138916
overhead4:: 2.915726661682129
overhead5:: 0
memory usage:: 3018788864
time_provenance:: 3.7519688606262207
curr_diff: 0 tensor(4.9476e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9476e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.022875308990478516
overhead3:: 0.06444120407104492
overhead4:: 3.00079345703125
overhead5:: 0
memory usage:: 3038760960
time_provenance:: 3.8642215728759766
curr_diff: 0 tensor(4.7534e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7534e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.025513648986816406
overhead3:: 0.07286739349365234
overhead4:: 3.33354115486145
overhead5:: 0
memory usage:: 3049934848
time_provenance:: 4.251475095748901
curr_diff: 0 tensor(4.6917e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6917e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.039073944091796875
overhead3:: 0.10930061340332031
overhead4:: 5.13722562789917
overhead5:: 0
memory usage:: 3008143360
time_provenance:: 6.438477516174316
curr_diff: 0 tensor(1.5136e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5136e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3556, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8133, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6827, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4732, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2560, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2048, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1079, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0724, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0360, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0030, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9761, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9522, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8856, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8370, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8172, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7910, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7601, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7401, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7124, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7038, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6977, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6938, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6843, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6798, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6647, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6558, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6491, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6456, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6343, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6359, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6366, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6223, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6182, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6186, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6130, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5977, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.6008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.370478630065918
training time full:: 6.370546340942383
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.934764623641968
overhead:: 0
overhead2:: 0.39196181297302246
overhead3:: 0
time_baseline:: 4.934797525405884
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.006772756576538086
overhead3:: 0.029738187789916992
overhead4:: 0.9802899360656738
overhead5:: 0
memory usage:: 3041382400
time_provenance:: 1.3806672096252441
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.009659767150878906
overhead3:: 0.03614997863769531
overhead4:: 1.3306913375854492
overhead5:: 0
memory usage:: 3015704576
time_provenance:: 1.8253557682037354
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.013143062591552734
overhead3:: 0.04392075538635254
overhead4:: 1.7316436767578125
overhead5:: 0
memory usage:: 3030880256
time_provenance:: 2.30062198638916
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.010463953018188477
overhead3:: 0.03866314888000488
overhead4:: 1.4375925064086914
overhead5:: 0
memory usage:: 3031035904
time_provenance:: 1.954758644104004
curr_diff: 0 tensor(1.4017e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4017e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.013337135314941406
overhead3:: 0.045725345611572266
overhead4:: 1.8173551559448242
overhead5:: 0
memory usage:: 3043008512
time_provenance:: 2.398420572280884
curr_diff: 0 tensor(1.3343e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3343e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.016290903091430664
overhead3:: 0.05126023292541504
overhead4:: 2.1092007160186768
overhead5:: 0
memory usage:: 3030835200
time_provenance:: 2.762439727783203
curr_diff: 0 tensor(1.2944e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2944e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.020960330963134766
overhead3:: 0.06350159645080566
overhead4:: 2.849470615386963
overhead5:: 0
memory usage:: 3034169344
time_provenance:: 3.6885814666748047
curr_diff: 0 tensor(4.0460e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0460e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.022043704986572266
overhead3:: 0.06776571273803711
overhead4:: 3.170825242996216
overhead5:: 0
memory usage:: 3105792000
time_provenance:: 4.049980878829956
curr_diff: 0 tensor(3.8115e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8115e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.0238494873046875
overhead3:: 0.07065224647521973
overhead4:: 3.2994046211242676
overhead5:: 0
memory usage:: 3026640896
time_provenance:: 4.21902871131897
curr_diff: 0 tensor(3.8222e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8222e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
overhead:: 0
overhead2:: 0.03916192054748535
overhead3:: 0.11097288131713867
overhead4:: 5.088825941085815
overhead5:: 0
memory usage:: 3019165696
time_provenance:: 6.390763759613037
curr_diff: 0 tensor(1.5017e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5017e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  MNIST5 0
tensor([31235, 30147, 38022, 42701, 15245, 33934, 57870, 31313, 40653, 48845,
        40859, 44444, 27036, 59036, 34014, 11616, 56738, 44642, 49444, 26282,
        50026, 19947, 10606, 59248, 13424, 57010, 41525, 54456, 56185, 30267])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3131, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9320, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6592, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1824, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9196, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8762, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7861, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7661, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6308, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6196, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6191, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5836, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5865, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2598729133605957
training time full:: 3.2599422931671143
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6639411449432373
overhead:: 0
overhead2:: 0.19648218154907227
overhead3:: 0
time_baseline:: 2.663975954055786
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.007010221481323242
overhead3:: 0.026035547256469727
overhead4:: 0.4999046325683594
overhead5:: 0
memory usage:: 3046621184
time_provenance:: 0.8159127235412598
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.010973453521728516
overhead3:: 0.03227424621582031
overhead4:: 0.6881189346313477
overhead5:: 0
memory usage:: 3018145792
time_provenance:: 1.0628530979156494
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.014592647552490234
overhead3:: 0.039267778396606445
overhead4:: 0.8445556163787842
overhead5:: 0
memory usage:: 3012030464
time_provenance:: 1.2451581954956055
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011614561080932617
overhead3:: 0.0332486629486084
overhead4:: 0.6994566917419434
overhead5:: 0
memory usage:: 3025379328
time_provenance:: 1.0631718635559082
curr_diff: 0 tensor(5.3401e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3401e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013881683349609375
overhead3:: 0.04047894477844238
overhead4:: 0.8673408031463623
overhead5:: 0
memory usage:: 3018207232
time_provenance:: 1.2791945934295654
curr_diff: 0 tensor(4.6708e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6708e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015949010848999023
overhead3:: 0.047226905822753906
overhead4:: 1.0123088359832764
overhead5:: 0
memory usage:: 3018043392
time_provenance:: 1.4550786018371582
curr_diff: 0 tensor(4.4601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.025478124618530273
overhead3:: 0.05673933029174805
overhead4:: 1.5753185749053955
overhead5:: 0
memory usage:: 3048120320
time_provenance:: 2.1403777599334717
curr_diff: 0 tensor(1.4698e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4698e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02346181869506836
overhead3:: 0.06696295738220215
overhead4:: 1.622830867767334
overhead5:: 0
memory usage:: 3019591680
time_provenance:: 2.2144734859466553
curr_diff: 0 tensor(1.4255e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4255e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.026164531707763672
overhead3:: 0.06524491310119629
overhead4:: 1.6882803440093994
overhead5:: 0
memory usage:: 3013500928
time_provenance:: 2.289743185043335
curr_diff: 0 tensor(1.3490e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3490e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.04020977020263672
overhead3:: 0.10159683227539062
overhead4:: 2.470892906188965
overhead5:: 0
memory usage:: 3009187840
time_provenance:: 3.232163190841675
curr_diff: 0 tensor(1.6188e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6188e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3114, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9370, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0348, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9825, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9324, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8884, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8521, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.8000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7089, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6813, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6790, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6585, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6306, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6212, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5890, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5740, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2722434997558594
training time full:: 3.272311210632324
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 137, 145, 148, 155, 158, 162, 163, 173,
        175, 176, 183, 184, 186, 187, 188, 200])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.642503261566162
overhead:: 0
overhead2:: 0.1994948387145996
overhead3:: 0
time_baseline:: 2.642536163330078
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.007643222808837891
overhead3:: 0.025419235229492188
overhead4:: 0.4899446964263916
overhead5:: 0
memory usage:: 3014348800
time_provenance:: 0.8034710884094238
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.00991964340209961
overhead3:: 0.03116297721862793
overhead4:: 0.6498856544494629
overhead5:: 0
memory usage:: 3047243776
time_provenance:: 1.0117268562316895
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013501167297363281
overhead3:: 0.039443016052246094
overhead4:: 0.8339307308197021
overhead5:: 0
memory usage:: 3014791168
time_provenance:: 1.2289557456970215
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.010827302932739258
overhead3:: 0.03461098670959473
overhead4:: 0.7115414142608643
overhead5:: 0
memory usage:: 3041366016
time_provenance:: 1.0796072483062744
curr_diff: 0 tensor(6.1428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1428e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.014665603637695312
overhead3:: 0.041587114334106445
overhead4:: 0.8931262493133545
overhead5:: 0
memory usage:: 3038658560
time_provenance:: 1.3025949001312256
curr_diff: 0 tensor(5.6517e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6517e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016277313232421875
overhead3:: 0.044652462005615234
overhead4:: 1.0333194732666016
overhead5:: 0
memory usage:: 3047567360
time_provenance:: 1.4734249114990234
curr_diff: 0 tensor(5.3058e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3058e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021715641021728516
overhead3:: 0.05906105041503906
overhead4:: 1.5688879489898682
overhead5:: 0
memory usage:: 3047575552
time_provenance:: 2.1382017135620117
curr_diff: 0 tensor(1.7463e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7463e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.025101661682128906
overhead3:: 0.06520271301269531
overhead4:: 1.615671157836914
overhead5:: 0
memory usage:: 3019411456
time_provenance:: 2.197091817855835
curr_diff: 0 tensor(1.6759e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6759e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.025357961654663086
overhead3:: 0.07777833938598633
overhead4:: 1.7241692543029785
overhead5:: 0
memory usage:: 3047731200
time_provenance:: 2.3308093547821045
curr_diff: 0 tensor(1.6152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03782987594604492
overhead3:: 0.10449385643005371
overhead4:: 2.4663569927215576
overhead5:: 0
memory usage:: 3021553664
time_provenance:: 3.2249937057495117
curr_diff: 0 tensor(1.5933e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5933e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9522, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6751, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4711, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3189, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.2063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1085, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9293, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8517, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7316, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7116, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6738, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6326, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6284, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6234, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6079, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5982, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5945, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5782, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.26690673828125
training time full:: 3.2669730186462402
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6329760551452637
overhead:: 0
overhead2:: 0.1974201202392578
overhead3:: 0
time_baseline:: 2.6330108642578125
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008600234985351562
overhead3:: 0.02632880210876465
overhead4:: 0.5149075984954834
overhead5:: 0
memory usage:: 3013570560
time_provenance:: 0.8511826992034912
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.010429620742797852
overhead3:: 0.031716108322143555
overhead4:: 0.6455297470092773
overhead5:: 0
memory usage:: 3017977856
time_provenance:: 1.005225419998169
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012919425964355469
overhead3:: 0.0382535457611084
overhead4:: 0.8237640857696533
overhead5:: 0
memory usage:: 3018448896
time_provenance:: 1.2204031944274902
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012105226516723633
overhead3:: 0.03325796127319336
overhead4:: 0.7068555355072021
overhead5:: 0
memory usage:: 3014987776
time_provenance:: 1.067908763885498
curr_diff: 0 tensor(4.8751e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8751e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013680458068847656
overhead3:: 0.0406341552734375
overhead4:: 0.877575159072876
overhead5:: 0
memory usage:: 3013115904
time_provenance:: 1.2877449989318848
curr_diff: 0 tensor(4.6037e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6037e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016590595245361328
overhead3:: 0.04578971862792969
overhead4:: 1.0199503898620605
overhead5:: 0
memory usage:: 3023679488
time_provenance:: 1.461768388748169
curr_diff: 0 tensor(4.2355e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2355e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02273082733154297
overhead3:: 0.06091594696044922
overhead4:: 1.5878064632415771
overhead5:: 0
memory usage:: 3038011392
time_provenance:: 2.1654868125915527
curr_diff: 0 tensor(1.5466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02436518669128418
overhead3:: 0.06669306755065918
overhead4:: 1.6265907287597656
overhead5:: 0
memory usage:: 3011436544
time_provenance:: 2.2172839641571045
curr_diff: 0 tensor(1.4908e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4908e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02549910545349121
overhead3:: 0.06747627258300781
overhead4:: 1.7241265773773193
overhead5:: 0
memory usage:: 3014807552
time_provenance:: 2.321089267730713
curr_diff: 0 tensor(1.4578e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4578e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03933858871459961
overhead3:: 0.09939789772033691
overhead4:: 2.4402410984039307
overhead5:: 0
memory usage:: 3029446656
time_provenance:: 3.1990323066711426
curr_diff: 0 tensor(1.6085e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6085e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3390, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6690, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4618, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0315, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9776, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8833, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8481, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7867, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7672, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6670, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6576, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6497, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6430, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6287, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5841, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2491023540496826
training time full:: 3.2491719722747803
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.622133493423462
overhead:: 0
overhead2:: 0.1974947452545166
overhead3:: 0
time_baseline:: 2.6221656799316406
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.006942272186279297
overhead3:: 0.025045156478881836
overhead4:: 0.4812307357788086
overhead5:: 0
memory usage:: 3032252416
time_provenance:: 0.8071203231811523
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011025667190551758
overhead3:: 0.033327579498291016
overhead4:: 0.6728839874267578
overhead5:: 0
memory usage:: 3036147712
time_provenance:: 1.0372276306152344
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.0131988525390625
overhead3:: 0.03896784782409668
overhead4:: 0.8263232707977295
overhead5:: 0
memory usage:: 3012284416
time_provenance:: 1.2295806407928467
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011841773986816406
overhead3:: 0.03324460983276367
overhead4:: 0.7183670997619629
overhead5:: 0
memory usage:: 3023032320
time_provenance:: 1.0812606811523438
curr_diff: 0 tensor(5.8107e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8107e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01388239860534668
overhead3:: 0.038210153579711914
overhead4:: 0.8620598316192627
overhead5:: 0
memory usage:: 3014914048
time_provenance:: 1.2590327262878418
curr_diff: 0 tensor(5.4474e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4474e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016941547393798828
overhead3:: 0.0462646484375
overhead4:: 1.0273239612579346
overhead5:: 0
memory usage:: 3018645504
time_provenance:: 1.4714584350585938
curr_diff: 0 tensor(5.2578e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2578e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.024991750717163086
overhead3:: 0.05743718147277832
overhead4:: 1.5529723167419434
overhead5:: 0
memory usage:: 3030396928
time_provenance:: 2.127474308013916
curr_diff: 0 tensor(1.3936e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3936e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02342391014099121
overhead3:: 0.06476712226867676
overhead4:: 1.6515882015228271
overhead5:: 0
memory usage:: 3014504448
time_provenance:: 2.23608136177063
curr_diff: 0 tensor(1.2894e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2894e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02471756935119629
overhead3:: 0.06703972816467285
overhead4:: 1.7155959606170654
overhead5:: 0
memory usage:: 3051008000
time_provenance:: 2.3181982040405273
curr_diff: 0 tensor(1.2362e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2362e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.045064687728881836
overhead3:: 0.10823869705200195
overhead4:: 2.5453522205352783
overhead5:: 0
memory usage:: 3005120512
time_provenance:: 3.306648015975952
curr_diff: 0 tensor(1.6109e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6109e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3166, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9416, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3100, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0330, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9251, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7277, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6983, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6850, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6687, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6497, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6396, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6096, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5974, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5872, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5724, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2565276622772217
training time full:: 3.2566068172454834
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 177, 180, 188, 190])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.632110595703125
overhead:: 0
overhead2:: 0.19742822647094727
overhead3:: 0
time_baseline:: 2.632143974304199
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.007391691207885742
overhead3:: 0.02507162094116211
overhead4:: 0.526972770690918
overhead5:: 0
memory usage:: 3015208960
time_provenance:: 0.8572244644165039
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.009804010391235352
overhead3:: 0.03195500373840332
overhead4:: 0.6506798267364502
overhead5:: 0
memory usage:: 3012083712
time_provenance:: 1.0117926597595215
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013266801834106445
overhead3:: 0.0378413200378418
overhead4:: 0.826932430267334
overhead5:: 0
memory usage:: 3064197120
time_provenance:: 1.2201335430145264
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012050628662109375
overhead3:: 0.03345465660095215
overhead4:: 0.72499680519104
overhead5:: 0
memory usage:: 3010543616
time_provenance:: 1.0883471965789795
curr_diff: 0 tensor(5.1793e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1793e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013512134552001953
overhead3:: 0.038811445236206055
overhead4:: 0.8714010715484619
overhead5:: 0
memory usage:: 3024592896
time_provenance:: 1.2744488716125488
curr_diff: 0 tensor(4.9572e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9572e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015677452087402344
overhead3:: 0.045361995697021484
overhead4:: 1.0152509212493896
overhead5:: 0
memory usage:: 3032735744
time_provenance:: 1.4527478218078613
curr_diff: 0 tensor(4.7305e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7305e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02052903175354004
overhead3:: 0.06168198585510254
overhead4:: 1.5706026554107666
overhead5:: 0
memory usage:: 3014225920
time_provenance:: 2.136577606201172
curr_diff: 0 tensor(1.6224e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6224e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.023534536361694336
overhead3:: 0.06238245964050293
overhead4:: 1.6616325378417969
overhead5:: 0
memory usage:: 3016757248
time_provenance:: 2.251206636428833
curr_diff: 0 tensor(1.5661e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5661e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02622246742248535
overhead3:: 0.0666966438293457
overhead4:: 1.7113392353057861
overhead5:: 0
memory usage:: 3039821824
time_provenance:: 2.3161368370056152
curr_diff: 0 tensor(1.4917e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4917e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.04042696952819824
overhead3:: 0.10203242301940918
overhead4:: 2.4530460834503174
overhead5:: 0
memory usage:: 3003920384
time_provenance:: 3.217447519302368
curr_diff: 0 tensor(1.6128e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6128e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3385, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9376, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7872, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6614, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5495, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4565, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3741, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3064, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0997, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0612, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9701, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9463, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8797, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8132, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7775, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7668, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7637, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7099, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6938, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6852, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6565, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6492, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6437, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6357, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6340, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6289, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6146, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6137, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6050, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5990, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5891, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5881, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.366041660308838
training time full:: 6.366109848022461
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.862661838531494
overhead:: 0
overhead2:: 0.39260125160217285
overhead3:: 0
time_baseline:: 4.862692594528198
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.0069768428802490234
overhead3:: 0.02930140495300293
overhead4:: 0.9700798988342285
overhead5:: 0
memory usage:: 3026087936
time_provenance:: 1.385115146636963
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.00997161865234375
overhead3:: 0.03752446174621582
overhead4:: 1.3503034114837646
overhead5:: 0
memory usage:: 3050061824
time_provenance:: 1.8610270023345947
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013123273849487305
overhead3:: 0.04242372512817383
overhead4:: 1.6930549144744873
overhead5:: 0
memory usage:: 3026161664
time_provenance:: 2.280700922012329
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.010960578918457031
overhead3:: 0.03894209861755371
overhead4:: 1.458777904510498
overhead5:: 0
memory usage:: 3012673536
time_provenance:: 1.9833204746246338
curr_diff: 0 tensor(4.0856e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0856e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013220787048339844
overhead3:: 0.04517054557800293
overhead4:: 1.7940783500671387
overhead5:: 0
memory usage:: 3039133696
time_provenance:: 2.3985371589660645
curr_diff: 0 tensor(3.9275e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9275e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.015967607498168945
overhead3:: 0.050858497619628906
overhead4:: 2.1326112747192383
overhead5:: 0
memory usage:: 3023802368
time_provenance:: 2.805748462677002
curr_diff: 0 tensor(3.8957e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8957e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.020952939987182617
overhead3:: 0.06354045867919922
overhead4:: 2.978163480758667
overhead5:: 0
memory usage:: 3049648128
time_provenance:: 3.828033447265625
curr_diff: 0 tensor(9.3569e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3569e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02272319793701172
overhead3:: 0.06658387184143066
overhead4:: 3.1593945026397705
overhead5:: 0
memory usage:: 3031547904
time_provenance:: 4.057078838348389
curr_diff: 0 tensor(9.1395e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1395e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.024554014205932617
overhead3:: 0.0699155330657959
overhead4:: 3.3092963695526123
overhead5:: 0
memory usage:: 3015118848
time_provenance:: 4.2518227100372314
curr_diff: 0 tensor(9.0002e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0002e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.04057431221008301
overhead3:: 0.10857009887695312
overhead4:: 5.211326360702515
overhead5:: 0
memory usage:: 3004092416
time_provenance:: 6.542480945587158
curr_diff: 0 tensor(1.5062e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5062e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3328, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8031, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6746, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5637, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3865, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3146, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1484, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0350, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9744, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9056, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8658, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8526, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8033, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7820, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7696, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7507, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7351, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7126, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6944, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6919, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6844, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6598, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6348, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6400, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6250, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6142, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6116, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6093, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5923, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.445404767990112
training time full:: 6.4454731941223145
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         80,  82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112,
        113, 114, 117, 118, 119, 122, 124, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.960821628570557
overhead:: 0
overhead2:: 0.39365220069885254
overhead3:: 0
time_baseline:: 4.960853576660156
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.0071964263916015625
overhead3:: 0.029494285583496094
overhead4:: 0.9629380702972412
overhead5:: 0
memory usage:: 3030159360
time_provenance:: 1.377626895904541
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.009960174560546875
overhead3:: 0.037363529205322266
overhead4:: 1.3688349723815918
overhead5:: 0
memory usage:: 3030695936
time_provenance:: 1.8648815155029297
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013394594192504883
overhead3:: 0.04518747329711914
overhead4:: 1.7261006832122803
overhead5:: 0
memory usage:: 3048665088
time_provenance:: 2.31417179107666
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.010994911193847656
overhead3:: 0.03724336624145508
overhead4:: 1.4346837997436523
overhead5:: 0
memory usage:: 3038720000
time_provenance:: 1.9581882953643799
curr_diff: 0 tensor(2.4141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013885736465454102
overhead3:: 0.046098947525024414
overhead4:: 1.756509780883789
overhead5:: 0
memory usage:: 3015794688
time_provenance:: 2.3533384799957275
curr_diff: 0 tensor(2.3448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.015522003173828125
overhead3:: 0.05105328559875488
overhead4:: 2.106570243835449
overhead5:: 0
memory usage:: 3014660096
time_provenance:: 2.7800421714782715
curr_diff: 0 tensor(2.2185e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2185e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02139115333557129
overhead3:: 0.06406426429748535
overhead4:: 2.9809467792510986
overhead5:: 0
memory usage:: 3013185536
time_provenance:: 3.832995653152466
curr_diff: 0 tensor(9.9202e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9202e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02322077751159668
overhead3:: 0.06746649742126465
overhead4:: 3.1539552211761475
overhead5:: 0
memory usage:: 3030196224
time_provenance:: 4.04797625541687
curr_diff: 0 tensor(9.7440e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7440e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.024286508560180664
overhead3:: 0.07104253768920898
overhead4:: 3.3651580810546875
overhead5:: 0
memory usage:: 3045396480
time_provenance:: 4.306647300720215
curr_diff: 0 tensor(9.6359e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6359e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.04048752784729004
overhead3:: 0.10993742942810059
overhead4:: 5.070911169052124
overhead5:: 0
memory usage:: 3004768256
time_provenance:: 6.401322364807129
curr_diff: 0 tensor(1.5017e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5017e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9413, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7929, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5550, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4616, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3756, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2500, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1912, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1452, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0639, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9732, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9437, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8820, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8486, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8296, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8151, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7894, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7799, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7655, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7592, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7381, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7030, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6532, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6374, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6234, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6066, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5913, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5866, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.439838886260986
training time full:: 6.4399073123931885
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.901407718658447
overhead:: 0
overhead2:: 0.3904435634613037
overhead3:: 0
time_baseline:: 4.901440143585205
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.006832122802734375
overhead3:: 0.029607772827148438
overhead4:: 0.9542331695556641
overhead5:: 0
memory usage:: 3049385984
time_provenance:: 1.371889352798462
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.01006627082824707
overhead3:: 0.03724861145019531
overhead4:: 1.346405267715454
overhead5:: 0
memory usage:: 3031568384
time_provenance:: 1.8580574989318848
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.01234745979309082
overhead3:: 0.04407954216003418
overhead4:: 1.7493672370910645
overhead5:: 0
memory usage:: 3013402624
time_provenance:: 2.3406832218170166
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.010633230209350586
overhead3:: 0.03905916213989258
overhead4:: 1.4989972114562988
overhead5:: 0
memory usage:: 3014582272
time_provenance:: 2.0190467834472656
curr_diff: 0 tensor(2.3358e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3358e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013155698776245117
overhead3:: 0.04401898384094238
overhead4:: 1.795483112335205
overhead5:: 0
memory usage:: 3024592896
time_provenance:: 2.3934454917907715
curr_diff: 0 tensor(2.1690e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1690e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.01613140106201172
overhead3:: 0.05191683769226074
overhead4:: 2.0597198009490967
overhead5:: 0
memory usage:: 3016429568
time_provenance:: 2.7285561561584473
curr_diff: 0 tensor(2.0840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02158069610595703
overhead3:: 0.06519269943237305
overhead4:: 2.90158748626709
overhead5:: 0
memory usage:: 3011067904
time_provenance:: 3.751654863357544
curr_diff: 0 tensor(9.5514e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5514e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.022737503051757812
overhead3:: 0.06751322746276855
overhead4:: 3.1726248264312744
overhead5:: 0
memory usage:: 3014725632
time_provenance:: 4.06826639175415
curr_diff: 0 tensor(9.1826e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1826e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02409672737121582
overhead3:: 0.07007956504821777
overhead4:: 3.3021342754364014
overhead5:: 0
memory usage:: 3012108288
time_provenance:: 4.237148284912109
curr_diff: 0 tensor(9.0123e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0123e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.03824019432067871
overhead3:: 0.11077213287353516
overhead4:: 5.158955097198486
overhead5:: 0
memory usage:: 3006619648
time_provenance:: 6.494166851043701
curr_diff: 0 tensor(1.5092e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5092e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1115, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9387, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6623, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5544, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4601, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3114, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1463, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0350, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9746, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9501, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9267, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8806, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8647, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8397, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8153, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8031, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7833, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7687, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7572, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7423, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7293, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7118, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7035, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6824, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6683, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6366, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6366, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6235, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6127, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6146, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6099, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6079, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5927, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.498042821884155
training time full:: 6.498108148574829
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.945008039474487
overhead:: 0
overhead2:: 0.3937985897064209
overhead3:: 0
time_baseline:: 4.945041656494141
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.007174015045166016
overhead3:: 0.029695749282836914
overhead4:: 0.9609806537628174
overhead5:: 0
memory usage:: 3040362496
time_provenance:: 1.3712522983551025
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.009775400161743164
overhead3:: 0.037398338317871094
overhead4:: 1.3444099426269531
overhead5:: 0
memory usage:: 3011710976
time_provenance:: 1.8519742488861084
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.012506484985351562
overhead3:: 0.0434420108795166
overhead4:: 1.6800196170806885
overhead5:: 0
memory usage:: 3013373952
time_provenance:: 2.266615152359009
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.01056361198425293
overhead3:: 0.03849506378173828
overhead4:: 1.4491798877716064
overhead5:: 0
memory usage:: 3051692032
time_provenance:: 1.9753437042236328
curr_diff: 0 tensor(3.1155e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1155e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.012923955917358398
overhead3:: 0.045345306396484375
overhead4:: 1.8001365661621094
overhead5:: 0
memory usage:: 3012694016
time_provenance:: 2.403435707092285
curr_diff: 0 tensor(3.1200e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1200e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.015491962432861328
overhead3:: 0.04959607124328613
overhead4:: 2.086092472076416
overhead5:: 0
memory usage:: 3018252288
time_provenance:: 2.756192207336426
curr_diff: 0 tensor(2.8259e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8259e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.021573781967163086
overhead3:: 0.0619504451751709
overhead4:: 2.9845917224884033
overhead5:: 0
memory usage:: 3019087872
time_provenance:: 3.836395263671875
curr_diff: 0 tensor(1.0929e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0929e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.022854328155517578
overhead3:: 0.06798124313354492
overhead4:: 3.053722620010376
overhead5:: 0
memory usage:: 3047804928
time_provenance:: 3.9506752490997314
curr_diff: 0 tensor(1.0802e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0802e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.02390003204345703
overhead3:: 0.06932187080383301
overhead4:: 3.325754165649414
overhead5:: 0
memory usage:: 3021856768
time_provenance:: 4.264892816543579
curr_diff: 0 tensor(1.0670e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0670e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.03932929039001465
overhead3:: 0.10933566093444824
overhead4:: 5.110381841659546
overhead5:: 0
memory usage:: 3010031616
time_provenance:: 6.440340518951416
curr_diff: 0 tensor(1.5023e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5023e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9264, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4530, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3710, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3055, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2418, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1931, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0644, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9703, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9229, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8819, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8655, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8336, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7636, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7578, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7384, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7314, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7173, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7108, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6926, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6787, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6760, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6547, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6477, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6350, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6176, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6062, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5930, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5865, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.380070686340332
training time full:: 6.380138158798218
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.93905234336853
overhead:: 0
overhead2:: 0.39383888244628906
overhead3:: 0
time_baseline:: 4.939085960388184
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.0071489810943603516
overhead3:: 0.030070781707763672
overhead4:: 0.9774518013000488
overhead5:: 0
memory usage:: 3045658624
time_provenance:: 1.3975443840026855
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.009743690490722656
overhead3:: 0.037004709243774414
overhead4:: 1.330017328262329
overhead5:: 0
memory usage:: 3057422336
time_provenance:: 1.8329541683197021
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.014679670333862305
overhead3:: 0.04536890983581543
overhead4:: 1.7434990406036377
overhead5:: 0
memory usage:: 3015852032
time_provenance:: 2.3430187702178955
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.01040029525756836
overhead3:: 0.03827977180480957
overhead4:: 1.4366455078125
overhead5:: 0
memory usage:: 3052617728
time_provenance:: 1.9607510566711426
curr_diff: 0 tensor(2.2308e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2308e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.013657093048095703
overhead3:: 0.04539155960083008
overhead4:: 1.7955105304718018
overhead5:: 0
memory usage:: 3024834560
time_provenance:: 2.3918449878692627
curr_diff: 0 tensor(1.9506e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9506e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.015847206115722656
overhead3:: 0.04961276054382324
overhead4:: 2.108461618423462
overhead5:: 0
memory usage:: 3016003584
time_provenance:: 2.788578987121582
curr_diff: 0 tensor(1.7437e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7437e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.021341800689697266
overhead3:: 0.06261301040649414
overhead4:: 2.972057342529297
overhead5:: 0
memory usage:: 3053404160
time_provenance:: 3.8247060775756836
curr_diff: 0 tensor(8.3709e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3709e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.023563385009765625
overhead3:: 0.06879448890686035
overhead4:: 3.178565263748169
overhead5:: 0
memory usage:: 3047841792
time_provenance:: 4.076488256454468
curr_diff: 0 tensor(7.9465e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9465e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.025593042373657227
overhead3:: 0.07041144371032715
overhead4:: 3.2369327545166016
overhead5:: 0
memory usage:: 3030806528
time_provenance:: 4.179250240325928
curr_diff: 0 tensor(7.8586e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8586e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
overhead:: 0
overhead2:: 0.03970766067504883
overhead3:: 0.10253143310546875
overhead4:: 5.06650710105896
overhead5:: 0
memory usage:: 3020005376
time_provenance:: 6.40135645866394
curr_diff: 0 tensor(1.5013e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5013e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
deletion rate:: 0.001
python3 generate_rand_ids 0.001  MNIST5 0
tensor([31235, 38022, 58888, 15245, 33934, 57870,  3599, 42512,  2451, 52505,
        40859, 59036, 27036, 44444, 51740, 15262, 38689, 56738, 49444, 45609,
        26282, 35193, 57010, 41525, 46005, 54456, 30267, 16827, 28091, 45755,
          450, 30147,  6985, 38346, 42701, 40653, 48845, 10061, 31313, 27343,
        40273, 17617, 19284, 34014,  6238, 11616, 44642, 10213, 58983, 59881,
        50026, 19947, 10606, 59248, 13424, 26737, 17780, 27509, 10869, 56185])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3349, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6709, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3109, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9219, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8277, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7695, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7300, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6926, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6892, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6654, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6617, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6332, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6220, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6273, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6057, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6036, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5858, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2702760696411133
training time full:: 3.2703444957733154
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6272971630096436
overhead:: 0
overhead2:: 0.19522404670715332
overhead3:: 0
time_baseline:: 2.62736177444458
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007603168487548828
overhead3:: 0.025088787078857422
overhead4:: 0.4847562313079834
overhead5:: 0
memory usage:: 3033616384
time_provenance:: 0.8128876686096191
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01002812385559082
overhead3:: 0.03167986869812012
overhead4:: 0.6460280418395996
overhead5:: 0
memory usage:: 3032649728
time_provenance:: 1.0251944065093994
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01402592658996582
overhead3:: 0.03849387168884277
overhead4:: 0.867659330368042
overhead5:: 0
memory usage:: 3047903232
time_provenance:: 1.2741444110870361
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011357545852661133
overhead3:: 0.03474545478820801
overhead4:: 0.7031431198120117
overhead5:: 0
memory usage:: 3030278144
time_provenance:: 1.0895509719848633
curr_diff: 0 tensor(6.0298e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0298e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013941526412963867
overhead3:: 0.03977489471435547
overhead4:: 0.8528668880462646
overhead5:: 0
memory usage:: 3011842048
time_provenance:: 1.274017333984375
curr_diff: 0 tensor(4.9947e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9947e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.018039226531982422
overhead3:: 0.04476332664489746
overhead4:: 1.0043375492095947
overhead5:: 0
memory usage:: 3048800256
time_provenance:: 1.4587697982788086
curr_diff: 0 tensor(4.4483e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4483e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02245783805847168
overhead3:: 0.06753396987915039
overhead4:: 1.5925378799438477
overhead5:: 0
memory usage:: 3048210432
time_provenance:: 2.1939034461975098
curr_diff: 0 tensor(2.0781e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0781e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.023585081100463867
overhead3:: 0.06536030769348145
overhead4:: 1.6302602291107178
overhead5:: 0
memory usage:: 3047559168
time_provenance:: 2.2389087677001953
curr_diff: 0 tensor(2.0060e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0060e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02796626091003418
overhead3:: 0.06598782539367676
overhead4:: 1.6901912689208984
overhead5:: 0
memory usage:: 3011145728
time_provenance:: 2.309499740600586
curr_diff: 0 tensor(1.9292e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9292e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.0437312126159668
overhead3:: 0.10498642921447754
overhead4:: 2.501401901245117
overhead5:: 0
memory usage:: 3037933568
time_provenance:: 3.2878971099853516
curr_diff: 0 tensor(1.6175e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6175e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873600
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3127, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9380, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6666, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4626, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1891, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0279, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9757, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9267, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8830, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8456, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7963, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7681, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7257, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6753, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6554, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6466, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6279, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5871, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2721385955810547
training time full:: 3.272207736968994
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 137, 145, 148, 155, 158, 162, 163, 173,
        175, 176, 183, 184, 186, 187, 188, 200])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.644338846206665
overhead:: 0
overhead2:: 0.1951289176940918
overhead3:: 0
time_baseline:: 2.6443710327148438
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007983684539794922
overhead3:: 0.02525043487548828
overhead4:: 0.47514891624450684
overhead5:: 0
memory usage:: 3011366912
time_provenance:: 0.8041841983795166
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01113128662109375
overhead3:: 0.03087902069091797
overhead4:: 0.6466434001922607
overhead5:: 0
memory usage:: 3054678016
time_provenance:: 1.024688720703125
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013847112655639648
overhead3:: 0.038587093353271484
overhead4:: 0.827526330947876
overhead5:: 0
memory usage:: 3011776512
time_provenance:: 1.2384166717529297
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.012039661407470703
overhead3:: 0.033521413803100586
overhead4:: 0.6922810077667236
overhead5:: 0
memory usage:: 3019825152
time_provenance:: 1.0761849880218506
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01431584358215332
overhead3:: 0.04067206382751465
overhead4:: 0.874340295791626
overhead5:: 0
memory usage:: 3019816960
time_provenance:: 1.30413818359375
curr_diff: 0 tensor(9.4297e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4297e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01676774024963379
overhead3:: 0.04454541206359863
overhead4:: 1.0313127040863037
overhead5:: 0
memory usage:: 3040276480
time_provenance:: 1.4866447448730469
curr_diff: 0 tensor(8.8990e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8990e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02269268035888672
overhead3:: 0.05991530418395996
overhead4:: 1.5343828201293945
overhead5:: 0
memory usage:: 3029086208
time_provenance:: 2.1300292015075684
curr_diff: 0 tensor(2.1634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.026372432708740234
overhead3:: 0.06176590919494629
overhead4:: 1.6045889854431152
overhead5:: 0
memory usage:: 3030458368
time_provenance:: 2.214752435684204
curr_diff: 0 tensor(2.0813e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0813e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.027157068252563477
overhead3:: 0.06868219375610352
overhead4:: 1.6951956748962402
overhead5:: 0
memory usage:: 3011129344
time_provenance:: 2.3187429904937744
curr_diff: 0 tensor(2.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.04625082015991211
overhead3:: 0.10950684547424316
overhead4:: 2.5422656536102295
overhead5:: 0
memory usage:: 3029540864
time_provenance:: 3.32667875289917
curr_diff: 0 tensor(1.6232e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6232e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9260, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6564, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8820, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8491, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8203, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7932, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7742, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7108, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6942, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6830, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6728, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6679, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6279, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5944, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5781, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.270737409591675
training time full:: 3.270803928375244
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6748759746551514
overhead:: 0
overhead2:: 0.20490288734436035
overhead3:: 0
time_baseline:: 2.6749091148376465
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007488727569580078
overhead3:: 0.02553272247314453
overhead4:: 0.4880495071411133
overhead5:: 0
memory usage:: 3012935680
time_provenance:: 0.8270840644836426
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01085352897644043
overhead3:: 0.03307986259460449
overhead4:: 0.6585292816162109
overhead5:: 0
memory usage:: 3012763648
time_provenance:: 1.029390573501587
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01358175277709961
overhead3:: 0.03902602195739746
overhead4:: 0.8363659381866455
overhead5:: 0
memory usage:: 3034726400
time_provenance:: 1.2646818161010742
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.0111541748046875
overhead3:: 0.034638166427612305
overhead4:: 0.7110745906829834
overhead5:: 0
memory usage:: 3015233536
time_provenance:: 1.103700876235962
curr_diff: 0 tensor(8.5876e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5876e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013471126556396484
overhead3:: 0.04032611846923828
overhead4:: 0.873166561126709
overhead5:: 0
memory usage:: 3047419904
time_provenance:: 1.3030171394348145
curr_diff: 0 tensor(7.7939e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7939e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.016040563583374023
overhead3:: 0.04497575759887695
overhead4:: 1.0131471157073975
overhead5:: 0
memory usage:: 3049041920
time_provenance:: 1.4679358005523682
curr_diff: 0 tensor(7.4835e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4835e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02327132225036621
overhead3:: 0.060999393463134766
overhead4:: 1.5454497337341309
overhead5:: 0
memory usage:: 3047022592
time_provenance:: 2.134925603866577
curr_diff: 0 tensor(2.4335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.027673006057739258
overhead3:: 0.061222076416015625
overhead4:: 1.655616283416748
overhead5:: 0
memory usage:: 3047559168
time_provenance:: 2.260115146636963
curr_diff: 0 tensor(2.3501e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3501e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.026492595672607422
overhead3:: 0.07041358947753906
overhead4:: 1.7004969120025635
overhead5:: 0
memory usage:: 3018403840
time_provenance:: 2.3305418491363525
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.04770803451538086
overhead3:: 0.10455894470214844
overhead4:: 2.548278331756592
overhead5:: 0
memory usage:: 3015405568
time_provenance:: 3.3394198417663574
curr_diff: 0 tensor(1.6110e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6110e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3024, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9230, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1854, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9743, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9200, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7851, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7397, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6800, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6563, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6492, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5889, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5841, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2505695819854736
training time full:: 3.2506368160247803
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.627744436264038
overhead:: 0
overhead2:: 0.19619989395141602
overhead3:: 0
time_baseline:: 2.6277811527252197
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007398366928100586
overhead3:: 0.02542591094970703
overhead4:: 0.4864687919616699
overhead5:: 0
memory usage:: 3011276800
time_provenance:: 0.8217146396636963
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010442972183227539
overhead3:: 0.031310319900512695
overhead4:: 0.636002779006958
overhead5:: 0
memory usage:: 3032895488
time_provenance:: 1.0163135528564453
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014079809188842773
overhead3:: 0.03973746299743652
overhead4:: 0.8344528675079346
overhead5:: 0
memory usage:: 3015036928
time_provenance:: 1.244927167892456
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011113166809082031
overhead3:: 0.03487992286682129
overhead4:: 0.7106180191040039
overhead5:: 0
memory usage:: 3031519232
time_provenance:: 1.1048521995544434
curr_diff: 0 tensor(7.1147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014653921127319336
overhead3:: 0.04048323631286621
overhead4:: 0.8842229843139648
overhead5:: 0
memory usage:: 3014893568
time_provenance:: 1.313570261001587
curr_diff: 0 tensor(6.5671e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5671e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01635265350341797
overhead3:: 0.04540681838989258
overhead4:: 1.037221908569336
overhead5:: 0
memory usage:: 3015774208
time_provenance:: 1.4931817054748535
curr_diff: 0 tensor(6.0982e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0982e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021827220916748047
overhead3:: 0.0618133544921875
overhead4:: 1.5346107482910156
overhead5:: 0
memory usage:: 3020906496
time_provenance:: 2.125746011734009
curr_diff: 0 tensor(2.0986e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0986e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02479696273803711
overhead3:: 0.0651092529296875
overhead4:: 1.64680814743042
overhead5:: 0
memory usage:: 3090415616
time_provenance:: 2.2542731761932373
curr_diff: 0 tensor(1.9850e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9850e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02777719497680664
overhead3:: 0.06680107116699219
overhead4:: 1.7147495746612549
overhead5:: 0
memory usage:: 3030458368
time_provenance:: 2.340259075164795
curr_diff: 0 tensor(1.9428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9428e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.04355573654174805
overhead3:: 0.10537862777709961
overhead4:: 2.508824348449707
overhead5:: 0
memory usage:: 3013570560
time_provenance:: 3.297722339630127
curr_diff: 0 tensor(1.6157e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6157e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6528, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4538, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1877, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9263, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8915, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7872, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7456, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7158, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6987, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6856, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6685, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6403, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5983, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5975, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5871, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5841, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5726, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.25704026222229
training time full:: 3.257105827331543
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 177, 180, 188, 190])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.629802703857422
overhead:: 0
overhead2:: 0.19516611099243164
overhead3:: 0
time_baseline:: 2.629838466644287
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.00741124153137207
overhead3:: 0.025658130645751953
overhead4:: 0.4894564151763916
overhead5:: 0
memory usage:: 3071762432
time_provenance:: 0.8266005516052246
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010286569595336914
overhead3:: 0.031775474548339844
overhead4:: 0.6456539630889893
overhead5:: 0
memory usage:: 3014774784
time_provenance:: 1.0135469436645508
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013191699981689453
overhead3:: 0.0382537841796875
overhead4:: 0.8413727283477783
overhead5:: 0
memory usage:: 3047165952
time_provenance:: 1.2623696327209473
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011799097061157227
overhead3:: 0.034125566482543945
overhead4:: 0.7184224128723145
overhead5:: 0
memory usage:: 3012726784
time_provenance:: 1.1029975414276123
curr_diff: 0 tensor(8.2661e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2661e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014569282531738281
overhead3:: 0.03861260414123535
overhead4:: 0.8635857105255127
overhead5:: 0
memory usage:: 3016114176
time_provenance:: 1.2785255908966064
curr_diff: 0 tensor(7.7444e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7444e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.017159700393676758
overhead3:: 0.045058488845825195
overhead4:: 0.9943606853485107
overhead5:: 0
memory usage:: 3057659904
time_provenance:: 1.4443304538726807
curr_diff: 0 tensor(7.6325e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6325e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02485346794128418
overhead3:: 0.06286954879760742
overhead4:: 1.5515894889831543
overhead5:: 0
memory usage:: 3033661440
time_provenance:: 2.1479806900024414
curr_diff: 0 tensor(2.9374e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9374e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02454209327697754
overhead3:: 0.06378436088562012
overhead4:: 1.6167171001434326
overhead5:: 0
memory usage:: 3023728640
time_provenance:: 2.226350784301758
curr_diff: 0 tensor(2.8515e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8515e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.0275266170501709
overhead3:: 0.06502771377563477
overhead4:: 1.7135844230651855
overhead5:: 0
memory usage:: 3014991872
time_provenance:: 2.3441860675811768
curr_diff: 0 tensor(2.7669e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7669e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.04113960266113281
overhead3:: 0.10617709159851074
overhead4:: 2.49194598197937
overhead5:: 0
memory usage:: 3042308096
time_provenance:: 3.283339023590088
curr_diff: 0 tensor(1.6299e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6299e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7904, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6644, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5536, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4609, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3101, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2480, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1923, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9481, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8813, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8283, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7783, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7416, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7299, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7101, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6855, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6794, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6360, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6342, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6077, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6050, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.365151405334473
training time full:: 6.365220069885254
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.900815963745117
overhead:: 0
overhead2:: 0.3965027332305908
overhead3:: 0
time_baseline:: 4.900849342346191
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.0072784423828125
overhead3:: 0.030347347259521484
overhead4:: 0.9474618434906006
overhead5:: 0
memory usage:: 3051319296
time_provenance:: 1.3826963901519775
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010072708129882812
overhead3:: 0.03697967529296875
overhead4:: 1.3849101066589355
overhead5:: 0
memory usage:: 3019296768
time_provenance:: 1.906860113143921
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.01436161994934082
overhead3:: 0.04543924331665039
overhead4:: 1.7558996677398682
overhead5:: 0
memory usage:: 3015430144
time_provenance:: 2.3894245624542236
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.011081933975219727
overhead3:: 0.03875541687011719
overhead4:: 1.48044753074646
overhead5:: 0
memory usage:: 3012595712
time_provenance:: 2.0214319229125977
curr_diff: 0 tensor(5.1863e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1863e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.014377355575561523
overhead3:: 0.045209646224975586
overhead4:: 1.8429701328277588
overhead5:: 0
memory usage:: 3023261696
time_provenance:: 2.472484827041626
curr_diff: 0 tensor(4.9652e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9652e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.017049074172973633
overhead3:: 0.05221724510192871
overhead4:: 2.098701000213623
overhead5:: 0
memory usage:: 3048140800
time_provenance:: 2.793832302093506
curr_diff: 0 tensor(4.8750e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8750e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.02204585075378418
overhead3:: 0.061531782150268555
overhead4:: 2.957448720932007
overhead5:: 0
memory usage:: 3033481216
time_provenance:: 3.847074031829834
curr_diff: 0 tensor(1.7617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.023439884185791016
overhead3:: 0.06817102432250977
overhead4:: 3.1847586631774902
overhead5:: 0
memory usage:: 3030265856
time_provenance:: 4.113258600234985
curr_diff: 0 tensor(1.7453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.025226593017578125
overhead3:: 0.07263064384460449
overhead4:: 3.390942096710205
overhead5:: 0
memory usage:: 3015323648
time_provenance:: 4.368512868881226
curr_diff: 0 tensor(1.7328e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7328e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.040863752365112305
overhead3:: 0.10267043113708496
overhead4:: 5.1247944831848145
overhead5:: 0
memory usage:: 3016482816
time_provenance:: 6.507189512252808
curr_diff: 0 tensor(1.5013e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5013e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9324, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7850, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6593, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4560, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3763, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3050, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0626, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0293, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9983, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9702, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8837, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8620, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8487, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8280, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7787, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7598, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7477, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7269, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7197, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7102, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6817, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6764, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6622, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6581, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6424, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6379, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6197, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6125, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6097, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6036, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5933, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5946, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.355417013168335
training time full:: 6.355481386184692
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         80,  82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112,
        113, 114, 117, 118, 119, 122, 124, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.881906032562256
overhead:: 0
overhead2:: 0.39484238624572754
overhead3:: 0
time_baseline:: 4.881940126419067
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.0075800418853759766
overhead3:: 0.030591249465942383
overhead4:: 0.9742400646209717
overhead5:: 0
memory usage:: 3018207232
time_provenance:: 1.4059734344482422
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010709285736083984
overhead3:: 0.037329912185668945
overhead4:: 1.3570196628570557
overhead5:: 0
memory usage:: 3030405120
time_provenance:: 1.8766043186187744
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.013883113861083984
overhead3:: 0.045384883880615234
overhead4:: 1.759093999862671
overhead5:: 0
memory usage:: 3056939008
time_provenance:: 2.368664503097534
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.012241840362548828
overhead3:: 0.03713393211364746
overhead4:: 1.4803845882415771
overhead5:: 0
memory usage:: 3025092608
time_provenance:: 2.0177199840545654
curr_diff: 0 tensor(4.6241e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6241e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.014425039291381836
overhead3:: 0.04611063003540039
overhead4:: 1.7787351608276367
overhead5:: 0
memory usage:: 3022856192
time_provenance:: 2.4024548530578613
curr_diff: 0 tensor(4.3404e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3404e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.01623058319091797
overhead3:: 0.05071759223937988
overhead4:: 2.0962789058685303
overhead5:: 0
memory usage:: 3024171008
time_provenance:: 2.7977445125579834
curr_diff: 0 tensor(4.2852e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2852e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.023160934448242188
overhead3:: 0.06174302101135254
overhead4:: 2.948584794998169
overhead5:: 0
memory usage:: 3031543808
time_provenance:: 3.8402209281921387
curr_diff: 0 tensor(1.4167e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4167e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.026749849319458008
overhead3:: 0.06711387634277344
overhead4:: 3.1016204357147217
overhead5:: 0
memory usage:: 3024326656
time_provenance:: 4.037009000778198
curr_diff: 0 tensor(1.3749e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3749e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.025878190994262695
overhead3:: 0.07237601280212402
overhead4:: 3.3481998443603516
overhead5:: 0
memory usage:: 3044003840
time_provenance:: 4.328416109085083
curr_diff: 0 tensor(1.3581e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3581e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.04056906700134277
overhead3:: 0.10843276977539062
overhead4:: 5.0547449588775635
overhead5:: 0
memory usage:: 3007885312
time_provenance:: 6.437102317810059
curr_diff: 0 tensor(1.4990e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4990e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0845, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9137, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5338, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4452, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3598, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.2940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2375, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0963, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0560, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0262, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9387, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.8980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8607, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8263, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8131, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7869, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7780, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7577, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7173, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7126, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7020, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6806, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6752, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6655, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6629, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6526, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6563, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6397, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6372, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6300, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6233, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6201, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6209, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6155, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5915, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5908, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.346230506896973
training time full:: 6.346297979354858
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.973683595657349
overhead:: 0
overhead2:: 0.3939058780670166
overhead3:: 0
time_baseline:: 4.973715782165527
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.007181406021118164
overhead3:: 0.02972102165222168
overhead4:: 0.9662227630615234
overhead5:: 0
memory usage:: 3011362816
time_provenance:: 1.4028499126434326
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010373592376708984
overhead3:: 0.03635382652282715
overhead4:: 1.333268404006958
overhead5:: 0
memory usage:: 3048517632
time_provenance:: 1.8548622131347656
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.013645172119140625
overhead3:: 0.045290470123291016
overhead4:: 1.7500145435333252
overhead5:: 0
memory usage:: 3014115328
time_provenance:: 2.3568544387817383
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.011820554733276367
overhead3:: 0.038709402084350586
overhead4:: 1.464073657989502
overhead5:: 0
memory usage:: 3015114752
time_provenance:: 2.0002999305725098
curr_diff: 0 tensor(5.0194e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0194e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.014154911041259766
overhead3:: 0.04566025733947754
overhead4:: 1.810194730758667
overhead5:: 0
memory usage:: 3015688192
time_provenance:: 2.4348268508911133
curr_diff: 0 tensor(4.9223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.016655921936035156
overhead3:: 0.050931692123413086
overhead4:: 2.102067232131958
overhead5:: 0
memory usage:: 3047587840
time_provenance:: 2.7983131408691406
curr_diff: 0 tensor(4.8350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.021349430084228516
overhead3:: 0.062116146087646484
overhead4:: 2.931751251220703
overhead5:: 0
memory usage:: 3030487040
time_provenance:: 3.8223230838775635
curr_diff: 0 tensor(1.5168e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5168e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.023527860641479492
overhead3:: 0.0681006908416748
overhead4:: 3.137340545654297
overhead5:: 0
memory usage:: 3039199232
time_provenance:: 4.070213317871094
curr_diff: 0 tensor(1.4615e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4615e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.026009559631347656
overhead3:: 0.07221341133117676
overhead4:: 3.3474082946777344
overhead5:: 0
memory usage:: 3012661248
time_provenance:: 4.329725980758667
curr_diff: 0 tensor(1.4466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.0409693717956543
overhead3:: 0.10887861251831055
overhead4:: 5.1031599044799805
overhead5:: 0
memory usage:: 3014213632
time_provenance:: 6.485320329666138
curr_diff: 0 tensor(1.5074e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5074e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1300, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9555, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6733, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3166, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1991, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0698, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9764, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9050, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8392, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7890, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7692, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7577, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7473, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7119, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7091, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7033, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6787, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6532, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6418, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6369, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6236, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6144, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5954, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5922, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.378041505813599
training time full:: 6.37811279296875
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.902994632720947
overhead:: 0
overhead2:: 0.3886125087738037
overhead3:: 0
time_baseline:: 4.903027534484863
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.0072002410888671875
overhead3:: 0.02989649772644043
overhead4:: 0.966315507888794
overhead5:: 0
memory usage:: 3011440640
time_provenance:: 1.4092822074890137
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010164976119995117
overhead3:: 0.03722834587097168
overhead4:: 1.3546502590179443
overhead5:: 0
memory usage:: 3013902336
time_provenance:: 1.8719758987426758
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.013244152069091797
overhead3:: 0.044394493103027344
overhead4:: 1.739274501800537
overhead5:: 0
memory usage:: 3013681152
time_provenance:: 2.343357801437378
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.01081085205078125
overhead3:: 0.03766989707946777
overhead4:: 1.4205641746520996
overhead5:: 0
memory usage:: 3018838016
time_provenance:: 1.9602036476135254
curr_diff: 0 tensor(5.2080e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2080e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.014003515243530273
overhead3:: 0.04559683799743652
overhead4:: 1.7810719013214111
overhead5:: 0
memory usage:: 3051257856
time_provenance:: 2.3960390090942383
curr_diff: 0 tensor(5.0472e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0472e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.016913890838623047
overhead3:: 0.051657676696777344
overhead4:: 2.159533739089966
overhead5:: 0
memory usage:: 3048599552
time_provenance:: 2.8619344234466553
curr_diff: 0 tensor(4.9788e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9788e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.021389245986938477
overhead3:: 0.06429457664489746
overhead4:: 2.9716274738311768
overhead5:: 0
memory usage:: 3048386560
time_provenance:: 3.8570163249969482
curr_diff: 0 tensor(1.7344e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7344e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.02426433563232422
overhead3:: 0.06922149658203125
overhead4:: 3.187425374984741
overhead5:: 0
memory usage:: 3012067328
time_provenance:: 4.127028226852417
curr_diff: 0 tensor(1.7126e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7126e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.025389909744262695
overhead3:: 0.0724785327911377
overhead4:: 3.344963312149048
overhead5:: 0
memory usage:: 3038187520
time_provenance:: 4.326506614685059
curr_diff: 0 tensor(1.7003e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7003e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.0431058406829834
overhead3:: 0.11134505271911621
overhead4:: 5.086259365081787
overhead5:: 0
memory usage:: 3040104448
time_provenance:: 6.471794128417969
curr_diff: 0 tensor(1.5085e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5085e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1244, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6699, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5587, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4650, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3817, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1475, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0689, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9492, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8678, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8354, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7799, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7591, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7390, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7321, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7184, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7119, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6933, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6837, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6767, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6553, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6452, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6354, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5937, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5867, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.401767015457153
training time full:: 6.401834726333618
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.9029700756073
overhead:: 0
overhead2:: 0.38978028297424316
overhead3:: 0
time_baseline:: 4.903006076812744
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.007426261901855469
overhead3:: 0.029366731643676758
overhead4:: 0.963707447052002
overhead5:: 0
memory usage:: 3049664512
time_provenance:: 1.3974676132202148
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010068893432617188
overhead3:: 0.03664422035217285
overhead4:: 1.3471310138702393
overhead5:: 0
memory usage:: 3016372224
time_provenance:: 1.8685555458068848
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.013634920120239258
overhead3:: 0.04385566711425781
overhead4:: 1.7096834182739258
overhead5:: 0
memory usage:: 3013365760
time_provenance:: 2.3193650245666504
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.010886907577514648
overhead3:: 0.037843942642211914
overhead4:: 1.4897480010986328
overhead5:: 0
memory usage:: 3048345600
time_provenance:: 2.032895803451538
curr_diff: 0 tensor(4.4714e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4714e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.013672590255737305
overhead3:: 0.04520606994628906
overhead4:: 1.7795240879058838
overhead5:: 0
memory usage:: 3023597568
time_provenance:: 2.4029369354248047
curr_diff: 0 tensor(4.0549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0549e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.016536712646484375
overhead3:: 0.0520167350769043
overhead4:: 2.128166437149048
overhead5:: 0
memory usage:: 3066929152
time_provenance:: 2.831545352935791
curr_diff: 0 tensor(3.7916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.021968841552734375
overhead3:: 0.06444144248962402
overhead4:: 2.9679200649261475
overhead5:: 0
memory usage:: 3050135552
time_provenance:: 3.8586337566375732
curr_diff: 0 tensor(1.3954e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3954e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.023942947387695312
overhead3:: 0.0686349868774414
overhead4:: 3.1631367206573486
overhead5:: 0
memory usage:: 3023478784
time_provenance:: 4.101195812225342
curr_diff: 0 tensor(1.3438e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3438e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.026188135147094727
overhead3:: 0.0708322525024414
overhead4:: 3.214052438735962
overhead5:: 0
memory usage:: 3014316032
time_provenance:: 4.19321346282959
curr_diff: 0 tensor(1.3396e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3396e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
overhead:: 0
overhead2:: 0.046384334564208984
overhead3:: 0.09571075439453125
overhead4:: 5.114667177200317
overhead5:: 0
memory usage:: 3013246976
time_provenance:: 6.4961419105529785
curr_diff: 0 tensor(1.5095e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5095e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
deletion rate:: 0.005
python3 generate_rand_ids 0.005  MNIST5 0
tensor([19456, 13313, 24579, 48132, 14341,  2061, 29709, 27663, 59411, 13342,
        32803, 42020, 22567, 58408, 34860, 39981, 37934,  5171, 48180, 14393,
        51268, 44101, 11335, 35912, 18510, 56399,  8274, 24660, 16472, 30813,
         6238, 50275,  1133, 13424, 26737,  5241,  6267, 38022, 22663,  9351,
        16525, 33934, 20626, 27795,  4244,  4250,  6299,  1185, 41122, 10401,
         8356, 29867, 21683, 54456, 16568, 41148, 33984, 45248, 36037, 20679,
        30920, 55498,  8395, 10447, 17617, 29910,  1238, 17627, 11485, 34014,
        17629, 28896,  8418, 40169,  9451, 34034, 49407, 42248, 58633, 43275,
         4369,  3347, 52505, 43290, 27936, 49443, 49444, 20779, 25902, 36144,
        30015, 43328, 17732, 44359,  8525, 40273, 22865, 56658, 54609,  4438,
        51545, 19804, 38237, 11616, 49512, 43373, 10606, 21871, 26990, 43377,
        17780, 35193, 18810, 49531, 32137,  9610, 53645, 43406, 44432, 59793,
        57746,  2451, 49555, 19860, 59800, 44444, 27036, 36255, 19872, 56738,
         2467, 42410, 39344, 16827, 28091, 34238,   450, 30147, 57794,   455,
         9673, 38346, 59850, 58828, 54736, 52705, 52706, 59881, 19947, 39404,
        10732, 19959,  8705, 31235, 58888, 53771, 57870,  3599, 42512, 22039,
        31255, 29208, 51740, 48668, 52765,  9758, 12830, 18978, 32295, 45609,
        39465, 41525, 59961, 30267, 51773, 50750, 14915,  8773,  8774,  5704,
        42571, 28240, 31313, 19031, 58971, 44642, 58983, 51816, 40552, 34419,
         2676, 10869,  4724, 36472, 14970, 31354, 47738, 57980, 20095, 38531,
        50819, 24207, 59036, 19107, 36515, 13989, 46756,  8869, 40616, 26282,
        39598, 41646, 57010, 31412, 38582, 57017, 45755, 30396, 24254, 47809,
        21187, 24265, 42701, 40653, 48845, 27343, 47822, 13007, 40659, 15078,
        37606,  4842,  4851,  7927, 54008, 30457, 33530, 43772, 55039, 36611,
         6915, 55048, 41737,  8970, 37646, 51997, 28448, 38689, 40738, 26401,
         4903, 38697, 31530, 55087, 22323, 11066,  5956, 18245,  6985,  1866,
        56140, 10061, 11086, 19284, 43866, 25437, 12127, 55139, 28520, 50026,
        19310, 59248,  2930, 20339, 11122, 27509, 25462, 56185,  9094, 19336,
        15245, 50073, 40859, 23452, 15262, 47014, 45996, 46005, 38843, 51134,
        39880, 27610, 10213, 24551, 50152, 52204, 37874, 20468, 11256, 16381])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6679, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4612, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1840, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8755, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7850, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7170, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6859, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6628, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6592, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6183, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6239, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6033, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5954, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2839159965515137
training time full:: 3.2839818000793457
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6282925605773926
overhead:: 0
overhead2:: 0.19840192794799805
overhead3:: 0
time_baseline:: 2.6283273696899414
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.007700920104980469
overhead3:: 0.02591872215270996
overhead4:: 0.5354313850402832
overhead5:: 0
memory usage:: 3049713664
time_provenance:: 0.9131267070770264
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011466026306152344
overhead3:: 0.031891822814941406
overhead4:: 0.6805095672607422
overhead5:: 0
memory usage:: 3076997120
time_provenance:: 1.1358294486999512
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.014092683792114258
overhead3:: 0.03787398338317871
overhead4:: 0.9122226238250732
overhead5:: 0
memory usage:: 3033812992
time_provenance:: 1.3677208423614502
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011851072311401367
overhead3:: 0.03388667106628418
overhead4:: 0.7374241352081299
overhead5:: 0
memory usage:: 3056553984
time_provenance:: 1.1772668361663818
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.015224695205688477
overhead3:: 0.0391232967376709
overhead4:: 0.8943650722503662
overhead5:: 0
memory usage:: 3032633344
time_provenance:: 1.3793509006500244
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.0169677734375
overhead3:: 0.04485940933227539
overhead4:: 1.0735726356506348
overhead5:: 0
memory usage:: 3059134464
time_provenance:: 1.5650403499603271
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.025934457778930664
overhead3:: 0.057096242904663086
overhead4:: 1.5630378723144531
overhead5:: 0
memory usage:: 3018457088
time_provenance:: 2.1962034702301025
curr_diff: 0 tensor(5.1748e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1748e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02375006675720215
overhead3:: 0.06440997123718262
overhead4:: 1.6791660785675049
overhead5:: 0
memory usage:: 3026448384
time_provenance:: 2.3334803581237793
curr_diff: 0 tensor(5.0070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02631831169128418
overhead3:: 0.06655001640319824
overhead4:: 1.7894501686096191
overhead5:: 0
memory usage:: 3014807552
time_provenance:: 2.4616410732269287
curr_diff: 0 tensor(4.8890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.041548967361450195
overhead3:: 0.10358858108520508
overhead4:: 2.552440643310547
overhead5:: 0
memory usage:: 3028291584
time_provenance:: 3.3963327407836914
curr_diff: 0 tensor(1.6103e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6103e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6845, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1090, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0349, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9819, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6793, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6565, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6279, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6065, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5722, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.268389940261841
training time full:: 3.268460988998413
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 137, 145, 148, 155, 158, 162, 163, 173,
        175, 176, 183, 184, 186, 187, 188, 200])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.621903896331787
overhead:: 0
overhead2:: 0.19716119766235352
overhead3:: 0
time_baseline:: 2.6219727993011475
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.007839202880859375
overhead3:: 0.026633262634277344
overhead4:: 0.5366406440734863
overhead5:: 0
memory usage:: 3027668992
time_provenance:: 0.9078004360198975
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.010411977767944336
overhead3:: 0.03226041793823242
overhead4:: 0.7047293186187744
overhead5:: 0
memory usage:: 3082600448
time_provenance:: 1.1234006881713867
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.013187646865844727
overhead3:: 0.03820180892944336
overhead4:: 0.8938219547271729
overhead5:: 0
memory usage:: 3024187392
time_provenance:: 1.3375768661499023
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011690616607666016
overhead3:: 0.03503847122192383
overhead4:: 0.7590398788452148
overhead5:: 0
memory usage:: 3016646656
time_provenance:: 1.1841745376586914
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.01568603515625
overhead3:: 0.03842568397521973
overhead4:: 0.9148709774017334
overhead5:: 0
memory usage:: 3039965184
time_provenance:: 1.3773939609527588
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.017866134643554688
overhead3:: 0.047725677490234375
overhead4:: 1.0991029739379883
overhead5:: 0
memory usage:: 3033612288
time_provenance:: 1.5919322967529297
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.026514053344726562
overhead3:: 0.05596280097961426
overhead4:: 1.5593841075897217
overhead5:: 0
memory usage:: 3034136576
time_provenance:: 2.1995139122009277
curr_diff: 0 tensor(5.9861e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9861e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02506709098815918
overhead3:: 0.06160855293273926
overhead4:: 1.696685791015625
overhead5:: 0
memory usage:: 3040006144
time_provenance:: 2.3461194038391113
curr_diff: 0 tensor(5.7925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02750396728515625
overhead3:: 0.0661020278930664
overhead4:: 1.7316803932189941
overhead5:: 0
memory usage:: 3015962624
time_provenance:: 2.4000911712646484
curr_diff: 0 tensor(5.6803e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6803e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.05115914344787598
overhead3:: 0.09977340698242188
overhead4:: 2.548678159713745
overhead5:: 0
memory usage:: 3005100032
time_provenance:: 3.395505905151367
curr_diff: 0 tensor(1.6044e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6044e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6717, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.2041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9297, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8844, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8510, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7757, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7552, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7117, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6954, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6082, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5791, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5738, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.268033981323242
training time full:: 3.2681026458740234
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.627441644668579
overhead:: 0
overhead2:: 0.20046639442443848
overhead3:: 0
time_baseline:: 2.627479314804077
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.008411169052124023
overhead3:: 0.025711536407470703
overhead4:: 0.5701923370361328
overhead5:: 0
memory usage:: 3025133568
time_provenance:: 0.9480888843536377
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0109, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0109, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.010065078735351562
overhead3:: 0.031531572341918945
overhead4:: 0.7049407958984375
overhead5:: 0
memory usage:: 3013775360
time_provenance:: 1.105156421661377
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.01398158073425293
overhead3:: 0.039849042892456055
overhead4:: 0.8831644058227539
overhead5:: 0
memory usage:: 3020038144
time_provenance:: 1.3359293937683105
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0107, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.012237548828125
overhead3:: 0.03344988822937012
overhead4:: 0.7607295513153076
overhead5:: 0
memory usage:: 3040153600
time_provenance:: 1.1871747970581055
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.014130830764770508
overhead3:: 0.040488243103027344
overhead4:: 0.9258551597595215
overhead5:: 0
memory usage:: 3032768512
time_provenance:: 1.3758571147918701
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.017415523529052734
overhead3:: 0.046625375747680664
overhead4:: 1.0730936527252197
overhead5:: 0
memory usage:: 3021631488
time_provenance:: 1.5682575702667236
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02307748794555664
overhead3:: 0.0600285530090332
overhead4:: 1.5833520889282227
overhead5:: 0
memory usage:: 3045470208
time_provenance:: 2.2198145389556885
curr_diff: 0 tensor(6.1797e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1797e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.02628469467163086
overhead3:: 0.06261157989501953
overhead4:: 1.6623330116271973
overhead5:: 0
memory usage:: 3013709824
time_provenance:: 2.3172757625579834
curr_diff: 0 tensor(5.9285e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9285e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.025780677795410156
overhead3:: 0.0659186840057373
overhead4:: 1.7533602714538574
overhead5:: 0
memory usage:: 3015929856
time_provenance:: 2.4201252460479736
curr_diff: 0 tensor(5.8613e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8613e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.0440213680267334
overhead3:: 0.10268115997314453
overhead4:: 2.5154497623443604
overhead5:: 0
memory usage:: 3030118400
time_provenance:: 3.354686737060547
curr_diff: 0 tensor(1.6055e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6055e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3296, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9368, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6675, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4620, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3161, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1963, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9827, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8196, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7698, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7433, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7165, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6818, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6681, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6586, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6033, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5904, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5846, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2720961570739746
training time full:: 3.2721636295318604
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.632904291152954
overhead:: 0
overhead2:: 0.19651269912719727
overhead3:: 0
time_baseline:: 2.6329426765441895
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.007973670959472656
overhead3:: 0.025115013122558594
overhead4:: 0.5281901359558105
overhead5:: 0
memory usage:: 3026767872
time_provenance:: 0.9055161476135254
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.010394573211669922
overhead3:: 0.03257584571838379
overhead4:: 0.7234089374542236
overhead5:: 0
memory usage:: 3016417280
time_provenance:: 1.1310179233551025
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0109, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0109, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.014150857925415039
overhead3:: 0.03821706771850586
overhead4:: 0.9068236351013184
overhead5:: 0
memory usage:: 3025911808
time_provenance:: 1.3594021797180176
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011464595794677734
overhead3:: 0.03313636779785156
overhead4:: 0.7647008895874023
overhead5:: 0
memory usage:: 3026468864
time_provenance:: 1.1855509281158447
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.014510393142700195
overhead3:: 0.038451433181762695
overhead4:: 0.9268698692321777
overhead5:: 0
memory usage:: 3044749312
time_provenance:: 1.3860414028167725
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.017538070678710938
overhead3:: 0.04598593711853027
overhead4:: 1.062103509902954
overhead5:: 0
memory usage:: 3043110912
time_provenance:: 1.5617873668670654
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.022693872451782227
overhead3:: 0.062191009521484375
overhead4:: 1.5996854305267334
overhead5:: 0
memory usage:: 3014447104
time_provenance:: 2.2307889461517334
curr_diff: 0 tensor(6.3394e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3394e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.026062726974487305
overhead3:: 0.06172943115234375
overhead4:: 1.6779062747955322
overhead5:: 0
memory usage:: 3026558976
time_provenance:: 2.340101480484009
curr_diff: 0 tensor(6.1600e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1600e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.027335405349731445
overhead3:: 0.06656360626220703
overhead4:: 1.7488253116607666
overhead5:: 0
memory usage:: 3020763136
time_provenance:: 2.4271583557128906
curr_diff: 0 tensor(6.0806e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0806e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.04790139198303223
overhead3:: 0.09793567657470703
overhead4:: 2.549229621887207
overhead5:: 0
memory usage:: 3008794624
time_provenance:: 3.3876636028289795
curr_diff: 0 tensor(1.6123e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6123e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3381, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9601, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6756, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4720, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1988, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0386, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9830, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8205, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7894, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7171, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6697, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6608, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6511, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6405, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6308, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6200, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6097, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5845, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2507245540618896
training time full:: 3.250792980194092
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 177, 180, 188, 190])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.638651132583618
overhead:: 0
overhead2:: 0.19692254066467285
overhead3:: 0
time_baseline:: 2.6386919021606445
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.007699489593505859
overhead3:: 0.025392770767211914
overhead4:: 0.5218307971954346
overhead5:: 0
memory usage:: 3032289280
time_provenance:: 0.9226293563842773
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0107, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011778831481933594
overhead3:: 0.03132748603820801
overhead4:: 0.6800251007080078
overhead5:: 0
memory usage:: 3035381760
time_provenance:: 1.140320062637329
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0107, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.013788938522338867
overhead3:: 0.03871297836303711
overhead4:: 0.8740112781524658
overhead5:: 0
memory usage:: 3014090752
time_provenance:: 1.3322339057922363
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0107, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.011784553527832031
overhead3:: 0.03499865531921387
overhead4:: 0.7782135009765625
overhead5:: 0
memory usage:: 3026821120
time_provenance:: 1.197418212890625
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.01466512680053711
overhead3:: 0.040712594985961914
overhead4:: 0.9328773021697998
overhead5:: 0
memory usage:: 3054718976
time_provenance:: 1.3942532539367676
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.016722679138183594
overhead3:: 0.046297550201416016
overhead4:: 1.1026382446289062
overhead5:: 0
memory usage:: 3016568832
time_provenance:: 1.595259666442871
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.024338722229003906
overhead3:: 0.05864834785461426
overhead4:: 1.5469691753387451
overhead5:: 0
memory usage:: 3016585216
time_provenance:: 2.1788408756256104
curr_diff: 0 tensor(6.1840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.025055885314941406
overhead3:: 0.06540179252624512
overhead4:: 1.7181670665740967
overhead5:: 0
memory usage:: 3016728576
time_provenance:: 2.3702504634857178
curr_diff: 0 tensor(6.0458e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0458e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.025528669357299805
overhead3:: 0.0692451000213623
overhead4:: 1.7626287937164307
overhead5:: 0
memory usage:: 3013832704
time_provenance:: 2.4431259632110596
curr_diff: 0 tensor(5.9086e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9086e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 300
max_epoch:: 32
overhead:: 0
overhead2:: 0.04732918739318848
overhead3:: 0.10051298141479492
overhead4:: 2.571120262145996
overhead5:: 0
memory usage:: 3015335936
time_provenance:: 3.4174580574035645
curr_diff: 0 tensor(1.6223e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6223e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3056, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9340, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6574, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5463, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4543, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3726, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2436, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1881, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0611, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0283, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9954, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9710, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9463, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8799, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8675, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8450, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8273, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8141, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7913, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7781, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7474, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7412, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7296, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7104, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7051, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6945, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6857, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6801, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6686, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6637, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6571, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6518, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6499, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6346, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6292, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6213, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6151, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6082, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6057, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5976, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.407843351364136
training time full:: 6.407909154891968
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.966975212097168
overhead:: 0
overhead2:: 0.3974723815917969
overhead3:: 0
time_baseline:: 4.967013120651245
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.00768280029296875
overhead3:: 0.0315704345703125
overhead4:: 1.0114247798919678
overhead5:: 0
memory usage:: 3084406784
time_provenance:: 1.4697961807250977
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.010669469833374023
overhead3:: 0.03841805458068848
overhead4:: 1.3744163513183594
overhead5:: 0
memory usage:: 3020210176
time_provenance:: 1.9158084392547607
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01378178596496582
overhead3:: 0.045584678649902344
overhead4:: 1.776627779006958
overhead5:: 0
memory usage:: 3012874240
time_provenance:: 2.4094772338867188
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.011323213577270508
overhead3:: 0.0377192497253418
overhead4:: 1.5193355083465576
overhead5:: 0
memory usage:: 3055767552
time_provenance:: 2.091578483581543
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013923168182373047
overhead3:: 0.04703092575073242
overhead4:: 1.8519871234893799
overhead5:: 0
memory usage:: 3026391040
time_provenance:: 2.4937164783477783
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.016971588134765625
overhead3:: 0.051882266998291016
overhead4:: 2.170971393585205
overhead5:: 0
memory usage:: 3014582272
time_provenance:: 2.8816661834716797
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.022919416427612305
overhead3:: 0.06518959999084473
overhead4:: 2.9997596740722656
overhead5:: 0
memory usage:: 3016876032
time_provenance:: 3.9081063270568848
curr_diff: 0 tensor(3.4536e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4536e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.023977994918823242
overhead3:: 0.06956672668457031
overhead4:: 3.234992027282715
overhead5:: 0
memory usage:: 3016839168
time_provenance:: 4.183620929718018
curr_diff: 0 tensor(3.3961e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3961e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.027128219604492188
overhead3:: 0.07481145858764648
overhead4:: 3.3899598121643066
overhead5:: 0
memory usage:: 3015200768
time_provenance:: 4.390032529830933
curr_diff: 0 tensor(3.3223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.04131722450256348
overhead3:: 0.11040472984313965
overhead4:: 5.229227066040039
overhead5:: 0
memory usage:: 3021799424
time_provenance:: 6.628865003585815
curr_diff: 0 tensor(1.5072e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5072e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6524, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4500, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3715, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2415, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1844, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1387, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9389, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9205, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8826, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8267, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8151, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7988, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7782, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7657, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7590, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7474, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7319, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7186, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6912, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6761, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6617, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6420, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6373, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6225, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6119, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6066, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5999, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6030, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5931, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5944, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5875, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.438927173614502
training time full:: 6.438990592956543
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         80,  82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112,
        113, 114, 117, 118, 119, 122, 124, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.950410842895508
overhead:: 0
overhead2:: 0.3994429111480713
overhead3:: 0
time_baseline:: 4.950448989868164
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.007849931716918945
overhead3:: 0.030817508697509766
overhead4:: 1.0084636211395264
overhead5:: 0
memory usage:: 3049279488
time_provenance:: 1.460989236831665
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01049351692199707
overhead3:: 0.037767648696899414
overhead4:: 1.4167437553405762
overhead5:: 0
memory usage:: 3045097472
time_provenance:: 1.950138807296753
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013147592544555664
overhead3:: 0.04456377029418945
overhead4:: 1.7683064937591553
overhead5:: 0
memory usage:: 3040067584
time_provenance:: 2.3977649211883545
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.011147260665893555
overhead3:: 0.03891944885253906
overhead4:: 1.5325860977172852
overhead5:: 0
memory usage:: 3021254656
time_provenance:: 2.0913233757019043
curr_diff: 0 tensor(8.9775e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9775e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013935327529907227
overhead3:: 0.04552459716796875
overhead4:: 1.880521535873413
overhead5:: 0
memory usage:: 3040985088
time_provenance:: 2.5193111896514893
curr_diff: 0 tensor(8.7211e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7211e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01683330535888672
overhead3:: 0.05204343795776367
overhead4:: 2.185532331466675
overhead5:: 0
memory usage:: 3027156992
time_provenance:: 2.8977110385894775
curr_diff: 0 tensor(8.3895e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3895e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.022544145584106445
overhead3:: 0.06514406204223633
overhead4:: 3.025010108947754
overhead5:: 0
memory usage:: 3029225472
time_provenance:: 3.9312868118286133
curr_diff: 0 tensor(3.2966e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2966e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.023380041122436523
overhead3:: 0.06714463233947754
overhead4:: 3.2157835960388184
overhead5:: 0
memory usage:: 3024240640
time_provenance:: 4.164978742599487
curr_diff: 0 tensor(3.2503e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2503e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.024782896041870117
overhead3:: 0.07165098190307617
overhead4:: 3.4281957149505615
overhead5:: 0
memory usage:: 3052285952
time_provenance:: 4.4290032386779785
curr_diff: 0 tensor(3.1864e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1864e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.040439605712890625
overhead3:: 0.11099576950073242
overhead4:: 5.13750696182251
overhead5:: 0
memory usage:: 3039252480
time_provenance:: 6.531632661819458
curr_diff: 0 tensor(1.5111e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5111e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3109, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7861, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6589, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5497, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1895, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1441, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9731, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8819, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8644, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8492, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7808, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7660, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7596, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7385, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7280, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7142, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7035, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6901, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6818, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6765, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6642, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6539, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6576, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6385, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6243, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6246, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5875, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.361940383911133
training time full:: 6.362007141113281
provenance prepare time:: 7.152557373046875e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.904489040374756
overhead:: 0
overhead2:: 0.3975796699523926
overhead3:: 0
time_baseline:: 4.904528856277466
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.008198261260986328
overhead3:: 0.0320279598236084
overhead4:: 1.0085322856903076
overhead5:: 0
memory usage:: 3051159552
time_provenance:: 1.4605114459991455
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.010518550872802734
overhead3:: 0.03725910186767578
overhead4:: 1.375105619430542
overhead5:: 0
memory usage:: 3028332544
time_provenance:: 1.9127089977264404
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013850688934326172
overhead3:: 0.04561662673950195
overhead4:: 1.7457573413848877
overhead5:: 0
memory usage:: 3017199616
time_provenance:: 2.3954081535339355
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.010720491409301758
overhead3:: 0.03905797004699707
overhead4:: 1.5097150802612305
overhead5:: 0
memory usage:: 3021139968
time_provenance:: 2.074948787689209
curr_diff: 0 tensor(9.0007e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0007e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.014451026916503906
overhead3:: 0.04566764831542969
overhead4:: 1.8278968334197998
overhead5:: 0
memory usage:: 3031343104
time_provenance:: 2.4678397178649902
curr_diff: 0 tensor(8.2865e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2865e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01745009422302246
overhead3:: 0.05238223075866699
overhead4:: 2.186674118041992
overhead5:: 0
memory usage:: 3050606592
time_provenance:: 2.8956661224365234
curr_diff: 0 tensor(7.8153e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8153e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.02202439308166504
overhead3:: 0.06413674354553223
overhead4:: 3.000276565551758
overhead5:: 0
memory usage:: 3021152256
time_provenance:: 3.9035251140594482
curr_diff: 0 tensor(4.0937e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0937e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.024567604064941406
overhead3:: 0.06625604629516602
overhead4:: 3.2183618545532227
overhead5:: 0
memory usage:: 3093368832
time_provenance:: 4.169839859008789
curr_diff: 0 tensor(4.0168e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0168e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.026584148406982422
overhead3:: 0.07368850708007812
overhead4:: 3.417778491973877
overhead5:: 0
memory usage:: 3021447168
time_provenance:: 4.413969278335571
curr_diff: 0 tensor(3.9553e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9553e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.04072308540344238
overhead3:: 0.1086125373840332
overhead4:: 5.2225987911224365
overhead5:: 0
memory usage:: 3016122368
time_provenance:: 6.616631031036377
curr_diff: 0 tensor(1.5024e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5024e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3331, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1265, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6708, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5620, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4657, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2511, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1990, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0697, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0366, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0012, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9514, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8395, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7894, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7836, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7576, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7418, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7292, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7189, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7114, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7031, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6784, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6699, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6527, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6415, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6366, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6359, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6127, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6096, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6071, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.382172346115112
training time full:: 6.382238149642944
provenance prepare time:: 7.152557373046875e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.900543451309204
overhead:: 0
overhead2:: 0.3955717086791992
overhead3:: 0
time_baseline:: 4.900579929351807
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.007606029510498047
overhead3:: 0.03076934814453125
overhead4:: 1.0094218254089355
overhead5:: 0
memory usage:: 3016331264
time_provenance:: 1.4609456062316895
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.010381698608398438
overhead3:: 0.03870248794555664
overhead4:: 1.4074816703796387
overhead5:: 0
memory usage:: 3016871936
time_provenance:: 1.9510471820831299
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01370692253112793
overhead3:: 0.04551887512207031
overhead4:: 1.7873456478118896
overhead5:: 0
memory usage:: 3028381696
time_provenance:: 2.422581672668457
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.011241674423217773
overhead3:: 0.03965568542480469
overhead4:: 1.498061180114746
overhead5:: 0
memory usage:: 3016609792
time_provenance:: 2.0607922077178955
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.01383209228515625
overhead3:: 0.045725107192993164
overhead4:: 1.8362908363342285
overhead5:: 0
memory usage:: 3020402688
time_provenance:: 2.4776771068573
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.018230915069580078
overhead3:: 0.05141401290893555
overhead4:: 2.1887753009796143
overhead5:: 0
memory usage:: 3018964992
time_provenance:: 2.9093003273010254
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.023061513900756836
overhead3:: 0.0665290355682373
overhead4:: 3.0327751636505127
overhead5:: 0
memory usage:: 3022376960
time_provenance:: 3.941166877746582
curr_diff: 0 tensor(4.3386e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3386e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.024163484573364258
overhead3:: 0.06942868232727051
overhead4:: 3.2250173091888428
overhead5:: 0
memory usage:: 3041656832
time_provenance:: 4.181272745132446
curr_diff: 0 tensor(4.2846e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2846e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.02529430389404297
overhead3:: 0.0713045597076416
overhead4:: 3.4392616748809814
overhead5:: 0
memory usage:: 3058528256
time_provenance:: 4.4374096393585205
curr_diff: 0 tensor(4.2542e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2542e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.04058051109313965
overhead3:: 0.10955262184143066
overhead4:: 5.18338418006897
overhead5:: 0
memory usage:: 3003539456
time_provenance:: 6.576406240463257
curr_diff: 0 tensor(1.4870e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4870e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3134, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1093, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9345, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6589, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4556, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3722, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2433, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1938, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1404, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0983, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9683, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8801, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8120, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7862, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7771, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7619, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7562, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7158, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6909, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6813, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6765, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6742, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6615, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6530, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6461, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6486, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6429, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6315, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6337, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6344, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6162, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6107, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5845, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.33672833442688
training time full:: 6.336791753768921
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.8904945850372314
overhead:: 0
overhead2:: 0.39329028129577637
overhead3:: 0
time_baseline:: 4.890531778335571
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.007083892822265625
overhead3:: 0.0308380126953125
overhead4:: 1.0113787651062012
overhead5:: 0
memory usage:: 3017457664
time_provenance:: 1.468754529953003
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.011063575744628906
overhead3:: 0.037262678146362305
overhead4:: 1.390282392501831
overhead5:: 0
memory usage:: 3013996544
time_provenance:: 1.9684767723083496
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013498783111572266
overhead3:: 0.04556560516357422
overhead4:: 1.7543301582336426
overhead5:: 0
memory usage:: 3020726272
time_provenance:: 2.3820676803588867
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0103, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0103, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.011009931564331055
overhead3:: 0.03950047492980957
overhead4:: 1.4872934818267822
overhead5:: 0
memory usage:: 3049996288
time_provenance:: 2.0407280921936035
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.013929367065429688
overhead3:: 0.0449984073638916
overhead4:: 1.8285751342773438
overhead5:: 0
memory usage:: 3109335040
time_provenance:: 2.495767593383789
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.016335248947143555
overhead3:: 0.05303764343261719
overhead4:: 2.215590715408325
overhead5:: 0
memory usage:: 3025362944
time_provenance:: 2.935551881790161
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.02243971824645996
overhead3:: 0.06402468681335449
overhead4:: 3.0386054515838623
overhead5:: 0
memory usage:: 3070783488
time_provenance:: 3.949512481689453
curr_diff: 0 tensor(3.4278e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4278e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.02267289161682129
overhead3:: 0.06853604316711426
overhead4:: 3.2362253665924072
overhead5:: 0
memory usage:: 3023474688
time_provenance:: 4.19173789024353
curr_diff: 0 tensor(3.3955e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3955e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.02647542953491211
overhead3:: 0.07349681854248047
overhead4:: 3.3949472904205322
overhead5:: 0
memory usage:: 3027202048
time_provenance:: 4.390230655670166
curr_diff: 0 tensor(3.3564e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3564e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 300
max_epoch:: 60
overhead:: 0
overhead2:: 0.04238319396972656
overhead3:: 0.10904693603515625
overhead4:: 5.214632034301758
overhead5:: 0
memory usage:: 3007205376
time_provenance:: 6.606904983520508
curr_diff: 0 tensor(1.5161e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5161e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
deletion rate:: 0.01
python3 generate_rand_ids 0.01  MNIST5 0
tensor([19456, 13313, 23552, 24579, 48132, 14341,  3074, 49156,  3082, 24588,
         2061, 29709, 27663, 55308, 59411, 36883, 44053, 19481, 20508, 13342,
        53278, 26654, 34850, 32803, 42020, 19492, 15397, 22567, 58408, 58409,
        34860, 39981, 37934, 44080,  5171, 48180, 10293, 26678,  7223,  8247,
        14393, 27701,  3131, 13369, 45121,  2113, 51268, 44101, 28742, 11335,
        35912, 16459, 18510, 56399,  8274, 31827, 24660, 16472, 51289, 30813,
         6238, 41055,  3166, 17505,  7266, 50275, 57442, 43110,  1133, 20590,
        13424, 26737, 54386, 58482, 48242,  5241,  6267, 57468,  7296, 38022,
        22663,  9351, 14473,  5258,  8331, 16525, 33934, 24719, 41105, 20626,
        27795,  4244, 44178, 31893,  4250,  6299, 54428, 33950,  2207,  1185,
        41122, 10401,  8356, 45221, 33960, 29867, 29871, 26800, 39087, 58546,
        21683, 54456, 16568, 14520, 24760, 41148, 54459,  7160, 33984, 45248,
        11457, 13507, 36037, 57542, 20679, 30920, 43209, 55498,  8395,  5323,
         3277, 10447, 44240, 17617,  1236, 29910,  1238, 17627, 24795, 11485,
        34014, 17629, 28896, 52448,  8418,  9441, 59619, 41188, 45288, 40169,
         9451, 26865, 34034, 54520, 22777,  5369, 11515,  4348, 49407, 56576,
        47362, 42248, 58633, 53514, 43275, 20746, 45327,  3344,  4369,  3347,
        34067, 52503, 52505, 43290, 22812, 40221, 24861, 27936, 49443, 49444,
        38180, 47397, 34090, 20779, 25902, 36144, 25907, 26935, 43321, 30015,
        43328, 31040, 17732, 20806, 44359, 39241,  8525, 46414, 40273, 22865,
        56658, 54609,  4438, 51545, 43354, 19804, 38237, 38238, 11616, 18784,
        49512, 50539, 43373, 10606, 21871, 26990, 43377, 28013,  7536, 17780,
        37234, 41335, 35193, 18810, 49531, 43386, 38269, 25985, 32137,  9610,
        40329, 53645, 43406, 44432, 59793, 57746,  2451, 49555, 19860, 46481,
        22931, 59800,  3476, 23957, 32154, 44444, 27036, 36251, 36255, 19872,
        56738,  2467, 28068, 42410, 26026, 33195, 39344, 45490, 26034,  3508,
        13748, 46518, 51639, 59830, 37304,  4538, 16827, 28091, 38332, 34238,
        31165, 40383,   450, 30147, 57794,   455, 22984,  9673, 38346, 59850,
        58828, 47560, 33227,  9678, 54736, 44494, 40402, 37327, 39375,  8669,
        52705, 52706, 59881, 19947, 39404, 10732, 40429, 19959, 41463, 29178,
        55806, 48638,  8705, 31234, 31235,  2563, 34309, 49670,  5639, 58888,
         9736,  1546, 53771, 57870,  3599, 42512,  5650, 50709, 45589, 22039,
        31255, 29208,  2582, 41493, 51740, 48668, 52765,  9758, 12830, 25117,
        18978, 23073, 24102, 32295, 37414, 45609, 39465,  4648, 24112,   562,
        41525, 26165, 34358, 59961, 30267, 41531, 51773, 50750, 21053, 14915,
        52803,  8773,  8774, 29252,  5704, 46661, 44615, 42571, 39496, 51786,
        27215, 28240, 31313, 29263, 57937, 39507, 30293, 19031, 58971, 56923,
        49756, 56926, 44642, 58983, 51816, 40552, 37481, 47722, 26219, 37483,
        55917, 36461, 47726, 28271, 15985, 34419,  2676, 10869,  4724,  3700,
        36472, 51830, 14970, 31354, 47738, 57980, 22133, 20095, 30329, 35451,
        27265, 38531, 50819, 40579, 52868, 46730, 24207, 42641, 10901, 59036,
         6813, 30370, 19107, 36515, 13989, 46756,  8869, 40616, 36517, 26282,
         7853, 39598, 41646, 57010, 31412, 38582, 57017, 48826, 45755, 30396,
         8890, 24254, 53948, 42687, 47809, 21187, 33480, 24265, 53961, 41674,
        21195, 42701, 40653, 48845, 27343, 47822, 13007, 40659,  3788, 41686,
        27362,  8931, 15078, 37606,  6887, 10984,  4842,  8942, 13040,  4851,
        13044,  7927, 54008, 30457, 33530, 59130, 43772, 28412, 18174, 55039,
        56064, 36611,  6915, 43781, 55048, 41737,  8970, 16136, 51978, 24171,
        37646, 22286, 44816,  7957, 46875, 51997, 28448, 38689, 40738, 26401,
        48930,  4903, 38697, 31530,   812, 55087, 28463, 31536, 22323,  1843,
        17208, 52025, 11066, 22330,  2876, 12097,  5956, 18245,  6985,  1866,
        56140, 10061, 11086, 21331, 19284, 34643, 42837, 43859, 35670, 48984,
        43866, 58990, 25437, 12127, 20319, 36704, 55139, 29543, 28520, 50026,
        19310,  4975, 59248,  2930, 20339, 11122, 27509, 25462, 56185, 42879,
         9094, 19336, 15245,  2958, 17300, 30614, 50073, 22425, 40859, 23452,
         1947, 15262,  1952, 30625, 45989, 47014,  5035, 45996, 16301,  8109,
        10161, 43954, 46005, 56245, 44983, 28598, 59320, 38843, 53179, 39869,
        51134, 27588, 11206, 58310, 39880, 34764, 28624, 56273, 30678, 27610,
        55259, 18399, 52193, 14306, 35812, 10213,  3044, 24551, 50152, 52204,
        44014, 22512, 48113, 37874, 19442, 20468, 11256, 14329, 16381, 40958])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3398, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9501, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3103, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1865, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1035, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0321, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8769, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8516, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8237, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7175, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6860, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6196, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6249, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6188, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5956, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5837, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.262491226196289
training time full:: 3.2625601291656494
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  75,  76,  77,  78,  79,  80,  89,  91,  92,
         93,  95,  97, 102, 105, 106, 107, 108, 109, 118, 121, 123, 134, 135,
        136, 143, 150, 152, 173, 174, 176, 183])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.606837749481201
overhead:: 0
overhead2:: 0.20182490348815918
overhead3:: 0
time_baseline:: 2.6068859100341797
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.008045196533203125
overhead3:: 0.027234554290771484
overhead4:: 0.5342800617218018
overhead5:: 0
memory usage:: 3028512768
time_provenance:: 0.9126007556915283
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010213136672973633
overhead3:: 0.033677101135253906
overhead4:: 0.6892282962799072
overhead5:: 0
memory usage:: 3033841664
time_provenance:: 1.1496214866638184
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013447284698486328
overhead3:: 0.03884696960449219
overhead4:: 0.8644659519195557
overhead5:: 0
memory usage:: 3024842752
time_provenance:: 1.3760030269622803
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011543989181518555
overhead3:: 0.03533291816711426
overhead4:: 0.7529561519622803
overhead5:: 0
memory usage:: 3024850944
time_provenance:: 1.2131600379943848
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014514923095703125
overhead3:: 0.04179501533508301
overhead4:: 0.8841004371643066
overhead5:: 0
memory usage:: 3015602176
time_provenance:: 1.4100110530853271
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.0182039737701416
overhead3:: 0.04695582389831543
overhead4:: 1.097445011138916
overhead5:: 0
memory usage:: 3017764864
time_provenance:: 1.613638162612915
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02303481101989746
overhead3:: 0.06194663047790527
overhead4:: 1.5799148082733154
overhead5:: 0
memory usage:: 3018092544
time_provenance:: 2.231034517288208
curr_diff: 0 tensor(8.9324e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9324e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.023626089096069336
overhead3:: 0.06413555145263672
overhead4:: 1.671311616897583
overhead5:: 0
memory usage:: 3025989632
time_provenance:: 2.3300771713256836
curr_diff: 0 tensor(8.6580e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6580e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.026201963424682617
overhead3:: 0.06739258766174316
overhead4:: 1.6979343891143799
overhead5:: 0
memory usage:: 3013632000
time_provenance:: 2.384571075439453
curr_diff: 0 tensor(8.4551e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4551e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04143071174621582
overhead3:: 0.1049497127532959
overhead4:: 2.5038647651672363
overhead5:: 0
memory usage:: 3002765312
time_provenance:: 3.3481290340423584
curr_diff: 0 tensor(1.6188e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6188e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4552, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0977, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0240, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7938, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7657, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7239, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6739, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6541, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6450, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6317, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6266, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6122, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6171, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5857, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.254272937774658
training time full:: 3.254342555999756
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 137, 145, 148, 155, 158, 162, 163, 173,
        175, 176, 183, 184, 186, 187, 188, 200])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.624403238296509
overhead:: 0
overhead2:: 0.20020604133605957
overhead3:: 0
time_baseline:: 2.624446392059326
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007172107696533203
overhead3:: 0.026304006576538086
overhead4:: 0.5087766647338867
overhead5:: 0
memory usage:: 3021295616
time_provenance:: 0.9379177093505859
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0145, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0145, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010252952575683594
overhead3:: 0.03299355506896973
overhead4:: 0.6864051818847656
overhead5:: 0
memory usage:: 3015987200
time_provenance:: 1.106902837753296
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013112545013427734
overhead3:: 0.03940987586975098
overhead4:: 0.860114574432373
overhead5:: 0
memory usage:: 3020996608
time_provenance:: 1.373927116394043
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011490345001220703
overhead3:: 0.03497195243835449
overhead4:: 0.7609243392944336
overhead5:: 0
memory usage:: 3014660096
time_provenance:: 1.192162275314331
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013750076293945312
overhead3:: 0.04027581214904785
overhead4:: 0.9020969867706299
overhead5:: 0
memory usage:: 3026325504
time_provenance:: 1.3834123611450195
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017018556594848633
overhead3:: 0.04697251319885254
overhead4:: 1.0681214332580566
overhead5:: 0
memory usage:: 3059122176
time_provenance:: 1.575516700744629
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.022350788116455078
overhead3:: 0.06005287170410156
overhead4:: 1.525428056716919
overhead5:: 0
memory usage:: 3025334272
time_provenance:: 2.1812119483947754
curr_diff: 0 tensor(7.8350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.023789405822753906
overhead3:: 0.06573128700256348
overhead4:: 1.6359062194824219
overhead5:: 0
memory usage:: 3015327744
time_provenance:: 2.3090250492095947
curr_diff: 0 tensor(7.4653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4653e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02703094482421875
overhead3:: 0.06753063201904297
overhead4:: 1.7351555824279785
overhead5:: 0
memory usage:: 3016994816
time_provenance:: 2.4176838397979736
curr_diff: 0 tensor(7.2973e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2973e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04728817939758301
overhead3:: 0.09547758102416992
overhead4:: 2.5317697525024414
overhead5:: 0
memory usage:: 3003129856
time_provenance:: 3.377397060394287
curr_diff: 0 tensor(1.6266e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6266e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877300
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9401, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3150, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.2016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0289, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8833, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7747, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7312, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7108, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6947, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6728, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6675, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5975, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5783, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2681033611297607
training time full:: 3.268172025680542
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 108, 114, 115, 117, 120, 127, 129, 134, 146,
        152, 154, 157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192,
        194, 195, 197, 201, 204, 206, 210, 215])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6445348262786865
overhead:: 0
overhead2:: 0.20761752128601074
overhead3:: 0
time_baseline:: 2.6445772647857666
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007817506790161133
overhead3:: 0.02590155601501465
overhead4:: 0.5105843544006348
overhead5:: 0
memory usage:: 3013353472
time_provenance:: 0.9335086345672607
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011376380920410156
overhead3:: 0.03305411338806152
overhead4:: 0.6891384124755859
overhead5:: 0
memory usage:: 3015086080
time_provenance:: 1.1587855815887451
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013172149658203125
overhead3:: 0.03965163230895996
overhead4:: 0.8560137748718262
overhead5:: 0
memory usage:: 3031973888
time_provenance:: 1.3714804649353027
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012624979019165039
overhead3:: 0.03747272491455078
overhead4:: 0.7889401912689209
overhead5:: 0
memory usage:: 3041443840
time_provenance:: 1.2346556186676025
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014306306838989258
overhead3:: 0.040609121322631836
overhead4:: 0.8712060451507568
overhead5:: 0
memory usage:: 3039895552
time_provenance:: 1.3862659931182861
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017301559448242188
overhead3:: 0.06134653091430664
overhead4:: 1.0487480163574219
overhead5:: 0
memory usage:: 3018203136
time_provenance:: 1.5728399753570557
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.022800683975219727
overhead3:: 0.06971597671508789
overhead4:: 1.5266039371490479
overhead5:: 0
memory usage:: 3032281088
time_provenance:: 2.1922268867492676
curr_diff: 0 tensor(8.4236e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4236e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024408817291259766
overhead3:: 0.06607389450073242
overhead4:: 1.6764779090881348
overhead5:: 0
memory usage:: 3065044992
time_provenance:: 2.335908889770508
curr_diff: 0 tensor(8.0278e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0278e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02550506591796875
overhead3:: 0.06884002685546875
overhead4:: 1.7328412532806396
overhead5:: 0
memory usage:: 3013791744
time_provenance:: 2.4110186100006104
curr_diff: 0 tensor(7.9255e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9255e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04914712905883789
overhead3:: 0.09746670722961426
overhead4:: 2.550692558288574
overhead5:: 0
memory usage:: 3001171968
time_provenance:: 3.4019927978515625
curr_diff: 0 tensor(1.6147e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6147e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4620, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3150, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9244, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8483, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8176, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7418, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7306, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7152, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6670, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6500, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5883, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.247567653656006
training time full:: 3.2476320266723633
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  75,  77,  87,  93,  97, 101, 103, 104, 106, 107,
        109, 110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163,
        170, 176, 178, 192, 194, 195, 204, 205])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.612168788909912
overhead:: 0
overhead2:: 0.19751238822937012
overhead3:: 0
time_baseline:: 2.6122138500213623
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.0074045658111572266
overhead3:: 0.027100563049316406
overhead4:: 0.5506055355072021
overhead5:: 0
memory usage:: 3049263104
time_provenance:: 0.9236066341400146
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010437726974487305
overhead3:: 0.03204154968261719
overhead4:: 0.7033424377441406
overhead5:: 0
memory usage:: 3012632576
time_provenance:: 1.1229727268218994
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013282537460327148
overhead3:: 0.03925371170043945
overhead4:: 0.8782608509063721
overhead5:: 0
memory usage:: 3013853184
time_provenance:: 1.3574504852294922
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012327432632446289
overhead3:: 0.034481048583984375
overhead4:: 0.7557387351989746
overhead5:: 0
memory usage:: 3021443072
time_provenance:: 1.2042171955108643
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013597488403320312
overhead3:: 0.0404055118560791
overhead4:: 0.9238593578338623
overhead5:: 0
memory usage:: 3019337728
time_provenance:: 1.3877191543579102
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017502307891845703
overhead3:: 0.04652094841003418
overhead4:: 1.0477547645568848
overhead5:: 0
memory usage:: 3032584192
time_provenance:: 1.5653204917907715
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024174213409423828
overhead3:: 0.05871748924255371
overhead4:: 1.5549161434173584
overhead5:: 0
memory usage:: 3026014208
time_provenance:: 2.2003068923950195
curr_diff: 0 tensor(9.9390e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9390e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.023934125900268555
overhead3:: 0.0650029182434082
overhead4:: 1.6394212245941162
overhead5:: 0
memory usage:: 3049930752
time_provenance:: 2.2999391555786133
curr_diff: 0 tensor(9.7052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7052e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02664923667907715
overhead3:: 0.0673978328704834
overhead4:: 1.744913101196289
overhead5:: 0
memory usage:: 3021430784
time_provenance:: 2.423684597015381
curr_diff: 0 tensor(9.6374e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6374e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04224658012390137
overhead3:: 0.10182523727416992
overhead4:: 2.5254669189453125
overhead5:: 0
memory usage:: 3037720576
time_provenance:: 3.377518653869629
curr_diff: 0 tensor(1.6160e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6160e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2987, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.2981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0263, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9718, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8867, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8439, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7832, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7624, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7424, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7253, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7133, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6966, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6828, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6666, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6578, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6384, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6078, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5966, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5860, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5829, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5716, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2780702114105225
training time full:: 3.278139352798462
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 177, 180, 188, 190])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.6061785221099854
overhead:: 0
overhead2:: 0.20089244842529297
overhead3:: 0
time_baseline:: 2.6062259674072266
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007612466812133789
overhead3:: 0.025800466537475586
overhead4:: 0.5174946784973145
overhead5:: 0
memory usage:: 3021045760
time_provenance:: 0.9259052276611328
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010396003723144531
overhead3:: 0.033719778060913086
overhead4:: 0.6902432441711426
overhead5:: 0
memory usage:: 3065479168
time_provenance:: 1.1635429859161377
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014117956161499023
overhead3:: 0.040021657943725586
overhead4:: 0.8525588512420654
overhead5:: 0
memory usage:: 3014230016
time_provenance:: 1.36448073387146
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0145, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0145, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01236867904663086
overhead3:: 0.03691220283508301
overhead4:: 0.7585024833679199
overhead5:: 0
memory usage:: 3016130560
time_provenance:: 1.1890792846679688
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.015287637710571289
overhead3:: 0.039853811264038086
overhead4:: 0.9017062187194824
overhead5:: 0
memory usage:: 3023249408
time_provenance:: 1.3787028789520264
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.018280982971191406
overhead3:: 0.04811525344848633
overhead4:: 1.0620131492614746
overhead5:: 0
memory usage:: 3013599232
time_provenance:: 1.585174560546875
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021699905395507812
overhead3:: 0.060826778411865234
overhead4:: 1.5476090908050537
overhead5:: 0
memory usage:: 3020623872
time_provenance:: 2.2060952186584473
curr_diff: 0 tensor(8.6130e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6130e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02565789222717285
overhead3:: 0.0632174015045166
overhead4:: 1.6437110900878906
overhead5:: 0
memory usage:: 3049082880
time_provenance:: 2.304835557937622
curr_diff: 0 tensor(8.3365e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3365e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.025348424911499023
overhead3:: 0.06680464744567871
overhead4:: 1.7015013694763184
overhead5:: 0
memory usage:: 3012345856
time_provenance:: 2.385807514190674
curr_diff: 0 tensor(8.1405e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1405e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04485321044921875
overhead3:: 0.10438275337219238
overhead4:: 2.573038339614868
overhead5:: 0
memory usage:: 3008741376
time_provenance:: 3.4253618717193604
curr_diff: 0 tensor(1.6126e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6126e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0932, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9246, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7780, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6541, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5436, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4531, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3720, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9722, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9475, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8812, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8683, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8152, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8033, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7683, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7648, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7306, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7110, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7049, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6862, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6802, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6689, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6520, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6501, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6450, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6368, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6350, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6298, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6218, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6153, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6056, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5997, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.442784547805786
training time full:: 6.442851781845093
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 75, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.8983094692230225
overhead:: 0
overhead2:: 0.39978861808776855
overhead3:: 0
time_baseline:: 4.8983540534973145
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.0073359012603759766
overhead3:: 0.03206205368041992
overhead4:: 0.9219508171081543
overhead5:: 0
memory usage:: 3011915776
time_provenance:: 1.4464449882507324
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.010277032852172852
overhead3:: 0.03893160820007324
overhead4:: 1.2882399559020996
overhead5:: 0
memory usage:: 3022184448
time_provenance:: 1.9192254543304443
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013830184936523438
overhead3:: 0.04658389091491699
overhead4:: 1.6920900344848633
overhead5:: 0
memory usage:: 3016388608
time_provenance:: 2.4032585620880127
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.011346578598022461
overhead3:: 0.04093670845031738
overhead4:: 1.4063527584075928
overhead5:: 0
memory usage:: 3049537536
time_provenance:: 1.982530117034912
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.014475107192993164
overhead3:: 0.04773688316345215
overhead4:: 1.713169813156128
overhead5:: 0
memory usage:: 3040083968
time_provenance:: 2.4331088066101074
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.017148494720458984
overhead3:: 0.05184483528137207
overhead4:: 2.0764265060424805
overhead5:: 0
memory usage:: 3025985536
time_provenance:: 2.8378210067749023
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02244091033935547
overhead3:: 0.06650972366333008
overhead4:: 2.7948482036590576
overhead5:: 0
memory usage:: 3060228096
time_provenance:: 3.7181429862976074
curr_diff: 0 tensor(5.7748e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7748e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02395153045654297
overhead3:: 0.07050299644470215
overhead4:: 3.0004682540893555
overhead5:: 0
memory usage:: 3019943936
time_provenance:: 3.9639601707458496
curr_diff: 0 tensor(5.6927e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6927e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.027439355850219727
overhead3:: 0.07075095176696777
overhead4:: 3.2253270149230957
overhead5:: 0
memory usage:: 3025707008
time_provenance:: 4.23177170753479
curr_diff: 0 tensor(5.6341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.041290283203125
overhead3:: 0.10944271087646484
overhead4:: 5.132799386978149
overhead5:: 0
memory usage:: 3028213760
time_provenance:: 6.544831037521362
curr_diff: 0 tensor(1.5032e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5032e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3012, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0945, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7797, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6555, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4531, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3032, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1400, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0995, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0610, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9696, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9404, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8833, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8620, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8487, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7795, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7597, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7199, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7104, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6927, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6906, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6829, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6628, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6583, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6461, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6386, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6239, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6203, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6130, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6104, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.38514518737793
training time full:: 6.385213613510132
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         80,  82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112,
        113, 114, 117, 118, 119, 122, 124, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.90929651260376
overhead:: 0
overhead2:: 0.40149760246276855
overhead3:: 0
time_baseline:: 4.909340143203735
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.007506370544433594
overhead3:: 0.032105445861816406
overhead4:: 0.9470810890197754
overhead5:: 0
memory usage:: 3013242880
time_provenance:: 1.4194674491882324
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.01036381721496582
overhead3:: 0.039916038513183594
overhead4:: 1.320295810699463
overhead5:: 0
memory usage:: 3019128832
time_provenance:: 1.931265115737915
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013663291931152344
overhead3:: 0.04640960693359375
overhead4:: 1.6734766960144043
overhead5:: 0
memory usage:: 3021369344
time_provenance:: 2.375889778137207
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.010715484619140625
overhead3:: 0.04025387763977051
overhead4:: 1.4050939083099365
overhead5:: 0
memory usage:: 3040124928
time_provenance:: 1.987502098083496
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013679742813110352
overhead3:: 0.0471804141998291
overhead4:: 1.7482028007507324
overhead5:: 0
memory usage:: 3048988672
time_provenance:: 2.40118408203125
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.01630544662475586
overhead3:: 0.05313253402709961
overhead4:: 2.030179023742676
overhead5:: 0
memory usage:: 3038740480
time_provenance:: 2.823195695877075
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02210378646850586
overhead3:: 0.06609773635864258
overhead4:: 2.7796103954315186
overhead5:: 0
memory usage:: 3049267200
time_provenance:: 3.6996870040893555
curr_diff: 0 tensor(6.0388e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0388e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.024305343627929688
overhead3:: 0.07064104080200195
overhead4:: 2.993180513381958
overhead5:: 0
memory usage:: 3012276224
time_provenance:: 3.9885878562927246
curr_diff: 0 tensor(5.9639e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9639e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02636575698852539
overhead3:: 0.0758354663848877
overhead4:: 3.2112910747528076
overhead5:: 0
memory usage:: 3049566208
time_provenance:: 4.22364354133606
curr_diff: 0 tensor(5.9204e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9204e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.04059600830078125
overhead3:: 0.10980367660522461
overhead4:: 5.114243268966675
overhead5:: 0
memory usage:: 3008491520
time_provenance:: 6.525863409042358
curr_diff: 0 tensor(1.5068e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5068e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0822, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7703, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6463, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5398, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3654, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0610, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9726, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9227, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7902, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7808, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7661, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7594, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7383, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7273, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6762, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6577, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6408, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6381, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6308, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6241, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6244, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6161, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5869, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.457841157913208
training time full:: 6.4579057693481445
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  35,
         39,  40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,
         70,  76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98,
        100, 105, 108, 112, 113, 114, 115, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.945239782333374
overhead:: 0
overhead2:: 0.4053158760070801
overhead3:: 0
time_baseline:: 4.945285797119141
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.007616758346557617
overhead3:: 0.03260016441345215
overhead4:: 0.9322850704193115
overhead5:: 0
memory usage:: 3035058176
time_provenance:: 1.4598443508148193
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.011063575744628906
overhead3:: 0.037314414978027344
overhead4:: 1.3368148803710938
overhead5:: 0
memory usage:: 3013328896
time_provenance:: 1.8953971862792969
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013323307037353516
overhead3:: 0.04605507850646973
overhead4:: 1.6840977668762207
overhead5:: 0
memory usage:: 3038584832
time_provenance:: 2.3951416015625
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.011295080184936523
overhead3:: 0.0411069393157959
overhead4:: 1.4369428157806396
overhead5:: 0
memory usage:: 3014684672
time_provenance:: 2.016961097717285
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.01426076889038086
overhead3:: 0.04767036437988281
overhead4:: 1.7274541854858398
overhead5:: 0
memory usage:: 3020718080
time_provenance:: 2.381584882736206
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.016944408416748047
overhead3:: 0.05111980438232422
overhead4:: 2.0324041843414307
overhead5:: 0
memory usage:: 3024662528
time_provenance:: 2.8345046043395996
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.023026704788208008
overhead3:: 0.0669550895690918
overhead4:: 2.807924509048462
overhead5:: 0
memory usage:: 3019149312
time_provenance:: 3.727717638015747
curr_diff: 0 tensor(4.4350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02356576919555664
overhead3:: 0.07064509391784668
overhead4:: 2.9995827674865723
overhead5:: 0
memory usage:: 3039412224
time_provenance:: 3.962341070175171
curr_diff: 0 tensor(4.2607e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2607e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.025269031524658203
overhead3:: 0.07402896881103516
overhead4:: 3.2058255672454834
overhead5:: 0
memory usage:: 3048796160
time_provenance:: 4.218740701675415
curr_diff: 0 tensor(4.1403e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1403e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.043634891510009766
overhead3:: 0.11413073539733887
overhead4:: 5.135988473892212
overhead5:: 0
memory usage:: 3014975488
time_provenance:: 6.544936895370483
curr_diff: 0 tensor(1.5110e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5110e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3133, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1032, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9320, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6558, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5496, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4555, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2418, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1907, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1411, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1006, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0305, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9706, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9003, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8767, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8612, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8354, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8115, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7857, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7806, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7654, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7438, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7384, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7162, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6793, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6673, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6611, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6498, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6393, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6344, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6342, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6203, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6109, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6121, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6079, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6055, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5938, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.443037986755371
training time full:: 6.443104267120361
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  48,  50,  55,
         57,  58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  75,  77,  84,
         85,  87,  90,  93,  94,  97, 100, 101])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.881952524185181
overhead:: 0
overhead2:: 0.4008059501647949
overhead3:: 0
time_baseline:: 4.881998300552368
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.007223367691040039
overhead3:: 0.031122446060180664
overhead4:: 0.905113697052002
overhead5:: 0
memory usage:: 3023867904
time_provenance:: 1.4314744472503662
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.010285615921020508
overhead3:: 0.039101362228393555
overhead4:: 1.2989153861999512
overhead5:: 0
memory usage:: 3020324864
time_provenance:: 1.9143543243408203
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013279199600219727
overhead3:: 0.04628777503967285
overhead4:: 1.6759018898010254
overhead5:: 0
memory usage:: 3025305600
time_provenance:: 2.3839271068573
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.011757373809814453
overhead3:: 0.03888535499572754
overhead4:: 1.391585350036621
overhead5:: 0
memory usage:: 3039997952
time_provenance:: 1.9985332489013672
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.014538764953613281
overhead3:: 0.04607200622558594
overhead4:: 1.7099480628967285
overhead5:: 0
memory usage:: 3026784256
time_provenance:: 2.4017701148986816
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.016234159469604492
overhead3:: 0.053038597106933594
overhead4:: 2.0769007205963135
overhead5:: 0
memory usage:: 3049775104
time_provenance:: 2.8320364952087402
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.022133350372314453
overhead3:: 0.06695961952209473
overhead4:: 2.795750379562378
overhead5:: 0
memory usage:: 3049021440
time_provenance:: 3.7172677516937256
curr_diff: 0 tensor(5.7617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02394866943359375
overhead3:: 0.06815600395202637
overhead4:: 3.0083274841308594
overhead5:: 0
memory usage:: 3013697536
time_provenance:: 3.9697425365448
curr_diff: 0 tensor(5.6590e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6590e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.02715897560119629
overhead3:: 0.07066106796264648
overhead4:: 3.228466510772705
overhead5:: 0
memory usage:: 3048275968
time_provenance:: 4.238492250442505
curr_diff: 0 tensor(5.6016e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6016e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.041814565658569336
overhead3:: 0.11195635795593262
overhead4:: 5.0964415073394775
overhead5:: 0
memory usage:: 3022745600
time_provenance:: 6.503164768218994
curr_diff: 0 tensor(1.5140e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5140e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2995, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7754, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5421, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3686, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3035, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2404, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1928, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1396, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9953, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9692, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9455, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9212, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8649, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8129, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7982, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7780, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7625, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7572, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7436, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7377, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7308, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7167, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7096, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6752, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6628, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6538, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6498, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6343, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6353, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6168, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6170, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6118, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6056, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5964, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5993, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5856, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.464199542999268
training time full:: 6.4642674922943115
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 3,  5,  6,  8, 10, 11, 14, 15, 17, 19, 21, 28, 29, 31, 33, 35, 36, 38,
        39, 40, 41, 43, 45, 46, 47, 48, 50, 52, 53, 54, 55, 56, 57, 58, 61, 64,
        66, 72, 74, 75, 76, 78, 79, 80, 81, 83, 85, 86, 90, 92])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.977709054946899
overhead:: 0
overhead2:: 0.40370988845825195
overhead3:: 0
time_baseline:: 4.977802991867065
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.007351398468017578
overhead3:: 0.032097578048706055
overhead4:: 0.9051275253295898
overhead5:: 0
memory usage:: 3025420288
time_provenance:: 1.4371230602264404
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.010464906692504883
overhead3:: 0.038535356521606445
overhead4:: 1.2916955947875977
overhead5:: 0
memory usage:: 3018301440
time_provenance:: 1.9089112281799316
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013799667358398438
overhead3:: 0.04571223258972168
overhead4:: 1.6916327476501465
overhead5:: 0
memory usage:: 3031826432
time_provenance:: 2.398911476135254
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0139, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0139, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.011719465255737305
overhead3:: 0.03907132148742676
overhead4:: 1.376676082611084
overhead5:: 0
memory usage:: 3032621056
time_provenance:: 2.027895450592041
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.013686418533325195
overhead3:: 0.046332359313964844
overhead4:: 1.7061867713928223
overhead5:: 0
memory usage:: 3049451520
time_provenance:: 2.4117166996002197
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.01648879051208496
overhead3:: 0.05283951759338379
overhead4:: 2.0714035034179688
overhead5:: 0
memory usage:: 3024334848
time_provenance:: 2.8334238529205322
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.0232546329498291
overhead3:: 0.06490278244018555
overhead4:: 2.820638418197632
overhead5:: 0
memory usage:: 3020328960
time_provenance:: 3.737046718597412
curr_diff: 0 tensor(4.9706e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9706e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.024677038192749023
overhead3:: 0.06857562065124512
overhead4:: 2.9737212657928467
overhead5:: 0
memory usage:: 3042807808
time_provenance:: 3.9718849658966064
curr_diff: 0 tensor(4.9006e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9006e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.026341915130615234
overhead3:: 0.07551431655883789
overhead4:: 3.217475175857544
overhead5:: 0
memory usage:: 3020120064
time_provenance:: 4.239711284637451
curr_diff: 0 tensor(4.8221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
overhead:: 0
overhead2:: 0.04053688049316406
overhead3:: 0.10920381546020508
overhead4:: 5.111914396286011
overhead5:: 0
memory usage:: 3015409664
time_provenance:: 6.5154595375061035
curr_diff: 0 tensor(1.4960e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4960e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0133, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0133, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
deletion rate:: 0.05
python3 generate_rand_ids 0.05  MNIST5 0
tensor([24579, 49156, 16387,  ..., 32758, 16381, 40958])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3497, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9501, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6775, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4690, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1919, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0342, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8522, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8254, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7175, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6866, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6625, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6593, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6241, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6186, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5827, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2887651920318604
training time full:: 3.2888340950012207
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  76,  77,  78,  79,  80,  89,  91,  92,  93,
         95,  97, 102, 105, 106, 107, 109, 118, 121, 123, 134, 135, 136, 143,
        150, 152, 173, 174, 183, 188, 190, 191])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5330960750579834
overhead:: 0
overhead2:: 0.21222901344299316
overhead3:: 0
time_baseline:: 2.533179998397827
curr_diff: 0 tensor(0.0320, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0320, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007719516754150391
overhead3:: 0.032132863998413086
overhead4:: 0.43901538848876953
overhead5:: 0
memory usage:: 3052208128
time_provenance:: 0.9449594020843506
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0340, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0340, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010876655578613281
overhead3:: 0.039095401763916016
overhead4:: 0.6076686382293701
overhead5:: 0
memory usage:: 3031269376
time_provenance:: 1.1646382808685303
curr_diff: 0 tensor(0.0033, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0033, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014310598373413086
overhead3:: 0.046623945236206055
overhead4:: 0.7942264080047607
overhead5:: 0
memory usage:: 3060318208
time_provenance:: 1.38932204246521
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0339, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0339, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011378288269042969
overhead3:: 0.04073929786682129
overhead4:: 0.6636412143707275
overhead5:: 0
memory usage:: 3056009216
time_provenance:: 1.2480120658874512
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015352249145507812
overhead3:: 0.046161651611328125
overhead4:: 0.826711893081665
overhead5:: 0
memory usage:: 3019935744
time_provenance:: 1.4380831718444824
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.019438982009887695
overhead3:: 0.05475783348083496
overhead4:: 0.9759690761566162
overhead5:: 0
memory usage:: 3055783936
time_provenance:: 1.6287665367126465
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025963306427001953
overhead3:: 0.06643223762512207
overhead4:: 1.4329094886779785
overhead5:: 0
memory usage:: 3039428608
time_provenance:: 2.2289481163024902
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0320, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0320, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025395631790161133
overhead3:: 0.07168292999267578
overhead4:: 1.4969432353973389
overhead5:: 0
memory usage:: 3027628032
time_provenance:: 2.295370578765869
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0320, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0320, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.027245283126831055
overhead3:: 0.07405495643615723
overhead4:: 1.6252472400665283
overhead5:: 0
memory usage:: 3026120704
time_provenance:: 2.444197177886963
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0320, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0320, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04214119911193848
overhead3:: 0.11352157592773438
overhead4:: 2.3297715187072754
overhead5:: 0
memory usage:: 3012841472
time_provenance:: 3.279754638671875
curr_diff: 0 tensor(1.6248e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6248e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0320, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0320, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6618, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3071, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0283, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9288, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8846, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8473, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8161, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6967, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6559, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6147, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6195, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6065, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5876, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5723, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.26143479347229
training time full:: 3.261504888534546
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 137, 145, 148, 155, 158, 162, 163, 173,
        175, 183, 184, 186, 187, 188, 203, 204])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5297152996063232
overhead:: 0
overhead2:: 0.2127058506011963
overhead3:: 0
time_baseline:: 2.5298027992248535
curr_diff: 0 tensor(0.0318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.008158683776855469
overhead3:: 0.03225970268249512
overhead4:: 0.44664931297302246
overhead5:: 0
memory usage:: 3028680704
time_provenance:: 0.961738109588623
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0336, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0336, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010928153991699219
overhead3:: 0.03767991065979004
overhead4:: 0.5939202308654785
overhead5:: 0
memory usage:: 3018731520
time_provenance:: 1.169731616973877
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0338, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0338, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013920068740844727
overhead3:: 0.04630422592163086
overhead4:: 0.7648451328277588
overhead5:: 0
memory usage:: 3056668672
time_provenance:: 1.3607258796691895
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0338, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0338, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011125802993774414
overhead3:: 0.040863037109375
overhead4:: 0.6494548320770264
overhead5:: 0
memory usage:: 3021426688
time_provenance:: 1.2751994132995605
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015015840530395508
overhead3:: 0.04474329948425293
overhead4:: 0.7815611362457275
overhead5:: 0
memory usage:: 3045265408
time_provenance:: 1.3970229625701904
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0322, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0322, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.017459869384765625
overhead3:: 0.05247354507446289
overhead4:: 0.9250643253326416
overhead5:: 0
memory usage:: 3019427840
time_provenance:: 1.5867598056793213
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02256011962890625
overhead3:: 0.06640386581420898
overhead4:: 1.4690828323364258
overhead5:: 0
memory usage:: 3032563712
time_provenance:: 2.2578747272491455
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025331735610961914
overhead3:: 0.07216191291809082
overhead4:: 1.4993486404418945
overhead5:: 0
memory usage:: 3018334208
time_provenance:: 2.270726203918457
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.028863906860351562
overhead3:: 0.07669591903686523
overhead4:: 1.5993993282318115
overhead5:: 0
memory usage:: 3046920192
time_provenance:: 2.400890588760376
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.05453181266784668
overhead3:: 0.12652111053466797
overhead4:: 2.4942400455474854
overhead5:: 0
memory usage:: 3015569408
time_provenance:: 3.4383108615875244
curr_diff: 0 tensor(1.6181e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6181e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2844, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4500, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0981, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0217, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9234, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8784, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8466, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8179, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7904, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7714, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7088, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6926, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6815, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6709, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6662, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6262, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6058, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5930, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5731, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2610130310058594
training time full:: 3.261077404022217
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 114, 115, 117, 120, 127, 129, 134, 152, 154,
        157, 158, 160, 166, 170, 172, 173, 180, 181, 182, 187, 192, 194, 195,
        197, 201, 204, 206, 210, 215, 224, 228])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.519092082977295
overhead:: 0
overhead2:: 0.21437740325927734
overhead3:: 0
time_baseline:: 2.5191867351531982
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007719993591308594
overhead3:: 0.03259086608886719
overhead4:: 0.4424753189086914
overhead5:: 0
memory usage:: 3044282368
time_provenance:: 0.948683500289917
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0340, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0340, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010701179504394531
overhead3:: 0.03833627700805664
overhead4:: 0.6026723384857178
overhead5:: 0
memory usage:: 3025383424
time_provenance:: 1.1506969928741455
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0340, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0340, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013261556625366211
overhead3:: 0.04694652557373047
overhead4:: 0.7614502906799316
overhead5:: 0
memory usage:: 3064139776
time_provenance:: 1.3551709651947021
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0340, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0340, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01209259033203125
overhead3:: 0.040549516677856445
overhead4:: 0.6499419212341309
overhead5:: 0
memory usage:: 3052072960
time_provenance:: 1.2778394222259521
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0322, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0322, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015413522720336914
overhead3:: 0.04658055305480957
overhead4:: 0.8049485683441162
overhead5:: 0
memory usage:: 3022913536
time_provenance:: 1.4153850078582764
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01659107208251953
overhead3:: 0.052173614501953125
overhead4:: 0.9408218860626221
overhead5:: 0
memory usage:: 3021832192
time_provenance:: 1.6045663356781006
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0322, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0322, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02330636978149414
overhead3:: 0.06776118278503418
overhead4:: 1.4588596820831299
overhead5:: 0
memory usage:: 3014778880
time_provenance:: 2.228015422821045
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02727341651916504
overhead3:: 0.06998014450073242
overhead4:: 1.4791789054870605
overhead5:: 0
memory usage:: 3017105408
time_provenance:: 2.273266077041626
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02584528923034668
overhead3:: 0.07467389106750488
overhead4:: 1.597992181777954
overhead5:: 0
memory usage:: 3034480640
time_provenance:: 2.401763916015625
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04643893241882324
overhead3:: 0.10980606079101562
overhead4:: 2.373919725418091
overhead5:: 0
memory usage:: 3015630848
time_provenance:: 3.318143367767334
curr_diff: 0 tensor(1.6276e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6276e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2844, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9089, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6430, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.2996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1820, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9189, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7395, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7131, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6650, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6564, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6486, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6275, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6137, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5881, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5873, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5832, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2621703147888184
training time full:: 3.262235403060913
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  77,  87,  93,  97, 101, 103, 104, 106, 107, 109,
        110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 163, 170,
        178, 192, 194, 195, 204, 205, 210, 212])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.533583402633667
overhead:: 0
overhead2:: 0.21441435813903809
overhead3:: 0
time_baseline:: 2.533693313598633
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.008069753646850586
overhead3:: 0.03190016746520996
overhead4:: 0.44896435737609863
overhead5:: 0
memory usage:: 3051847680
time_provenance:: 0.9567656517028809
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010731220245361328
overhead3:: 0.03870654106140137
overhead4:: 0.5887205600738525
overhead5:: 0
memory usage:: 3017859072
time_provenance:: 1.1334364414215088
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013527154922485352
overhead3:: 0.04496622085571289
overhead4:: 0.7611377239227295
overhead5:: 0
memory usage:: 3017068544
time_provenance:: 1.3426105976104736
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0340, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0340, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011411190032958984
overhead3:: 0.039977073669433594
overhead4:: 0.6668024063110352
overhead5:: 0
memory usage:: 3020271616
time_provenance:: 1.2316977977752686
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014970779418945312
overhead3:: 0.045977115631103516
overhead4:: 0.796511173248291
overhead5:: 0
memory usage:: 3042357248
time_provenance:: 1.4103896617889404
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.016910552978515625
overhead3:: 0.05124330520629883
overhead4:: 0.9347500801086426
overhead5:: 0
memory usage:: 3036454912
time_provenance:: 1.5737659931182861
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0235292911529541
overhead3:: 0.0640726089477539
overhead4:: 1.4575612545013428
overhead5:: 0
memory usage:: 3059335168
time_provenance:: 2.249582052230835
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.023970842361450195
overhead3:: 0.07068037986755371
overhead4:: 1.535142183303833
overhead5:: 0
memory usage:: 3051704320
time_provenance:: 2.32979416847229
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02782273292541504
overhead3:: 0.07314562797546387
overhead4:: 1.6265778541564941
overhead5:: 0
memory usage:: 3034718208
time_provenance:: 2.4421679973602295
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04255223274230957
overhead3:: 0.11209726333618164
overhead4:: 2.31535005569458
overhead5:: 0
memory usage:: 3009654784
time_provenance:: 3.2659952640533447
curr_diff: 0 tensor(1.6174e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6174e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877000
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4588, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3108, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1907, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0352, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9796, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8935, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7881, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7300, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7170, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.7000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6866, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6610, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6511, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6411, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6305, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6110, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5993, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.295336961746216
training time full:: 3.2954025268554688
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  80,  81,  86,  90,  92, 102, 105, 106, 109, 116,
        117, 120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162,
        163, 165, 166, 174, 180, 188, 190, 194])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.564277172088623
overhead:: 0
overhead2:: 0.216796875
overhead3:: 0
time_baseline:: 2.564359426498413
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.008063077926635742
overhead3:: 0.03151869773864746
overhead4:: 0.4601616859436035
overhead5:: 0
memory usage:: 3019087872
time_provenance:: 1.023592472076416
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010383367538452148
overhead3:: 0.04009652137756348
overhead4:: 0.6170008182525635
overhead5:: 0
memory usage:: 3027546112
time_provenance:: 1.1677415370941162
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0159604549407959
overhead3:: 0.0487978458404541
overhead4:: 0.8173458576202393
overhead5:: 0
memory usage:: 3043237888
time_provenance:: 1.445727825164795
curr_diff: 0 tensor(0.0033, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0033, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0341, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011688232421875
overhead3:: 0.041999101638793945
overhead4:: 0.6972057819366455
overhead5:: 0
memory usage:: 3031035904
time_provenance:: 1.2846195697784424
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0322, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0322, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014207839965820312
overhead3:: 0.0466310977935791
overhead4:: 0.8031930923461914
overhead5:: 0
memory usage:: 3056365568
time_provenance:: 1.4116554260253906
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.017184019088745117
overhead3:: 0.05252838134765625
overhead4:: 0.9393560886383057
overhead5:: 0
memory usage:: 3055992832
time_provenance:: 1.6064677238464355
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0322, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0322, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.022793292999267578
overhead3:: 0.06903600692749023
overhead4:: 1.4610857963562012
overhead5:: 0
memory usage:: 3031494656
time_provenance:: 2.2414395809173584
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02473306655883789
overhead3:: 0.07053613662719727
overhead4:: 1.5179789066314697
overhead5:: 0
memory usage:: 3030654976
time_provenance:: 2.328324317932129
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.027144908905029297
overhead3:: 0.07565855979919434
overhead4:: 1.5972537994384766
overhead5:: 0
memory usage:: 3043725312
time_provenance:: 2.409975290298462
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04651379585266113
overhead3:: 0.11279082298278809
overhead4:: 2.3742728233337402
overhead5:: 0
memory usage:: 3013398528
time_provenance:: 3.3229565620422363
curr_diff: 0 tensor(1.6208e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6208e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3126, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7795, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6539, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3686, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2391, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1392, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0970, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0242, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9431, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.8978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8646, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8106, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7997, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7650, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7615, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7446, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7390, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7178, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6936, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6924, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6779, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6661, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6617, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6554, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6428, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6347, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6324, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6273, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6133, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6124, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6064, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5976, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5990, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.3631579875946045
training time full:: 6.363222599029541
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([ 1,  2,  3,  4,  5,  7,  8, 15, 17, 18, 19, 20, 22, 24, 25, 27, 29, 31,
        32, 33, 34, 36, 38, 40, 44, 45, 47, 57, 58, 62, 64, 65, 66, 68, 70, 71,
        73, 74, 76, 77, 78, 79, 80, 81, 89, 91, 92, 93, 95, 97])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.741546154022217
overhead:: 0
overhead2:: 0.4280362129211426
overhead3:: 0
time_baseline:: 4.741623401641846
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007779359817504883
overhead3:: 0.04256081581115723
overhead4:: 0.8190538883209229
overhead5:: 0
memory usage:: 3031232512
time_provenance:: 1.5240981578826904
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011230945587158203
overhead3:: 0.04981875419616699
overhead4:: 1.1424274444580078
overhead5:: 0
memory usage:: 3030614016
time_provenance:: 1.9495351314544678
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014048337936401367
overhead3:: 0.057125091552734375
overhead4:: 1.48628830909729
overhead5:: 0
memory usage:: 3024859136
time_provenance:: 2.3811728954315186
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012698650360107422
overhead3:: 0.04873299598693848
overhead4:: 1.240144968032837
overhead5:: 0
memory usage:: 3019149312
time_provenance:: 2.007756471633911
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0303, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0303, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015300750732421875
overhead3:: 0.05469989776611328
overhead4:: 1.5388462543487549
overhead5:: 0
memory usage:: 3047960576
time_provenance:: 2.3856990337371826
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017444610595703125
overhead3:: 0.06295609474182129
overhead4:: 1.7947440147399902
overhead5:: 0
memory usage:: 3058958336
time_provenance:: 2.774050235748291
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0303, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0303, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.023462533950805664
overhead3:: 0.07659626007080078
overhead4:: 2.533256769180298
overhead5:: 0
memory usage:: 3038101504
time_provenance:: 3.6719491481781006
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0303, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0303, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02629256248474121
overhead3:: 0.08103823661804199
overhead4:: 2.7405829429626465
overhead5:: 0
memory usage:: 3047378944
time_provenance:: 3.908245086669922
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0303, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0303, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.028128385543823242
overhead3:: 0.08238840103149414
overhead4:: 2.899329900741577
overhead5:: 0
memory usage:: 3031314432
time_provenance:: 4.104797601699829
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0303, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0303, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.042403221130371094
overhead3:: 0.11937403678894043
overhead4:: 4.452049493789673
overhead5:: 0
memory usage:: 3044552704
time_provenance:: 6.019536256790161
curr_diff: 0 tensor(1.4976e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4976e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3423, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1352, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5740, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4770, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3946, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3225, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1544, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0396, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9779, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9489, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9078, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8674, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8543, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7830, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7510, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7358, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7299, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7132, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7049, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6946, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6919, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6788, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6473, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6342, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6398, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6213, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6134, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6114, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5945, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.366497278213501
training time full:: 6.3665666580200195
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         80,  82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112,
        113, 114, 117, 118, 119, 122, 124, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.74255108833313
overhead:: 0
overhead2:: 0.42961716651916504
overhead3:: 0
time_baseline:: 4.742633104324341
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008121490478515625
overhead3:: 0.04261970520019531
overhead4:: 0.8281066417694092
overhead5:: 0
memory usage:: 3023798272
time_provenance:: 1.5636262893676758
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0326, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0326, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011126041412353516
overhead3:: 0.04906797409057617
overhead4:: 1.1306593418121338
overhead5:: 0
memory usage:: 3024924672
time_provenance:: 1.9130527973175049
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0325, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0325, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014049291610717773
overhead3:: 0.056354522705078125
overhead4:: 1.4800214767456055
overhead5:: 0
memory usage:: 3026628608
time_provenance:: 2.366262674331665
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012483835220336914
overhead3:: 0.052474260330200195
overhead4:: 1.264976978302002
overhead5:: 0
memory usage:: 3053891584
time_provenance:: 2.045395851135254
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014414072036743164
overhead3:: 0.05542278289794922
overhead4:: 1.534902572631836
overhead5:: 0
memory usage:: 3031633920
time_provenance:: 2.4411966800689697
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.018248319625854492
overhead3:: 0.062227725982666016
overhead4:: 1.7768304347991943
overhead5:: 0
memory usage:: 3018133504
time_provenance:: 2.7528159618377686
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.023678302764892578
overhead3:: 0.07791972160339355
overhead4:: 2.515075445175171
overhead5:: 0
memory usage:: 3021336576
time_provenance:: 3.6336169242858887
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026073455810546875
overhead3:: 0.08120536804199219
overhead4:: 2.675293207168579
overhead5:: 0
memory usage:: 3026272256
time_provenance:: 3.828752040863037
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.027399778366088867
overhead3:: 0.08320856094360352
overhead4:: 2.8409194946289062
overhead5:: 0
memory usage:: 3044790272
time_provenance:: 4.038133859634399
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0304, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0304, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04213309288024902
overhead3:: 0.11977243423461914
overhead4:: 4.4407432079315186
overhead5:: 0
memory usage:: 3029635072
time_provenance:: 6.005799770355225
curr_diff: 0 tensor(1.4977e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4977e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3480, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1376, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9625, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8119, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6828, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5708, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4767, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2608, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1542, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1132, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0719, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0415, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9802, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9504, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9296, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9081, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8692, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8538, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8344, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8117, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7841, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7696, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7628, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7414, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7166, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7065, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6927, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6843, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6690, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6555, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6594, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6488, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6426, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6399, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6326, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6259, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6178, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5933, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5926, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5884, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.406251907348633
training time full:: 6.406320571899414
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  39,
         40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,  70,
         76,  80,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98, 100,
        105, 112, 113, 114, 115, 116, 117, 119])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.715476989746094
overhead:: 0
overhead2:: 0.4206051826477051
overhead3:: 0
time_baseline:: 4.715560436248779
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.00799560546875
overhead3:: 0.04218411445617676
overhead4:: 0.7981367111206055
overhead5:: 0
memory usage:: 3024809984
time_provenance:: 1.5052781105041504
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0328, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0328, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01112508773803711
overhead3:: 0.04914450645446777
overhead4:: 1.1416571140289307
overhead5:: 0
memory usage:: 3036852224
time_provenance:: 1.9042174816131592
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0327, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0327, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014434576034545898
overhead3:: 0.05629992485046387
overhead4:: 1.4676589965820312
overhead5:: 0
memory usage:: 3032551424
time_provenance:: 2.3402953147888184
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011783123016357422
overhead3:: 0.04981040954589844
overhead4:: 1.2623682022094727
overhead5:: 0
memory usage:: 3020365824
time_provenance:: 2.0395171642303467
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0307, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0307, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015273809432983398
overhead3:: 0.05625009536743164
overhead4:: 1.5475575923919678
overhead5:: 0
memory usage:: 3018129408
time_provenance:: 2.3918650150299072
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0307, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0307, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017164230346679688
overhead3:: 0.06424117088317871
overhead4:: 1.8544418811798096
overhead5:: 0
memory usage:: 3016912896
time_provenance:: 2.767667293548584
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02323627471923828
overhead3:: 0.07434296607971191
overhead4:: 2.4981765747070312
overhead5:: 0
memory usage:: 3027050496
time_provenance:: 3.613835334777832
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026078224182128906
overhead3:: 0.07719588279724121
overhead4:: 2.670077323913574
overhead5:: 0
memory usage:: 3045273600
time_provenance:: 3.829866886138916
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02751755714416504
overhead3:: 0.084808349609375
overhead4:: 2.8992772102355957
overhead5:: 0
memory usage:: 3039739904
time_provenance:: 4.104889869689941
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04210662841796875
overhead3:: 0.11990499496459961
overhead4:: 4.409247398376465
overhead5:: 0
memory usage:: 3019808768
time_provenance:: 5.976426839828491
curr_diff: 0 tensor(1.5058e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5058e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3400, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1209, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5569, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4608, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3792, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3119, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0336, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9735, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9031, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8793, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8375, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7876, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7679, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7557, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7456, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7284, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7182, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7105, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7080, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6780, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6690, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6676, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6525, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6411, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6357, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6266, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6095, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5882, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.4068708419799805
training time full:: 6.406942367553711
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  50,  55,  57,
         58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  77,  84,  85,  87,
         90,  93,  94,  97, 100, 101, 103, 104])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.789914608001709
overhead:: 0
overhead2:: 0.42984819412231445
overhead3:: 0
time_baseline:: 4.78999924659729
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007814645767211914
overhead3:: 0.04250693321228027
overhead4:: 0.7817380428314209
overhead5:: 0
memory usage:: 3053117440
time_provenance:: 1.4907610416412354
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0327, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0327, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.010852336883544922
overhead3:: 0.04941368103027344
overhead4:: 1.163332223892212
overhead5:: 0
memory usage:: 3049488384
time_provenance:: 1.9605388641357422
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0325, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0325, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015160083770751953
overhead3:: 0.05705404281616211
overhead4:: 1.4885921478271484
overhead5:: 0
memory usage:: 3020578816
time_provenance:: 2.3758184909820557
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0323, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0323, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01394200325012207
overhead3:: 0.05205941200256348
overhead4:: 1.250241756439209
overhead5:: 0
memory usage:: 3053735936
time_provenance:: 2.0233640670776367
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014968395233154297
overhead3:: 0.05731821060180664
overhead4:: 1.5181899070739746
overhead5:: 0
memory usage:: 3054551040
time_provenance:: 2.4208195209503174
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.018845081329345703
overhead3:: 0.0658717155456543
overhead4:: 1.8786678314208984
overhead5:: 0
memory usage:: 3023884288
time_provenance:: 2.80564022064209
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025287628173828125
overhead3:: 0.0749204158782959
overhead4:: 2.5696449279785156
overhead5:: 0
memory usage:: 3026399232
time_provenance:: 3.7057154178619385
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02498793601989746
overhead3:: 0.08159446716308594
overhead4:: 2.7340645790100098
overhead5:: 0
memory usage:: 3019341824
time_provenance:: 3.88871693611145
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02649235725402832
overhead3:: 0.08347463607788086
overhead4:: 2.843191623687744
overhead5:: 0
memory usage:: 3053805568
time_provenance:: 4.047819137573242
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.0434417724609375
overhead3:: 0.12412500381469727
overhead4:: 4.445016384124756
overhead5:: 0
memory usage:: 3066249216
time_provenance:: 6.010731220245361
curr_diff: 0 tensor(1.5120e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5120e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871800
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3260, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1166, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9409, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7928, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3801, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3140, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1470, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9745, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9501, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9262, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9057, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8851, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8692, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8499, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8167, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7902, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7652, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7399, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7335, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7032, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6845, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6791, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6555, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6485, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6453, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6363, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6221, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6180, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6179, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6129, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6066, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5974, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.420126914978027
training time full:: 6.420193433761597
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  3,   5,   8,  10,  11,  14,  15,  17,  19,  21,  28,  29,  31,  33,
         36,  38,  39,  40,  41,  43,  45,  46,  47,  50,  52,  53,  54,  55,
         56,  57,  58,  61,  64,  66,  72,  74,  76,  78,  79,  80,  81,  83,
         85,  86,  90,  92,  96, 102, 105, 106])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.765809774398804
overhead:: 0
overhead2:: 0.42728495597839355
overhead3:: 0
time_baseline:: 4.765890121459961
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008139848709106445
overhead3:: 0.04260683059692383
overhead4:: 0.8144326210021973
overhead5:: 0
memory usage:: 3100057600
time_provenance:: 1.5380239486694336
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0327, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0327, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011225700378417969
overhead3:: 0.04858517646789551
overhead4:: 1.1194844245910645
overhead5:: 0
memory usage:: 3021111296
time_provenance:: 1.9048824310302734
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0326, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0326, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014050722122192383
overhead3:: 0.056611061096191406
overhead4:: 1.4642164707183838
overhead5:: 0
memory usage:: 3033088000
time_provenance:: 2.346163034439087
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011574506759643555
overhead3:: 0.05124950408935547
overhead4:: 1.2922375202178955
overhead5:: 0
memory usage:: 3041705984
time_provenance:: 2.071497917175293
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01530313491821289
overhead3:: 0.057753562927246094
overhead4:: 1.5259325504302979
overhead5:: 0
memory usage:: 3029045248
time_provenance:: 2.375307559967041
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01752185821533203
overhead3:: 0.06391668319702148
overhead4:: 1.823287010192871
overhead5:: 0
memory usage:: 3019718656
time_provenance:: 2.7456278800964355
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0306, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0306, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02413487434387207
overhead3:: 0.07548642158508301
overhead4:: 2.4899253845214844
overhead5:: 0
memory usage:: 3053010944
time_provenance:: 3.615530014038086
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026472091674804688
overhead3:: 0.07926034927368164
overhead4:: 2.7054572105407715
overhead5:: 0
memory usage:: 3041792000
time_provenance:: 3.8624043464660645
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02737569808959961
overhead3:: 0.08375787734985352
overhead4:: 2.837322473526001
overhead5:: 0
memory usage:: 3033931776
time_provenance:: 4.037199258804321
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04279136657714844
overhead3:: 0.12150859832763672
overhead4:: 4.437593221664429
overhead5:: 0
memory usage:: 3063111680
time_provenance:: 6.004907846450806
curr_diff: 0 tensor(1.5084e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5084e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0305, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0305, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
deletion rate:: 0.1
python3 generate_rand_ids 0.1  MNIST5 0
tensor([49152, 49153, 16387,  ..., 16380, 16381, 32766])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3496, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9535, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4713, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1095, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0372, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9258, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8809, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8549, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7486, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7305, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7210, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6922, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6892, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6659, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6619, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6473, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6213, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6269, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5976, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5854, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5880, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2692651748657227
training time full:: 3.2693307399749756
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  76,  77,  78,  79,  89,  91,  92,  93,  95,
         97, 102, 105, 106, 107, 109, 118, 121, 123, 134, 135, 136, 143, 150,
        152, 173, 174, 183, 188, 190, 191, 192])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.4345922470092773
overhead:: 0
overhead2:: 0.2226266860961914
overhead3:: 0
time_baseline:: 2.434743881225586
curr_diff: 0 tensor(0.0466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007231235504150391
overhead3:: 0.036843061447143555
overhead4:: 0.425797700881958
overhead5:: 0
memory usage:: 3043250176
time_provenance:: 1.054589033126831
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0499, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0499, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01054692268371582
overhead3:: 0.044836997985839844
overhead4:: 0.5780706405639648
overhead5:: 0
memory usage:: 3062624256
time_provenance:: 1.2184255123138428
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013814449310302734
overhead3:: 0.05048537254333496
overhead4:: 0.7306945323944092
overhead5:: 0
memory usage:: 3091644416
time_provenance:: 1.3929884433746338
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0499, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0499, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.012977838516235352
overhead3:: 0.04415559768676758
overhead4:: 0.6150593757629395
overhead5:: 0
memory usage:: 3060322304
time_provenance:: 1.3236446380615234
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0473, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0473, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014981269836425781
overhead3:: 0.051943302154541016
overhead4:: 0.7989091873168945
overhead5:: 0
memory usage:: 3072049152
time_provenance:: 1.4921503067016602
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0473, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0473, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.017331600189208984
overhead3:: 0.05681347846984863
overhead4:: 0.8969480991363525
overhead5:: 0
memory usage:: 3043303424
time_provenance:: 1.618342399597168
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0473, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0473, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02394700050354004
overhead3:: 0.07286810874938965
overhead4:: 1.4047353267669678
overhead5:: 0
memory usage:: 3031314432
time_provenance:: 2.268749237060547
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02533698081970215
overhead3:: 0.07442808151245117
overhead4:: 1.4566221237182617
overhead5:: 0
memory usage:: 3066335232
time_provenance:: 2.3576784133911133
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026827573776245117
overhead3:: 0.07860541343688965
overhead4:: 1.5386791229248047
overhead5:: 0
memory usage:: 3031027712
time_provenance:: 2.433581829071045
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.05437588691711426
overhead3:: 0.1288437843322754
overhead4:: 2.3398256301879883
overhead5:: 0
memory usage:: 3025666048
time_provenance:: 3.3564720153808594
curr_diff: 0 tensor(1.6099e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6099e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3172, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4637, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3071, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1880, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0262, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9749, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9253, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8441, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8135, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7933, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7437, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7239, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7030, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6929, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6751, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6528, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6446, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6256, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6118, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6039, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.290923833847046
training time full:: 3.2909905910491943
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  42,  47,  52,  56,  59,  63,  64,  65,
         77,  78,  82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124,
        127, 129, 131, 132, 133, 135, 145, 148, 155, 158, 162, 173, 175, 183,
        184, 186, 187, 188, 203, 204, 218, 224])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.442821502685547
overhead:: 0
overhead2:: 0.2247452735900879
overhead3:: 0
time_baseline:: 2.4429774284362793
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007913351058959961
overhead3:: 0.039644718170166016
overhead4:: 0.42095041275024414
overhead5:: 0
memory usage:: 3069030400
time_provenance:: 1.0151779651641846
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0495, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0495, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013258695602416992
overhead3:: 0.04762554168701172
overhead4:: 0.6574282646179199
overhead5:: 0
memory usage:: 3057803264
time_provenance:: 1.3267874717712402
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014109373092651367
overhead3:: 0.050498008728027344
overhead4:: 0.7310605049133301
overhead5:: 0
memory usage:: 3048587264
time_provenance:: 1.410090446472168
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0493, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0493, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011856317520141602
overhead3:: 0.04689908027648926
overhead4:: 0.6187784671783447
overhead5:: 0
memory usage:: 3022200832
time_provenance:: 1.3203527927398682
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0469, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0469, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014486551284790039
overhead3:: 0.05171608924865723
overhead4:: 0.7637526988983154
overhead5:: 0
memory usage:: 3060146176
time_provenance:: 1.5128953456878662
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0469, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0469, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01699090003967285
overhead3:: 0.05771899223327637
overhead4:: 0.9095749855041504
overhead5:: 0
memory usage:: 3021680640
time_provenance:: 1.6258676052093506
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0468, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0468, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02500128746032715
overhead3:: 0.06815361976623535
overhead4:: 1.3439607620239258
overhead5:: 0
memory usage:: 3076939776
time_provenance:: 2.2213103771209717
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026061534881591797
overhead3:: 0.07564401626586914
overhead4:: 1.454493522644043
overhead5:: 0
memory usage:: 3080384512
time_provenance:: 2.3232414722442627
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025930404663085938
overhead3:: 0.07930517196655273
overhead4:: 1.550234317779541
overhead5:: 0
memory usage:: 3052453888
time_provenance:: 2.447840452194214
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04223132133483887
overhead3:: 0.11600637435913086
overhead4:: 2.237579822540283
overhead5:: 0
memory usage:: 3032006656
time_provenance:: 3.2535250186920166
curr_diff: 0 tensor(1.6203e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6203e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6716, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4671, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3144, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.2017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1064, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8825, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8496, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8214, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7928, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7747, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7100, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6828, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6731, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6267, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6225, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6063, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5934, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.261606216430664
training time full:: 3.26167368888855
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  69,  83,  85,
         89,  91,  92,  94, 105, 114, 115, 117, 120, 127, 129, 134, 152, 154,
        157, 158, 160, 166, 170, 172, 173, 180, 182, 187, 192, 194, 195, 197,
        204, 206, 210, 215, 224, 228, 229, 231])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.4287850856781006
overhead:: 0
overhead2:: 0.22422552108764648
overhead3:: 0
time_baseline:: 2.4289348125457764
curr_diff: 0 tensor(0.0467, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0467, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0075876712799072266
overhead3:: 0.037415504455566406
overhead4:: 0.41898059844970703
overhead5:: 0
memory usage:: 3036643328
time_provenance:: 1.0094928741455078
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0501, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0501, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010672569274902344
overhead3:: 0.043625593185424805
overhead4:: 0.571648120880127
overhead5:: 0
memory usage:: 3049398272
time_provenance:: 1.2131938934326172
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0500, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0500, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014443159103393555
overhead3:: 0.05169367790222168
overhead4:: 0.7442317008972168
overhead5:: 0
memory usage:: 3037212672
time_provenance:: 1.4168915748596191
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0499, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0499, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010848760604858398
overhead3:: 0.04561281204223633
overhead4:: 0.6357583999633789
overhead5:: 0
memory usage:: 3049562112
time_provenance:: 1.3535149097442627
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0471, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0471, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01408076286315918
overhead3:: 0.05236554145812988
overhead4:: 0.7921006679534912
overhead5:: 0
memory usage:: 3067039744
time_provenance:: 1.527951717376709
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0471, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0471, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.018111467361450195
overhead3:: 0.05910801887512207
overhead4:: 0.9336485862731934
overhead5:: 0
memory usage:: 3038359552
time_provenance:: 1.6918721199035645
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0470, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0470, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02274179458618164
overhead3:: 0.07457947731018066
overhead4:: 1.399925708770752
overhead5:: 0
memory usage:: 3077115904
time_provenance:: 2.2604784965515137
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0467, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0467, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025252819061279297
overhead3:: 0.07570481300354004
overhead4:: 1.4381308555603027
overhead5:: 0
memory usage:: 3066880000
time_provenance:: 2.3437340259552
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0467, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0467, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.029156923294067383
overhead3:: 0.0773169994354248
overhead4:: 1.5292956829071045
overhead5:: 0
memory usage:: 3068321792
time_provenance:: 2.427786111831665
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0467, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0467, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04210209846496582
overhead3:: 0.11787891387939453
overhead4:: 2.222099542617798
overhead5:: 0
memory usage:: 3033268224
time_provenance:: 3.2305312156677246
curr_diff: 0 tensor(1.6090e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6090e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0467, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0467, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.873800
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3401, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9526, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6737, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4636, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3179, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0349, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9258, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8854, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8495, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7432, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7165, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6824, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6681, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6588, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6515, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6152, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5906, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2702796459198
training time full:: 3.270362138748169
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  77,  87,  93,  97, 101, 103, 104, 106, 107, 109,
        110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 170, 178,
        192, 194, 195, 204, 205, 210, 213, 214])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.4381895065307617
overhead:: 0
overhead2:: 0.22478914260864258
overhead3:: 0
time_baseline:: 2.4383420944213867
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0074269771575927734
overhead3:: 0.03739643096923828
overhead4:: 0.42099976539611816
overhead5:: 0
memory usage:: 3027357696
time_provenance:: 1.0546388626098633
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0496, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0496, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010522127151489258
overhead3:: 0.043108224868774414
overhead4:: 0.5672085285186768
overhead5:: 0
memory usage:: 3031584768
time_provenance:: 1.2559375762939453
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0496, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0496, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01403355598449707
overhead3:: 0.05113625526428223
overhead4:: 0.737830638885498
overhead5:: 0
memory usage:: 3066540032
time_provenance:: 1.4096662998199463
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011892318725585938
overhead3:: 0.046004533767700195
overhead4:: 0.6351213455200195
overhead5:: 0
memory usage:: 3030290432
time_provenance:: 1.3149442672729492
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0474, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0474, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015563249588012695
overhead3:: 0.04976916313171387
overhead4:: 0.7681088447570801
overhead5:: 0
memory usage:: 3034648576
time_provenance:: 1.4519309997558594
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0474, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0474, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.017176151275634766
overhead3:: 0.058444976806640625
overhead4:: 0.9287559986114502
overhead5:: 0
memory usage:: 3052642304
time_provenance:: 1.6861116886138916
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0473, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0473, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02423381805419922
overhead3:: 0.07133030891418457
overhead4:: 1.4134244918823242
overhead5:: 0
memory usage:: 3042586624
time_provenance:: 2.283008337020874
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.024495363235473633
overhead3:: 0.07355523109436035
overhead4:: 1.4572079181671143
overhead5:: 0
memory usage:: 3042168832
time_provenance:: 2.350231885910034
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.028354883193969727
overhead3:: 0.07941341400146484
overhead4:: 1.512037992477417
overhead5:: 0
memory usage:: 3060097024
time_provenance:: 2.4085798263549805
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.042420387268066406
overhead3:: 0.11317205429077148
overhead4:: 2.2231061458587646
overhead5:: 0
memory usage:: 3056189440
time_provenance:: 3.231710195541382
curr_diff: 0 tensor(1.6165e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6165e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3199, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9388, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6597, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4582, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3110, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1925, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1081, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9805, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8928, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8515, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8200, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7679, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.7007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6710, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6320, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6112, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5856, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5743, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.272308588027954
training time full:: 3.272376775741577
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  81,  86,  90,  92, 102, 105, 106, 109, 116, 117,
        120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162, 165,
        166, 174, 180, 188, 190, 194, 195, 198])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.4269020557403564
overhead:: 0
overhead2:: 0.2259664535522461
overhead3:: 0
time_baseline:: 2.4270577430725098
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.00815582275390625
overhead3:: 0.03832578659057617
overhead4:: 0.42353391647338867
overhead5:: 0
memory usage:: 3037900800
time_provenance:: 1.0894577503204346
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0499, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0499, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011170387268066406
overhead3:: 0.04378533363342285
overhead4:: 0.5920538902282715
overhead5:: 0
memory usage:: 3030409216
time_provenance:: 1.2957172393798828
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0497, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0497, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013848066329956055
overhead3:: 0.050443410873413086
overhead4:: 0.7271173000335693
overhead5:: 0
memory usage:: 3038457856
time_provenance:: 1.3972580432891846
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0497, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0497, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011510372161865234
overhead3:: 0.04545140266418457
overhead4:: 0.6369543075561523
overhead5:: 0
memory usage:: 3071627264
time_provenance:: 1.2846956253051758
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0470, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0470, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014290571212768555
overhead3:: 0.05232357978820801
overhead4:: 0.777982234954834
overhead5:: 0
memory usage:: 3062960128
time_provenance:: 1.4829609394073486
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0469, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0469, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.016967058181762695
overhead3:: 0.05708718299865723
overhead4:: 0.898643970489502
overhead5:: 0
memory usage:: 3034431488
time_provenance:: 1.6381645202636719
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0469, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0469, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02493453025817871
overhead3:: 0.07697439193725586
overhead4:: 1.371018648147583
overhead5:: 0
memory usage:: 3048923136
time_provenance:: 2.246659517288208
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0465, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0465, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02671337127685547
overhead3:: 0.07515883445739746
overhead4:: 1.4622981548309326
overhead5:: 0
memory usage:: 3041103872
time_provenance:: 2.3670907020568848
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0274655818939209
overhead3:: 0.07814574241638184
overhead4:: 1.4990050792694092
overhead5:: 0
memory usage:: 3030396928
time_provenance:: 2.3891468048095703
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04154658317565918
overhead3:: 0.1108393669128418
overhead4:: 2.1963605880737305
overhead5:: 0
memory usage:: 3017015296
time_provenance:: 3.207693099975586
curr_diff: 0 tensor(1.6122e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6122e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0464, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0464, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3802, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1594, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4850, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3271, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2059, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1148, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0737, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0394, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0061, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9801, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9546, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8738, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8509, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8327, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7955, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7717, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7678, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7506, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7330, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7236, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7130, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6974, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6883, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6821, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6661, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6594, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6521, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6382, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6368, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6319, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6169, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6156, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6012, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5907, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.297463655471802
training time full:: 6.29753041267395
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   3,   4,   5,   7,   8,  15,  17,  18,  19,  20,  22,  24,
         25,  27,  29,  31,  32,  33,  34,  36,  38,  40,  44,  45,  47,  57,
         58,  62,  64,  65,  66,  68,  70,  71,  73,  74,  76,  77,  78,  79,
         81,  89,  91,  92,  93,  95,  97, 100])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.536823749542236
overhead:: 0
overhead2:: 0.4441540241241455
overhead3:: 0
time_baseline:: 4.53696870803833
curr_diff: 0 tensor(0.0447, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0447, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007971763610839844
overhead3:: 0.052573442459106445
overhead4:: 0.7978732585906982
overhead5:: 0
memory usage:: 3101499392
time_provenance:: 1.8127110004425049
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0480, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0480, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011234283447265625
overhead3:: 0.058930397033691406
overhead4:: 1.078230381011963
overhead5:: 0
memory usage:: 3065204736
time_provenance:: 2.160907030105591
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0480, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0480, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014402151107788086
overhead3:: 0.06645989418029785
overhead4:: 1.401597261428833
overhead5:: 0
memory usage:: 3057573888
time_provenance:: 2.5593619346618652
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0474, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0474, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011533498764038086
overhead3:: 0.059992074966430664
overhead4:: 1.1581535339355469
overhead5:: 0
memory usage:: 3066793984
time_provenance:: 2.2431674003601074
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014746904373168945
overhead3:: 0.06537055969238281
overhead4:: 1.4363770484924316
overhead5:: 0
memory usage:: 3065786368
time_provenance:: 2.6101462841033936
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017146587371826172
overhead3:: 0.07313966751098633
overhead4:: 1.7153146266937256
overhead5:: 0
memory usage:: 3073392640
time_provenance:: 2.9536187648773193
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0447, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0447, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.022006750106811523
overhead3:: 0.08432173728942871
overhead4:: 2.4218883514404297
overhead5:: 0
memory usage:: 3071332352
time_provenance:: 3.800830364227295
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024610519409179688
overhead3:: 0.09328103065490723
overhead4:: 2.5914950370788574
overhead5:: 0
memory usage:: 3076501504
time_provenance:: 4.000495910644531
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.027421236038208008
overhead3:: 0.09111833572387695
overhead4:: 2.747079372406006
overhead5:: 0
memory usage:: 3100848128
time_provenance:: 4.210623264312744
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04265332221984863
overhead3:: 0.12956690788269043
overhead4:: 4.25221061706543
overhead5:: 0
memory usage:: 3062145024
time_provenance:: 6.03145694732666
curr_diff: 0 tensor(1.5026e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5026e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0447, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0447, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1189, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7971, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6699, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5587, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4637, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3819, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3111, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1930, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0658, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0328, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9734, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9438, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9237, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8862, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8639, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8297, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8021, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7803, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7681, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7612, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7494, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7341, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7209, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7114, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6935, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6834, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6778, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6672, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6632, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6589, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6452, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6435, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6338, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6389, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6244, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6205, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6107, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6082, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.42350959777832
training time full:: 6.423574209213257
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  42,  43,  44,  47,  52,  53,  56,  59,  63,  64,  65,  77,  78,
         82,  84,  86,  91,  92, 100, 101, 104, 105, 106, 110, 111, 112, 113,
        114, 117, 118, 119, 122, 124, 126, 127])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.549391746520996
overhead:: 0
overhead2:: 0.4423954486846924
overhead3:: 0
time_baseline:: 4.549542427062988
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007861614227294922
overhead3:: 0.05227923393249512
overhead4:: 0.7595875263214111
overhead5:: 0
memory usage:: 3064971264
time_provenance:: 1.7623741626739502
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0479, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0479, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011749744415283203
overhead3:: 0.058620452880859375
overhead4:: 1.0754625797271729
overhead5:: 0
memory usage:: 3048968192
time_provenance:: 2.1437528133392334
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0478, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0478, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014692306518554688
overhead3:: 0.06746649742126465
overhead4:: 1.4091122150421143
overhead5:: 0
memory usage:: 3081297920
time_provenance:: 2.568582534790039
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0474, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0474, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011661529541015625
overhead3:: 0.05932784080505371
overhead4:: 1.1395983695983887
overhead5:: 0
memory usage:: 3047325696
time_provenance:: 2.233736991882324
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014554977416992188
overhead3:: 0.06701207160949707
overhead4:: 1.477051019668579
overhead5:: 0
memory usage:: 3071729664
time_provenance:: 2.6634209156036377
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017696142196655273
overhead3:: 0.07305359840393066
overhead4:: 1.7281155586242676
overhead5:: 0
memory usage:: 3084115968
time_provenance:: 2.9646010398864746
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02480030059814453
overhead3:: 0.08135247230529785
overhead4:: 2.384999990463257
overhead5:: 0
memory usage:: 3065503744
time_provenance:: 3.756432056427002
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024852752685546875
overhead3:: 0.0879526138305664
overhead4:: 2.5481784343719482
overhead5:: 0
memory usage:: 3082092544
time_provenance:: 3.951873302459717
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02844834327697754
overhead3:: 0.0889437198638916
overhead4:: 2.7195708751678467
overhead5:: 0
memory usage:: 3057885184
time_provenance:: 4.164186477661133
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04166603088378906
overhead3:: 0.1305248737335205
overhead4:: 4.242239475250244
overhead5:: 0
memory usage:: 3039784960
time_provenance:: 6.026684284210205
curr_diff: 0 tensor(1.5124e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5124e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3300, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1087, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9357, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6611, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3742, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2495, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1910, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0649, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0339, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9446, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9028, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8823, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8645, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8165, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7899, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7809, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7659, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7597, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7387, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7277, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6818, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6765, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6668, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6576, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6405, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6379, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6306, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6237, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6246, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6212, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6217, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6164, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6070, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6006, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5914, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5871, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.418198347091675
training time full:: 6.418267011642456
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  39,
         40,  43,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  69,  70,
         76,  83,  84,  85,  86,  87,  88,  89,  91,  92,  94,  98, 100, 105,
        112, 113, 114, 115, 116, 117, 119, 120])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.548086881637573
overhead:: 0
overhead2:: 0.44507479667663574
overhead3:: 0
time_baseline:: 4.5482423305511475
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008452415466308594
overhead3:: 0.05141305923461914
overhead4:: 0.7595434188842773
overhead5:: 0
memory usage:: 3064143872
time_provenance:: 1.740100622177124
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0482, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0482, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.010985612869262695
overhead3:: 0.05898451805114746
overhead4:: 1.0805206298828125
overhead5:: 0
memory usage:: 3064705024
time_provenance:: 2.1506059169769287
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0478, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0478, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014626741409301758
overhead3:: 0.06791281700134277
overhead4:: 1.3950252532958984
overhead5:: 0
memory usage:: 3074039808
time_provenance:: 2.5609443187713623
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0475, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0475, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01178741455078125
overhead3:: 0.06009316444396973
overhead4:: 1.1546971797943115
overhead5:: 0
memory usage:: 3048787968
time_provenance:: 2.2384135723114014
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0448, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0448, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014600038528442383
overhead3:: 0.06675219535827637
overhead4:: 1.477332353591919
overhead5:: 0
memory usage:: 3060948992
time_provenance:: 2.6591811180114746
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017774105072021484
overhead3:: 0.07348418235778809
overhead4:: 1.7273356914520264
overhead5:: 0
memory usage:: 3040591872
time_provenance:: 2.9659883975982666
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024285554885864258
overhead3:: 0.08507847785949707
overhead4:: 2.3996598720550537
overhead5:: 0
memory usage:: 3053125632
time_provenance:: 3.7711501121520996
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.027139902114868164
overhead3:: 0.08419322967529297
overhead4:: 2.607011318206787
overhead5:: 0
memory usage:: 3049160704
time_provenance:: 4.029419183731079
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026421785354614258
overhead3:: 0.09016919136047363
overhead4:: 2.7705466747283936
overhead5:: 0
memory usage:: 3064242176
time_provenance:: 4.220645427703857
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.042875051498413086
overhead3:: 0.1318349838256836
overhead4:: 4.23887300491333
overhead5:: 0
memory usage:: 3036950528
time_provenance:: 6.02098274230957
curr_diff: 0 tensor(1.5002e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5002e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2983, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9121, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7678, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6417, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5360, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3644, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.2996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2371, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1875, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1384, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0614, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0296, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9945, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9703, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9238, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8781, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8626, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8373, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8132, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8017, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7875, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7824, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7676, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7563, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7408, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7282, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7186, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7108, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7085, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6822, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6782, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6696, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6677, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6639, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6518, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6528, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6415, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6367, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6362, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6235, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6127, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5957, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5922, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.332045555114746
training time full:: 6.332114219665527
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  42,  44,  45,  46,  50,  55,  57,
         58,  60,  61,  62,  63,  64,  68,  71,  73,  74,  77,  84,  85,  87,
         90,  93,  94,  97, 100, 101, 103, 104])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.540100812911987
overhead:: 0
overhead2:: 0.43840670585632324
overhead3:: 0
time_baseline:: 4.540253639221191
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007861614227294922
overhead3:: 0.052580833435058594
overhead4:: 0.7676427364349365
overhead5:: 0
memory usage:: 3056336896
time_provenance:: 1.755690336227417
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0480, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0480, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011176586151123047
overhead3:: 0.059947967529296875
overhead4:: 1.0776572227478027
overhead5:: 0
memory usage:: 3045306368
time_provenance:: 2.1471848487854004
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0477, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0477, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.013991832733154297
overhead3:: 0.06695866584777832
overhead4:: 1.4176931381225586
overhead5:: 0
memory usage:: 3045982208
time_provenance:: 2.6199591159820557
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0473, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0473, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012787580490112305
overhead3:: 0.06186032295227051
overhead4:: 1.189936637878418
overhead5:: 0
memory usage:: 3072233472
time_provenance:: 2.3004777431488037
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.0156857967376709
overhead3:: 0.06400823593139648
overhead4:: 1.450042486190796
overhead5:: 0
memory usage:: 3049086976
time_provenance:: 2.6082327365875244
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017563581466674805
overhead3:: 0.07332968711853027
overhead4:: 1.808441400527954
overhead5:: 0
memory usage:: 3087953920
time_provenance:: 3.0227816104888916
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.023876667022705078
overhead3:: 0.08424544334411621
overhead4:: 2.450324296951294
overhead5:: 0
memory usage:: 3052355584
time_provenance:: 3.8228721618652344
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026800155639648438
overhead3:: 0.08619880676269531
overhead4:: 2.547332763671875
overhead5:: 0
memory usage:: 3044642816
time_provenance:: 3.9472434520721436
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.027272939682006836
overhead3:: 0.0920252799987793
overhead4:: 2.8201801776885986
overhead5:: 0
memory usage:: 3051966464
time_provenance:: 4.288796663284302
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04236721992492676
overhead3:: 0.13075947761535645
overhead4:: 4.270038604736328
overhead5:: 0
memory usage:: 3082539008
time_provenance:: 6.060176134109497
curr_diff: 0 tensor(1.4919e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4919e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874100
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9353, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7872, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5539, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4607, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3788, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3128, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2496, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1468, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0693, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9739, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9499, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9258, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9048, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8841, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8686, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8354, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8161, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8006, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7898, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7646, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7595, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7454, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7391, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7184, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7113, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7024, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6963, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6931, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6764, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6636, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6550, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6478, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6351, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6176, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6122, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6058, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5964, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5969, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5995, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5996, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5932, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5862, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.425803184509277
training time full:: 6.425869703292847
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  3,   5,   8,  10,  11,  14,  15,  17,  19,  21,  28,  29,  31,  33,
         36,  38,  39,  40,  41,  43,  45,  46,  47,  50,  52,  53,  54,  55,
         56,  57,  58,  61,  64,  66,  72,  74,  76,  78,  79,  81,  83,  85,
         86,  90,  92,  96, 102, 105, 106, 109])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.514869213104248
overhead:: 0
overhead2:: 0.43909120559692383
overhead3:: 0
time_baseline:: 4.515014410018921
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007929086685180664
overhead3:: 0.052106380462646484
overhead4:: 0.77089524269104
overhead5:: 0
memory usage:: 3045023744
time_provenance:: 1.760988473892212
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0479, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0479, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011115550994873047
overhead3:: 0.058570146560668945
overhead4:: 1.0772058963775635
overhead5:: 0
memory usage:: 3111493632
time_provenance:: 2.137425422668457
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0477, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0477, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014300107955932617
overhead3:: 0.06618714332580566
overhead4:: 1.4145402908325195
overhead5:: 0
memory usage:: 3085012992
time_provenance:: 2.6037886142730713
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0474, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0474, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011773109436035156
overhead3:: 0.06034731864929199
overhead4:: 1.1848070621490479
overhead5:: 0
memory usage:: 3049074688
time_provenance:: 2.2891056537628174
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014220714569091797
overhead3:: 0.06679558753967285
overhead4:: 1.4427406787872314
overhead5:: 0
memory usage:: 3052544000
time_provenance:: 2.604475498199463
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.018555879592895508
overhead3:: 0.07429790496826172
overhead4:: 1.7400932312011719
overhead5:: 0
memory usage:: 3073191936
time_provenance:: 2.9212841987609863
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0446, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0446, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025333166122436523
overhead3:: 0.07958412170410156
overhead4:: 2.4094388484954834
overhead5:: 0
memory usage:: 3045097472
time_provenance:: 3.794597625732422
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025061845779418945
overhead3:: 0.08805060386657715
overhead4:: 2.5478198528289795
overhead5:: 0
memory usage:: 3073769472
time_provenance:: 3.9604411125183105
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.027866601943969727
overhead3:: 0.09313631057739258
overhead4:: 2.7545955181121826
overhead5:: 0
memory usage:: 3078430720
time_provenance:: 4.2171242237091064
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0444, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0444, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6000
max_epoch:: 60
overhead:: 0
overhead2:: 0.042330265045166016
overhead3:: 0.1309971809387207
overhead4:: 4.242905139923096
overhead5:: 0
memory usage:: 3041157120
time_provenance:: 6.022013187408447
curr_diff: 0 tensor(1.5040e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5040e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0445, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0445, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872900
deletion rate:: 0.15
python3 generate_rand_ids 0.15  MNIST5 0
tensor([49152, 49153, 16386,  ..., 16380, 16381, 32766])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3124, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9304, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6604, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4554, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3050, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9675, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9176, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8750, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8493, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8220, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7657, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7449, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7277, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7175, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6892, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6633, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6305, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6193, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6187, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.269895315170288
training time full:: 3.2699615955352783
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  76,  77,  78,  79,  89,  91,  92,  93,  95,
         97, 102, 105, 106, 107, 109, 118, 121, 123, 135, 136, 143, 150, 152,
        173, 174, 183, 188, 190, 191, 192, 196])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.3218767642974854
overhead:: 0
overhead2:: 0.23749041557312012
overhead3:: 0
time_baseline:: 2.32206654548645
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007286548614501953
overhead3:: 0.040712594985961914
overhead4:: 0.4077630043029785
overhead5:: 0
memory usage:: 3042668544
time_provenance:: 1.1728184223175049
curr_diff: 0 tensor(0.0081, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0081, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0637, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0637, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010955810546875
overhead3:: 0.04777264595031738
overhead4:: 0.5363674163818359
overhead5:: 0
memory usage:: 3043491840
time_provenance:: 1.3184139728546143
curr_diff: 0 tensor(0.0081, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0081, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0645, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0645, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013106822967529297
overhead3:: 0.05430150032043457
overhead4:: 0.6858112812042236
overhead5:: 0
memory usage:: 3044392960
time_provenance:: 1.5095899105072021
curr_diff: 0 tensor(0.0079, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0079, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0643, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0643, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011229991912841797
overhead3:: 0.05130887031555176
overhead4:: 0.6062328815460205
overhead5:: 0
memory usage:: 3048505344
time_provenance:: 1.4196686744689941
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014291524887084961
overhead3:: 0.05625438690185547
overhead4:: 0.716550350189209
overhead5:: 0
memory usage:: 3061006336
time_provenance:: 1.56498384475708
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0178680419921875
overhead3:: 0.06146526336669922
overhead4:: 0.8636102676391602
overhead5:: 0
memory usage:: 3041058816
time_provenance:: 1.7244842052459717
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026057958602905273
overhead3:: 0.07403254508972168
overhead4:: 1.2922561168670654
overhead5:: 0
memory usage:: 3062054912
time_provenance:: 2.258603096008301
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02558159828186035
overhead3:: 0.08048105239868164
overhead4:: 1.4221837520599365
overhead5:: 0
memory usage:: 3041968128
time_provenance:: 2.4026474952697754
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026569128036499023
overhead3:: 0.0836038589477539
overhead4:: 1.440999984741211
overhead5:: 0
memory usage:: 3052851200
time_provenance:: 2.422973155975342
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.042664289474487305
overhead3:: 0.11435985565185547
overhead4:: 2.111757755279541
overhead5:: 0
memory usage:: 3051290624
time_provenance:: 3.1819956302642822
curr_diff: 0 tensor(1.6243e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6243e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3546, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9558, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6802, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4760, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1973, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9805, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8860, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8487, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7484, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7065, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6764, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6561, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6325, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6274, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6141, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6185, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6054, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6020, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5868, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5719, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.307838201522827
training time full:: 3.3079075813293457
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  26,  37,  47,  52,  59,  63,  64,  65,  77,  78,
         82,  84,  86,  91,  92, 104, 106, 111, 114, 117, 119, 124, 127, 129,
        131, 132, 133, 135, 145, 148, 155, 158, 162, 173, 175, 183, 184, 186,
        187, 188, 204, 218, 224, 227, 231, 233])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.3301799297332764
overhead:: 0
overhead2:: 0.23873353004455566
overhead3:: 0
time_baseline:: 2.330371379852295
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007748126983642578
overhead3:: 0.04178214073181152
overhead4:: 0.4048957824707031
overhead5:: 0
memory usage:: 3044261888
time_provenance:: 1.1829888820648193
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0635, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0635, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011491060256958008
overhead3:: 0.04728198051452637
overhead4:: 0.5428950786590576
overhead5:: 0
memory usage:: 3077877760
time_provenance:: 1.3245759010314941
curr_diff: 0 tensor(0.0074, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0074, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0641, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0641, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013756275177001953
overhead3:: 0.053772926330566406
overhead4:: 0.6889371871948242
overhead5:: 0
memory usage:: 3043954688
time_provenance:: 1.5196311473846436
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0638, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0638, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.012347221374511719
overhead3:: 0.048212289810180664
overhead4:: 0.5815200805664062
overhead5:: 0
memory usage:: 3077345280
time_provenance:: 1.401780605316162
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0602, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0602, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014758825302124023
overhead3:: 0.05534672737121582
overhead4:: 0.7498681545257568
overhead5:: 0
memory usage:: 3060092928
time_provenance:: 1.5864171981811523
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0601, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0601, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.018340587615966797
overhead3:: 0.06004738807678223
overhead4:: 0.8560628890991211
overhead5:: 0
memory usage:: 3031805952
time_provenance:: 1.6989293098449707
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0600, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0600, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02555108070373535
overhead3:: 0.07287931442260742
overhead4:: 1.3106982707977295
overhead5:: 0
memory usage:: 3082092544
time_provenance:: 2.264923095703125
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0244903564453125
overhead3:: 0.08193850517272949
overhead4:: 1.383209466934204
overhead5:: 0
memory usage:: 3069153280
time_provenance:: 2.3707408905029297
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.027285099029541016
overhead3:: 0.08346128463745117
overhead4:: 1.4811375141143799
overhead5:: 0
memory usage:: 3052507136
time_provenance:: 2.4905343055725098
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.040566444396972656
overhead3:: 0.1202695369720459
overhead4:: 2.119934558868408
overhead5:: 0
memory usage:: 3031400448
time_provenance:: 3.1975598335266113
curr_diff: 0 tensor(1.6088e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6088e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9382, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6638, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4595, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3104, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0230, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9715, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9242, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8788, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8181, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7721, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7500, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6916, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6708, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6658, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6421, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6309, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6257, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6217, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6056, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5959, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5927, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5765, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5723, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.253934860229492
training time full:: 3.2540032863616943
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  83,  85,  89,
         91,  92, 105, 114, 115, 117, 120, 127, 129, 152, 154, 157, 158, 160,
        166, 170, 172, 173, 180, 182, 187, 192, 194, 195, 197, 204, 206, 210,
        215, 224, 228, 229, 231, 238, 247, 248])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.338444709777832
overhead:: 0
overhead2:: 0.2353658676147461
overhead3:: 0
time_baseline:: 2.338635206222534
curr_diff: 0 tensor(0.0593, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0593, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007602691650390625
overhead3:: 0.04149174690246582
overhead4:: 0.3919656276702881
overhead5:: 0
memory usage:: 3076726784
time_provenance:: 1.1586189270019531
curr_diff: 0 tensor(0.0080, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0080, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0642, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0642, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010549068450927734
overhead3:: 0.04807424545288086
overhead4:: 0.5533046722412109
overhead5:: 0
memory usage:: 3052523520
time_provenance:: 1.3647205829620361
curr_diff: 0 tensor(0.0074, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0074, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0641, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0641, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013409614562988281
overhead3:: 0.055085182189941406
overhead4:: 0.7195508480072021
overhead5:: 0
memory usage:: 3053350912
time_provenance:: 1.5581424236297607
curr_diff: 0 tensor(0.0074, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0074, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0637, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0637, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.012091636657714844
overhead3:: 0.0506587028503418
overhead4:: 0.6084144115447998
overhead5:: 0
memory usage:: 3041947648
time_provenance:: 1.4420931339263916
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0598, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0598, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015922069549560547
overhead3:: 0.05584239959716797
overhead4:: 0.7191298007965088
overhead5:: 0
memory usage:: 3054104576
time_provenance:: 1.5141303539276123
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0597, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0597, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.018262386322021484
overhead3:: 0.06332278251647949
overhead4:: 0.8527874946594238
overhead5:: 0
memory usage:: 3040514048
time_provenance:: 1.7228543758392334
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0597, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0597, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026537656784057617
overhead3:: 0.07367157936096191
overhead4:: 1.2952330112457275
overhead5:: 0
memory usage:: 3102339072
time_provenance:: 2.23210072517395
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0592, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0592, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025081157684326172
overhead3:: 0.07974648475646973
overhead4:: 1.393503189086914
overhead5:: 0
memory usage:: 3045634048
time_provenance:: 2.38015079498291
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0592, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0592, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.027436494827270508
overhead3:: 0.08414578437805176
overhead4:: 1.4714417457580566
overhead5:: 0
memory usage:: 3054780416
time_provenance:: 2.4671316146850586
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0592, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0592, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.041704416275024414
overhead3:: 0.1206367015838623
overhead4:: 2.1366539001464844
overhead5:: 0
memory usage:: 3041722368
time_provenance:: 3.2098286151885986
curr_diff: 0 tensor(1.6100e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6100e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0593, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0593, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.878100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3421, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9576, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4659, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3173, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0322, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9778, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9237, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8834, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8466, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8162, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7867, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7667, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7139, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6975, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6802, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6566, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6498, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6426, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6145, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6014, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5883, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2584807872772217
training time full:: 3.2585489749908447
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  77,  87,  93,  97, 101, 103, 104, 106, 107, 109,
        110, 111, 113, 124, 125, 138, 148, 149, 150, 151, 154, 159, 170, 178,
        192, 194, 195, 204, 205, 210, 214, 216])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.423124074935913
overhead:: 0
overhead2:: 0.2385237216949463
overhead3:: 0
time_baseline:: 2.423326253890991
curr_diff: 0 tensor(0.0592, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0592, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007946014404296875
overhead3:: 0.04204893112182617
overhead4:: 0.4167790412902832
overhead5:: 0
memory usage:: 3060436992
time_provenance:: 1.156451940536499
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0635, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0635, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010864973068237305
overhead3:: 0.04881095886230469
overhead4:: 0.5429120063781738
overhead5:: 0
memory usage:: 3059445760
time_provenance:: 1.3598580360412598
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0637, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0637, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013135671615600586
overhead3:: 0.05426144599914551
overhead4:: 0.6921932697296143
overhead5:: 0
memory usage:: 3077005312
time_provenance:: 1.532425880432129
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0636, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0636, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011319637298583984
overhead3:: 0.05083942413330078
overhead4:: 0.6012125015258789
overhead5:: 0
memory usage:: 3025969152
time_provenance:: 1.3777287006378174
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0600, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0600, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014127254486083984
overhead3:: 0.05525970458984375
overhead4:: 0.7247302532196045
overhead5:: 0
memory usage:: 3059744768
time_provenance:: 1.5498323440551758
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0599, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0599, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01876354217529297
overhead3:: 0.06318902969360352
overhead4:: 0.8777787685394287
overhead5:: 0
memory usage:: 3044106240
time_provenance:: 1.753828763961792
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0599, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0599, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02548503875732422
overhead3:: 0.07528018951416016
overhead4:: 1.2763609886169434
overhead5:: 0
memory usage:: 3116142592
time_provenance:: 2.2296319007873535
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0591, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0591, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025050878524780273
overhead3:: 0.07907581329345703
overhead4:: 1.3796205520629883
overhead5:: 0
memory usage:: 3101683712
time_provenance:: 2.365586519241333
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0591, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0591, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02601790428161621
overhead3:: 0.08372831344604492
overhead4:: 1.4867408275604248
overhead5:: 0
memory usage:: 3084242944
time_provenance:: 2.496694326400757
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0590, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0590, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04266095161437988
overhead3:: 0.1160888671875
overhead4:: 2.1071128845214844
overhead5:: 0
memory usage:: 3039629312
time_provenance:: 3.1880407333374023
curr_diff: 0 tensor(1.6112e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6112e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0592, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0592, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9376, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6571, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4588, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3101, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0329, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9777, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8908, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8488, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8187, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7871, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7456, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7284, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6993, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6851, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6687, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6599, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6502, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6396, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6098, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.5978, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5837, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5734, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.285099983215332
training time full:: 3.2851829528808594
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  81,  86,  90,  92, 102, 105, 106, 109, 116, 117,
        120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 162, 165,
        166, 174, 180, 188, 190, 194, 195, 198])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.329493522644043
overhead:: 0
overhead2:: 0.23684334754943848
overhead3:: 0
time_baseline:: 2.3296852111816406
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007073402404785156
overhead3:: 0.04212379455566406
overhead4:: 0.40778183937072754
overhead5:: 0
memory usage:: 3066327040
time_provenance:: 1.189790964126587
curr_diff: 0 tensor(0.0074, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0074, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0642, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0642, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.010885953903198242
overhead3:: 0.048802852630615234
overhead4:: 0.5793042182922363
overhead5:: 0
memory usage:: 3062640640
time_provenance:: 1.4150190353393555
curr_diff: 0 tensor(0.0080, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0080, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0646, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0646, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013149738311767578
overhead3:: 0.05356431007385254
overhead4:: 0.6885693073272705
overhead5:: 0
memory usage:: 3039956992
time_provenance:: 1.5069217681884766
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0642, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0642, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.877100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01174020767211914
overhead3:: 0.04958224296569824
overhead4:: 0.5840682983398438
overhead5:: 0
memory usage:: 3076067328
time_provenance:: 1.4046809673309326
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0600, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0600, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01480412483215332
overhead3:: 0.05630898475646973
overhead4:: 0.718085527420044
overhead5:: 0
memory usage:: 3062525952
time_provenance:: 1.5617499351501465
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0600, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0600, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.016276121139526367
overhead3:: 0.061498403549194336
overhead4:: 0.8755381107330322
overhead5:: 0
memory usage:: 3062865920
time_provenance:: 1.6976385116577148
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0599, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0599, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.022208690643310547
overhead3:: 0.07807326316833496
overhead4:: 1.2840652465820312
overhead5:: 0
memory usage:: 3064750080
time_provenance:: 2.244668960571289
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.028254985809326172
overhead3:: 0.0759432315826416
overhead4:: 1.3276073932647705
overhead5:: 0
memory usage:: 3076931584
time_provenance:: 2.3121368885040283
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026114463806152344
overhead3:: 0.08295392990112305
overhead4:: 1.4844346046447754
overhead5:: 0
memory usage:: 3039932416
time_provenance:: 2.4669957160949707
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0594, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0594, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 9000
max_epoch:: 32
overhead:: 0
overhead2:: 0.05463743209838867
overhead3:: 0.13261055946350098
overhead4:: 2.2812600135803223
overhead5:: 0
memory usage:: 3046887424
time_provenance:: 3.3650405406951904
curr_diff: 0 tensor(1.6027e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6027e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0595, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0595, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876600
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0990, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7825, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6584, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5481, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4567, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1918, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0316, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9739, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9499, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9041, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8835, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8479, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8301, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8167, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8049, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7931, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7809, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7702, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7436, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7124, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7067, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6976, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6962, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6881, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6816, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6657, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6592, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6536, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6384, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6364, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6310, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6315, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6228, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6168, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6154, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6094, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6010, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5991, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5907, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5900, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.33556342124939
training time full:: 6.335629224777222
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   3,   4,   5,   7,   8,  15,  17,  18,  19,  20,  22,  24,
         25,  27,  29,  31,  32,  33,  34,  36,  38,  40,  44,  45,  47,  57,
         58,  62,  64,  65,  66,  68,  70,  71,  73,  74,  76,  77,  78,  79,
         81,  89,  91,  92,  93,  95,  97, 100])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.292165040969849
overhead:: 0
overhead2:: 0.4604947566986084
overhead3:: 0
time_baseline:: 4.292465925216675
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007698774337768555
overhead3:: 0.05944538116455078
overhead4:: 0.745833158493042
overhead5:: 0
memory usage:: 3102613504
time_provenance:: 1.8187963962554932
curr_diff: 0 tensor(0.0087, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0087, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011939287185668945
overhead3:: 0.06762528419494629
overhead4:: 1.0571768283843994
overhead5:: 0
memory usage:: 3066130432
time_provenance:: 2.3375372886657715
curr_diff: 0 tensor(0.0077, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0077, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0616, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0616, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01473379135131836
overhead3:: 0.07515764236450195
overhead4:: 1.3249363899230957
overhead5:: 0
memory usage:: 3083161600
time_provenance:: 2.5574023723602295
curr_diff: 0 tensor(0.0075, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0075, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012703418731689453
overhead3:: 0.06736135482788086
overhead4:: 1.1395032405853271
overhead5:: 0
memory usage:: 3074342912
time_provenance:: 2.3313143253326416
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0574, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0574, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015049219131469727
overhead3:: 0.07392573356628418
overhead4:: 1.3985865116119385
overhead5:: 0
memory usage:: 3066085376
time_provenance:: 2.6327221393585205
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017487764358520508
overhead3:: 0.07999086380004883
overhead4:: 1.6297190189361572
overhead5:: 0
memory usage:: 3074666496
time_provenance:: 2.950066328048706
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0570, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0570, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02295660972595215
overhead3:: 0.09244704246520996
overhead4:: 2.264498233795166
overhead5:: 0
memory usage:: 3072745472
time_provenance:: 3.6980462074279785
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024684906005859375
overhead3:: 0.09812569618225098
overhead4:: 2.4775190353393555
overhead5:: 0
memory usage:: 3066556416
time_provenance:: 3.9382193088531494
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026815176010131836
overhead3:: 0.09956979751586914
overhead4:: 2.6338682174682617
overhead5:: 0
memory usage:: 3075010560
time_provenance:: 4.13835597038269
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04312872886657715
overhead3:: 0.13799595832824707
overhead4:: 3.983724355697632
overhead5:: 0
memory usage:: 3030253568
time_provenance:: 5.746241092681885
curr_diff: 0 tensor(1.4956e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4956e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3365, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9549, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5646, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4692, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3874, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3162, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2553, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1504, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0691, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0369, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0040, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9766, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9471, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8888, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8668, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8541, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8321, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8203, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8039, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7989, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7828, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7702, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7361, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7299, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7127, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6789, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6678, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6647, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6600, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6474, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6346, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6402, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6315, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6214, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6141, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6115, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6093, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6052, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5964, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5923, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5895, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.388465404510498
training time full:: 6.3885297775268555
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  26,  34,  37,  38,
         40,  44,  47,  52,  53,  59,  63,  64,  65,  77,  78,  82,  84,  86,
         91,  92, 100, 101, 104, 105, 106, 110, 111, 113, 114, 117, 118, 119,
        122, 124, 126, 127, 129, 130, 131, 132])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.278468132019043
overhead:: 0
overhead2:: 0.4645421504974365
overhead3:: 0
time_baseline:: 4.278658866882324
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.007715940475463867
overhead3:: 0.05949664115905762
overhead4:: 0.731724739074707
overhead5:: 0
memory usage:: 3044847616
time_provenance:: 1.8284718990325928
curr_diff: 0 tensor(0.0086, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0086, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011758089065551758
overhead3:: 0.06781268119812012
overhead4:: 1.030578851699829
overhead5:: 0
memory usage:: 3066261504
time_provenance:: 2.1987521648406982
curr_diff: 0 tensor(0.0079, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0079, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0616, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0616, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015035152435302734
overhead3:: 0.07488298416137695
overhead4:: 1.3465638160705566
overhead5:: 0
memory usage:: 3113721856
time_provenance:: 2.662179470062256
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0609, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0609, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011795520782470703
overhead3:: 0.06775283813476562
overhead4:: 1.1736927032470703
overhead5:: 0
memory usage:: 3065221120
time_provenance:: 2.4205870628356934
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014282941818237305
overhead3:: 0.07416772842407227
overhead4:: 1.3562543392181396
overhead5:: 0
memory usage:: 3091685376
time_provenance:: 2.5961992740631104
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0574, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0574, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017322778701782227
overhead3:: 0.08079767227172852
overhead4:: 1.6483986377716064
overhead5:: 0
memory usage:: 3068837888
time_provenance:: 2.9766314029693604
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0574, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0574, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02276158332824707
overhead3:: 0.09081196784973145
overhead4:: 2.3167691230773926
overhead5:: 0
memory usage:: 3037859840
time_provenance:: 3.7513301372528076
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024822235107421875
overhead3:: 0.0936739444732666
overhead4:: 2.430607557296753
overhead5:: 0
memory usage:: 3100512256
time_provenance:: 3.891437530517578
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026758670806884766
overhead3:: 0.10152411460876465
overhead4:: 2.635800361633301
overhead5:: 0
memory usage:: 3084922880
time_provenance:: 4.1379125118255615
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04143071174621582
overhead3:: 0.13799047470092773
overhead4:: 4.0161943435668945
overhead5:: 0
memory usage:: 3067797504
time_provenance:: 5.78447699546814
curr_diff: 0 tensor(1.4996e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4996e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7807, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6547, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5469, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4560, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3717, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2477, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1045, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0645, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0349, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0038, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9743, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9455, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9251, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8829, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8662, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8505, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8312, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8174, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7822, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7673, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7610, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7455, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7395, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7287, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7153, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6966, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6915, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6828, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6773, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6682, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6651, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6547, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6581, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6416, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6392, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6320, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6250, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6252, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6218, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6171, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6016, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5929, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5878, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.373931646347046
training time full:: 6.37399697303772
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  39,
         40,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  70,  76,  83,
         84,  85,  86,  87,  88,  89,  91,  92,  98, 100, 105, 113, 114, 115,
        116, 117, 119, 120, 126, 127, 128, 129])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.302513837814331
overhead:: 0
overhead2:: 0.4638254642486572
overhead3:: 0
time_baseline:: 4.302700757980347
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008515596389770508
overhead3:: 0.06017255783081055
overhead4:: 0.7395060062408447
overhead5:: 0
memory usage:: 3038937088
time_provenance:: 1.9359426498413086
curr_diff: 0 tensor(0.0085, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0085, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0625, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0625, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.010910749435424805
overhead3:: 0.06637859344482422
overhead4:: 1.0269865989685059
overhead5:: 0
memory usage:: 3064573952
time_provenance:: 2.207357406616211
curr_diff: 0 tensor(0.0076, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0076, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0618, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0618, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.013991594314575195
overhead3:: 0.07327389717102051
overhead4:: 1.3351480960845947
overhead5:: 0
memory usage:: 3050700800
time_provenance:: 2.571387529373169
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012209415435791016
overhead3:: 0.06661605834960938
overhead4:: 1.2208478450775146
overhead5:: 0
memory usage:: 3085250560
time_provenance:: 2.434422016143799
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0574, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0574, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014879465103149414
overhead3:: 0.07453489303588867
overhead4:: 1.3906702995300293
overhead5:: 0
memory usage:: 3064381440
time_provenance:: 2.668059825897217
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017218828201293945
overhead3:: 0.07992053031921387
overhead4:: 1.6893515586853027
overhead5:: 0
memory usage:: 3058581504
time_provenance:: 3.0328028202056885
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02393364906311035
overhead3:: 0.09102582931518555
overhead4:: 2.3204801082611084
overhead5:: 0
memory usage:: 3066032128
time_provenance:: 3.755829334259033
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024807453155517578
overhead3:: 0.09450578689575195
overhead4:: 2.4733569622039795
overhead5:: 0
memory usage:: 3063975936
time_provenance:: 3.965769052505493
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026341915130615234
overhead3:: 0.10197329521179199
overhead4:: 2.6528871059417725
overhead5:: 0
memory usage:: 3136749568
time_provenance:: 4.1703126430511475
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04367256164550781
overhead3:: 0.13927054405212402
overhead4:: 4.048302412033081
overhead5:: 0
memory usage:: 3067461632
time_provenance:: 5.821844577789307
curr_diff: 0 tensor(1.4992e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4992e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.871900
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3286, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1254, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5605, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4643, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3821, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3138, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2483, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1038, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0667, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0337, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9730, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9481, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9247, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9028, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8784, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8630, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8369, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8133, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7872, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7815, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7670, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7550, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7177, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7099, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7073, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7020, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6884, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6775, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6628, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6516, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6353, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6266, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6224, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6216, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6117, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6090, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6038, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5879, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.39822244644165
training time full:: 6.398287534713745
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  44,  45,  46,  50,  55,  57,  58,
         60,  61,  62,  63,  64,  68,  71,  73,  74,  77,  84,  85,  87,  90,
         93,  97, 100, 101, 103, 104, 105, 106])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.246978521347046
overhead:: 0
overhead2:: 0.45803403854370117
overhead3:: 0
time_baseline:: 4.247161865234375
curr_diff: 0 tensor(0.0570, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0570, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01007843017578125
overhead3:: 0.05925273895263672
overhead4:: 0.7272613048553467
overhead5:: 0
memory usage:: 3144093696
time_provenance:: 1.8530075550079346
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0619, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0619, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011088848114013672
overhead3:: 0.06672310829162598
overhead4:: 1.0286023616790771
overhead5:: 0
memory usage:: 3168337920
time_provenance:: 2.258127212524414
curr_diff: 0 tensor(0.0075, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0075, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0617, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0617, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01380777359008789
overhead3:: 0.07306385040283203
overhead4:: 1.3309283256530762
overhead5:: 0
memory usage:: 3092205568
time_provenance:: 2.577609062194824
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.012343883514404297
overhead3:: 0.0668630599975586
overhead4:: 1.0973396301269531
overhead5:: 0
memory usage:: 3041574912
time_provenance:: 2.29528546333313
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0570, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0570, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014300346374511719
overhead3:: 0.07258057594299316
overhead4:: 1.3568735122680664
overhead5:: 0
memory usage:: 3039371264
time_provenance:: 2.6171329021453857
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.019282817840576172
overhead3:: 0.07976222038269043
overhead4:: 1.6588068008422852
overhead5:: 0
memory usage:: 3066232832
time_provenance:: 2.959841012954712
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025388717651367188
overhead3:: 0.08623051643371582
overhead4:: 2.279425859451294
overhead5:: 0
memory usage:: 3047927808
time_provenance:: 3.7096197605133057
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0568, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0568, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02614307403564453
overhead3:: 0.09937548637390137
overhead4:: 2.4333066940307617
overhead5:: 0
memory usage:: 3039797248
time_provenance:: 3.9004456996917725
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0568, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0568, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026946306228637695
overhead3:: 0.09998130798339844
overhead4:: 2.6403396129608154
overhead5:: 0
memory usage:: 3144589312
time_provenance:: 4.1489105224609375
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0568, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0568, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04059123992919922
overhead3:: 0.13597607612609863
overhead4:: 3.999337911605835
overhead5:: 0
memory usage:: 3084697600
time_provenance:: 5.761783123016357
curr_diff: 0 tensor(1.5144e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5144e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0570, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0570, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2940, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.0864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6484, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5410, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3686, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3030, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2410, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1921, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1381, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0967, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0257, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9678, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.8994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8793, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8445, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8314, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8118, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7968, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7858, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7763, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7613, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7552, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7422, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7358, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7293, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7155, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7086, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.6994, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6901, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6808, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6761, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6738, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6611, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6523, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6482, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6427, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6331, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6337, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6192, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6155, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6103, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6046, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5944, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5909, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5847, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.37233829498291
training time full:: 6.372421979904175
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  3,   5,   8,  10,  11,  14,  15,  17,  19,  21,  28,  29,  31,  33,
         36,  38,  39,  40,  41,  45,  46,  47,  50,  52,  53,  54,  55,  57,
         58,  61,  64,  66,  72,  74,  76,  78,  79,  81,  83,  85,  86,  90,
         92,  96, 102, 105, 106, 109, 113, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.254210472106934
overhead:: 0
overhead2:: 0.4632151126861572
overhead3:: 0
time_baseline:: 4.254402160644531
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.0073833465576171875
overhead3:: 0.0588688850402832
overhead4:: 0.7366437911987305
overhead5:: 0
memory usage:: 3048075264
time_provenance:: 1.8990421295166016
curr_diff: 0 tensor(0.0082, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0082, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0620, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0620, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011199951171875
overhead3:: 0.06615996360778809
overhead4:: 1.0375230312347412
overhead5:: 0
memory usage:: 3119538176
time_provenance:: 2.2351040840148926
curr_diff: 0 tensor(0.0081, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0081, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0617, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0617, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014713525772094727
overhead3:: 0.0738065242767334
overhead4:: 1.3296804428100586
overhead5:: 0
memory usage:: 3077636096
time_provenance:: 2.5862767696380615
curr_diff: 0 tensor(0.0082, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0082, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0615, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0615, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011832475662231445
overhead3:: 0.06717133522033691
overhead4:: 1.1059696674346924
overhead5:: 0
memory usage:: 3058917376
time_provenance:: 2.3005242347717285
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0573, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0573, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014653444290161133
overhead3:: 0.07411336898803711
overhead4:: 1.3907477855682373
overhead5:: 0
memory usage:: 3041312768
time_provenance:: 2.66474986076355
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01812434196472168
overhead3:: 0.07962751388549805
overhead4:: 1.6554045677185059
overhead5:: 0
memory usage:: 3065696256
time_provenance:: 2.9628076553344727
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.023416519165039062
overhead3:: 0.09417343139648438
overhead4:: 2.309393882751465
overhead5:: 0
memory usage:: 3042033664
time_provenance:: 3.739733934402466
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025263547897338867
overhead3:: 0.09559512138366699
overhead4:: 2.433143138885498
overhead5:: 0
memory usage:: 3133923328
time_provenance:: 3.8849735260009766
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026530742645263672
overhead3:: 0.10079574584960938
overhead4:: 2.666440486907959
overhead5:: 0
memory usage:: 3086815232
time_provenance:: 4.18242073059082
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0569, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0569, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.15 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 9000
max_epoch:: 60
overhead:: 0
overhead2:: 0.042484283447265625
overhead3:: 0.13873720169067383
overhead4:: 4.009292125701904
overhead5:: 0
memory usage:: 3031785472
time_provenance:: 5.773118495941162
curr_diff: 0 tensor(1.5094e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5094e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0571, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0571, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
deletion rate:: 0.2
python3 generate_rand_ids 0.2  MNIST5 0
tensor([32769, 32773,     6,  ..., 32761, 32764, 32766])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3039, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6529, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4516, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1793, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0979, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0272, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9659, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9162, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8736, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8219, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7645, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7254, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6876, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6848, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6573, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6430, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6178, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6232, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6171, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6026, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5829, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2551960945129395
training time full:: 3.255262613296509
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,   3,   5,  19,  20,  24,  27,  29,  31,  33,  38,  45,  57,
         58,  65,  66,  71,  73,  76,  77,  78,  79,  89,  91,  92,  93,  95,
         97, 102, 105, 106, 107, 109, 118, 121, 123, 135, 136, 143, 152, 173,
        174, 183, 188, 190, 191, 192, 196, 199])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.2410621643066406
overhead:: 0
overhead2:: 0.2515847682952881
overhead3:: 0
time_baseline:: 2.241281032562256
curr_diff: 0 tensor(0.0720, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0720, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.008469820022583008
overhead3:: 0.04544997215270996
overhead4:: 0.37392163276672363
overhead5:: 0
memory usage:: 3058733056
time_provenance:: 1.277306079864502
curr_diff: 0 tensor(0.0099, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0099, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0771, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0771, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.012025117874145508
overhead3:: 0.05230975151062012
overhead4:: 0.5230932235717773
overhead5:: 0
memory usage:: 3042758656
time_provenance:: 1.4684703350067139
curr_diff: 0 tensor(0.0109, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0109, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0788, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0788, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01394796371459961
overhead3:: 0.0605165958404541
overhead4:: 0.675518274307251
overhead5:: 0
memory usage:: 3031220224
time_provenance:: 1.6586382389068604
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0783, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0783, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.013362646102905273
overhead3:: 0.0542750358581543
overhead4:: 0.597273588180542
overhead5:: 0
memory usage:: 3060457472
time_provenance:: 1.5267333984375
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0734, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0734, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015744924545288086
overhead3:: 0.05992722511291504
overhead4:: 0.685920000076294
overhead5:: 0
memory usage:: 3041976320
time_provenance:: 1.6594114303588867
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0733, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0733, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.017006874084472656
overhead3:: 0.06766414642333984
overhead4:: 0.8389701843261719
overhead5:: 0
memory usage:: 3055570944
time_provenance:: 1.828064203262329
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0732, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0732, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0233001708984375
overhead3:: 0.08168435096740723
overhead4:: 1.2813489437103271
overhead5:: 0
memory usage:: 3069964288
time_provenance:: 2.4172329902648926
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.026778459548950195
overhead3:: 0.08425521850585938
overhead4:: 1.3212554454803467
overhead5:: 0
memory usage:: 3033542656
time_provenance:: 2.447284698486328
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.027158260345458984
overhead3:: 0.0898904800415039
overhead4:: 1.3971359729766846
overhead5:: 0
memory usage:: 3064066048
time_provenance:: 2.5344607830047607
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.048485755920410156
overhead3:: 0.11856222152709961
overhead4:: 1.9920587539672852
overhead5:: 0
memory usage:: 3030478848
time_provenance:: 3.1888909339904785
curr_diff: 0 tensor(1.6214e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6214e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0720, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0720, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6519, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3007, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1842, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0971, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0245, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9743, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9253, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8814, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8131, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7243, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6759, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6737, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6536, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6450, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6311, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6261, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6118, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6169, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6043, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6002, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5941, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5854, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5709, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2707674503326416
training time full:: 3.2708375453948975
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.876000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  3,  11,  16,  18,  37,  47,  52,  59,  63,  64,  65,  77,  78,  82,
         86,  91,  92, 104, 106, 111, 114, 117, 119, 124, 127, 129, 131, 132,
        133, 135, 145, 148, 155, 158, 173, 175, 183, 184, 186, 187, 188, 218,
        224, 227, 231, 233, 237, 245, 254, 269])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.284320592880249
overhead:: 0
overhead2:: 0.25188112258911133
overhead3:: 0
time_baseline:: 2.2845516204833984
curr_diff: 0 tensor(0.0716, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0716, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007863759994506836
overhead3:: 0.04623603820800781
overhead4:: 0.43123388290405273
overhead5:: 0
memory usage:: 3065856000
time_provenance:: 1.4382843971252441
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0769, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0769, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011313199996948242
overhead3:: 0.052451372146606445
overhead4:: 0.5661923885345459
overhead5:: 0
memory usage:: 3052384256
time_provenance:: 1.6095242500305176
curr_diff: 0 tensor(0.0098, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0098, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0778, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0778, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015355348587036133
overhead3:: 0.05829572677612305
overhead4:: 0.6533422470092773
overhead5:: 0
memory usage:: 3127238656
time_provenance:: 1.6144375801086426
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0772, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0772, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011497259140014648
overhead3:: 0.05610823631286621
overhead4:: 0.5939946174621582
overhead5:: 0
memory usage:: 3031203840
time_provenance:: 1.5618600845336914
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0724, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0724, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015457630157470703
overhead3:: 0.06113576889038086
overhead4:: 0.6968140602111816
overhead5:: 0
memory usage:: 3078291456
time_provenance:: 1.7111730575561523
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0724, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0724, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.019779682159423828
overhead3:: 0.0654439926147461
overhead4:: 0.8352508544921875
overhead5:: 0
memory usage:: 3062378496
time_provenance:: 1.8584585189819336
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0722, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0722, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0232694149017334
overhead3:: 0.0818932056427002
overhead4:: 1.290766716003418
overhead5:: 0
memory usage:: 3054751744
time_provenance:: 2.411324977874756
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0714, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0714, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02630138397216797
overhead3:: 0.08316373825073242
overhead4:: 1.3210129737854004
overhead5:: 0
memory usage:: 3072475136
time_provenance:: 2.431614398956299
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0714, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0714, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.028936147689819336
overhead3:: 0.08470535278320312
overhead4:: 1.4093797206878662
overhead5:: 0
memory usage:: 3049914368
time_provenance:: 2.552335023880005
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0714, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0714, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04588603973388672
overhead3:: 0.12513065338134766
overhead4:: 2.0095326900482178
overhead5:: 0
memory usage:: 3032875008
time_provenance:: 3.214226245880127
curr_diff: 0 tensor(1.6326e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6326e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0716, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0716, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2790, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9000, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6358, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4415, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.2948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1861, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0927, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0175, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8761, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8160, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7889, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7704, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7496, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7276, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7075, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6912, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6803, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6703, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6653, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6421, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6302, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6255, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6208, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6052, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5922, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5768, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5720, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.29803204536438
training time full:: 3.298100471496582
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  4,  10,  13,  20,  24,  31,  45,  49,  58,  59,  65,  83,  85,  89,
         91,  92, 105, 114, 117, 120, 127, 129, 152, 154, 157, 158, 160, 166,
        172, 173, 180, 182, 187, 192, 194, 195, 197, 206, 210, 215, 224, 228,
        229, 231, 247, 248, 254, 265, 269, 273])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.2501161098480225
overhead:: 0
overhead2:: 0.25321292877197266
overhead3:: 0
time_baseline:: 2.2503483295440674
curr_diff: 0 tensor(0.0714, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0714, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.008072853088378906
overhead3:: 0.04615068435668945
overhead4:: 0.3795344829559326
overhead5:: 0
memory usage:: 3057614848
time_provenance:: 1.3053669929504395
curr_diff: 0 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0107, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0776, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0776, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01147603988647461
overhead3:: 0.05320906639099121
overhead4:: 0.5645458698272705
overhead5:: 0
memory usage:: 3090481152
time_provenance:: 1.5749914646148682
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0779, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0779, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01505589485168457
overhead3:: 0.060048580169677734
overhead4:: 0.6624436378479004
overhead5:: 0
memory usage:: 3057369088
time_provenance:: 1.6369740962982178
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0771, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0771, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011714458465576172
overhead3:: 0.05523085594177246
overhead4:: 0.5711629390716553
overhead5:: 0
memory usage:: 3038236672
time_provenance:: 1.4846959114074707
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015078306198120117
overhead3:: 0.06076216697692871
overhead4:: 0.6796507835388184
overhead5:: 0
memory usage:: 3066298368
time_provenance:: 1.6716036796569824
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01906752586364746
overhead3:: 0.06458663940429688
overhead4:: 0.8039834499359131
overhead5:: 0
memory usage:: 3031949312
time_provenance:: 1.8010661602020264
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02333664894104004
overhead3:: 0.08083200454711914
overhead4:: 1.2963590621948242
overhead5:: 0
memory usage:: 3099930624
time_provenance:: 2.400346040725708
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0712, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0712, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.028421401977539062
overhead3:: 0.08377718925476074
overhead4:: 1.3281769752502441
overhead5:: 0
memory usage:: 3070889984
time_provenance:: 2.4441018104553223
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0712, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0712, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.029525041580200195
overhead3:: 0.09205102920532227
overhead4:: 1.3708007335662842
overhead5:: 0
memory usage:: 3101040640
time_provenance:: 2.486588478088379
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0711, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0711, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.042322635650634766
overhead3:: 0.12594366073608398
overhead4:: 2.0083017349243164
overhead5:: 0
memory usage:: 3039334400
time_provenance:: 3.20930552482605
curr_diff: 0 tensor(1.6138e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6138e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0714, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0714, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.2839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9084, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6451, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3032, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1859, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.0984, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9758, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9222, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8825, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8460, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7862, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7672, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7306, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7149, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.6980, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6665, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6570, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6498, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6426, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6295, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6148, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6022, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6008, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.5893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5901, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5838, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.263995409011841
training time full:: 3.2640652656555176
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  1,   2,  13,  30,  31,  34,  40,  41,  45,  50,  55,  57,  60,  62,
         68,  71,  73,  74,  77,  87,  93,  97, 101, 103, 104, 106, 107, 109,
        110, 111, 113, 124, 125, 138, 148, 149, 151, 154, 159, 192, 194, 195,
        205, 210, 214, 216, 217, 222, 230, 233])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.232045888900757
overhead:: 0
overhead2:: 0.25443077087402344
overhead3:: 0
time_baseline:: 2.232262372970581
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.007776975631713867
overhead3:: 0.04647946357727051
overhead4:: 0.38153815269470215
overhead5:: 0
memory usage:: 3127451648
time_provenance:: 1.2910330295562744
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0780, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0780, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.011555671691894531
overhead3:: 0.05246424674987793
overhead4:: 0.5194857120513916
overhead5:: 0
memory usage:: 3093434368
time_provenance:: 1.4107468128204346
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0777, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0777, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015080690383911133
overhead3:: 0.06003117561340332
overhead4:: 0.6605818271636963
overhead5:: 0
memory usage:: 3055181824
time_provenance:: 1.627805233001709
curr_diff: 0 tensor(0.0082, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0082, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0768, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0768, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01252293586730957
overhead3:: 0.054847002029418945
overhead4:: 0.5680549144744873
overhead5:: 0
memory usage:: 3071979520
time_provenance:: 1.5446789264678955
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0729, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0729, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.015439271926879883
overhead3:: 0.060225725173950195
overhead4:: 0.6812443733215332
overhead5:: 0
memory usage:: 3094876160
time_provenance:: 1.6327276229858398
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0728, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0728, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01952385902404785
overhead3:: 0.06679797172546387
overhead4:: 0.8209228515625
overhead5:: 0
memory usage:: 3119468544
time_provenance:: 1.8452825546264648
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0728, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0728, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02700018882751465
overhead3:: 0.07817387580871582
overhead4:: 1.2217059135437012
overhead5:: 0
memory usage:: 3085205504
time_provenance:: 2.327726125717163
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0715, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0715, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02831292152404785
overhead3:: 0.07993507385253906
overhead4:: 1.293830156326294
overhead5:: 0
memory usage:: 3078447104
time_provenance:: 2.394602060317993
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0715, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0715, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.029633283615112305
overhead3:: 0.08619046211242676
overhead4:: 1.3738417625427246
overhead5:: 0
memory usage:: 3120926720
time_provenance:: 2.498077869415283
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0715, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0715, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04137277603149414
overhead3:: 0.12835431098937988
overhead4:: 2.0127112865448
overhead5:: 0
memory usage:: 3093508096
time_provenance:: 3.2218432426452637
curr_diff: 0 tensor(1.6310e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6310e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874400
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(1.9512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.6684, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.4660, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.3169, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.1958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.1131, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.0389, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(0.9839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(0.9307, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(0.8950, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(0.8529, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(0.8225, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(0.7911, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(0.7688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.7495, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.7315, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.7194, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.7027, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.6889, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.6724, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.6635, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.6531, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.6433, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.6334, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.6223, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.6125, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.6011, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.5906, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.5864, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.5752, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 3.2366275787353516
training time full:: 3.2366976737976074
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
batch_size:: 16384
epoch:: 0
tensor([  5,   8,  10,  15,  17,  19,  29,  31,  39,  41,  45,  46,  50,  52,
         53,  55,  57,  72,  81,  86,  90,  92, 102, 105, 106, 109, 116, 117,
        120, 123, 125, 127, 132, 133, 143, 144, 147, 154, 159, 161, 166, 174,
        180, 188, 190, 194, 195, 198, 202, 206])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.35909104347229
overhead:: 0
overhead2:: 0.25379490852355957
overhead3:: 0
time_baseline:: 2.3593409061431885
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.00732111930847168
overhead3:: 0.046953678131103516
overhead4:: 0.38115978240966797
overhead5:: 0
memory usage:: 3073114112
time_provenance:: 1.3131494522094727
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0781, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0781, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01188969612121582
overhead3:: 0.052986860275268555
overhead4:: 0.5275564193725586
overhead5:: 0
memory usage:: 3073662976
time_provenance:: 1.4439325332641602
curr_diff: 0 tensor(0.0099, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0099, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0778, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0778, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.014791250228881836
overhead3:: 0.05838727951049805
overhead4:: 0.6547658443450928
overhead5:: 0
memory usage:: 3078746112
time_provenance:: 1.6360132694244385
curr_diff: 0 tensor(0.0102, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0102, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0784, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0784, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.012402772903442383
overhead3:: 0.05487465858459473
overhead4:: 0.5586395263671875
overhead5:: 0
memory usage:: 3072446464
time_provenance:: 1.5312819480895996
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0723, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0723, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.01484060287475586
overhead3:: 0.06329083442687988
overhead4:: 0.7078971862792969
overhead5:: 0
memory usage:: 3070509056
time_provenance:: 1.6503005027770996
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0726, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0726, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.0181276798248291
overhead3:: 0.06928539276123047
overhead4:: 0.8683555126190186
overhead5:: 0
memory usage:: 3099779072
time_provenance:: 1.8668320178985596
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0724, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0724, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.022836923599243164
overhead3:: 0.08377456665039062
overhead4:: 1.272108554840088
overhead5:: 0
memory usage:: 3077779456
time_provenance:: 2.407733678817749
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.025084733963012695
overhead3:: 0.08738470077514648
overhead4:: 1.367046594619751
overhead5:: 0
memory usage:: 3062120448
time_provenance:: 2.5005853176116943
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.02838420867919922
overhead3:: 0.08918404579162598
overhead4:: 1.4188804626464844
overhead5:: 0
memory usage:: 3038142464
time_provenance:: 2.537425994873047
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12000
max_epoch:: 32
overhead:: 0
overhead2:: 0.04962730407714844
overhead3:: 0.12177324295043945
overhead4:: 2.0129637718200684
overhead5:: 0
memory usage:: 3084283904
time_provenance:: 3.2170135974884033
curr_diff: 0 tensor(1.6115e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6115e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000055, Accuracy: 0.874100
batch size:: 30000
repetition 0
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3376, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9457, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5540, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4603, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3771, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3082, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2459, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1019, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0622, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0287, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9709, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9211, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8796, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8669, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8447, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8269, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8132, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8015, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7906, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7663, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7634, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7462, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7406, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7191, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7092, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7036, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6952, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6937, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6849, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6785, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6675, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6627, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6563, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6503, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6491, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6440, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6355, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6336, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6285, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6290, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6206, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6143, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6072, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6047, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5985, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5886, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5877, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.46417760848999
training time full:: 6.464250564575195
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.877100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   3,   4,   5,   7,   8,  15,  17,  18,  19,  20,  22,  24,
         25,  27,  29,  31,  32,  33,  34,  36,  38,  40,  44,  45,  47,  57,
         58,  62,  64,  65,  66,  68,  70,  71,  73,  74,  76,  77,  78,  79,
         81,  89,  91,  92,  93,  95,  97, 100])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.169060707092285
overhead:: 0
overhead2:: 0.5012617111206055
overhead3:: 0
time_baseline:: 4.169274806976318
curr_diff: 0 tensor(0.0687, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0687, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008199214935302734
overhead3:: 0.06942582130432129
overhead4:: 0.6912460327148438
overhead5:: 0
memory usage:: 3015942144
time_provenance:: 2.270206928253174
curr_diff: 0 tensor(0.0117, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0117, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0746, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0746, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.010876893997192383
overhead3:: 0.07589554786682129
overhead4:: 0.9829328060150146
overhead5:: 0
memory usage:: 3049467904
time_provenance:: 2.622868299484253
curr_diff: 0 tensor(0.0106, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0106, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0743, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0743, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014281511306762695
overhead3:: 0.08192324638366699
overhead4:: 1.2698297500610352
overhead5:: 0
memory usage:: 3015417856
time_provenance:: 3.000532865524292
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0739, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0739, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011415481567382812
overhead3:: 0.07648158073425293
overhead4:: 1.0698299407958984
overhead5:: 0
memory usage:: 3032608768
time_provenance:: 2.7263193130493164
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0695, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0695, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015280961990356445
overhead3:: 0.08159875869750977
overhead4:: 1.3210651874542236
overhead5:: 0
memory usage:: 3050115072
time_provenance:: 3.0123612880706787
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0693, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0693, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017868995666503906
overhead3:: 0.08924460411071777
overhead4:: 1.5606651306152344
overhead5:: 0
memory usage:: 3018842112
time_provenance:: 3.2949085235595703
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0689, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0689, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.023845434188842773
overhead3:: 0.0988612174987793
overhead4:: 2.1508798599243164
overhead5:: 0
memory usage:: 3020627968
time_provenance:: 3.9563510417938232
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0684, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0684, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.024642467498779297
overhead3:: 0.10667014122009277
overhead4:: 2.3665382862091064
overhead5:: 0
memory usage:: 3041820672
time_provenance:: 4.209282636642456
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0684, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0684, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02747797966003418
overhead3:: 0.1072378158569336
overhead4:: 2.4764201641082764
overhead5:: 0
memory usage:: 3017510912
time_provenance:: 4.335737943649292
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0684, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0684, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04439568519592285
overhead3:: 0.14806199073791504
overhead4:: 3.861208200454712
overhead5:: 0
memory usage:: 3009794048
time_provenance:: 5.932257413864136
curr_diff: 0 tensor(1.5029e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5029e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0687, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0687, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.876300
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3117, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1074, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9357, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7885, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4595, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3786, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2476, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1908, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1443, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0656, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0001, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9727, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9436, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9236, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9042, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8863, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8513, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8304, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8189, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7972, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7815, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7688, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7504, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7348, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7291, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7123, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7044, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6939, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6912, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6839, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6784, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6680, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6640, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6596, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6472, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6458, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6444, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6343, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6398, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6313, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6248, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6214, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6137, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6115, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6089, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6023, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6051, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5949, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5961, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5922, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5897, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.3883912563323975
training time full:: 6.388457298278809
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  0,   2,   3,  11,  13,  16,  18,  19,  22,  24,  34,  37,  38,  40,
         44,  47,  52,  53,  59,  63,  64,  65,  77,  78,  82,  86,  91,  92,
        100, 101, 104, 105, 106, 110, 111, 113, 114, 117, 118, 119, 124, 127,
        129, 130, 131, 132, 133, 135, 138, 139])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.124646186828613
overhead:: 0
overhead2:: 0.5013117790222168
overhead3:: 0
time_baseline:: 4.124861240386963
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008516550064086914
overhead3:: 0.06746244430541992
overhead4:: 0.7030565738677979
overhead5:: 0
memory usage:: 3016462336
time_provenance:: 2.2744758129119873
curr_diff: 0 tensor(0.0113, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0113, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0758, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0758, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01155996322631836
overhead3:: 0.07607078552246094
overhead4:: 0.9761397838592529
overhead5:: 0
memory usage:: 3035508736
time_provenance:: 2.581650733947754
curr_diff: 0 tensor(0.0109, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0109, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0755, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0755, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014374017715454102
overhead3:: 0.08150959014892578
overhead4:: 1.2942509651184082
overhead5:: 0
memory usage:: 3073941504
time_provenance:: 2.971993923187256
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0747, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0747, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.872400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011809110641479492
overhead3:: 0.07710909843444824
overhead4:: 1.0905327796936035
overhead5:: 0
memory usage:: 3015065600
time_provenance:: 2.7706174850463867
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015393733978271484
overhead3:: 0.0818939208984375
overhead4:: 1.3130457401275635
overhead5:: 0
memory usage:: 3013754880
time_provenance:: 2.9886786937713623
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0696, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0696, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01786041259765625
overhead3:: 0.08763957023620605
overhead4:: 1.571979284286499
overhead5:: 0
memory usage:: 3014389760
time_provenance:: 3.3038346767425537
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0691, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0691, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.0230405330657959
overhead3:: 0.1028742790222168
overhead4:: 2.173722505569458
overhead5:: 0
memory usage:: 3064496128
time_provenance:: 3.9844133853912354
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0689, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0689, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02430891990661621
overhead3:: 0.10543632507324219
overhead4:: 2.363797903060913
overhead5:: 0
memory usage:: 3039838208
time_provenance:: 4.202441930770874
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0689, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0689, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.026101350784301758
overhead3:: 0.11029982566833496
overhead4:: 2.4761133193969727
overhead5:: 0
memory usage:: 3040415744
time_provenance:: 4.352790117263794
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0689, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0689, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.043114423751831055
overhead3:: 0.1463022232055664
overhead4:: 3.8523690700531006
overhead5:: 0
memory usage:: 3039461376
time_provenance:: 5.917456865310669
curr_diff: 0 tensor(1.5050e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5050e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873400
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3203, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1100, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9375, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7896, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6613, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5512, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4598, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3739, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3060, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2488, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1903, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1448, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1034, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0631, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9729, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9435, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9226, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9018, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8810, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8636, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8487, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8292, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8157, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8068, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7893, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7804, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7655, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7590, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7442, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7379, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7270, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7190, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7032, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6951, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6904, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6811, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6763, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6662, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6635, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6533, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6572, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6467, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6407, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6377, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6304, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6237, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6241, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6207, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6215, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6159, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6064, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6005, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5998, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.6004, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5917, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5910, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5870, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.391297340393066
training time full:: 6.3913657665252686
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   4,  10,  13,  14,  17,  18,  20,  23,  24,  27,  28,  31,  39,
         40,  45,  49,  54,  58,  59,  61,  63,  64,  65,  68,  70,  76,  83,
         85,  86,  87,  88,  89,  91,  92,  98, 100, 105, 113, 114, 116, 117,
        119, 120, 127, 128, 129, 130, 139, 140])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.125593185424805
overhead:: 0
overhead2:: 0.5006017684936523
overhead3:: 0
time_baseline:: 4.125805616378784
curr_diff: 0 tensor(0.0691, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0691, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.00837850570678711
overhead3:: 0.06882023811340332
overhead4:: 0.6857297420501709
overhead5:: 0
memory usage:: 3013582848
time_provenance:: 2.254756212234497
curr_diff: 0 tensor(0.0109, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0109, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0760, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0760, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011164188385009766
overhead3:: 0.07564663887023926
overhead4:: 0.9727761745452881
overhead5:: 0
memory usage:: 3013349376
time_provenance:: 2.591339588165283
curr_diff: 0 tensor(0.0099, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0099, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0749, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0749, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014194250106811523
overhead3:: 0.08355426788330078
overhead4:: 1.2743360996246338
overhead5:: 0
memory usage:: 3015548928
time_provenance:: 2.96630597114563
curr_diff: 0 tensor(0.0101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0750, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0750, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011799097061157227
overhead3:: 0.07707762718200684
overhead4:: 1.0699043273925781
overhead5:: 0
memory usage:: 3013709824
time_provenance:: 2.7142555713653564
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0695, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0695, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015303850173950195
overhead3:: 0.08189272880554199
overhead4:: 1.3176934719085693
overhead5:: 0
memory usage:: 3020894208
time_provenance:: 2.9965364933013916
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0693, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0693, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.0174710750579834
overhead3:: 0.09033393859863281
overhead4:: 1.5770583152770996
overhead5:: 0
memory usage:: 3053662208
time_provenance:: 3.3037331104278564
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02279210090637207
overhead3:: 0.10010576248168945
overhead4:: 2.2056570053100586
overhead5:: 0
memory usage:: 3068002304
time_provenance:: 4.034793853759766
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.025025606155395508
overhead3:: 0.10512447357177734
overhead4:: 2.361027956008911
overhead5:: 0
memory usage:: 3012964352
time_provenance:: 4.224963903427124
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.028889179229736328
overhead3:: 0.1080622673034668
overhead4:: 2.5112836360931396
overhead5:: 0
memory usage:: 3024822272
time_provenance:: 4.372615337371826
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04165959358215332
overhead3:: 0.14604544639587402
overhead4:: 3.839141607284546
overhead5:: 0
memory usage:: 3041296384
time_provenance:: 5.91109037399292
curr_diff: 0 tensor(1.5013e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5013e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0691, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0691, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3425, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1304, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9553, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.8069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6774, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5664, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4715, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3197, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2549, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.2025, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1524, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.1100, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0721, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0392, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(1.0037, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9782, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9537, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9294, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9069, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8830, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8673, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8508, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8410, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8169, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.8049, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7854, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7700, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7585, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7480, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7429, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7202, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7124, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7101, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.7039, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6905, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6831, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6798, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6705, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6689, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6641, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6530, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6538, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6424, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6370, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6369, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6278, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6240, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6231, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6134, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6148, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6107, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.6076, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.6048, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5926, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5887, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.360307216644287
training time full:: 6.360374450683594
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  1,   2,   4,   5,   7,   8,  11,  13,  18,  19,  21,  22,  28,  29,
         30,  31,  32,  34,  37,  40,  41,  44,  45,  46,  50,  55,  57,  58,
         60,  61,  62,  63,  64,  68,  71,  73,  74,  77,  85,  87,  90,  93,
         97, 100, 101, 103, 104, 105, 106, 107])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.115705251693726
overhead:: 0
overhead2:: 0.49881410598754883
overhead3:: 0
time_baseline:: 4.115912437438965
curr_diff: 0 tensor(0.0691, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0691, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008415699005126953
overhead3:: 0.06816840171813965
overhead4:: 0.6894490718841553
overhead5:: 0
memory usage:: 3051540480
time_provenance:: 2.2558400630950928
curr_diff: 0 tensor(0.0118, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0118, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0757, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0757, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01139521598815918
overhead3:: 0.07540321350097656
overhead4:: 0.975846529006958
overhead5:: 0
memory usage:: 3049537536
time_provenance:: 2.588006019592285
curr_diff: 0 tensor(0.0097, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0097, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0751, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0751, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014402389526367188
overhead3:: 0.08376431465148926
overhead4:: 1.2936248779296875
overhead5:: 0
memory usage:: 3012612096
time_provenance:: 2.9613656997680664
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0746, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0746, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.011520147323608398
overhead3:: 0.07648634910583496
overhead4:: 1.0597598552703857
overhead5:: 0
memory usage:: 3012939776
time_provenance:: 2.683067798614502
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0694, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0694, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.015755176544189453
overhead3:: 0.08220338821411133
overhead4:: 1.3107857704162598
overhead5:: 0
memory usage:: 3093602304
time_provenance:: 2.9741814136505127
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017127513885498047
overhead3:: 0.08790302276611328
overhead4:: 1.579991102218628
overhead5:: 0
memory usage:: 3039813632
time_provenance:: 3.3134493827819824
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0689, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0689, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02341151237487793
overhead3:: 0.10286188125610352
overhead4:: 2.2210001945495605
overhead5:: 0
memory usage:: 3066830848
time_provenance:: 4.035595417022705
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02590799331665039
overhead3:: 0.1048729419708252
overhead4:: 2.3453989028930664
overhead5:: 0
memory usage:: 3038097408
time_provenance:: 4.192977428436279
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02650284767150879
overhead3:: 0.11005663871765137
overhead4:: 2.484100341796875
overhead5:: 0
memory usage:: 3014086656
time_provenance:: 4.343425035476685
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0688, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0688, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.045746803283691406
overhead3:: 0.15014195442199707
overhead4:: 3.8521728515625
overhead5:: 0
memory usage:: 3024396288
time_provenance:: 5.920488357543945
curr_diff: 0 tensor(1.5085e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5085e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0691, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0691, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873700
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss:
tensor(2.3153, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 1, Batch: 0, Loss:
tensor(2.1055, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 2, Batch: 0, Loss:
tensor(1.9309, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 3, Batch: 0, Loss:
tensor(1.7822, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 4, Batch: 0, Loss:
tensor(1.6575, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 5, Batch: 0, Loss:
tensor(1.5481, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 6, Batch: 0, Loss:
tensor(1.4552, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 7, Batch: 0, Loss:
tensor(1.3725, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 8, Batch: 0, Loss:
tensor(1.3066, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 9, Batch: 0, Loss:
tensor(1.2434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 10, Batch: 0, Loss:
tensor(1.1943, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 11, Batch: 0, Loss:
tensor(1.1419, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 12, Batch: 0, Loss:
tensor(1.0992, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 13, Batch: 0, Loss:
tensor(1.0647, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 14, Batch: 0, Loss:
tensor(1.0281, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 15, Batch: 0, Loss:
tensor(0.9965, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 16, Batch: 0, Loss:
tensor(0.9701, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 17, Batch: 0, Loss:
tensor(0.9464, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 18, Batch: 0, Loss:
tensor(0.9223, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 19, Batch: 0, Loss:
tensor(0.9013, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 20, Batch: 0, Loss:
tensor(0.8815, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 21, Batch: 0, Loss:
tensor(0.8649, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 22, Batch: 0, Loss:
tensor(0.8465, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 23, Batch: 0, Loss:
tensor(0.8333, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 24, Batch: 0, Loss:
tensor(0.8136, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 25, Batch: 0, Loss:
tensor(0.7986, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 26, Batch: 0, Loss:
tensor(0.7873, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 27, Batch: 0, Loss:
tensor(0.7777, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 28, Batch: 0, Loss:
tensor(0.7626, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 29, Batch: 0, Loss:
tensor(0.7568, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 30, Batch: 0, Loss:
tensor(0.7434, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 31, Batch: 0, Loss:
tensor(0.7373, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 32, Batch: 0, Loss:
tensor(0.7303, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 33, Batch: 0, Loss:
tensor(0.7167, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 34, Batch: 0, Loss:
tensor(0.7100, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 35, Batch: 0, Loss:
tensor(0.7009, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 36, Batch: 0, Loss:
tensor(0.6948, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 37, Batch: 0, Loss:
tensor(0.6917, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 38, Batch: 0, Loss:
tensor(0.6817, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 39, Batch: 0, Loss:
tensor(0.6772, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 40, Batch: 0, Loss:
tensor(0.6750, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 41, Batch: 0, Loss:
tensor(0.6621, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 42, Batch: 0, Loss:
tensor(0.6534, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 43, Batch: 0, Loss:
tensor(0.6466, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 44, Batch: 0, Loss:
tensor(0.6490, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 45, Batch: 0, Loss:
tensor(0.6438, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 46, Batch: 0, Loss:
tensor(0.6318, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 47, Batch: 0, Loss:
tensor(0.6340, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 48, Batch: 0, Loss:
tensor(0.6348, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 49, Batch: 0, Loss:
tensor(0.6204, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 50, Batch: 0, Loss:
tensor(0.6166, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 51, Batch: 0, Loss:
tensor(0.6163, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 52, Batch: 0, Loss:
tensor(0.6112, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 53, Batch: 0, Loss:
tensor(0.6053, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 54, Batch: 0, Loss:
tensor(0.5958, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 55, Batch: 0, Loss:
tensor(0.5960, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 56, Batch: 0, Loss:
tensor(0.5988, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 57, Batch: 0, Loss:
tensor(0.5987, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 58, Batch: 0, Loss:
tensor(0.5920, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
Train - Epoch 59, Batch: 0, Loss:
tensor(0.5853, device='cuda:1', dtype=torch.float64, grad_fn=<NllLossBackward>)
training_time:: 6.507995367050171
training time full:: 6.508060693740845
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
batch_size:: 30000
epoch:: 0
tensor([  3,   5,   8,  10,  11,  14,  15,  17,  19,  21,  28,  29,  31,  33,
         36,  38,  39,  40,  41,  45,  46,  47,  50,  52,  53,  54,  55,  57,
         58,  61,  64,  66,  72,  74,  76,  78,  79,  81,  83,  85,  86,  90,
         92,  96, 102, 105, 106, 109, 113, 116])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
training time is 4.126635551452637
overhead:: 0
overhead2:: 0.49826741218566895
overhead3:: 0
time_baseline:: 4.126951694488525
curr_diff: 0 tensor(0.0693, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0693, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.008045434951782227
overhead3:: 0.06636714935302734
overhead4:: 0.6906311511993408
overhead5:: 0
memory usage:: 3040997376
time_provenance:: 2.247931957244873
curr_diff: 0 tensor(0.0118, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0118, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0760, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0760, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.875400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.010828256607055664
overhead3:: 0.07485032081604004
overhead4:: 0.9795117378234863
overhead5:: 0
memory usage:: 3034415104
time_provenance:: 2.6058130264282227
curr_diff: 0 tensor(0.0108, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0108, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0756, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0756, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.014804363250732422
overhead3:: 0.08156561851501465
overhead4:: 1.270798683166504
overhead5:: 0
memory usage:: 3025711104
time_provenance:: 2.9308390617370605
curr_diff: 0 tensor(0.0110, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0110, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0751, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0751, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01203608512878418
overhead3:: 0.07553791999816895
overhead4:: 1.068009853363037
overhead5:: 0
memory usage:: 3031924736
time_provenance:: 2.6995625495910645
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0697, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0697, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.01453399658203125
overhead3:: 0.08214855194091797
overhead4:: 1.313868761062622
overhead5:: 0
memory usage:: 3034021888
time_provenance:: 2.982879877090454
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0696, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0696, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.017116308212280273
overhead3:: 0.089202880859375
overhead4:: 1.5720124244689941
overhead5:: 0
memory usage:: 3026051072
time_provenance:: 3.3053245544433594
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02369236946105957
overhead3:: 0.10142230987548828
overhead4:: 2.2395386695861816
overhead5:: 0
memory usage:: 3025457152
time_provenance:: 4.073289155960083
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0690, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0690, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02483963966369629
overhead3:: 0.10510778427124023
overhead4:: 2.354926109313965
overhead5:: 0
memory usage:: 3032154112
time_provenance:: 4.185839414596558
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0690, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0690, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.02644062042236328
overhead3:: 0.10866713523864746
overhead4:: 2.556621551513672
overhead5:: 0
memory usage:: 3013357568
time_provenance:: 4.442705392837524
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0690, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0690, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.2 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12000
max_epoch:: 60
overhead:: 0
overhead2:: 0.04240822792053223
overhead3:: 0.1475517749786377
overhead4:: 3.8758342266082764
overhead5:: 0
memory usage:: 3005616128
time_provenance:: 5.948324918746948
curr_diff: 0 tensor(1.5092e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5092e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0693, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0693, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000056, Accuracy: 0.873800
