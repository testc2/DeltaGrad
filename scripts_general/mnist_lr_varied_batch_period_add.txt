period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression MNIST5 16384 120 5
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  MNIST5 1
tensor([43661])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.309013
Train - Epoch 1, Batch: 0, Loss: 1.935851
Train - Epoch 2, Batch: 0, Loss: 1.660786
Train - Epoch 3, Batch: 0, Loss: 1.459530
Train - Epoch 4, Batch: 0, Loss: 1.308374
Train - Epoch 5, Batch: 0, Loss: 1.196649
Train - Epoch 6, Batch: 0, Loss: 1.107970
Train - Epoch 7, Batch: 0, Loss: 1.034634
Train - Epoch 8, Batch: 0, Loss: 0.977920
Train - Epoch 9, Batch: 0, Loss: 0.929253
Train - Epoch 10, Batch: 0, Loss: 0.883290
Train - Epoch 11, Batch: 0, Loss: 0.854763
Train - Epoch 12, Batch: 0, Loss: 0.818191
Train - Epoch 13, Batch: 0, Loss: 0.797669
Train - Epoch 14, Batch: 0, Loss: 0.766400
Train - Epoch 15, Batch: 0, Loss: 0.749383
Train - Epoch 16, Batch: 0, Loss: 0.729587
Train - Epoch 17, Batch: 0, Loss: 0.709430
Train - Epoch 18, Batch: 0, Loss: 0.698377
Train - Epoch 19, Batch: 0, Loss: 0.682170
Train - Epoch 20, Batch: 0, Loss: 0.678728
Train - Epoch 21, Batch: 0, Loss: 0.659303
Train - Epoch 22, Batch: 0, Loss: 0.651349
Train - Epoch 23, Batch: 0, Loss: 0.637748
Train - Epoch 24, Batch: 0, Loss: 0.635038
Train - Epoch 25, Batch: 0, Loss: 0.616117
Train - Epoch 26, Batch: 0, Loss: 0.610128
Train - Epoch 27, Batch: 0, Loss: 0.610190
Train - Epoch 28, Batch: 0, Loss: 0.603821
Train - Epoch 29, Batch: 0, Loss: 0.595543
Train - Epoch 30, Batch: 0, Loss: 0.581720
Train - Epoch 31, Batch: 0, Loss: 0.578105
training_time:: 3.4395976066589355
training time full:: 3.4396495819091797
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1
training time is 2.589717149734497
overhead:: 0
overhead2:: 0
time_baseline:: 2.589962959289551
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0010445117950439453
overhead3:: 0.020869970321655273
overhead4:: 0.30203771591186523
overhead5:: 0
time_provenance:: 0.42617011070251465
curr_diff: 0 tensor(5.2346e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2346e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0020716190338134766
overhead3:: 0.027891159057617188
overhead4:: 0.5443680286407471
overhead5:: 0
time_provenance:: 0.7050015926361084
curr_diff: 0 tensor(2.9294e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9294e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0024919509887695312
overhead3:: 0.03732728958129883
overhead4:: 0.6406552791595459
overhead5:: 0
time_provenance:: 0.7989981174468994
curr_diff: 0 tensor(4.3137e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3137e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003468751907348633
overhead3:: 0.040825843811035156
overhead4:: 0.8840019702911377
overhead5:: 0
time_provenance:: 1.068345546722412
curr_diff: 0 tensor(2.1503e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1503e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004140138626098633
overhead3:: 0.04944634437561035
overhead4:: 1.0166893005371094
overhead5:: 0
time_provenance:: 1.2064003944396973
curr_diff: 0 tensor(2.4660e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4660e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0017216205596923828
overhead3:: 0.0245664119720459
overhead4:: 0.4607217311859131
overhead5:: 0
time_provenance:: 0.6076309680938721
curr_diff: 0 tensor(3.1127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1127e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0018947124481201172
overhead3:: 0.031905412673950195
overhead4:: 0.6084692478179932
overhead5:: 0
time_provenance:: 0.7727601528167725
curr_diff: 0 tensor(2.8737e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8737e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0034945011138916016
overhead3:: 0.04150533676147461
overhead4:: 0.7585372924804688
overhead5:: 0
time_provenance:: 0.938859224319458
curr_diff: 0 tensor(2.7766e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7766e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003426790237426758
overhead3:: 0.04558205604553223
overhead4:: 0.9194843769073486
overhead5:: 0
time_provenance:: 1.1096806526184082
curr_diff: 0 tensor(2.2738e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2738e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.00401759147644043
overhead3:: 0.05397987365722656
overhead4:: 1.0978286266326904
overhead5:: 0
time_provenance:: 1.304241418838501
curr_diff: 0 tensor(2.1851e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1851e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0015194416046142578
overhead3:: 0.03487062454223633
overhead4:: 0.6893095970153809
overhead5:: 0
time_provenance:: 0.9108974933624268
curr_diff: 0 tensor(7.5154e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5154e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0019211769104003906
overhead3:: 0.04620623588562012
overhead4:: 0.8516097068786621
overhead5:: 0
time_provenance:: 1.0824635028839111
curr_diff: 0 tensor(7.2069e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2069e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0029366016387939453
overhead3:: 0.048035621643066406
overhead4:: 0.9901134967803955
overhead5:: 0
time_provenance:: 1.2296009063720703
curr_diff: 0 tensor(7.0070e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0070e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003415346145629883
overhead3:: 0.052520036697387695
overhead4:: 1.106137990951538
overhead5:: 0
time_provenance:: 1.3568980693817139
curr_diff: 0 tensor(6.5372e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5372e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004106760025024414
overhead3:: 0.05953478813171387
overhead4:: 1.2847437858581543
overhead5:: 0
time_provenance:: 1.5340845584869385
curr_diff: 0 tensor(6.0617e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0617e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0055048465728759766
overhead3:: 0.0593724250793457
overhead4:: 1.2890172004699707
overhead5:: 0
time_provenance:: 1.6772089004516602
curr_diff: 0 tensor(1.9940e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9940e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005827665328979492
overhead3:: 0.07193613052368164
overhead4:: 1.3826451301574707
overhead5:: 0
time_provenance:: 1.7670550346374512
curr_diff: 0 tensor(1.9324e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9324e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.006665706634521484
overhead3:: 0.06638312339782715
overhead4:: 1.4878230094909668
overhead5:: 0
time_provenance:: 1.8607866764068604
curr_diff: 0 tensor(1.8780e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8780e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.00661015510559082
overhead3:: 0.07446050643920898
overhead4:: 1.5822710990905762
overhead5:: 0
time_provenance:: 1.9502124786376953
curr_diff: 0 tensor(1.6585e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6585e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007391452789306641
overhead3:: 0.07843947410583496
overhead4:: 1.6682560443878174
overhead5:: 0
time_provenance:: 2.0252938270568848
curr_diff: 0 tensor(1.6103e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6103e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.00975799560546875
overhead3:: 0.1066896915435791
overhead4:: 2.347437620162964
overhead5:: 0
time_provenance:: 2.6391921043395996
curr_diff: 0 tensor(5.5711e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5711e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.335536
Train - Epoch 1, Batch: 0, Loss: 1.947438
Train - Epoch 2, Batch: 0, Loss: 1.668316
Train - Epoch 3, Batch: 0, Loss: 1.459280
Train - Epoch 4, Batch: 0, Loss: 1.317914
Train - Epoch 5, Batch: 0, Loss: 1.194309
Train - Epoch 6, Batch: 0, Loss: 1.109575
Train - Epoch 7, Batch: 0, Loss: 1.036822
Train - Epoch 8, Batch: 0, Loss: 0.978806
Train - Epoch 9, Batch: 0, Loss: 0.927754
Train - Epoch 10, Batch: 0, Loss: 0.887080
Train - Epoch 11, Batch: 0, Loss: 0.845036
Train - Epoch 12, Batch: 0, Loss: 0.816354
Train - Epoch 13, Batch: 0, Loss: 0.791728
Train - Epoch 14, Batch: 0, Loss: 0.774002
Train - Epoch 15, Batch: 0, Loss: 0.749085
Train - Epoch 16, Batch: 0, Loss: 0.728534
Train - Epoch 17, Batch: 0, Loss: 0.711336
Train - Epoch 18, Batch: 0, Loss: 0.697286
Train - Epoch 19, Batch: 0, Loss: 0.686220
Train - Epoch 20, Batch: 0, Loss: 0.673828
Train - Epoch 21, Batch: 0, Loss: 0.657681
Train - Epoch 22, Batch: 0, Loss: 0.647668
Train - Epoch 23, Batch: 0, Loss: 0.639817
Train - Epoch 24, Batch: 0, Loss: 0.633381
Train - Epoch 25, Batch: 0, Loss: 0.627265
Train - Epoch 26, Batch: 0, Loss: 0.606351
Train - Epoch 27, Batch: 0, Loss: 0.596677
Train - Epoch 28, Batch: 0, Loss: 0.598044
Train - Epoch 29, Batch: 0, Loss: 0.593169
Train - Epoch 30, Batch: 0, Loss: 0.591199
Train - Epoch 31, Batch: 0, Loss: 0.567787
training_time:: 3.392435312271118
training time full:: 3.3924825191497803
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1
training time is 2.5705478191375732
overhead:: 0
overhead2:: 0
time_baseline:: 2.57077693939209
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0015625953674316406
overhead3:: 0.021209001541137695
overhead4:: 0.30058765411376953
overhead5:: 0
time_provenance:: 0.4256322383880615
curr_diff: 0 tensor(4.4067e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4067e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002312183380126953
overhead3:: 0.026476621627807617
overhead4:: 0.5326766967773438
overhead5:: 0
time_provenance:: 0.691960334777832
curr_diff: 0 tensor(5.0291e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0291e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002830982208251953
overhead3:: 0.035902976989746094
overhead4:: 0.6519169807434082
overhead5:: 0
time_provenance:: 0.8131511211395264
curr_diff: 0 tensor(5.1444e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1444e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003975629806518555
overhead3:: 0.042540788650512695
overhead4:: 0.8999395370483398
overhead5:: 0
time_provenance:: 1.094848394393921
curr_diff: 0 tensor(4.5006e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5006e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004080533981323242
overhead3:: 0.05058002471923828
overhead4:: 0.9920341968536377
overhead5:: 0
time_provenance:: 1.1851623058319092
curr_diff: 0 tensor(5.5023e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5023e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0018362998962402344
overhead3:: 0.024577617645263672
overhead4:: 0.44350409507751465
overhead5:: 0
time_provenance:: 0.5933845043182373
curr_diff: 0 tensor(3.2614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002766847610473633
overhead3:: 0.03370475769042969
overhead4:: 0.6386442184448242
overhead5:: 0
time_provenance:: 0.8134703636169434
curr_diff: 0 tensor(3.3314e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3314e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002841472625732422
overhead3:: 0.03987383842468262
overhead4:: 0.7589616775512695
overhead5:: 0
time_provenance:: 0.9356920719146729
curr_diff: 0 tensor(2.9601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004183292388916016
overhead3:: 0.046439170837402344
overhead4:: 0.9213311672210693
overhead5:: 0
time_provenance:: 1.1121642589569092
curr_diff: 0 tensor(3.0608e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0608e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004785299301147461
overhead3:: 0.055077314376831055
overhead4:: 1.0880956649780273
overhead5:: 0
time_provenance:: 1.2942867279052734
curr_diff: 0 tensor(1.9668e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9668e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0024814605712890625
overhead3:: 0.03418993949890137
overhead4:: 0.7023172378540039
overhead5:: 0
time_provenance:: 0.9250521659851074
curr_diff: 0 tensor(1.1662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002981901168823242
overhead3:: 0.03975415229797363
overhead4:: 0.8509631156921387
overhead5:: 0
time_provenance:: 1.0868597030639648
curr_diff: 0 tensor(1.2138e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2138e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0033960342407226562
overhead3:: 0.04821944236755371
overhead4:: 1.0084359645843506
overhead5:: 0
time_provenance:: 1.24741792678833
curr_diff: 0 tensor(1.0946e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0946e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004263639450073242
overhead3:: 0.05243372917175293
overhead4:: 1.1567082405090332
overhead5:: 0
time_provenance:: 1.4002463817596436
curr_diff: 0 tensor(1.1465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004488229751586914
overhead3:: 0.05842709541320801
overhead4:: 1.2780542373657227
overhead5:: 0
time_provenance:: 1.5255544185638428
curr_diff: 0 tensor(9.6374e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6374e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005685091018676758
overhead3:: 0.06354594230651855
overhead4:: 1.3275165557861328
overhead5:: 0
time_provenance:: 1.7217292785644531
curr_diff: 0 tensor(2.6251e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6251e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005867719650268555
overhead3:: 0.0683434009552002
overhead4:: 1.375251054763794
overhead5:: 0
time_provenance:: 1.7590844631195068
curr_diff: 0 tensor(2.4880e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4880e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0066127777099609375
overhead3:: 0.06557154655456543
overhead4:: 1.47499680519104
overhead5:: 0
time_provenance:: 1.8491370677947998
curr_diff: 0 tensor(2.2831e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2831e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.006616115570068359
overhead3:: 0.07157325744628906
overhead4:: 1.5919504165649414
overhead5:: 0
time_provenance:: 1.9573960304260254
curr_diff: 0 tensor(2.3091e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3091e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007964134216308594
overhead3:: 0.07315301895141602
overhead4:: 1.6992981433868408
overhead5:: 0
time_provenance:: 2.051154375076294
curr_diff: 0 tensor(2.1252e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1252e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.009899616241455078
overhead3:: 0.10750389099121094
overhead4:: 2.3539414405822754
overhead5:: 0
time_provenance:: 2.6499645709991455
curr_diff: 0 tensor(5.5246e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5246e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.352385
Train - Epoch 1, Batch: 0, Loss: 1.961150
Train - Epoch 2, Batch: 0, Loss: 1.683371
Train - Epoch 3, Batch: 0, Loss: 1.477669
Train - Epoch 4, Batch: 0, Loss: 1.319171
Train - Epoch 5, Batch: 0, Loss: 1.205109
Train - Epoch 6, Batch: 0, Loss: 1.108883
Train - Epoch 7, Batch: 0, Loss: 1.038303
Train - Epoch 8, Batch: 0, Loss: 0.977019
Train - Epoch 9, Batch: 0, Loss: 0.931529
Train - Epoch 10, Batch: 0, Loss: 0.894415
Train - Epoch 11, Batch: 0, Loss: 0.850701
Train - Epoch 12, Batch: 0, Loss: 0.821988
Train - Epoch 13, Batch: 0, Loss: 0.799992
Train - Epoch 14, Batch: 0, Loss: 0.771626
Train - Epoch 15, Batch: 0, Loss: 0.747440
Train - Epoch 16, Batch: 0, Loss: 0.729161
Train - Epoch 17, Batch: 0, Loss: 0.719792
Train - Epoch 18, Batch: 0, Loss: 0.697201
Train - Epoch 19, Batch: 0, Loss: 0.676311
Train - Epoch 20, Batch: 0, Loss: 0.673954
Train - Epoch 21, Batch: 0, Loss: 0.662746
Train - Epoch 22, Batch: 0, Loss: 0.658540
Train - Epoch 23, Batch: 0, Loss: 0.638371
Train - Epoch 24, Batch: 0, Loss: 0.630210
Train - Epoch 25, Batch: 0, Loss: 0.623142
Train - Epoch 26, Batch: 0, Loss: 0.607751
Train - Epoch 27, Batch: 0, Loss: 0.618542
Train - Epoch 28, Batch: 0, Loss: 0.592647
Train - Epoch 29, Batch: 0, Loss: 0.599580
Train - Epoch 30, Batch: 0, Loss: 0.584485
Train - Epoch 31, Batch: 0, Loss: 0.578987
training_time:: 3.388173818588257
training time full:: 3.388219118118286
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1
training time is 2.53122878074646
overhead:: 0
overhead2:: 0
time_baseline:: 2.531449794769287
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0013926029205322266
overhead3:: 0.018004894256591797
overhead4:: 0.3225991725921631
overhead5:: 0
time_provenance:: 0.4628117084503174
curr_diff: 0 tensor(4.8371e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8371e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003130197525024414
overhead3:: 0.027753591537475586
overhead4:: 0.5353143215179443
overhead5:: 0
time_provenance:: 0.6956071853637695
curr_diff: 0 tensor(3.5105e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5105e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0030045509338378906
overhead3:: 0.03469252586364746
overhead4:: 0.6707420349121094
overhead5:: 0
time_provenance:: 0.8457295894622803
curr_diff: 0 tensor(3.7323e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7323e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004554033279418945
overhead3:: 0.03887486457824707
overhead4:: 0.8616206645965576
overhead5:: 0
time_provenance:: 1.0518467426300049
curr_diff: 0 tensor(2.9640e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9640e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004426717758178711
overhead3:: 0.050458669662475586
overhead4:: 1.0289874076843262
overhead5:: 0
time_provenance:: 1.2395036220550537
curr_diff: 0 tensor(1.8562e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8562e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002653360366821289
overhead3:: 0.023894786834716797
overhead4:: 0.4582793712615967
overhead5:: 0
time_provenance:: 0.621884822845459
curr_diff: 0 tensor(2.4793e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4793e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0031523704528808594
overhead3:: 0.030632734298706055
overhead4:: 0.6482133865356445
overhead5:: 0
time_provenance:: 0.8297367095947266
curr_diff: 0 tensor(2.1134e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1134e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0037107467651367188
overhead3:: 0.03797101974487305
overhead4:: 0.8414452075958252
overhead5:: 0
time_provenance:: 1.0359632968902588
curr_diff: 0 tensor(2.2819e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2819e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004042387008666992
overhead3:: 0.045165061950683594
overhead4:: 0.9472603797912598
overhead5:: 0
time_provenance:: 1.1478095054626465
curr_diff: 0 tensor(2.1207e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1207e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004909992218017578
overhead3:: 0.05248904228210449
overhead4:: 1.1329667568206787
overhead5:: 0
time_provenance:: 1.3508837223052979
curr_diff: 0 tensor(9.1725e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1725e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002973318099975586
overhead3:: 0.033365488052368164
overhead4:: 0.6714599132537842
overhead5:: 0
time_provenance:: 0.8951005935668945
curr_diff: 0 tensor(8.5410e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5410e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0032701492309570312
overhead3:: 0.04323101043701172
overhead4:: 0.8574497699737549
overhead5:: 0
time_provenance:: 1.0884504318237305
curr_diff: 0 tensor(7.9890e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9890e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0040280818939208984
overhead3:: 0.047525882720947266
overhead4:: 0.9920120239257812
overhead5:: 0
time_provenance:: 1.2294747829437256
curr_diff: 0 tensor(7.8926e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8926e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004456758499145508
overhead3:: 0.054068803787231445
overhead4:: 1.1392979621887207
overhead5:: 0
time_provenance:: 1.388631820678711
curr_diff: 0 tensor(8.0690e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0690e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005205631256103516
overhead3:: 0.059105634689331055
overhead4:: 1.2728664875030518
overhead5:: 0
time_provenance:: 1.5257799625396729
curr_diff: 0 tensor(7.7584e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7584e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007416963577270508
overhead3:: 0.05600285530090332
overhead4:: 1.438507080078125
overhead5:: 0
time_provenance:: 1.8821220397949219
curr_diff: 0 tensor(2.2812e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2812e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007787942886352539
overhead3:: 0.06285452842712402
overhead4:: 1.5258684158325195
overhead5:: 0
time_provenance:: 1.9568588733673096
curr_diff: 0 tensor(2.1641e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1641e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.008203983306884766
overhead3:: 0.06464195251464844
overhead4:: 1.6117231845855713
overhead5:: 0
time_provenance:: 2.0285069942474365
curr_diff: 0 tensor(2.1891e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1891e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007810354232788086
overhead3:: 0.07260322570800781
overhead4:: 1.6736884117126465
overhead5:: 0
time_provenance:: 2.0843586921691895
curr_diff: 0 tensor(2.0871e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0871e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.008642911911010742
overhead3:: 0.07255315780639648
overhead4:: 1.8037936687469482
overhead5:: 0
time_provenance:: 2.1984751224517822
curr_diff: 0 tensor(2.0108e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0108e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.009955644607543945
overhead3:: 0.10582876205444336
overhead4:: 2.334954023361206
overhead5:: 0
time_provenance:: 2.634312391281128
curr_diff: 0 tensor(6.0839e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0839e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.334790
Train - Epoch 1, Batch: 0, Loss: 1.950694
Train - Epoch 2, Batch: 0, Loss: 1.670835
Train - Epoch 3, Batch: 0, Loss: 1.463325
Train - Epoch 4, Batch: 0, Loss: 1.310955
Train - Epoch 5, Batch: 0, Loss: 1.191692
Train - Epoch 6, Batch: 0, Loss: 1.107613
Train - Epoch 7, Batch: 0, Loss: 1.028296
Train - Epoch 8, Batch: 0, Loss: 0.972180
Train - Epoch 9, Batch: 0, Loss: 0.919330
Train - Epoch 10, Batch: 0, Loss: 0.884801
Train - Epoch 11, Batch: 0, Loss: 0.853735
Train - Epoch 12, Batch: 0, Loss: 0.813495
Train - Epoch 13, Batch: 0, Loss: 0.787208
Train - Epoch 14, Batch: 0, Loss: 0.768082
Train - Epoch 15, Batch: 0, Loss: 0.748875
Train - Epoch 16, Batch: 0, Loss: 0.725827
Train - Epoch 17, Batch: 0, Loss: 0.714368
Train - Epoch 18, Batch: 0, Loss: 0.704912
Train - Epoch 19, Batch: 0, Loss: 0.677761
Train - Epoch 20, Batch: 0, Loss: 0.667611
Train - Epoch 21, Batch: 0, Loss: 0.663424
Train - Epoch 22, Batch: 0, Loss: 0.648540
Train - Epoch 23, Batch: 0, Loss: 0.632490
Train - Epoch 24, Batch: 0, Loss: 0.631507
Train - Epoch 25, Batch: 0, Loss: 0.609915
Train - Epoch 26, Batch: 0, Loss: 0.615075
Train - Epoch 27, Batch: 0, Loss: 0.601891
Train - Epoch 28, Batch: 0, Loss: 0.602317
Train - Epoch 29, Batch: 0, Loss: 0.590365
Train - Epoch 30, Batch: 0, Loss: 0.586176
Train - Epoch 31, Batch: 0, Loss: 0.580783
training_time:: 3.4028937816619873
training time full:: 3.4029407501220703
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1
training time is 2.548238754272461
overhead:: 0
overhead2:: 0
time_baseline:: 2.5484771728515625
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.001811981201171875
overhead3:: 0.019510984420776367
overhead4:: 0.3365349769592285
overhead5:: 0
time_provenance:: 0.47690534591674805
curr_diff: 0 tensor(4.9166e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9166e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0024263858795166016
overhead3:: 0.027215003967285156
overhead4:: 0.5212931632995605
overhead5:: 0
time_provenance:: 0.6806268692016602
curr_diff: 0 tensor(3.1349e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1349e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0028481483459472656
overhead3:: 0.034642696380615234
overhead4:: 0.6814358234405518
overhead5:: 0
time_provenance:: 0.8520710468292236
curr_diff: 0 tensor(4.4949e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4949e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0035927295684814453
overhead3:: 0.041115760803222656
overhead4:: 0.8709120750427246
overhead5:: 0
time_provenance:: 1.0566799640655518
curr_diff: 0 tensor(2.5565e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5565e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004565238952636719
overhead3:: 0.050041913986206055
overhead4:: 1.0427913665771484
overhead5:: 0
time_provenance:: 1.2496168613433838
curr_diff: 0 tensor(3.6581e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6581e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0024111270904541016
overhead3:: 0.023521900177001953
overhead4:: 0.48152804374694824
overhead5:: 0
time_provenance:: 0.6414566040039062
curr_diff: 0 tensor(3.2439e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2439e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0037593841552734375
overhead3:: 0.031207561492919922
overhead4:: 0.6396353244781494
overhead5:: 0
time_provenance:: 0.8164985179901123
curr_diff: 0 tensor(2.5687e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5687e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0032465457916259766
overhead3:: 0.03722977638244629
overhead4:: 0.8142983913421631
overhead5:: 0
time_provenance:: 1.0027985572814941
curr_diff: 0 tensor(2.2754e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2754e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0036940574645996094
overhead3:: 0.045223236083984375
overhead4:: 0.9524285793304443
overhead5:: 0
time_provenance:: 1.1516139507293701
curr_diff: 0 tensor(2.5642e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5642e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005342721939086914
overhead3:: 0.0535283088684082
overhead4:: 1.1311168670654297
overhead5:: 0
time_provenance:: 1.3422017097473145
curr_diff: 0 tensor(2.1095e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1095e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0025510787963867188
overhead3:: 0.0350642204284668
overhead4:: 0.708925724029541
overhead5:: 0
time_provenance:: 0.9325339794158936
curr_diff: 0 tensor(8.5611e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5611e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003241300582885742
overhead3:: 0.04118227958679199
overhead4:: 0.8584439754486084
overhead5:: 0
time_provenance:: 1.089587926864624
curr_diff: 0 tensor(8.2432e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2432e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003471851348876953
overhead3:: 0.04728507995605469
overhead4:: 1.0105390548706055
overhead5:: 0
time_provenance:: 1.2493999004364014
curr_diff: 0 tensor(8.1960e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1960e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0044367313385009766
overhead3:: 0.053827524185180664
overhead4:: 1.1581664085388184
overhead5:: 0
time_provenance:: 1.4021730422973633
curr_diff: 0 tensor(8.1829e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1829e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.00508880615234375
overhead3:: 0.059853315353393555
overhead4:: 1.2733190059661865
overhead5:: 0
time_provenance:: 1.522834062576294
curr_diff: 0 tensor(8.5750e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5750e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005321979522705078
overhead3:: 0.05926370620727539
overhead4:: 1.4781537055969238
overhead5:: 0
time_provenance:: 1.9252002239227295
curr_diff: 0 tensor(1.9949e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9949e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0061914920806884766
overhead3:: 0.06105852127075195
overhead4:: 1.5835037231445312
overhead5:: 0
time_provenance:: 2.0248336791992188
curr_diff: 0 tensor(1.6486e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6486e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.006562232971191406
overhead3:: 0.06804013252258301
overhead4:: 1.6766624450683594
overhead5:: 0
time_provenance:: 2.104598045349121
curr_diff: 0 tensor(1.5073e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5073e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.007442951202392578
overhead3:: 0.07019782066345215
overhead4:: 1.6994054317474365
overhead5:: 0
time_provenance:: 2.107671022415161
curr_diff: 0 tensor(1.3662e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3662e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0073320865631103516
overhead3:: 0.0739743709564209
overhead4:: 1.7934319972991943
overhead5:: 0
time_provenance:: 2.1936988830566406
curr_diff: 0 tensor(1.3465e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3465e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.009563684463500977
overhead3:: 0.10994672775268555
overhead4:: 2.375645160675049
overhead5:: 0
time_provenance:: 2.6681675910949707
curr_diff: 0 tensor(5.3544e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3544e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.336001
Train - Epoch 1, Batch: 0, Loss: 1.954979
Train - Epoch 2, Batch: 0, Loss: 1.677331
Train - Epoch 3, Batch: 0, Loss: 1.469458
Train - Epoch 4, Batch: 0, Loss: 1.318620
Train - Epoch 5, Batch: 0, Loss: 1.200517
Train - Epoch 6, Batch: 0, Loss: 1.109285
Train - Epoch 7, Batch: 0, Loss: 1.031094
Train - Epoch 8, Batch: 0, Loss: 0.973188
Train - Epoch 9, Batch: 0, Loss: 0.930803
Train - Epoch 10, Batch: 0, Loss: 0.882653
Train - Epoch 11, Batch: 0, Loss: 0.845385
Train - Epoch 12, Batch: 0, Loss: 0.808063
Train - Epoch 13, Batch: 0, Loss: 0.793111
Train - Epoch 14, Batch: 0, Loss: 0.762382
Train - Epoch 15, Batch: 0, Loss: 0.744971
Train - Epoch 16, Batch: 0, Loss: 0.729019
Train - Epoch 17, Batch: 0, Loss: 0.712313
Train - Epoch 18, Batch: 0, Loss: 0.701776
Train - Epoch 19, Batch: 0, Loss: 0.683475
Train - Epoch 20, Batch: 0, Loss: 0.676211
Train - Epoch 21, Batch: 0, Loss: 0.654809
Train - Epoch 22, Batch: 0, Loss: 0.647640
Train - Epoch 23, Batch: 0, Loss: 0.642928
Train - Epoch 24, Batch: 0, Loss: 0.626714
Train - Epoch 25, Batch: 0, Loss: 0.617769
Train - Epoch 26, Batch: 0, Loss: 0.615161
Train - Epoch 27, Batch: 0, Loss: 0.605919
Train - Epoch 28, Batch: 0, Loss: 0.608806
Train - Epoch 29, Batch: 0, Loss: 0.593030
Train - Epoch 30, Batch: 0, Loss: 0.585728
Train - Epoch 31, Batch: 0, Loss: 0.577259
training_time:: 3.382826566696167
training time full:: 3.3828718662261963
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.874600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1
training time is 2.5392189025878906
overhead:: 0
overhead2:: 0
time_baseline:: 2.5394351482391357
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0009806156158447266
overhead3:: 0.016722440719604492
overhead4:: 0.3464818000793457
overhead5:: 0
time_provenance:: 0.4804229736328125
curr_diff: 0 tensor(6.0938e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0938e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0027112960815429688
overhead3:: 0.02715015411376953
overhead4:: 0.475508451461792
overhead5:: 0
time_provenance:: 0.6157715320587158
curr_diff: 0 tensor(2.8713e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8713e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.003023386001586914
overhead3:: 0.030699491500854492
overhead4:: 0.6827518939971924
overhead5:: 0
time_provenance:: 0.8459339141845703
curr_diff: 0 tensor(5.1281e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1281e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004182577133178711
overhead3:: 0.041327476501464844
overhead4:: 0.8319516181945801
overhead5:: 0
time_provenance:: 1.0006864070892334
curr_diff: 0 tensor(2.3271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3271e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004087209701538086
overhead3:: 0.04772520065307617
overhead4:: 1.0240306854248047
overhead5:: 0
time_provenance:: 1.2173113822937012
curr_diff: 0 tensor(3.3423e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3423e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0017726421356201172
overhead3:: 0.024066448211669922
overhead4:: 0.44854736328125
overhead5:: 0
time_provenance:: 0.6009347438812256
curr_diff: 0 tensor(2.4668e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4668e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.002716064453125
overhead3:: 0.030820608139038086
overhead4:: 0.5865392684936523
overhead5:: 0
time_provenance:: 0.7450647354125977
curr_diff: 0 tensor(2.2205e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2205e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0034694671630859375
overhead3:: 0.0365757942199707
overhead4:: 0.7561681270599365
overhead5:: 0
time_provenance:: 0.9296631813049316
curr_diff: 0 tensor(1.9205e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9205e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0041921138763427734
overhead3:: 0.04402017593383789
overhead4:: 0.9252321720123291
overhead5:: 0
time_provenance:: 1.1101748943328857
curr_diff: 0 tensor(2.0669e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0669e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0047571659088134766
overhead3:: 0.051593780517578125
overhead4:: 1.1094179153442383
overhead5:: 0
time_provenance:: 1.3028004169464111
curr_diff: 0 tensor(1.6916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0023276805877685547
overhead3:: 0.03184771537780762
overhead4:: 0.6831557750701904
overhead5:: 0
time_provenance:: 0.8907501697540283
curr_diff: 0 tensor(1.1327e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1327e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0035729408264160156
overhead3:: 0.03761911392211914
overhead4:: 0.831794023513794
overhead5:: 0
time_provenance:: 1.050074577331543
curr_diff: 0 tensor(1.1416e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1416e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004511117935180664
overhead3:: 0.04453015327453613
overhead4:: 0.9732589721679688
overhead5:: 0
time_provenance:: 1.199359655380249
curr_diff: 0 tensor(1.1135e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1135e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.004835844039916992
overhead3:: 0.050406694412231445
overhead4:: 1.107865333557129
overhead5:: 0
time_provenance:: 1.3369638919830322
curr_diff: 0 tensor(1.0664e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0664e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.005534648895263672
overhead3:: 0.05767107009887695
overhead4:: 1.276639699935913
overhead5:: 0
time_provenance:: 1.5183734893798828
curr_diff: 0 tensor(1.0423e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0423e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.0076220035552978516
overhead3:: 0.05751991271972656
overhead4:: 1.265885829925537
overhead5:: 0
time_provenance:: 1.6389784812927246
curr_diff: 0 tensor(1.6496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.00750279426574707
overhead3:: 0.06104731559753418
overhead4:: 1.3486964702606201
overhead5:: 0
time_provenance:: 1.7144441604614258
curr_diff: 0 tensor(1.6263e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6263e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.008471488952636719
overhead3:: 0.0646824836730957
overhead4:: 1.4768991470336914
overhead5:: 0
time_provenance:: 1.839320182800293
curr_diff: 0 tensor(1.5572e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5572e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.008731365203857422
overhead3:: 0.07002139091491699
overhead4:: 1.5554313659667969
overhead5:: 0
time_provenance:: 1.9075124263763428
curr_diff: 0 tensor(1.4155e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4155e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.009518623352050781
overhead3:: 0.07085871696472168
overhead4:: 1.6814477443695068
overhead5:: 0
time_provenance:: 2.020875930786133
curr_diff: 0 tensor(1.3198e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3198e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
overhead:: 0
overhead2:: 0.010356664657592773
overhead3:: 0.09957075119018555
overhead4:: 2.351290225982666
overhead5:: 0
time_provenance:: 2.6366617679595947
curr_diff: 0 tensor(6.5910e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5910e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  MNIST5 0
tensor([25872, 39469, 43661])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.325149
Train - Epoch 1, Batch: 0, Loss: 1.931189
Train - Epoch 2, Batch: 0, Loss: 1.663759
Train - Epoch 3, Batch: 0, Loss: 1.458263
Train - Epoch 4, Batch: 0, Loss: 1.311572
Train - Epoch 5, Batch: 0, Loss: 1.198500
Train - Epoch 6, Batch: 0, Loss: 1.107333
Train - Epoch 7, Batch: 0, Loss: 1.035468
Train - Epoch 8, Batch: 0, Loss: 0.981879
Train - Epoch 9, Batch: 0, Loss: 0.936471
Train - Epoch 10, Batch: 0, Loss: 0.891686
Train - Epoch 11, Batch: 0, Loss: 0.855483
Train - Epoch 12, Batch: 0, Loss: 0.827973
Train - Epoch 13, Batch: 0, Loss: 0.797381
Train - Epoch 14, Batch: 0, Loss: 0.773636
Train - Epoch 15, Batch: 0, Loss: 0.748050
Train - Epoch 16, Batch: 0, Loss: 0.739350
Train - Epoch 17, Batch: 0, Loss: 0.721244
Train - Epoch 18, Batch: 0, Loss: 0.702533
Train - Epoch 19, Batch: 0, Loss: 0.689304
Train - Epoch 20, Batch: 0, Loss: 0.666773
Train - Epoch 21, Batch: 0, Loss: 0.653208
Train - Epoch 22, Batch: 0, Loss: 0.654488
Train - Epoch 23, Batch: 0, Loss: 0.646844
Train - Epoch 24, Batch: 0, Loss: 0.628071
Train - Epoch 25, Batch: 0, Loss: 0.620917
Train - Epoch 26, Batch: 0, Loss: 0.609916
Train - Epoch 27, Batch: 0, Loss: 0.601903
Train - Epoch 28, Batch: 0, Loss: 0.605957
Train - Epoch 29, Batch: 0, Loss: 0.591823
Train - Epoch 30, Batch: 0, Loss: 0.591078
Train - Epoch 31, Batch: 0, Loss: 0.575866
training_time:: 3.4605133533477783
training time full:: 3.460559606552124
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.874800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 3
training time is 2.5436782836914062
overhead:: 0
overhead2:: 0
time_baseline:: 2.5439043045043945
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00334930419921875
overhead3:: 0.01886272430419922
overhead4:: 0.33005356788635254
overhead5:: 0
time_provenance:: 0.5090799331665039
curr_diff: 0 tensor(9.0857e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0857e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004896879196166992
overhead3:: 0.02671980857849121
overhead4:: 0.5173373222351074
overhead5:: 0
time_provenance:: 0.7303833961486816
curr_diff: 0 tensor(7.5024e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5024e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0064697265625
overhead3:: 0.033226966857910156
overhead4:: 0.6834852695465088
overhead5:: 0
time_provenance:: 0.9162590503692627
curr_diff: 0 tensor(8.7522e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7522e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008175373077392578
overhead3:: 0.04130268096923828
overhead4:: 0.8562333583831787
overhead5:: 0
time_provenance:: 1.1209959983825684
curr_diff: 0 tensor(4.6140e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6140e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010070323944091797
overhead3:: 0.050191402435302734
overhead4:: 1.0676805973052979
overhead5:: 0
time_provenance:: 1.359119176864624
curr_diff: 0 tensor(8.0805e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0805e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004485607147216797
overhead3:: 0.02329874038696289
overhead4:: 0.4637753963470459
overhead5:: 0
time_provenance:: 0.6624865531921387
curr_diff: 0 tensor(5.0777e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0777e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005720853805541992
overhead3:: 0.03088831901550293
overhead4:: 0.6275889873504639
overhead5:: 0
time_provenance:: 0.8615689277648926
curr_diff: 0 tensor(4.9243e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9243e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007578611373901367
overhead3:: 0.03820538520812988
overhead4:: 0.7799575328826904
overhead5:: 0
time_provenance:: 1.0337002277374268
curr_diff: 0 tensor(5.2645e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2645e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009475469589233398
overhead3:: 0.04462146759033203
overhead4:: 0.9749805927276611
overhead5:: 0
time_provenance:: 1.2656941413879395
curr_diff: 0 tensor(4.6075e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6075e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011125564575195312
overhead3:: 0.05035400390625
overhead4:: 1.115307331085205
overhead5:: 0
time_provenance:: 1.4142436981201172
curr_diff: 0 tensor(3.6940e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6940e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006441593170166016
overhead3:: 0.033849239349365234
overhead4:: 0.6698360443115234
overhead5:: 0
time_provenance:: 0.9290790557861328
curr_diff: 0 tensor(1.6266e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6266e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008664608001708984
overhead3:: 0.041773319244384766
overhead4:: 0.8683311939239502
overhead5:: 0
time_provenance:: 1.1512351036071777
curr_diff: 0 tensor(1.5462e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5462e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008554458618164062
overhead3:: 0.0462493896484375
overhead4:: 0.9615414142608643
overhead5:: 0
time_provenance:: 1.2546312808990479
curr_diff: 0 tensor(1.5079e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5079e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01097249984741211
overhead3:: 0.051406145095825195
overhead4:: 1.0973436832427979
overhead5:: 0
time_provenance:: 1.4153532981872559
curr_diff: 0 tensor(1.4729e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4729e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011590242385864258
overhead3:: 0.05828213691711426
overhead4:: 1.2871556282043457
overhead5:: 0
time_provenance:: 1.622373104095459
curr_diff: 0 tensor(1.3753e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3753e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.013024330139160156
overhead3:: 0.05912160873413086
overhead4:: 1.4422578811645508
overhead5:: 0
time_provenance:: 1.9164693355560303
curr_diff: 0 tensor(2.8128e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8128e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01446390151977539
overhead3:: 0.06135225296020508
overhead4:: 1.5517935752868652
overhead5:: 0
time_provenance:: 2.0316460132598877
curr_diff: 0 tensor(2.5247e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5247e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.015088319778442383
overhead3:: 0.06735801696777344
overhead4:: 1.6164863109588623
overhead5:: 0
time_provenance:: 2.0966124534606934
curr_diff: 0 tensor(2.4566e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4566e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01601099967956543
overhead3:: 0.06969952583312988
overhead4:: 1.7684481143951416
overhead5:: 0
time_provenance:: 2.2459957599639893
curr_diff: 0 tensor(2.3696e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3696e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.017223358154296875
overhead3:: 0.07556605339050293
overhead4:: 1.7808589935302734
overhead5:: 0
time_provenance:: 2.2573082447052
curr_diff: 0 tensor(2.2446e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2446e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.02485489845275879
overhead3:: 0.10448598861694336
overhead4:: 2.3516128063201904
overhead5:: 0
time_provenance:: 2.834398031234741
curr_diff: 0 tensor(6.4596e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4596e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.326477
Train - Epoch 1, Batch: 0, Loss: 1.941448
Train - Epoch 2, Batch: 0, Loss: 1.666718
Train - Epoch 3, Batch: 0, Loss: 1.462519
Train - Epoch 4, Batch: 0, Loss: 1.308337
Train - Epoch 5, Batch: 0, Loss: 1.193694
Train - Epoch 6, Batch: 0, Loss: 1.106276
Train - Epoch 7, Batch: 0, Loss: 1.029188
Train - Epoch 8, Batch: 0, Loss: 0.971061
Train - Epoch 9, Batch: 0, Loss: 0.921280
Train - Epoch 10, Batch: 0, Loss: 0.883501
Train - Epoch 11, Batch: 0, Loss: 0.853899
Train - Epoch 12, Batch: 0, Loss: 0.811991
Train - Epoch 13, Batch: 0, Loss: 0.791664
Train - Epoch 14, Batch: 0, Loss: 0.768355
Train - Epoch 15, Batch: 0, Loss: 0.743686
Train - Epoch 16, Batch: 0, Loss: 0.729144
Train - Epoch 17, Batch: 0, Loss: 0.711741
Train - Epoch 18, Batch: 0, Loss: 0.696057
Train - Epoch 19, Batch: 0, Loss: 0.683223
Train - Epoch 20, Batch: 0, Loss: 0.668954
Train - Epoch 21, Batch: 0, Loss: 0.655550
Train - Epoch 22, Batch: 0, Loss: 0.647376
Train - Epoch 23, Batch: 0, Loss: 0.635530
Train - Epoch 24, Batch: 0, Loss: 0.623983
Train - Epoch 25, Batch: 0, Loss: 0.624261
Train - Epoch 26, Batch: 0, Loss: 0.619655
Train - Epoch 27, Batch: 0, Loss: 0.600500
Train - Epoch 28, Batch: 0, Loss: 0.600217
Train - Epoch 29, Batch: 0, Loss: 0.590121
Train - Epoch 30, Batch: 0, Loss: 0.590436
Train - Epoch 31, Batch: 0, Loss: 0.579108
training_time:: 3.4421825408935547
training time full:: 3.442227363586426
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 3
training time is 2.580881118774414
overhead:: 0
overhead2:: 0
time_baseline:: 2.5810978412628174
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0033109188079833984
overhead3:: 0.01922011375427246
overhead4:: 0.3251056671142578
overhead5:: 0
time_provenance:: 0.5018539428710938
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004499912261962891
overhead3:: 0.02592754364013672
overhead4:: 0.5146815776824951
overhead5:: 0
time_provenance:: 0.7219898700714111
curr_diff: 0 tensor(7.8713e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8713e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006407976150512695
overhead3:: 0.03391766548156738
overhead4:: 0.6839606761932373
overhead5:: 0
time_provenance:: 0.9149954319000244
curr_diff: 0 tensor(8.3583e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3583e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009474515914916992
overhead3:: 0.03884553909301758
overhead4:: 0.8574182987213135
overhead5:: 0
time_provenance:: 1.1259362697601318
curr_diff: 0 tensor(7.5986e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5986e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009583234786987305
overhead3:: 0.04850959777832031
overhead4:: 1.0172052383422852
overhead5:: 0
time_provenance:: 1.3118209838867188
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004004240036010742
overhead3:: 0.024254322052001953
overhead4:: 0.4743163585662842
overhead5:: 0
time_provenance:: 0.6876983642578125
curr_diff: 0 tensor(6.0286e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0286e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0056955814361572266
overhead3:: 0.03171968460083008
overhead4:: 0.6424553394317627
overhead5:: 0
time_provenance:: 0.8725318908691406
curr_diff: 0 tensor(6.6693e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6693e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0074045658111572266
overhead3:: 0.03765368461608887
overhead4:: 0.7919633388519287
overhead5:: 0
time_provenance:: 1.0445270538330078
curr_diff: 0 tensor(6.0792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010857343673706055
overhead3:: 0.04855608940124512
overhead4:: 0.9669849872589111
overhead5:: 0
time_provenance:: 1.2426855564117432
curr_diff: 0 tensor(5.4830e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4830e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010351181030273438
overhead3:: 0.053082942962646484
overhead4:: 1.1210036277770996
overhead5:: 0
time_provenance:: 1.4287118911743164
curr_diff: 0 tensor(5.5246e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5246e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007067441940307617
overhead3:: 0.033026933670043945
overhead4:: 0.7164690494537354
overhead5:: 0
time_provenance:: 0.9761247634887695
curr_diff: 0 tensor(1.7070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008417367935180664
overhead3:: 0.04128742218017578
overhead4:: 0.8287508487701416
overhead5:: 0
time_provenance:: 1.1029784679412842
curr_diff: 0 tensor(1.1296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1296e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010065317153930664
overhead3:: 0.04496598243713379
overhead4:: 1.0015530586242676
overhead5:: 0
time_provenance:: 1.3078460693359375
curr_diff: 0 tensor(1.0188e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0188e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011047840118408203
overhead3:: 0.052285194396972656
overhead4:: 1.104689598083496
overhead5:: 0
time_provenance:: 1.4246854782104492
curr_diff: 0 tensor(9.0058e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0058e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012125015258789062
overhead3:: 0.058914899826049805
overhead4:: 1.2644121646881104
overhead5:: 0
time_provenance:: 1.6104395389556885
curr_diff: 0 tensor(8.2013e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2013e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01233530044555664
overhead3:: 0.05752062797546387
overhead4:: 1.5019450187683105
overhead5:: 0
time_provenance:: 1.9833009243011475
curr_diff: 0 tensor(4.9246e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9246e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012680530548095703
overhead3:: 0.06398272514343262
overhead4:: 1.5605049133300781
overhead5:: 0
time_provenance:: 2.037902593612671
curr_diff: 0 tensor(4.3147e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3147e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014189720153808594
overhead3:: 0.06707882881164551
overhead4:: 1.6521861553192139
overhead5:: 0
time_provenance:: 2.1360790729522705
curr_diff: 0 tensor(3.8116e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8116e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01510477066040039
overhead3:: 0.07173275947570801
overhead4:: 1.7029943466186523
overhead5:: 0
time_provenance:: 2.1901283264160156
curr_diff: 0 tensor(3.7629e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7629e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01603388786315918
overhead3:: 0.07682943344116211
overhead4:: 1.8084783554077148
overhead5:: 0
time_provenance:: 2.300173759460449
curr_diff: 0 tensor(3.5310e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5310e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.022896766662597656
overhead3:: 0.10956120491027832
overhead4:: 2.3611838817596436
overhead5:: 0
time_provenance:: 2.8678958415985107
curr_diff: 0 tensor(5.5327e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5327e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.316701
Train - Epoch 1, Batch: 0, Loss: 1.933167
Train - Epoch 2, Batch: 0, Loss: 1.661028
Train - Epoch 3, Batch: 0, Loss: 1.456293
Train - Epoch 4, Batch: 0, Loss: 1.305090
Train - Epoch 5, Batch: 0, Loss: 1.188910
Train - Epoch 6, Batch: 0, Loss: 1.104534
Train - Epoch 7, Batch: 0, Loss: 1.024328
Train - Epoch 8, Batch: 0, Loss: 0.973109
Train - Epoch 9, Batch: 0, Loss: 0.927621
Train - Epoch 10, Batch: 0, Loss: 0.877378
Train - Epoch 11, Batch: 0, Loss: 0.849457
Train - Epoch 12, Batch: 0, Loss: 0.817659
Train - Epoch 13, Batch: 0, Loss: 0.790498
Train - Epoch 14, Batch: 0, Loss: 0.762597
Train - Epoch 15, Batch: 0, Loss: 0.751477
Train - Epoch 16, Batch: 0, Loss: 0.728281
Train - Epoch 17, Batch: 0, Loss: 0.712461
Train - Epoch 18, Batch: 0, Loss: 0.698993
Train - Epoch 19, Batch: 0, Loss: 0.679746
Train - Epoch 20, Batch: 0, Loss: 0.667905
Train - Epoch 21, Batch: 0, Loss: 0.650205
Train - Epoch 22, Batch: 0, Loss: 0.651638
Train - Epoch 23, Batch: 0, Loss: 0.631577
Train - Epoch 24, Batch: 0, Loss: 0.631878
Train - Epoch 25, Batch: 0, Loss: 0.614520
Train - Epoch 26, Batch: 0, Loss: 0.610824
Train - Epoch 27, Batch: 0, Loss: 0.598856
Train - Epoch 28, Batch: 0, Loss: 0.593537
Train - Epoch 29, Batch: 0, Loss: 0.593149
Train - Epoch 30, Batch: 0, Loss: 0.588917
Train - Epoch 31, Batch: 0, Loss: 0.566256
training_time:: 3.3813893795013428
training time full:: 3.381434679031372
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 3
training time is 2.5412473678588867
overhead:: 0
overhead2:: 0
time_baseline:: 2.5414764881134033
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.002778768539428711
overhead3:: 0.018408775329589844
overhead4:: 0.3390471935272217
overhead5:: 0
time_provenance:: 0.5228967666625977
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006578922271728516
overhead3:: 0.02658534049987793
overhead4:: 0.5172295570373535
overhead5:: 0
time_provenance:: 0.7372279167175293
curr_diff: 0 tensor(6.6332e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6332e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006499528884887695
overhead3:: 0.03315544128417969
overhead4:: 0.6732375621795654
overhead5:: 0
time_provenance:: 0.9197640419006348
curr_diff: 0 tensor(9.4589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009738922119140625
overhead3:: 0.04268598556518555
overhead4:: 0.8609294891357422
overhead5:: 0
time_provenance:: 1.1383821964263916
curr_diff: 0 tensor(5.3457e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3457e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009979724884033203
overhead3:: 0.04985499382019043
overhead4:: 1.0147912502288818
overhead5:: 0
time_provenance:: 1.3336615562438965
curr_diff: 0 tensor(8.7524e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7524e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004506111145019531
overhead3:: 0.023984909057617188
overhead4:: 0.4809417724609375
overhead5:: 0
time_provenance:: 0.6959981918334961
curr_diff: 0 tensor(5.6887e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6887e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006925821304321289
overhead3:: 0.030829906463623047
overhead4:: 0.6154141426086426
overhead5:: 0
time_provenance:: 0.8522799015045166
curr_diff: 0 tensor(5.7839e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7839e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008183002471923828
overhead3:: 0.03760218620300293
overhead4:: 0.8242659568786621
overhead5:: 0
time_provenance:: 1.1038713455200195
curr_diff: 0 tensor(5.6022e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6022e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00950002670288086
overhead3:: 0.0446619987487793
overhead4:: 0.944648265838623
overhead5:: 0
time_provenance:: 1.2362570762634277
curr_diff: 0 tensor(4.8395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8395e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010913610458374023
overhead3:: 0.05173659324645996
overhead4:: 1.1364295482635498
overhead5:: 0
time_provenance:: 1.4581360816955566
curr_diff: 0 tensor(4.6367e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6367e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00539708137512207
overhead3:: 0.03371405601501465
overhead4:: 0.6743819713592529
overhead5:: 0
time_provenance:: 0.9388821125030518
curr_diff: 0 tensor(1.7918e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7918e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007445573806762695
overhead3:: 0.03991079330444336
overhead4:: 0.8514833450317383
overhead5:: 0
time_provenance:: 1.1421427726745605
curr_diff: 0 tensor(1.6464e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6464e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00874185562133789
overhead3:: 0.04511284828186035
overhead4:: 0.996387243270874
overhead5:: 0
time_provenance:: 1.3109073638916016
curr_diff: 0 tensor(1.5440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010668039321899414
overhead3:: 0.05210161209106445
overhead4:: 1.1366288661956787
overhead5:: 0
time_provenance:: 1.4741249084472656
curr_diff: 0 tensor(1.3429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012654781341552734
overhead3:: 0.05869865417480469
overhead4:: 1.2561323642730713
overhead5:: 0
time_provenance:: 1.6141648292541504
curr_diff: 0 tensor(1.3294e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3294e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01340937614440918
overhead3:: 0.059830665588378906
overhead4:: 1.5112922191619873
overhead5:: 0
time_provenance:: 1.9977097511291504
curr_diff: 0 tensor(4.6747e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6747e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014078378677368164
overhead3:: 0.06285953521728516
overhead4:: 1.593217134475708
overhead5:: 0
time_provenance:: 2.083996534347534
curr_diff: 0 tensor(4.3104e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3104e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.015805482864379883
overhead3:: 0.06579446792602539
overhead4:: 1.6460189819335938
overhead5:: 0
time_provenance:: 2.1477880477905273
curr_diff: 0 tensor(4.2450e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2450e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.016221284866333008
overhead3:: 0.07096123695373535
overhead4:: 1.7202863693237305
overhead5:: 0
time_provenance:: 2.2201857566833496
curr_diff: 0 tensor(4.2230e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2230e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01780390739440918
overhead3:: 0.07625865936279297
overhead4:: 1.810342788696289
overhead5:: 0
time_provenance:: 2.320816993713379
curr_diff: 0 tensor(4.1758e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1758e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.023546457290649414
overhead3:: 0.10813069343566895
overhead4:: 2.394024610519409
overhead5:: 0
time_provenance:: 2.895481586456299
curr_diff: 0 tensor(5.8366e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8366e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.329699
Train - Epoch 1, Batch: 0, Loss: 1.943733
Train - Epoch 2, Batch: 0, Loss: 1.669411
Train - Epoch 3, Batch: 0, Loss: 1.459628
Train - Epoch 4, Batch: 0, Loss: 1.310911
Train - Epoch 5, Batch: 0, Loss: 1.192356
Train - Epoch 6, Batch: 0, Loss: 1.108479
Train - Epoch 7, Batch: 0, Loss: 1.032988
Train - Epoch 8, Batch: 0, Loss: 0.975467
Train - Epoch 9, Batch: 0, Loss: 0.927618
Train - Epoch 10, Batch: 0, Loss: 0.890601
Train - Epoch 11, Batch: 0, Loss: 0.840926
Train - Epoch 12, Batch: 0, Loss: 0.823520
Train - Epoch 13, Batch: 0, Loss: 0.795345
Train - Epoch 14, Batch: 0, Loss: 0.770001
Train - Epoch 15, Batch: 0, Loss: 0.746987
Train - Epoch 16, Batch: 0, Loss: 0.727460
Train - Epoch 17, Batch: 0, Loss: 0.711640
Train - Epoch 18, Batch: 0, Loss: 0.696288
Train - Epoch 19, Batch: 0, Loss: 0.686631
Train - Epoch 20, Batch: 0, Loss: 0.667093
Train - Epoch 21, Batch: 0, Loss: 0.664409
Train - Epoch 22, Batch: 0, Loss: 0.648465
Train - Epoch 23, Batch: 0, Loss: 0.640290
Train - Epoch 24, Batch: 0, Loss: 0.626504
Train - Epoch 25, Batch: 0, Loss: 0.612828
Train - Epoch 26, Batch: 0, Loss: 0.612628
Train - Epoch 27, Batch: 0, Loss: 0.605108
Train - Epoch 28, Batch: 0, Loss: 0.605626
Train - Epoch 29, Batch: 0, Loss: 0.586040
Train - Epoch 30, Batch: 0, Loss: 0.582723
Train - Epoch 31, Batch: 0, Loss: 0.580890
training_time:: 3.415654420852661
training time full:: 3.4157016277313232
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 3
training time is 2.536736488342285
overhead:: 0
overhead2:: 0
time_baseline:: 2.536972999572754
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0032939910888671875
overhead3:: 0.01887226104736328
overhead4:: 0.3358736038208008
overhead5:: 0
time_provenance:: 0.5066685676574707
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.003969669342041016
overhead3:: 0.0267183780670166
overhead4:: 0.51934814453125
overhead5:: 0
time_provenance:: 0.7181096076965332
curr_diff: 0 tensor(7.1758e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1758e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006987571716308594
overhead3:: 0.034834861755371094
overhead4:: 0.683922290802002
overhead5:: 0
time_provenance:: 0.9146702289581299
curr_diff: 0 tensor(8.5494e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5494e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007353782653808594
overhead3:: 0.041220903396606445
overhead4:: 0.8622000217437744
overhead5:: 0
time_provenance:: 1.1277003288269043
curr_diff: 0 tensor(6.0892e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0892e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009135246276855469
overhead3:: 0.048792123794555664
overhead4:: 1.0383985042572021
overhead5:: 0
time_provenance:: 1.3296241760253906
curr_diff: 0 tensor(7.1032e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1032e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.003534555435180664
overhead3:: 0.024230003356933594
overhead4:: 0.4812290668487549
overhead5:: 0
time_provenance:: 0.6854233741760254
curr_diff: 0 tensor(5.5007e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5007e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005480527877807617
overhead3:: 0.030954360961914062
overhead4:: 0.6819355487823486
overhead5:: 0
time_provenance:: 0.9109375476837158
curr_diff: 0 tensor(4.7443e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7443e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007012367248535156
overhead3:: 0.037827491760253906
overhead4:: 0.7923526763916016
overhead5:: 0
time_provenance:: 1.0486812591552734
curr_diff: 0 tensor(3.8661e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8661e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008524656295776367
overhead3:: 0.04452013969421387
overhead4:: 1.0151307582855225
overhead5:: 0
time_provenance:: 1.3028545379638672
curr_diff: 0 tensor(3.7726e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7726e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00966954231262207
overhead3:: 0.051483869552612305
overhead4:: 1.1040129661560059
overhead5:: 0
time_provenance:: 1.408280611038208
curr_diff: 0 tensor(3.6783e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6783e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.005186796188354492
overhead3:: 0.0352933406829834
overhead4:: 0.7114272117614746
overhead5:: 0
time_provenance:: 0.9787969589233398
curr_diff: 0 tensor(1.5870e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5870e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006570100784301758
overhead3:: 0.0395512580871582
overhead4:: 0.8182957172393799
overhead5:: 0
time_provenance:: 1.0934042930603027
curr_diff: 0 tensor(1.1458e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1458e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008893966674804688
overhead3:: 0.04327058792114258
overhead4:: 0.9620664119720459
overhead5:: 0
time_provenance:: 1.2589311599731445
curr_diff: 0 tensor(9.9456e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9456e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.00925302505493164
overhead3:: 0.05339860916137695
overhead4:: 1.1386947631835938
overhead5:: 0
time_provenance:: 1.4646499156951904
curr_diff: 0 tensor(9.2201e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2201e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01054525375366211
overhead3:: 0.057660818099975586
overhead4:: 1.2626540660858154
overhead5:: 0
time_provenance:: 1.6022584438323975
curr_diff: 0 tensor(8.5321e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5321e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012068510055541992
overhead3:: 0.05963945388793945
overhead4:: 1.4846439361572266
overhead5:: 0
time_provenance:: 1.9650800228118896
curr_diff: 0 tensor(4.1946e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1946e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01248025894165039
overhead3:: 0.06286096572875977
overhead4:: 1.5937447547912598
overhead5:: 0
time_provenance:: 2.0737721920013428
curr_diff: 0 tensor(3.6360e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6360e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012972354888916016
overhead3:: 0.06481337547302246
overhead4:: 1.6641852855682373
overhead5:: 0
time_provenance:: 2.1460556983947754
curr_diff: 0 tensor(3.4501e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4501e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014705181121826172
overhead3:: 0.07067990303039551
overhead4:: 1.7250862121582031
overhead5:: 0
time_provenance:: 2.212223529815674
curr_diff: 0 tensor(3.3114e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3114e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014438629150390625
overhead3:: 0.0748894214630127
overhead4:: 1.8024730682373047
overhead5:: 0
time_provenance:: 2.281158924102783
curr_diff: 0 tensor(3.2423e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2423e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.02206587791442871
overhead3:: 0.10192489624023438
overhead4:: 2.3787479400634766
overhead5:: 0
time_provenance:: 2.8586318492889404
curr_diff: 0 tensor(5.4068e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4068e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.342681
Train - Epoch 1, Batch: 0, Loss: 1.953831
Train - Epoch 2, Batch: 0, Loss: 1.676672
Train - Epoch 3, Batch: 0, Loss: 1.475526
Train - Epoch 4, Batch: 0, Loss: 1.318354
Train - Epoch 5, Batch: 0, Loss: 1.198855
Train - Epoch 6, Batch: 0, Loss: 1.105337
Train - Epoch 7, Batch: 0, Loss: 1.028313
Train - Epoch 8, Batch: 0, Loss: 0.974912
Train - Epoch 9, Batch: 0, Loss: 0.925239
Train - Epoch 10, Batch: 0, Loss: 0.890484
Train - Epoch 11, Batch: 0, Loss: 0.843499
Train - Epoch 12, Batch: 0, Loss: 0.816865
Train - Epoch 13, Batch: 0, Loss: 0.794839
Train - Epoch 14, Batch: 0, Loss: 0.775104
Train - Epoch 15, Batch: 0, Loss: 0.747062
Train - Epoch 16, Batch: 0, Loss: 0.725613
Train - Epoch 17, Batch: 0, Loss: 0.725637
Train - Epoch 18, Batch: 0, Loss: 0.686173
Train - Epoch 19, Batch: 0, Loss: 0.681964
Train - Epoch 20, Batch: 0, Loss: 0.665061
Train - Epoch 21, Batch: 0, Loss: 0.659358
Train - Epoch 22, Batch: 0, Loss: 0.646660
Train - Epoch 23, Batch: 0, Loss: 0.639252
Train - Epoch 24, Batch: 0, Loss: 0.623547
Train - Epoch 25, Batch: 0, Loss: 0.613585
Train - Epoch 26, Batch: 0, Loss: 0.614627
Train - Epoch 27, Batch: 0, Loss: 0.602355
Train - Epoch 28, Batch: 0, Loss: 0.596022
Train - Epoch 29, Batch: 0, Loss: 0.588723
Train - Epoch 30, Batch: 0, Loss: 0.585024
Train - Epoch 31, Batch: 0, Loss: 0.589730
training_time:: 3.442378520965576
training time full:: 3.44242525100708
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.873700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 3
training time is 2.5393173694610596
overhead:: 0
overhead2:: 0
time_baseline:: 2.5395498275756836
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0037741661071777344
overhead3:: 0.01871490478515625
overhead4:: 0.3289954662322998
overhead5:: 0
time_provenance:: 0.5092384815216064
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.004496574401855469
overhead3:: 0.02651047706604004
overhead4:: 0.5117294788360596
overhead5:: 0
time_provenance:: 0.721337080001831
curr_diff: 0 tensor(7.9367e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9367e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007091522216796875
overhead3:: 0.034432172775268555
overhead4:: 0.6748535633087158
overhead5:: 0
time_provenance:: 0.9138734340667725
curr_diff: 0 tensor(8.2140e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2140e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008571386337280273
overhead3:: 0.041784048080444336
overhead4:: 0.8559920787811279
overhead5:: 0
time_provenance:: 1.1390924453735352
curr_diff: 0 tensor(6.3105e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3105e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011970043182373047
overhead3:: 0.049355506896972656
overhead4:: 1.0084381103515625
overhead5:: 0
time_provenance:: 1.330463171005249
curr_diff: 0 tensor(7.7539e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7539e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0046465396881103516
overhead3:: 0.02389383316040039
overhead4:: 0.45948314666748047
overhead5:: 0
time_provenance:: 0.6634125709533691
curr_diff: 0 tensor(5.1997e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1997e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.006272792816162109
overhead3:: 0.03184652328491211
overhead4:: 0.6445307731628418
overhead5:: 0
time_provenance:: 0.8755671977996826
curr_diff: 0 tensor(5.3088e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3088e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.008334159851074219
overhead3:: 0.03584146499633789
overhead4:: 0.7949063777923584
overhead5:: 0
time_provenance:: 1.057999849319458
curr_diff: 0 tensor(4.9844e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9844e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.010258197784423828
overhead3:: 0.04491376876831055
overhead4:: 0.9867627620697021
overhead5:: 0
time_provenance:: 1.2877345085144043
curr_diff: 0 tensor(4.9085e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9085e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.011823654174804688
overhead3:: 0.05234980583190918
overhead4:: 1.1033909320831299
overhead5:: 0
time_provenance:: 1.4313154220581055
curr_diff: 0 tensor(3.9275e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9275e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007279634475708008
overhead3:: 0.034819841384887695
overhead4:: 0.7058532238006592
overhead5:: 0
time_provenance:: 0.9715960025787354
curr_diff: 0 tensor(1.2752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.007798433303833008
overhead3:: 0.040084123611450195
overhead4:: 0.8245882987976074
overhead5:: 0
time_provenance:: 1.1107141971588135
curr_diff: 0 tensor(9.6300e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6300e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.009972572326660156
overhead3:: 0.04600882530212402
overhead4:: 0.9706060886383057
overhead5:: 0
time_provenance:: 1.2752594947814941
curr_diff: 0 tensor(8.6115e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6115e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.012251138687133789
overhead3:: 0.05320024490356445
overhead4:: 1.1496379375457764
overhead5:: 0
time_provenance:: 1.4794065952301025
curr_diff: 0 tensor(7.9450e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9450e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.013889789581298828
overhead3:: 0.06039834022521973
overhead4:: 1.2776362895965576
overhead5:: 0
time_provenance:: 1.6442136764526367
curr_diff: 0 tensor(7.7473e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7473e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.014114618301391602
overhead3:: 0.05795907974243164
overhead4:: 1.4816889762878418
overhead5:: 0
time_provenance:: 1.9671177864074707
curr_diff: 0 tensor(3.8333e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8333e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.015139102935791016
overhead3:: 0.061086177825927734
overhead4:: 1.5149400234222412
overhead5:: 0
time_provenance:: 1.995171070098877
curr_diff: 0 tensor(3.3507e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3507e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.017034530639648438
overhead3:: 0.06412959098815918
overhead4:: 1.6172370910644531
overhead5:: 0
time_provenance:: 2.099421739578247
curr_diff: 0 tensor(3.1711e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1711e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.01710963249206543
overhead3:: 0.07032942771911621
overhead4:: 1.727708101272583
overhead5:: 0
time_provenance:: 2.225172758102417
curr_diff: 0 tensor(3.0486e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0486e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.0183103084564209
overhead3:: 0.0740807056427002
overhead4:: 1.8129091262817383
overhead5:: 0
time_provenance:: 2.321458578109741
curr_diff: 0 tensor(2.7485e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7485e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
overhead:: 0
overhead2:: 0.024682044982910156
overhead3:: 0.10217499732971191
overhead4:: 2.31475830078125
overhead5:: 0
time_provenance:: 2.8303773403167725
curr_diff: 0 tensor(6.4719e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4719e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  MNIST5 0
tensor([25872, 43661, 25162, 21339, 57084, 39469])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.320884
Train - Epoch 1, Batch: 0, Loss: 1.935221
Train - Epoch 2, Batch: 0, Loss: 1.661642
Train - Epoch 3, Batch: 0, Loss: 1.464254
Train - Epoch 4, Batch: 0, Loss: 1.313991
Train - Epoch 5, Batch: 0, Loss: 1.194837
Train - Epoch 6, Batch: 0, Loss: 1.101668
Train - Epoch 7, Batch: 0, Loss: 1.034109
Train - Epoch 8, Batch: 0, Loss: 0.975626
Train - Epoch 9, Batch: 0, Loss: 0.922607
Train - Epoch 10, Batch: 0, Loss: 0.888818
Train - Epoch 11, Batch: 0, Loss: 0.847596
Train - Epoch 12, Batch: 0, Loss: 0.823835
Train - Epoch 13, Batch: 0, Loss: 0.796600
Train - Epoch 14, Batch: 0, Loss: 0.760987
Train - Epoch 15, Batch: 0, Loss: 0.748350
Train - Epoch 16, Batch: 0, Loss: 0.724743
Train - Epoch 17, Batch: 0, Loss: 0.705584
Train - Epoch 18, Batch: 0, Loss: 0.698534
Train - Epoch 19, Batch: 0, Loss: 0.684380
Train - Epoch 20, Batch: 0, Loss: 0.664776
Train - Epoch 21, Batch: 0, Loss: 0.659617
Train - Epoch 22, Batch: 0, Loss: 0.640919
Train - Epoch 23, Batch: 0, Loss: 0.642302
Train - Epoch 24, Batch: 0, Loss: 0.632048
Train - Epoch 25, Batch: 0, Loss: 0.626153
Train - Epoch 26, Batch: 0, Loss: 0.612066
Train - Epoch 27, Batch: 0, Loss: 0.604015
Train - Epoch 28, Batch: 0, Loss: 0.591805
Train - Epoch 29, Batch: 0, Loss: 0.596576
Train - Epoch 30, Batch: 0, Loss: 0.581317
Train - Epoch 31, Batch: 0, Loss: 0.567665
training_time:: 3.3764021396636963
training time full:: 3.3764488697052
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 6
training time is 2.532099485397339
overhead:: 0
overhead2:: 0
time_baseline:: 2.532334327697754
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.004494667053222656
overhead3:: 0.019426822662353516
overhead4:: 0.3269071578979492
overhead5:: 0
time_provenance:: 0.5393764972686768
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007888555526733398
overhead3:: 0.027781248092651367
overhead4:: 0.5219411849975586
overhead5:: 0
time_provenance:: 0.7908022403717041
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010601520538330078
overhead3:: 0.034270286560058594
overhead4:: 0.6796243190765381
overhead5:: 0
time_provenance:: 0.9882540702819824
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01270437240600586
overhead3:: 0.04109001159667969
overhead4:: 0.8590576648712158
overhead5:: 0
time_provenance:: 1.2002108097076416
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014124870300292969
overhead3:: 0.048253774642944336
overhead4:: 1.0480852127075195
overhead5:: 0
time_provenance:: 1.4237346649169922
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006869077682495117
overhead3:: 0.023883819580078125
overhead4:: 0.4563140869140625
overhead5:: 0
time_provenance:: 0.6981265544891357
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009546995162963867
overhead3:: 0.030557632446289062
overhead4:: 0.6543090343475342
overhead5:: 0
time_provenance:: 0.9436769485473633
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011307001113891602
overhead3:: 0.03634190559387207
overhead4:: 0.7902154922485352
overhead5:: 0
time_provenance:: 1.1169371604919434
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013575315475463867
overhead3:: 0.044321298599243164
overhead4:: 0.9557623863220215
overhead5:: 0
time_provenance:: 1.311877727508545
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018580198287963867
overhead3:: 0.05259418487548828
overhead4:: 1.1525473594665527
overhead5:: 0
time_provenance:: 1.5427186489105225
curr_diff: 0 tensor(8.7918e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7918e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.00971364974975586
overhead3:: 0.03350520133972168
overhead4:: 0.6930642127990723
overhead5:: 0
time_provenance:: 0.9860024452209473
curr_diff: 0 tensor(2.9165e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9165e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01224517822265625
overhead3:: 0.03937816619873047
overhead4:: 0.817981481552124
overhead5:: 0
time_provenance:: 1.1471104621887207
curr_diff: 0 tensor(2.6265e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6265e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.015057086944580078
overhead3:: 0.04578542709350586
overhead4:: 0.9862439632415771
overhead5:: 0
time_provenance:: 1.3628270626068115
curr_diff: 0 tensor(2.5088e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5088e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.016866445541381836
overhead3:: 0.048967838287353516
overhead4:: 1.122171401977539
overhead5:: 0
time_provenance:: 1.5169129371643066
curr_diff: 0 tensor(2.1245e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1245e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018272876739501953
overhead3:: 0.05885958671569824
overhead4:: 1.2592597007751465
overhead5:: 0
time_provenance:: 1.6968398094177246
curr_diff: 0 tensor(2.1867e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1867e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018416404724121094
overhead3:: 0.057881832122802734
overhead4:: 1.5359551906585693
overhead5:: 0
time_provenance:: 2.0548603534698486
curr_diff: 0 tensor(9.0562e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0562e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.019573211669921875
overhead3:: 0.061087608337402344
overhead4:: 1.5789151191711426
overhead5:: 0
time_provenance:: 2.1118319034576416
curr_diff: 0 tensor(8.6176e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6176e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.021548748016357422
overhead3:: 0.06624960899353027
overhead4:: 1.6471545696258545
overhead5:: 0
time_provenance:: 2.19877552986145
curr_diff: 0 tensor(8.2527e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2527e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02200174331665039
overhead3:: 0.07072234153747559
overhead4:: 1.728539228439331
overhead5:: 0
time_provenance:: 2.2860233783721924
curr_diff: 0 tensor(7.9683e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9683e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02432394027709961
overhead3:: 0.0756986141204834
overhead4:: 1.8118515014648438
overhead5:: 0
time_provenance:: 2.388003349304199
curr_diff: 0 tensor(7.7857e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7857e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03631424903869629
overhead3:: 0.10882043838500977
overhead4:: 2.3545737266540527
overhead5:: 0
time_provenance:: 3.042233467102051
curr_diff: 0 tensor(6.1492e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1492e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.304233
Train - Epoch 1, Batch: 0, Loss: 1.916799
Train - Epoch 2, Batch: 0, Loss: 1.647359
Train - Epoch 3, Batch: 0, Loss: 1.441344
Train - Epoch 4, Batch: 0, Loss: 1.297408
Train - Epoch 5, Batch: 0, Loss: 1.178282
Train - Epoch 6, Batch: 0, Loss: 1.089823
Train - Epoch 7, Batch: 0, Loss: 1.023918
Train - Epoch 8, Batch: 0, Loss: 0.966895
Train - Epoch 9, Batch: 0, Loss: 0.922850
Train - Epoch 10, Batch: 0, Loss: 0.873471
Train - Epoch 11, Batch: 0, Loss: 0.843528
Train - Epoch 12, Batch: 0, Loss: 0.804866
Train - Epoch 13, Batch: 0, Loss: 0.788774
Train - Epoch 14, Batch: 0, Loss: 0.767792
Train - Epoch 15, Batch: 0, Loss: 0.735660
Train - Epoch 16, Batch: 0, Loss: 0.717505
Train - Epoch 17, Batch: 0, Loss: 0.708526
Train - Epoch 18, Batch: 0, Loss: 0.694131
Train - Epoch 19, Batch: 0, Loss: 0.681615
Train - Epoch 20, Batch: 0, Loss: 0.668931
Train - Epoch 21, Batch: 0, Loss: 0.651880
Train - Epoch 22, Batch: 0, Loss: 0.645188
Train - Epoch 23, Batch: 0, Loss: 0.629303
Train - Epoch 24, Batch: 0, Loss: 0.621538
Train - Epoch 25, Batch: 0, Loss: 0.614728
Train - Epoch 26, Batch: 0, Loss: 0.613340
Train - Epoch 27, Batch: 0, Loss: 0.609568
Train - Epoch 28, Batch: 0, Loss: 0.597240
Train - Epoch 29, Batch: 0, Loss: 0.584333
Train - Epoch 30, Batch: 0, Loss: 0.578363
Train - Epoch 31, Batch: 0, Loss: 0.576025
training_time:: 3.4318530559539795
training time full:: 3.431898832321167
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.873900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 6
training time is 2.582845449447632
overhead:: 0
overhead2:: 0
time_baseline:: 2.583052635192871
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.00392460823059082
overhead3:: 0.01970672607421875
overhead4:: 0.34169793128967285
overhead5:: 0
time_provenance:: 0.5445096492767334
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006252765655517578
overhead3:: 0.026779651641845703
overhead4:: 0.5134725570678711
overhead5:: 0
time_provenance:: 0.7516233921051025
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008649826049804688
overhead3:: 0.034368276596069336
overhead4:: 0.6750564575195312
overhead5:: 0
time_provenance:: 0.9578132629394531
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011677980422973633
overhead3:: 0.04108858108520508
overhead4:: 0.8681364059448242
overhead5:: 0
time_provenance:: 1.1952712535858154
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012791633605957031
overhead3:: 0.04811978340148926
overhead4:: 1.0129518508911133
overhead5:: 0
time_provenance:: 1.3708369731903076
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.005556583404541016
overhead3:: 0.023487091064453125
overhead4:: 0.46819615364074707
overhead5:: 0
time_provenance:: 0.7013051509857178
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007689952850341797
overhead3:: 0.03184318542480469
overhead4:: 0.6314992904663086
overhead5:: 0
time_provenance:: 0.9057157039642334
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011147022247314453
overhead3:: 0.03572678565979004
overhead4:: 0.7930569648742676
overhead5:: 0
time_provenance:: 1.0978915691375732
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012250185012817383
overhead3:: 0.04563260078430176
overhead4:: 0.9642243385314941
overhead5:: 0
time_provenance:: 1.32106351852417
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01422119140625
overhead3:: 0.052883148193359375
overhead4:: 1.1240007877349854
overhead5:: 0
time_provenance:: 1.5055980682373047
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008070945739746094
overhead3:: 0.0336911678314209
overhead4:: 0.717583417892456
overhead5:: 0
time_provenance:: 1.0018482208251953
curr_diff: 0 tensor(2.8301e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8301e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009884119033813477
overhead3:: 0.040186405181884766
overhead4:: 0.8212540149688721
overhead5:: 0
time_provenance:: 1.1362965106964111
curr_diff: 0 tensor(2.4968e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4968e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012699127197265625
overhead3:: 0.044855356216430664
overhead4:: 0.9646728038787842
overhead5:: 0
time_provenance:: 1.3182356357574463
curr_diff: 0 tensor(2.3604e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3604e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0140380859375
overhead3:: 0.0516965389251709
overhead4:: 1.1315407752990723
overhead5:: 0
time_provenance:: 1.5146143436431885
curr_diff: 0 tensor(1.9984e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9984e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01608729362487793
overhead3:: 0.0587613582611084
overhead4:: 1.2617027759552002
overhead5:: 0
time_provenance:: 1.666762113571167
curr_diff: 0 tensor(1.9926e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9926e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.016764163970947266
overhead3:: 0.059346914291381836
overhead4:: 1.5051255226135254
overhead5:: 0
time_provenance:: 2.018073081970215
curr_diff: 0 tensor(8.3923e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3923e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.017516613006591797
overhead3:: 0.06285786628723145
overhead4:: 1.5699849128723145
overhead5:: 0
time_provenance:: 2.0904853343963623
curr_diff: 0 tensor(7.7384e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7384e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.019442081451416016
overhead3:: 0.06669235229492188
overhead4:: 1.65824556350708
overhead5:: 0
time_provenance:: 2.1921403408050537
curr_diff: 0 tensor(7.6837e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6837e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.020223140716552734
overhead3:: 0.07074165344238281
overhead4:: 1.723280429840088
overhead5:: 0
time_provenance:: 2.268566608428955
curr_diff: 0 tensor(7.2508e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2508e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02352166175842285
overhead3:: 0.07622361183166504
overhead4:: 1.8256592750549316
overhead5:: 0
time_provenance:: 2.382901430130005
curr_diff: 0 tensor(6.9422e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9422e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.032598257064819336
overhead3:: 0.10661911964416504
overhead4:: 2.3566315174102783
overhead5:: 0
time_provenance:: 3.0037994384765625
curr_diff: 0 tensor(6.0501e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0501e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.306502
Train - Epoch 1, Batch: 0, Loss: 1.931959
Train - Epoch 2, Batch: 0, Loss: 1.656027
Train - Epoch 3, Batch: 0, Loss: 1.457948
Train - Epoch 4, Batch: 0, Loss: 1.311240
Train - Epoch 5, Batch: 0, Loss: 1.195310
Train - Epoch 6, Batch: 0, Loss: 1.099974
Train - Epoch 7, Batch: 0, Loss: 1.032846
Train - Epoch 8, Batch: 0, Loss: 0.972212
Train - Epoch 9, Batch: 0, Loss: 0.923515
Train - Epoch 10, Batch: 0, Loss: 0.878185
Train - Epoch 11, Batch: 0, Loss: 0.844736
Train - Epoch 12, Batch: 0, Loss: 0.816015
Train - Epoch 13, Batch: 0, Loss: 0.790654
Train - Epoch 14, Batch: 0, Loss: 0.768794
Train - Epoch 15, Batch: 0, Loss: 0.741060
Train - Epoch 16, Batch: 0, Loss: 0.729210
Train - Epoch 17, Batch: 0, Loss: 0.717797
Train - Epoch 18, Batch: 0, Loss: 0.698631
Train - Epoch 19, Batch: 0, Loss: 0.690102
Train - Epoch 20, Batch: 0, Loss: 0.667222
Train - Epoch 21, Batch: 0, Loss: 0.657317
Train - Epoch 22, Batch: 0, Loss: 0.643905
Train - Epoch 23, Batch: 0, Loss: 0.633848
Train - Epoch 24, Batch: 0, Loss: 0.627491
Train - Epoch 25, Batch: 0, Loss: 0.624550
Train - Epoch 26, Batch: 0, Loss: 0.618791
Train - Epoch 27, Batch: 0, Loss: 0.605813
Train - Epoch 28, Batch: 0, Loss: 0.605243
Train - Epoch 29, Batch: 0, Loss: 0.590516
Train - Epoch 30, Batch: 0, Loss: 0.586702
Train - Epoch 31, Batch: 0, Loss: 0.574691
training_time:: 3.368657350540161
training time full:: 3.368701696395874
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.874700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 6
training time is 2.558894634246826
overhead:: 0
overhead2:: 0
time_baseline:: 2.559093713760376
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.00403594970703125
overhead3:: 0.018715858459472656
overhead4:: 0.3313436508178711
overhead5:: 0
time_provenance:: 0.5367379188537598
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0070192813873291016
overhead3:: 0.027051210403442383
overhead4:: 0.5095503330230713
overhead5:: 0
time_provenance:: 0.7614009380340576
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009493589401245117
overhead3:: 0.034223318099975586
overhead4:: 0.6906781196594238
overhead5:: 0
time_provenance:: 0.982342004776001
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012187480926513672
overhead3:: 0.041556596755981445
overhead4:: 0.8522613048553467
overhead5:: 0
time_provenance:: 1.1900310516357422
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014220952987670898
overhead3:: 0.04851055145263672
overhead4:: 1.0193283557891846
overhead5:: 0
time_provenance:: 1.392721176147461
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0053272247314453125
overhead3:: 0.023692846298217773
overhead4:: 0.45755958557128906
overhead5:: 0
time_provenance:: 0.6950883865356445
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007967233657836914
overhead3:: 0.030672311782836914
overhead4:: 0.6089136600494385
overhead5:: 0
time_provenance:: 0.8763868808746338
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0109405517578125
overhead3:: 0.03836774826049805
overhead4:: 0.8120477199554443
overhead5:: 0
time_provenance:: 1.1352770328521729
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013051033020019531
overhead3:: 0.04478192329406738
overhead4:: 0.9494948387145996
overhead5:: 0
time_provenance:: 1.2956712245941162
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014851093292236328
overhead3:: 0.05051732063293457
overhead4:: 1.1089744567871094
overhead5:: 0
time_provenance:: 1.4960711002349854
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.00895547866821289
overhead3:: 0.033895254135131836
overhead4:: 0.7043900489807129
overhead5:: 0
time_provenance:: 0.9900040626525879
curr_diff: 0 tensor(3.4824e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4824e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010773658752441406
overhead3:: 0.03940105438232422
overhead4:: 0.8366813659667969
overhead5:: 0
time_provenance:: 1.161977767944336
curr_diff: 0 tensor(3.1222e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1222e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013647079467773438
overhead3:: 0.04562568664550781
overhead4:: 0.9801130294799805
overhead5:: 0
time_provenance:: 1.346872091293335
curr_diff: 0 tensor(2.9105e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9105e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014843463897705078
overhead3:: 0.052028656005859375
overhead4:: 1.1194593906402588
overhead5:: 0
time_provenance:: 1.505368709564209
curr_diff: 0 tensor(2.5050e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5050e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.017250776290893555
overhead3:: 0.05921030044555664
overhead4:: 1.2754571437835693
overhead5:: 0
time_provenance:: 1.7032339572906494
curr_diff: 0 tensor(2.0887e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0887e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018326520919799805
overhead3:: 0.06058931350708008
overhead4:: 1.510850429534912
overhead5:: 0
time_provenance:: 2.028204917907715
curr_diff: 0 tensor(9.3901e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3901e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02211761474609375
overhead3:: 0.06553101539611816
overhead4:: 1.5943872928619385
overhead5:: 0
time_provenance:: 2.124429225921631
curr_diff: 0 tensor(8.9259e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9259e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02427816390991211
overhead3:: 0.06442141532897949
overhead4:: 1.6387982368469238
overhead5:: 0
time_provenance:: 2.174333095550537
curr_diff: 0 tensor(8.5089e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5089e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.022003173828125
overhead3:: 0.07009267807006836
overhead4:: 1.7481606006622314
overhead5:: 0
time_provenance:: 2.2963554859161377
curr_diff: 0 tensor(8.3327e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3327e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.023861169815063477
overhead3:: 0.07527375221252441
overhead4:: 1.7965238094329834
overhead5:: 0
time_provenance:: 2.365846633911133
curr_diff: 0 tensor(8.1867e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1867e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03207588195800781
overhead3:: 0.1068882942199707
overhead4:: 2.3663249015808105
overhead5:: 0
time_provenance:: 3.0234122276306152
curr_diff: 0 tensor(6.0752e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0752e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.310440
Train - Epoch 1, Batch: 0, Loss: 1.929127
Train - Epoch 2, Batch: 0, Loss: 1.657600
Train - Epoch 3, Batch: 0, Loss: 1.462534
Train - Epoch 4, Batch: 0, Loss: 1.304692
Train - Epoch 5, Batch: 0, Loss: 1.193197
Train - Epoch 6, Batch: 0, Loss: 1.107972
Train - Epoch 7, Batch: 0, Loss: 1.034828
Train - Epoch 8, Batch: 0, Loss: 0.978561
Train - Epoch 9, Batch: 0, Loss: 0.930502
Train - Epoch 10, Batch: 0, Loss: 0.884497
Train - Epoch 11, Batch: 0, Loss: 0.854188
Train - Epoch 12, Batch: 0, Loss: 0.824728
Train - Epoch 13, Batch: 0, Loss: 0.796140
Train - Epoch 14, Batch: 0, Loss: 0.768622
Train - Epoch 15, Batch: 0, Loss: 0.751583
Train - Epoch 16, Batch: 0, Loss: 0.730079
Train - Epoch 17, Batch: 0, Loss: 0.710108
Train - Epoch 18, Batch: 0, Loss: 0.695337
Train - Epoch 19, Batch: 0, Loss: 0.686562
Train - Epoch 20, Batch: 0, Loss: 0.673199
Train - Epoch 21, Batch: 0, Loss: 0.665196
Train - Epoch 22, Batch: 0, Loss: 0.647017
Train - Epoch 23, Batch: 0, Loss: 0.641820
Train - Epoch 24, Batch: 0, Loss: 0.630448
Train - Epoch 25, Batch: 0, Loss: 0.613161
Train - Epoch 26, Batch: 0, Loss: 0.614452
Train - Epoch 27, Batch: 0, Loss: 0.614985
Train - Epoch 28, Batch: 0, Loss: 0.598339
Train - Epoch 29, Batch: 0, Loss: 0.588713
Train - Epoch 30, Batch: 0, Loss: 0.581171
Train - Epoch 31, Batch: 0, Loss: 0.584429
training_time:: 3.4866113662719727
training time full:: 3.486656665802002
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 6
training time is 2.5511438846588135
overhead:: 0
overhead2:: 0
time_baseline:: 2.5513644218444824
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.004735469818115234
overhead3:: 0.019160032272338867
overhead4:: 0.3605682849884033
overhead5:: 0
time_provenance:: 0.5686514377593994
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006375312805175781
overhead3:: 0.02626514434814453
overhead4:: 0.5087006092071533
overhead5:: 0
time_provenance:: 0.7494068145751953
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008755683898925781
overhead3:: 0.03349113464355469
overhead4:: 0.6825113296508789
overhead5:: 0
time_provenance:: 0.9577727317810059
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011020183563232422
overhead3:: 0.04202771186828613
overhead4:: 0.8589189052581787
overhead5:: 0
time_provenance:: 1.1797304153442383
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013614416122436523
overhead3:: 0.049242496490478516
overhead4:: 1.0298001766204834
overhead5:: 0
time_provenance:: 1.3835411071777344
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.006021976470947266
overhead3:: 0.023984670639038086
overhead4:: 0.49425625801086426
overhead5:: 0
time_provenance:: 0.7263634204864502
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.008133411407470703
overhead3:: 0.030279159545898438
overhead4:: 0.6222748756408691
overhead5:: 0
time_provenance:: 0.8866736888885498
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01018977165222168
overhead3:: 0.03841686248779297
overhead4:: 0.805389404296875
overhead5:: 0
time_provenance:: 1.1097798347473145
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012610673904418945
overhead3:: 0.04463768005371094
overhead4:: 0.9469716548919678
overhead5:: 0
time_provenance:: 1.2818984985351562
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014585494995117188
overhead3:: 0.05270981788635254
overhead4:: 1.1088228225708008
overhead5:: 0
time_provenance:: 1.4754056930541992
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009887218475341797
overhead3:: 0.03267312049865723
overhead4:: 0.6824445724487305
overhead5:: 0
time_provenance:: 0.9685192108154297
curr_diff: 0 tensor(2.7715e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7715e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012166976928710938
overhead3:: 0.03928256034851074
overhead4:: 0.8398387432098389
overhead5:: 0
time_provenance:: 1.1586265563964844
curr_diff: 0 tensor(2.4918e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4918e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012198925018310547
overhead3:: 0.050478219985961914
overhead4:: 0.9719028472900391
overhead5:: 0
time_provenance:: 1.3253536224365234
curr_diff: 0 tensor(2.1998e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1998e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.015466451644897461
overhead3:: 0.05266165733337402
overhead4:: 1.1078288555145264
overhead5:: 0
time_provenance:: 1.4755034446716309
curr_diff: 0 tensor(1.8335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.016457080841064453
overhead3:: 0.05865144729614258
overhead4:: 1.280947208404541
overhead5:: 0
time_provenance:: 1.6874439716339111
curr_diff: 0 tensor(1.6877e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6877e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.017192840576171875
overhead3:: 0.06035566329956055
overhead4:: 1.520862102508545
overhead5:: 0
time_provenance:: 2.0232903957366943
curr_diff: 0 tensor(9.8911e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8911e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.018312692642211914
overhead3:: 0.06416940689086914
overhead4:: 1.550567388534546
overhead5:: 0
time_provenance:: 2.073564291000366
curr_diff: 0 tensor(9.6702e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6702e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01950669288635254
overhead3:: 0.06795716285705566
overhead4:: 1.6468617916107178
overhead5:: 0
time_provenance:: 2.173339605331421
curr_diff: 0 tensor(9.2114e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2114e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.020311355590820312
overhead3:: 0.07123708724975586
overhead4:: 1.7450881004333496
overhead5:: 0
time_provenance:: 2.2847774028778076
curr_diff: 0 tensor(8.9063e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9063e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.021008014678955078
overhead3:: 0.07483506202697754
overhead4:: 1.8570606708526611
overhead5:: 0
time_provenance:: 2.4116501808166504
curr_diff: 0 tensor(8.6828e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6828e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03251981735229492
overhead3:: 0.10701823234558105
overhead4:: 2.3772354125976562
overhead5:: 0
time_provenance:: 3.0086960792541504
curr_diff: 0 tensor(5.9376e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9376e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.311166
Train - Epoch 1, Batch: 0, Loss: 1.932976
Train - Epoch 2, Batch: 0, Loss: 1.651221
Train - Epoch 3, Batch: 0, Loss: 1.451430
Train - Epoch 4, Batch: 0, Loss: 1.302497
Train - Epoch 5, Batch: 0, Loss: 1.186465
Train - Epoch 6, Batch: 0, Loss: 1.090129
Train - Epoch 7, Batch: 0, Loss: 1.021463
Train - Epoch 8, Batch: 0, Loss: 0.970067
Train - Epoch 9, Batch: 0, Loss: 0.923234
Train - Epoch 10, Batch: 0, Loss: 0.884252
Train - Epoch 11, Batch: 0, Loss: 0.846673
Train - Epoch 12, Batch: 0, Loss: 0.819291
Train - Epoch 13, Batch: 0, Loss: 0.786943
Train - Epoch 14, Batch: 0, Loss: 0.766365
Train - Epoch 15, Batch: 0, Loss: 0.739396
Train - Epoch 16, Batch: 0, Loss: 0.729256
Train - Epoch 17, Batch: 0, Loss: 0.717316
Train - Epoch 18, Batch: 0, Loss: 0.699319
Train - Epoch 19, Batch: 0, Loss: 0.684731
Train - Epoch 20, Batch: 0, Loss: 0.671527
Train - Epoch 21, Batch: 0, Loss: 0.664194
Train - Epoch 22, Batch: 0, Loss: 0.647884
Train - Epoch 23, Batch: 0, Loss: 0.644711
Train - Epoch 24, Batch: 0, Loss: 0.637754
Train - Epoch 25, Batch: 0, Loss: 0.627545
Train - Epoch 26, Batch: 0, Loss: 0.614486
Train - Epoch 27, Batch: 0, Loss: 0.604894
Train - Epoch 28, Batch: 0, Loss: 0.601602
Train - Epoch 29, Batch: 0, Loss: 0.590804
Train - Epoch 30, Batch: 0, Loss: 0.581437
Train - Epoch 31, Batch: 0, Loss: 0.579215
training_time:: 3.3448774814605713
training time full:: 3.344921827316284
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 6
training time is 2.529844284057617
overhead:: 0
overhead2:: 0
time_baseline:: 2.5300605297088623
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.004374265670776367
overhead3:: 0.017694711685180664
overhead4:: 0.338240385055542
overhead5:: 0
time_provenance:: 0.5502188205718994
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.0064008235931396484
overhead3:: 0.02689361572265625
overhead4:: 0.5180222988128662
overhead5:: 0
time_provenance:: 0.7521297931671143
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009133100509643555
overhead3:: 0.03355598449707031
overhead4:: 0.6717133522033691
overhead5:: 0
time_provenance:: 0.9478316307067871
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.011086463928222656
overhead3:: 0.04024529457092285
overhead4:: 0.856736421585083
overhead5:: 0
time_provenance:: 1.1714684963226318
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.013233423233032227
overhead3:: 0.04934573173522949
overhead4:: 1.0160889625549316
overhead5:: 0
time_provenance:: 1.3763070106506348
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.005254030227661133
overhead3:: 0.02224445343017578
overhead4:: 0.45741939544677734
overhead5:: 0
time_provenance:: 0.6804599761962891
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.007241487503051758
overhead3:: 0.030130624771118164
overhead4:: 0.6170740127563477
overhead5:: 0
time_provenance:: 0.8751087188720703
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.009417295455932617
overhead3:: 0.03780198097229004
overhead4:: 0.7804970741271973
overhead5:: 0
time_provenance:: 1.0838584899902344
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01202082633972168
overhead3:: 0.043755292892456055
overhead4:: 0.947925329208374
overhead5:: 0
time_provenance:: 1.2820043563842773
curr_diff: 0 tensor(9.8934e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8934e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014236211776733398
overhead3:: 0.050679922103881836
overhead4:: 1.1140937805175781
overhead5:: 0
time_provenance:: 1.4929511547088623
curr_diff: 0 tensor(8.2426e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2426e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.00926828384399414
overhead3:: 0.03227496147155762
overhead4:: 0.6711797714233398
overhead5:: 0
time_provenance:: 0.9456281661987305
curr_diff: 0 tensor(3.4078e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4078e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.010576725006103516
overhead3:: 0.040888071060180664
overhead4:: 0.8554625511169434
overhead5:: 0
time_provenance:: 1.162477731704712
curr_diff: 0 tensor(3.2536e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2536e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.012168407440185547
overhead3:: 0.047348976135253906
overhead4:: 0.9643821716308594
overhead5:: 0
time_provenance:: 1.3109416961669922
curr_diff: 0 tensor(3.0401e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0401e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.014449834823608398
overhead3:: 0.05212116241455078
overhead4:: 1.1354050636291504
overhead5:: 0
time_provenance:: 1.5163443088531494
curr_diff: 0 tensor(3.0095e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0095e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.016176939010620117
overhead3:: 0.05872488021850586
overhead4:: 1.2424559593200684
overhead5:: 0
time_provenance:: 1.642782211303711
curr_diff: 0 tensor(2.7493e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7493e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.015680789947509766
overhead3:: 0.05692338943481445
overhead4:: 1.4742100238800049
overhead5:: 0
time_provenance:: 1.9696245193481445
curr_diff: 0 tensor(8.7412e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7412e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.017231225967407227
overhead3:: 0.062294960021972656
overhead4:: 1.5340840816497803
overhead5:: 0
time_provenance:: 2.0421650409698486
curr_diff: 0 tensor(8.3645e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3645e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.01820206642150879
overhead3:: 0.06639695167541504
overhead4:: 1.600266456604004
overhead5:: 0
time_provenance:: 2.12438702583313
curr_diff: 0 tensor(7.8407e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8407e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.02132558822631836
overhead3:: 0.07222485542297363
overhead4:: 1.7444334030151367
overhead5:: 0
time_provenance:: 2.2820284366607666
curr_diff: 0 tensor(7.5689e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5689e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.021441221237182617
overhead3:: 0.07465505599975586
overhead4:: 1.7819547653198242
overhead5:: 0
time_provenance:: 2.32674503326416
curr_diff: 0 tensor(7.3053e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3053e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
overhead:: 0
overhead2:: 0.03059244155883789
overhead3:: 0.10732102394104004
overhead4:: 2.310652494430542
overhead5:: 0
time_provenance:: 2.9296936988830566
curr_diff: 0 tensor(6.5972e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5972e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  MNIST5 0
tensor([19233, 40355, 25162, 28652, 39469, 43661, 25872, 54608, 42001, 58837,
        21339, 57084])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.319283
Train - Epoch 1, Batch: 0, Loss: 1.933713
Train - Epoch 2, Batch: 0, Loss: 1.665818
Train - Epoch 3, Batch: 0, Loss: 1.458669
Train - Epoch 4, Batch: 0, Loss: 1.310824
Train - Epoch 5, Batch: 0, Loss: 1.198722
Train - Epoch 6, Batch: 0, Loss: 1.103391
Train - Epoch 7, Batch: 0, Loss: 1.035484
Train - Epoch 8, Batch: 0, Loss: 0.969630
Train - Epoch 9, Batch: 0, Loss: 0.925009
Train - Epoch 10, Batch: 0, Loss: 0.879662
Train - Epoch 11, Batch: 0, Loss: 0.858487
Train - Epoch 12, Batch: 0, Loss: 0.818996
Train - Epoch 13, Batch: 0, Loss: 0.792421
Train - Epoch 14, Batch: 0, Loss: 0.768187
Train - Epoch 15, Batch: 0, Loss: 0.756943
Train - Epoch 16, Batch: 0, Loss: 0.731878
Train - Epoch 17, Batch: 0, Loss: 0.717965
Train - Epoch 18, Batch: 0, Loss: 0.697251
Train - Epoch 19, Batch: 0, Loss: 0.687307
Train - Epoch 20, Batch: 0, Loss: 0.672155
Train - Epoch 21, Batch: 0, Loss: 0.657371
Train - Epoch 22, Batch: 0, Loss: 0.649315
Train - Epoch 23, Batch: 0, Loss: 0.633260
Train - Epoch 24, Batch: 0, Loss: 0.631524
Train - Epoch 25, Batch: 0, Loss: 0.617742
Train - Epoch 26, Batch: 0, Loss: 0.621937
Train - Epoch 27, Batch: 0, Loss: 0.608488
Train - Epoch 28, Batch: 0, Loss: 0.604905
Train - Epoch 29, Batch: 0, Loss: 0.582606
Train - Epoch 30, Batch: 0, Loss: 0.580826
Train - Epoch 31, Batch: 0, Loss: 0.581927
training_time:: 3.426940441131592
training time full:: 3.426987409591675
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 12
training time is 2.539759635925293
overhead:: 0
overhead2:: 0
time_baseline:: 2.53998064994812
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.005470991134643555
overhead3:: 0.01881241798400879
overhead4:: 0.3365025520324707
overhead5:: 0
time_provenance:: 0.5776371955871582
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.008770942687988281
overhead3:: 0.026165246963500977
overhead4:: 0.5080349445343018
overhead5:: 0
time_provenance:: 0.7794327735900879
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010749578475952148
overhead3:: 0.0344541072845459
overhead4:: 0.6721482276916504
overhead5:: 0
time_provenance:: 0.9867539405822754
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013870954513549805
overhead3:: 0.04096865653991699
overhead4:: 0.8547530174255371
overhead5:: 0
time_provenance:: 1.2227389812469482
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.016999483108520508
overhead3:: 0.04805326461791992
overhead4:: 1.019604206085205
overhead5:: 0
time_provenance:: 1.4297599792480469
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.007468223571777344
overhead3:: 0.023534774780273438
overhead4:: 0.47548890113830566
overhead5:: 0
time_provenance:: 0.7337379455566406
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.009391069412231445
overhead3:: 0.0301363468170166
overhead4:: 0.6085662841796875
overhead5:: 0
time_provenance:: 0.9049735069274902
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013129472732543945
overhead3:: 0.0377810001373291
overhead4:: 0.7942967414855957
overhead5:: 0
time_provenance:: 1.1353473663330078
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.014684200286865234
overhead3:: 0.04360508918762207
overhead4:: 0.9458260536193848
overhead5:: 0
time_provenance:: 1.3340156078338623
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.019128084182739258
overhead3:: 0.052080631256103516
overhead4:: 1.1195652484893799
overhead5:: 0
time_provenance:: 1.5476138591766357
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010938882827758789
overhead3:: 0.03312849998474121
overhead4:: 0.6893799304962158
overhead5:: 0
time_provenance:: 0.9925415515899658
curr_diff: 0 tensor(4.1063e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1063e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01335000991821289
overhead3:: 0.040547847747802734
overhead4:: 0.8473482131958008
overhead5:: 0
time_provenance:: 1.1914079189300537
curr_diff: 0 tensor(3.9577e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9577e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.016695022583007812
overhead3:: 0.046388864517211914
overhead4:: 1.0025057792663574
overhead5:: 0
time_provenance:: 1.383772611618042
curr_diff: 0 tensor(3.5356e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5356e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017688274383544922
overhead3:: 0.05243635177612305
overhead4:: 1.1240637302398682
overhead5:: 0
time_provenance:: 1.550626516342163
curr_diff: 0 tensor(2.7939e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7939e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020963191986083984
overhead3:: 0.05777096748352051
overhead4:: 1.2555222511291504
overhead5:: 0
time_provenance:: 1.7176949977874756
curr_diff: 0 tensor(2.8432e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8432e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.021971940994262695
overhead3:: 0.06126713752746582
overhead4:: 1.5198860168457031
overhead5:: 0
time_provenance:: 2.0525314807891846
curr_diff: 0 tensor(1.0010e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0010e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02316761016845703
overhead3:: 0.06273388862609863
overhead4:: 1.5883328914642334
overhead5:: 0
time_provenance:: 2.139064311981201
curr_diff: 0 tensor(9.2043e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2043e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02323174476623535
overhead3:: 0.06607937812805176
overhead4:: 1.6668660640716553
overhead5:: 0
time_provenance:: 2.2287240028381348
curr_diff: 0 tensor(8.9028e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9028e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02492380142211914
overhead3:: 0.07144308090209961
overhead4:: 1.7068588733673096
overhead5:: 0
time_provenance:: 2.2973358631134033
curr_diff: 0 tensor(8.4583e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4583e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.027218103408813477
overhead3:: 0.07456302642822266
overhead4:: 1.8383839130401611
overhead5:: 0
time_provenance:: 2.4451956748962402
curr_diff: 0 tensor(7.9194e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9194e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.03893280029296875
overhead3:: 0.10768318176269531
overhead4:: 2.3408303260803223
overhead5:: 0
time_provenance:: 3.081185817718506
curr_diff: 0 tensor(6.2824e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2824e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.311663
Train - Epoch 1, Batch: 0, Loss: 1.935360
Train - Epoch 2, Batch: 0, Loss: 1.655982
Train - Epoch 3, Batch: 0, Loss: 1.458638
Train - Epoch 4, Batch: 0, Loss: 1.300884
Train - Epoch 5, Batch: 0, Loss: 1.191931
Train - Epoch 6, Batch: 0, Loss: 1.094805
Train - Epoch 7, Batch: 0, Loss: 1.029019
Train - Epoch 8, Batch: 0, Loss: 0.972871
Train - Epoch 9, Batch: 0, Loss: 0.918884
Train - Epoch 10, Batch: 0, Loss: 0.879800
Train - Epoch 11, Batch: 0, Loss: 0.856067
Train - Epoch 12, Batch: 0, Loss: 0.821464
Train - Epoch 13, Batch: 0, Loss: 0.792843
Train - Epoch 14, Batch: 0, Loss: 0.765088
Train - Epoch 15, Batch: 0, Loss: 0.743767
Train - Epoch 16, Batch: 0, Loss: 0.727050
Train - Epoch 17, Batch: 0, Loss: 0.713014
Train - Epoch 18, Batch: 0, Loss: 0.692906
Train - Epoch 19, Batch: 0, Loss: 0.682938
Train - Epoch 20, Batch: 0, Loss: 0.672112
Train - Epoch 21, Batch: 0, Loss: 0.663980
Train - Epoch 22, Batch: 0, Loss: 0.651166
Train - Epoch 23, Batch: 0, Loss: 0.632722
Train - Epoch 24, Batch: 0, Loss: 0.631630
Train - Epoch 25, Batch: 0, Loss: 0.627748
Train - Epoch 26, Batch: 0, Loss: 0.601746
Train - Epoch 27, Batch: 0, Loss: 0.605767
Train - Epoch 28, Batch: 0, Loss: 0.600377
Train - Epoch 29, Batch: 0, Loss: 0.586040
Train - Epoch 30, Batch: 0, Loss: 0.579362
Train - Epoch 31, Batch: 0, Loss: 0.574243
training_time:: 3.372101068496704
training time full:: 3.3721470832824707
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.877000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 12
training time is 2.5457115173339844
overhead:: 0
overhead2:: 0
time_baseline:: 2.5459396839141846
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.004495382308959961
overhead3:: 0.01870417594909668
overhead4:: 0.3519124984741211
overhead5:: 0
time_provenance:: 0.5615384578704834
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.007881402969360352
overhead3:: 0.026566028594970703
overhead4:: 0.5103878974914551
overhead5:: 0
time_provenance:: 0.7714338302612305
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010459184646606445
overhead3:: 0.04264521598815918
overhead4:: 0.6888573169708252
overhead5:: 0
time_provenance:: 0.999243974685669
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013247251510620117
overhead3:: 0.04167532920837402
overhead4:: 0.8594369888305664
overhead5:: 0
time_provenance:: 1.2172465324401855
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.016326189041137695
overhead3:: 0.04876875877380371
overhead4:: 1.0580909252166748
overhead5:: 0
time_provenance:: 1.4618265628814697
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.006853818893432617
overhead3:: 0.02380514144897461
overhead4:: 0.4921450614929199
overhead5:: 0
time_provenance:: 0.7441699504852295
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.008981466293334961
overhead3:: 0.030575275421142578
overhead4:: 0.6351542472839355
overhead5:: 0
time_provenance:: 0.9356071949005127
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.011682748794555664
overhead3:: 0.0370333194732666
overhead4:: 0.7769374847412109
overhead5:: 0
time_provenance:: 1.1082639694213867
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.014032840728759766
overhead3:: 0.0435638427734375
overhead4:: 0.9465048313140869
overhead5:: 0
time_provenance:: 1.3161356449127197
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01641845703125
overhead3:: 0.05106186866760254
overhead4:: 1.1047914028167725
overhead5:: 0
time_provenance:: 1.5205962657928467
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010298013687133789
overhead3:: 0.03462815284729004
overhead4:: 0.6659231185913086
overhead5:: 0
time_provenance:: 0.9631774425506592
curr_diff: 0 tensor(4.0219e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0219e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.012313604354858398
overhead3:: 0.040004730224609375
overhead4:: 0.8299870491027832
overhead5:: 0
time_provenance:: 1.1661818027496338
curr_diff: 0 tensor(3.4350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.015172719955444336
overhead3:: 0.047061920166015625
overhead4:: 0.9840366840362549
overhead5:: 0
time_provenance:: 1.3608531951904297
curr_diff: 0 tensor(3.2323e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2323e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017973899841308594
overhead3:: 0.05180025100708008
overhead4:: 1.1162481307983398
overhead5:: 0
time_provenance:: 1.5244569778442383
curr_diff: 0 tensor(3.0120e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0120e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020065784454345703
overhead3:: 0.05765509605407715
overhead4:: 1.2776556015014648
overhead5:: 0
time_provenance:: 1.7232389450073242
curr_diff: 0 tensor(2.9117e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9117e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02348804473876953
overhead3:: 0.05466747283935547
overhead4:: 1.5248994827270508
overhead5:: 0
time_provenance:: 2.0497994422912598
curr_diff: 0 tensor(1.3747e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3747e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02227163314819336
overhead3:: 0.06415891647338867
overhead4:: 1.601576566696167
overhead5:: 0
time_provenance:: 2.147784948348999
curr_diff: 0 tensor(1.3051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.0240020751953125
overhead3:: 0.06760621070861816
overhead4:: 1.6324706077575684
overhead5:: 0
time_provenance:: 2.19181752204895
curr_diff: 0 tensor(1.2500e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2500e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.025361061096191406
overhead3:: 0.07204580307006836
overhead4:: 1.7600886821746826
overhead5:: 0
time_provenance:: 2.336552858352661
curr_diff: 0 tensor(1.2134e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2134e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.0265047550201416
overhead3:: 0.0752553939819336
overhead4:: 1.8029921054840088
overhead5:: 0
time_provenance:: 2.389249324798584
curr_diff: 0 tensor(1.2080e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2080e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.039197683334350586
overhead3:: 0.10485029220581055
overhead4:: 2.342491865158081
overhead5:: 0
time_provenance:: 3.075566530227661
curr_diff: 0 tensor(6.3273e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3273e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877000
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.251849
Train - Epoch 1, Batch: 0, Loss: 1.889738
Train - Epoch 2, Batch: 0, Loss: 1.627477
Train - Epoch 3, Batch: 0, Loss: 1.432291
Train - Epoch 4, Batch: 0, Loss: 1.292520
Train - Epoch 5, Batch: 0, Loss: 1.187972
Train - Epoch 6, Batch: 0, Loss: 1.088964
Train - Epoch 7, Batch: 0, Loss: 1.014044
Train - Epoch 8, Batch: 0, Loss: 0.963530
Train - Epoch 9, Batch: 0, Loss: 0.924737
Train - Epoch 10, Batch: 0, Loss: 0.876139
Train - Epoch 11, Batch: 0, Loss: 0.843729
Train - Epoch 12, Batch: 0, Loss: 0.817775
Train - Epoch 13, Batch: 0, Loss: 0.794341
Train - Epoch 14, Batch: 0, Loss: 0.764523
Train - Epoch 15, Batch: 0, Loss: 0.742067
Train - Epoch 16, Batch: 0, Loss: 0.728264
Train - Epoch 17, Batch: 0, Loss: 0.708915
Train - Epoch 18, Batch: 0, Loss: 0.695529
Train - Epoch 19, Batch: 0, Loss: 0.673547
Train - Epoch 20, Batch: 0, Loss: 0.673370
Train - Epoch 21, Batch: 0, Loss: 0.661686
Train - Epoch 22, Batch: 0, Loss: 0.647758
Train - Epoch 23, Batch: 0, Loss: 0.639378
Train - Epoch 24, Batch: 0, Loss: 0.632367
Train - Epoch 25, Batch: 0, Loss: 0.613646
Train - Epoch 26, Batch: 0, Loss: 0.610324
Train - Epoch 27, Batch: 0, Loss: 0.592311
Train - Epoch 28, Batch: 0, Loss: 0.597253
Train - Epoch 29, Batch: 0, Loss: 0.593996
Train - Epoch 30, Batch: 0, Loss: 0.581812
Train - Epoch 31, Batch: 0, Loss: 0.573664
training_time:: 3.4548451900482178
training time full:: 3.454890251159668
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 12
training time is 2.545833110809326
overhead:: 0
overhead2:: 0
time_baseline:: 2.5460727214813232
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.00518488883972168
overhead3:: 0.018482446670532227
overhead4:: 0.3308694362640381
overhead5:: 0
time_provenance:: 0.5496888160705566
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.00820016860961914
overhead3:: 0.02718806266784668
overhead4:: 0.5077486038208008
overhead5:: 0
time_provenance:: 0.7806668281555176
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.011474609375
overhead3:: 0.0327763557434082
overhead4:: 0.6756961345672607
overhead5:: 0
time_provenance:: 0.9929420948028564
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013391256332397461
overhead3:: 0.040697574615478516
overhead4:: 0.8498365879058838
overhead5:: 0
time_provenance:: 1.2196946144104004
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.015703678131103516
overhead3:: 0.04770922660827637
overhead4:: 1.0171163082122803
overhead5:: 0
time_provenance:: 1.4222538471221924
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.007147789001464844
overhead3:: 0.02344346046447754
overhead4:: 0.4655954837799072
overhead5:: 0
time_provenance:: 0.7128751277923584
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.009636163711547852
overhead3:: 0.03043675422668457
overhead4:: 0.6123692989349365
overhead5:: 0
time_provenance:: 0.9079136848449707
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.012518882751464844
overhead3:: 0.03674674034118652
overhead4:: 0.7804808616638184
overhead5:: 0
time_provenance:: 1.1186108589172363
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.015360593795776367
overhead3:: 0.044635772705078125
overhead4:: 0.9605677127838135
overhead5:: 0
time_provenance:: 1.350341796875
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017104625701904297
overhead3:: 0.050557613372802734
overhead4:: 1.111598253250122
overhead5:: 0
time_provenance:: 1.533376693725586
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010848522186279297
overhead3:: 0.0338284969329834
overhead4:: 0.6703667640686035
overhead5:: 0
time_provenance:: 0.9751999378204346
curr_diff: 0 tensor(3.7119e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7119e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013037443161010742
overhead3:: 0.0412442684173584
overhead4:: 0.8167641162872314
overhead5:: 0
time_provenance:: 1.1578810214996338
curr_diff: 0 tensor(3.4971e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4971e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01666545867919922
overhead3:: 0.04418802261352539
overhead4:: 0.9821083545684814
overhead5:: 0
time_provenance:: 1.366793155670166
curr_diff: 0 tensor(3.0679e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0679e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017479896545410156
overhead3:: 0.05044865608215332
overhead4:: 1.1014230251312256
overhead5:: 0
time_provenance:: 1.5207772254943848
curr_diff: 0 tensor(2.8689e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8689e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020483016967773438
overhead3:: 0.058028459548950195
overhead4:: 1.2468478679656982
overhead5:: 0
time_provenance:: 1.701988697052002
curr_diff: 0 tensor(2.7542e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7542e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020842552185058594
overhead3:: 0.059035301208496094
overhead4:: 1.4899909496307373
overhead5:: 0
time_provenance:: 2.023283004760742
curr_diff: 0 tensor(1.4706e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4706e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02276921272277832
overhead3:: 0.06471920013427734
overhead4:: 1.5571188926696777
overhead5:: 0
time_provenance:: 2.1116607189178467
curr_diff: 0 tensor(1.4076e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4076e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.023761510848999023
overhead3:: 0.06701898574829102
overhead4:: 1.6959691047668457
overhead5:: 0
time_provenance:: 2.2606186866760254
curr_diff: 0 tensor(1.3592e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3592e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.024975061416625977
overhead3:: 0.06992530822753906
overhead4:: 1.7266614437103271
overhead5:: 0
time_provenance:: 2.3134570121765137
curr_diff: 0 tensor(1.3129e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3129e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02637195587158203
overhead3:: 0.07372665405273438
overhead4:: 1.819892406463623
overhead5:: 0
time_provenance:: 2.4217894077301025
curr_diff: 0 tensor(1.2741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.03790760040283203
overhead3:: 0.10456061363220215
overhead4:: 2.3623905181884766
overhead5:: 0
time_provenance:: 3.1053478717803955
curr_diff: 0 tensor(6.1297e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1297e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.296162
Train - Epoch 1, Batch: 0, Loss: 1.922520
Train - Epoch 2, Batch: 0, Loss: 1.654173
Train - Epoch 3, Batch: 0, Loss: 1.448117
Train - Epoch 4, Batch: 0, Loss: 1.305777
Train - Epoch 5, Batch: 0, Loss: 1.195811
Train - Epoch 6, Batch: 0, Loss: 1.097288
Train - Epoch 7, Batch: 0, Loss: 1.040455
Train - Epoch 8, Batch: 0, Loss: 0.975344
Train - Epoch 9, Batch: 0, Loss: 0.919870
Train - Epoch 10, Batch: 0, Loss: 0.882576
Train - Epoch 11, Batch: 0, Loss: 0.850934
Train - Epoch 12, Batch: 0, Loss: 0.820024
Train - Epoch 13, Batch: 0, Loss: 0.790544
Train - Epoch 14, Batch: 0, Loss: 0.764013
Train - Epoch 15, Batch: 0, Loss: 0.744057
Train - Epoch 16, Batch: 0, Loss: 0.730933
Train - Epoch 17, Batch: 0, Loss: 0.720353
Train - Epoch 18, Batch: 0, Loss: 0.692184
Train - Epoch 19, Batch: 0, Loss: 0.678458
Train - Epoch 20, Batch: 0, Loss: 0.673062
Train - Epoch 21, Batch: 0, Loss: 0.657685
Train - Epoch 22, Batch: 0, Loss: 0.649465
Train - Epoch 23, Batch: 0, Loss: 0.635136
Train - Epoch 24, Batch: 0, Loss: 0.631635
Train - Epoch 25, Batch: 0, Loss: 0.619048
Train - Epoch 26, Batch: 0, Loss: 0.617214
Train - Epoch 27, Batch: 0, Loss: 0.614975
Train - Epoch 28, Batch: 0, Loss: 0.609636
Train - Epoch 29, Batch: 0, Loss: 0.592985
Train - Epoch 30, Batch: 0, Loss: 0.587985
Train - Epoch 31, Batch: 0, Loss: 0.571411
training_time:: 3.389665365219116
training time full:: 3.389711380004883
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 12
training time is 2.5489418506622314
overhead:: 0
overhead2:: 0
time_baseline:: 2.5491549968719482
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.005418062210083008
overhead3:: 0.01878523826599121
overhead4:: 0.3527083396911621
overhead5:: 0
time_provenance:: 0.5801880359649658
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.008501529693603516
overhead3:: 0.027136802673339844
overhead4:: 0.5218892097473145
overhead5:: 0
time_provenance:: 0.7888815402984619
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.012209653854370117
overhead3:: 0.03094315528869629
overhead4:: 0.6853973865509033
overhead5:: 0
time_provenance:: 1.0196783542633057
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013840436935424805
overhead3:: 0.041553497314453125
overhead4:: 0.8492376804351807
overhead5:: 0
time_provenance:: 1.2149901390075684
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017911911010742188
overhead3:: 0.04751467704772949
overhead4:: 1.04252028465271
overhead5:: 0
time_provenance:: 1.4616999626159668
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.006785154342651367
overhead3:: 0.023066043853759766
overhead4:: 0.46116042137145996
overhead5:: 0
time_provenance:: 0.7081148624420166
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.009671688079833984
overhead3:: 0.02988433837890625
overhead4:: 0.6236038208007812
overhead5:: 0
time_provenance:: 0.917461633682251
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.012212514877319336
overhead3:: 0.03741168975830078
overhead4:: 0.794727087020874
overhead5:: 0
time_provenance:: 1.1312463283538818
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.015148639678955078
overhead3:: 0.04454469680786133
overhead4:: 0.9544548988342285
overhead5:: 0
time_provenance:: 1.3434679508209229
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017859220504760742
overhead3:: 0.050664663314819336
overhead4:: 1.1144850254058838
overhead5:: 0
time_provenance:: 1.5441174507141113
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01091766357421875
overhead3:: 0.03383588790893555
overhead4:: 0.6948244571685791
overhead5:: 0
time_provenance:: 1.0054566860198975
curr_diff: 0 tensor(3.9078e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9078e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.014439105987548828
overhead3:: 0.03895974159240723
overhead4:: 0.8178572654724121
overhead5:: 0
time_provenance:: 1.1672372817993164
curr_diff: 0 tensor(3.4720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.015101194381713867
overhead3:: 0.04589438438415527
overhead4:: 0.9749469757080078
overhead5:: 0
time_provenance:: 1.3598341941833496
curr_diff: 0 tensor(3.2862e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2862e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01740741729736328
overhead3:: 0.051990509033203125
overhead4:: 1.1041572093963623
overhead5:: 0
time_provenance:: 1.5204362869262695
curr_diff: 0 tensor(3.2529e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2529e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.021303176879882812
overhead3:: 0.059317827224731445
overhead4:: 1.2470805644989014
overhead5:: 0
time_provenance:: 1.7091484069824219
curr_diff: 0 tensor(2.6208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6208e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020658493041992188
overhead3:: 0.060289621353149414
overhead4:: 1.4493777751922607
overhead5:: 0
time_provenance:: 1.9825739860534668
curr_diff: 0 tensor(1.6061e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6061e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.022883892059326172
overhead3:: 0.0616912841796875
overhead4:: 1.537858247756958
overhead5:: 0
time_provenance:: 2.08040189743042
curr_diff: 0 tensor(1.5600e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5600e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.023699283599853516
overhead3:: 0.0664525032043457
overhead4:: 1.6451222896575928
overhead5:: 0
time_provenance:: 2.2147128582000732
curr_diff: 0 tensor(1.5449e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5449e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.02491164207458496
overhead3:: 0.07103085517883301
overhead4:: 1.7152130603790283
overhead5:: 0
time_provenance:: 2.30094313621521
curr_diff: 0 tensor(1.5331e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5331e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.027448177337646484
overhead3:: 0.07532644271850586
overhead4:: 1.8023810386657715
overhead5:: 0
time_provenance:: 2.4068551063537598
curr_diff: 0 tensor(1.5165e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5165e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.03901863098144531
overhead3:: 0.10458970069885254
overhead4:: 2.352243661880493
overhead5:: 0
time_provenance:: 3.0827269554138184
curr_diff: 0 tensor(6.1949e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1949e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.302442
Train - Epoch 1, Batch: 0, Loss: 1.926687
Train - Epoch 2, Batch: 0, Loss: 1.654459
Train - Epoch 3, Batch: 0, Loss: 1.448340
Train - Epoch 4, Batch: 0, Loss: 1.300948
Train - Epoch 5, Batch: 0, Loss: 1.191213
Train - Epoch 6, Batch: 0, Loss: 1.097202
Train - Epoch 7, Batch: 0, Loss: 1.022645
Train - Epoch 8, Batch: 0, Loss: 0.972204
Train - Epoch 9, Batch: 0, Loss: 0.920379
Train - Epoch 10, Batch: 0, Loss: 0.880169
Train - Epoch 11, Batch: 0, Loss: 0.837162
Train - Epoch 12, Batch: 0, Loss: 0.824162
Train - Epoch 13, Batch: 0, Loss: 0.791717
Train - Epoch 14, Batch: 0, Loss: 0.763058
Train - Epoch 15, Batch: 0, Loss: 0.739248
Train - Epoch 16, Batch: 0, Loss: 0.724780
Train - Epoch 17, Batch: 0, Loss: 0.711231
Train - Epoch 18, Batch: 0, Loss: 0.687398
Train - Epoch 19, Batch: 0, Loss: 0.675423
Train - Epoch 20, Batch: 0, Loss: 0.666844
Train - Epoch 21, Batch: 0, Loss: 0.659724
Train - Epoch 22, Batch: 0, Loss: 0.651497
Train - Epoch 23, Batch: 0, Loss: 0.638783
Train - Epoch 24, Batch: 0, Loss: 0.633190
Train - Epoch 25, Batch: 0, Loss: 0.623305
Train - Epoch 26, Batch: 0, Loss: 0.609671
Train - Epoch 27, Batch: 0, Loss: 0.608254
Train - Epoch 28, Batch: 0, Loss: 0.594123
Train - Epoch 29, Batch: 0, Loss: 0.592421
Train - Epoch 30, Batch: 0, Loss: 0.585330
Train - Epoch 31, Batch: 0, Loss: 0.573041
training_time:: 3.3749003410339355
training time full:: 3.3749475479125977
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 12
training time is 2.551466226577759
overhead:: 0
overhead2:: 0
time_baseline:: 2.551694631576538
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.005436897277832031
overhead3:: 0.019230365753173828
overhead4:: 0.33119750022888184
overhead5:: 0
time_provenance:: 0.5473330020904541
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.008641958236694336
overhead3:: 0.02653217315673828
overhead4:: 0.5269205570220947
overhead5:: 0
time_provenance:: 0.7934651374816895
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.010133504867553711
overhead3:: 0.03240466117858887
overhead4:: 0.6743109226226807
overhead5:: 0
time_provenance:: 0.9898595809936523
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013023614883422852
overhead3:: 0.04081010818481445
overhead4:: 0.8579385280609131
overhead5:: 0
time_provenance:: 1.2117068767547607
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01576972007751465
overhead3:: 0.04665374755859375
overhead4:: 1.0411224365234375
overhead5:: 0
time_provenance:: 1.4473493099212646
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.0069882869720458984
overhead3:: 0.023079395294189453
overhead4:: 0.4516754150390625
overhead5:: 0
time_provenance:: 0.7041065692901611
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.009130716323852539
overhead3:: 0.029906272888183594
overhead4:: 0.6087851524353027
overhead5:: 0
time_provenance:: 0.9035403728485107
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.011647462844848633
overhead3:: 0.0379941463470459
overhead4:: 0.7903745174407959
overhead5:: 0
time_provenance:: 1.1159582138061523
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01422572135925293
overhead3:: 0.04485440254211426
overhead4:: 0.9467005729675293
overhead5:: 0
time_provenance:: 1.3252208232879639
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.017074108123779297
overhead3:: 0.05264997482299805
overhead4:: 1.1209862232208252
overhead5:: 0
time_provenance:: 1.5401232242584229
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.009739160537719727
overhead3:: 0.0334928035736084
overhead4:: 0.7014117240905762
overhead5:: 0
time_provenance:: 1.0013477802276611
curr_diff: 0 tensor(3.2559e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2559e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.013039112091064453
overhead3:: 0.0384981632232666
overhead4:: 0.82373046875
overhead5:: 0
time_provenance:: 1.1689386367797852
curr_diff: 0 tensor(2.8880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.014919042587280273
overhead3:: 0.04688429832458496
overhead4:: 0.9673335552215576
overhead5:: 0
time_provenance:: 1.3512544631958008
curr_diff: 0 tensor(2.7314e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7314e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01779770851135254
overhead3:: 0.052411794662475586
overhead4:: 1.1487903594970703
overhead5:: 0
time_provenance:: 1.5732636451721191
curr_diff: 0 tensor(2.2327e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2327e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020638227462768555
overhead3:: 0.05720973014831543
overhead4:: 1.274470329284668
overhead5:: 0
time_provenance:: 1.7220189571380615
curr_diff: 0 tensor(2.0306e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0306e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.01905345916748047
overhead3:: 0.05756568908691406
overhead4:: 1.50132155418396
overhead5:: 0
time_provenance:: 2.0309970378875732
curr_diff: 0 tensor(1.0752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.020337581634521484
overhead3:: 0.06391072273254395
overhead4:: 1.5382237434387207
overhead5:: 0
time_provenance:: 2.089198112487793
curr_diff: 0 tensor(1.0427e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0427e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.022743701934814453
overhead3:: 0.06680512428283691
overhead4:: 1.6863903999328613
overhead5:: 0
time_provenance:: 2.2472879886627197
curr_diff: 0 tensor(9.9075e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9075e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.023161888122558594
overhead3:: 0.0704343318939209
overhead4:: 1.725370168685913
overhead5:: 0
time_provenance:: 2.306272029876709
curr_diff: 0 tensor(9.4925e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4925e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.024944782257080078
overhead3:: 0.0745549201965332
overhead4:: 1.8035168647766113
overhead5:: 0
time_provenance:: 2.4030449390411377
curr_diff: 0 tensor(9.1588e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1588e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
overhead:: 0
overhead2:: 0.037668466567993164
overhead3:: 0.10690784454345703
overhead4:: 2.361147880554199
overhead5:: 0
time_provenance:: 3.087181568145752
curr_diff: 0 tensor(6.1072e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1072e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  MNIST5 0
tensor([42754, 25162, 28938, 43661, 25872, 54608, 42001, 58837, 45465, 21339,
        54780,  5403, 40155, 19548, 57504, 19233,  5729, 40355, 14049, 15208,
        44201,  9322, 28652, 39469, 44205,  5617, 52660, 45688, 57084, 16190])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.299129
Train - Epoch 1, Batch: 0, Loss: 1.925982
Train - Epoch 2, Batch: 0, Loss: 1.653499
Train - Epoch 3, Batch: 0, Loss: 1.450270
Train - Epoch 4, Batch: 0, Loss: 1.305526
Train - Epoch 5, Batch: 0, Loss: 1.183259
Train - Epoch 6, Batch: 0, Loss: 1.095444
Train - Epoch 7, Batch: 0, Loss: 1.024248
Train - Epoch 8, Batch: 0, Loss: 0.964924
Train - Epoch 9, Batch: 0, Loss: 0.922118
Train - Epoch 10, Batch: 0, Loss: 0.879601
Train - Epoch 11, Batch: 0, Loss: 0.844945
Train - Epoch 12, Batch: 0, Loss: 0.819233
Train - Epoch 13, Batch: 0, Loss: 0.791859
Train - Epoch 14, Batch: 0, Loss: 0.767070
Train - Epoch 15, Batch: 0, Loss: 0.743663
Train - Epoch 16, Batch: 0, Loss: 0.728677
Train - Epoch 17, Batch: 0, Loss: 0.714977
Train - Epoch 18, Batch: 0, Loss: 0.694453
Train - Epoch 19, Batch: 0, Loss: 0.675334
Train - Epoch 20, Batch: 0, Loss: 0.673769
Train - Epoch 21, Batch: 0, Loss: 0.662746
Train - Epoch 22, Batch: 0, Loss: 0.649656
Train - Epoch 23, Batch: 0, Loss: 0.637088
Train - Epoch 24, Batch: 0, Loss: 0.631139
Train - Epoch 25, Batch: 0, Loss: 0.616930
Train - Epoch 26, Batch: 0, Loss: 0.618245
Train - Epoch 27, Batch: 0, Loss: 0.601768
Train - Epoch 28, Batch: 0, Loss: 0.597348
Train - Epoch 29, Batch: 0, Loss: 0.589665
Train - Epoch 30, Batch: 0, Loss: 0.581318
Train - Epoch 31, Batch: 0, Loss: 0.571554
training_time:: 3.361556053161621
training time full:: 3.3615996837615967
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 30
training time is 2.5418238639831543
overhead:: 0
overhead2:: 0
time_baseline:: 2.5420382022857666
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.0052530765533447266
overhead3:: 0.019000768661499023
overhead4:: 0.3597695827484131
overhead5:: 0
time_provenance:: 0.5985214710235596
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008614301681518555
overhead3:: 0.02633380889892578
overhead4:: 0.5173327922821045
overhead5:: 0
time_provenance:: 0.786548376083374
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011403799057006836
overhead3:: 0.03410768508911133
overhead4:: 0.6923389434814453
overhead5:: 0
time_provenance:: 1.0141408443450928
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013879776000976562
overhead3:: 0.041288137435913086
overhead4:: 0.8466744422912598
overhead5:: 0
time_provenance:: 1.2216806411743164
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016575336456298828
overhead3:: 0.0482182502746582
overhead4:: 1.0185561180114746
overhead5:: 0
time_provenance:: 1.4344589710235596
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008069753646850586
overhead3:: 0.021821260452270508
overhead4:: 0.45894742012023926
overhead5:: 0
time_provenance:: 0.7123637199401855
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.009922266006469727
overhead3:: 0.03040480613708496
overhead4:: 0.6216299533843994
overhead5:: 0
time_provenance:: 0.9274306297302246
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012670755386352539
overhead3:: 0.03817415237426758
overhead4:: 0.8265602588653564
overhead5:: 0
time_provenance:: 1.1831693649291992
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01481485366821289
overhead3:: 0.04361534118652344
overhead4:: 0.9421970844268799
overhead5:: 0
time_provenance:: 1.3298962116241455
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.018597841262817383
overhead3:: 0.0522613525390625
overhead4:: 1.1004440784454346
overhead5:: 0
time_provenance:: 1.5260939598083496
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011603116989135742
overhead3:: 0.03470897674560547
overhead4:: 0.7005877494812012
overhead5:: 0
time_provenance:: 1.0204763412475586
curr_diff: 0 tensor(6.0304e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0304e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013606786727905273
overhead3:: 0.038399457931518555
overhead4:: 0.8262853622436523
overhead5:: 0
time_provenance:: 1.171727180480957
curr_diff: 0 tensor(5.3650e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3650e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01692509651184082
overhead3:: 0.04691267013549805
overhead4:: 0.9939239025115967
overhead5:: 0
time_provenance:: 1.3864672183990479
curr_diff: 0 tensor(4.6416e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6416e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01935291290283203
overhead3:: 0.05088472366333008
overhead4:: 1.1457948684692383
overhead5:: 0
time_provenance:: 1.572538137435913
curr_diff: 0 tensor(4.5489e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5489e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.022508859634399414
overhead3:: 0.06044125556945801
overhead4:: 1.3023266792297363
overhead5:: 0
time_provenance:: 1.7677834033966064
curr_diff: 0 tensor(4.0027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.020306110382080078
overhead3:: 0.05907082557678223
overhead4:: 1.512587308883667
overhead5:: 0
time_provenance:: 2.041792392730713
curr_diff: 0 tensor(1.7174e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7174e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.022971630096435547
overhead3:: 0.06331181526184082
overhead4:: 1.5756840705871582
overhead5:: 0
time_provenance:: 2.1231775283813477
curr_diff: 0 tensor(1.6121e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6121e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02436542510986328
overhead3:: 0.06810235977172852
overhead4:: 1.654482126235962
overhead5:: 0
time_provenance:: 2.228525161743164
curr_diff: 0 tensor(1.5722e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5722e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.026346206665039062
overhead3:: 0.07030344009399414
overhead4:: 1.7138636112213135
overhead5:: 0
time_provenance:: 2.300265073776245
curr_diff: 0 tensor(1.5141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.026814937591552734
overhead3:: 0.07546639442443848
overhead4:: 1.7992408275604248
overhead5:: 0
time_provenance:: 2.413581132888794
curr_diff: 0 tensor(1.4723e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4723e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03891277313232422
overhead3:: 0.10583758354187012
overhead4:: 2.360393524169922
overhead5:: 0
time_provenance:: 3.117626428604126
curr_diff: 0 tensor(6.1039e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1039e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.314008
Train - Epoch 1, Batch: 0, Loss: 1.936180
Train - Epoch 2, Batch: 0, Loss: 1.658597
Train - Epoch 3, Batch: 0, Loss: 1.451536
Train - Epoch 4, Batch: 0, Loss: 1.299492
Train - Epoch 5, Batch: 0, Loss: 1.184065
Train - Epoch 6, Batch: 0, Loss: 1.096389
Train - Epoch 7, Batch: 0, Loss: 1.024743
Train - Epoch 8, Batch: 0, Loss: 0.975503
Train - Epoch 9, Batch: 0, Loss: 0.926440
Train - Epoch 10, Batch: 0, Loss: 0.882825
Train - Epoch 11, Batch: 0, Loss: 0.837021
Train - Epoch 12, Batch: 0, Loss: 0.811006
Train - Epoch 13, Batch: 0, Loss: 0.785293
Train - Epoch 14, Batch: 0, Loss: 0.759619
Train - Epoch 15, Batch: 0, Loss: 0.744873
Train - Epoch 16, Batch: 0, Loss: 0.720932
Train - Epoch 17, Batch: 0, Loss: 0.709492
Train - Epoch 18, Batch: 0, Loss: 0.692543
Train - Epoch 19, Batch: 0, Loss: 0.681738
Train - Epoch 20, Batch: 0, Loss: 0.663028
Train - Epoch 21, Batch: 0, Loss: 0.658020
Train - Epoch 22, Batch: 0, Loss: 0.648790
Train - Epoch 23, Batch: 0, Loss: 0.640538
Train - Epoch 24, Batch: 0, Loss: 0.628331
Train - Epoch 25, Batch: 0, Loss: 0.612739
Train - Epoch 26, Batch: 0, Loss: 0.609525
Train - Epoch 27, Batch: 0, Loss: 0.610323
Train - Epoch 28, Batch: 0, Loss: 0.601267
Train - Epoch 29, Batch: 0, Loss: 0.581899
Train - Epoch 30, Batch: 0, Loss: 0.585624
Train - Epoch 31, Batch: 0, Loss: 0.577482
training_time:: 3.4721386432647705
training time full:: 3.4721829891204834
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 30
training time is 2.6644279956817627
overhead:: 0
overhead2:: 0
time_baseline:: 2.664642810821533
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.005113124847412109
overhead3:: 0.01879429817199707
overhead4:: 0.340238094329834
overhead5:: 0
time_provenance:: 0.5830163955688477
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008375167846679688
overhead3:: 0.025713682174682617
overhead4:: 0.5033533573150635
overhead5:: 0
time_provenance:: 0.7836291790008545
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011620044708251953
overhead3:: 0.034528255462646484
overhead4:: 0.6855995655059814
overhead5:: 0
time_provenance:: 1.0208518505096436
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.014132499694824219
overhead3:: 0.041723012924194336
overhead4:: 0.8565902709960938
overhead5:: 0
time_provenance:: 1.2299611568450928
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01642632484436035
overhead3:: 0.048746585845947266
overhead4:: 1.0236387252807617
overhead5:: 0
time_provenance:: 1.439117670059204
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.00714874267578125
overhead3:: 0.022855281829833984
overhead4:: 0.4635472297668457
overhead5:: 0
time_provenance:: 0.7104232311248779
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.010151863098144531
overhead3:: 0.029486656188964844
overhead4:: 0.6248691082000732
overhead5:: 0
time_provenance:: 0.9195172786712646
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01384878158569336
overhead3:: 0.03866720199584961
overhead4:: 0.8227009773254395
overhead5:: 0
time_provenance:: 1.175865888595581
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016271591186523438
overhead3:: 0.04230213165283203
overhead4:: 0.9343852996826172
overhead5:: 0
time_provenance:: 1.3231971263885498
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.018728017807006836
overhead3:: 0.05196642875671387
overhead4:: 1.1499128341674805
overhead5:: 0
time_provenance:: 1.591278314590454
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011543512344360352
overhead3:: 0.0337529182434082
overhead4:: 0.7079625129699707
overhead5:: 0
time_provenance:: 1.020599365234375
curr_diff: 0 tensor(8.2540e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2540e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015203475952148438
overhead3:: 0.03749418258666992
overhead4:: 0.8170506954193115
overhead5:: 0
time_provenance:: 1.1636879444122314
curr_diff: 0 tensor(7.5319e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5319e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01629352569580078
overhead3:: 0.04660606384277344
overhead4:: 0.9711523056030273
overhead5:: 0
time_provenance:: 1.3592100143432617
curr_diff: 0 tensor(7.3372e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3372e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.018076181411743164
overhead3:: 0.05115318298339844
overhead4:: 1.1062240600585938
overhead5:: 0
time_provenance:: 1.533250331878662
curr_diff: 0 tensor(6.6256e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6256e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.020683765411376953
overhead3:: 0.05841779708862305
overhead4:: 1.2555484771728516
overhead5:: 0
time_provenance:: 1.727052927017212
curr_diff: 0 tensor(6.2496e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2496e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021014690399169922
overhead3:: 0.05726504325866699
overhead4:: 1.4783318042755127
overhead5:: 0
time_provenance:: 2.0114362239837646
curr_diff: 0 tensor(1.8756e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8756e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.022601842880249023
overhead3:: 0.06177544593811035
overhead4:: 1.6016275882720947
overhead5:: 0
time_provenance:: 2.1528773307800293
curr_diff: 0 tensor(1.7834e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7834e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02398514747619629
overhead3:: 0.06695961952209473
overhead4:: 1.6543736457824707
overhead5:: 0
time_provenance:: 2.2245984077453613
curr_diff: 0 tensor(1.7145e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7145e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02625560760498047
overhead3:: 0.06897878646850586
overhead4:: 1.7252256870269775
overhead5:: 0
time_provenance:: 2.3109476566314697
curr_diff: 0 tensor(1.6530e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6530e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.027913331985473633
overhead3:: 0.0749967098236084
overhead4:: 1.82706880569458
overhead5:: 0
time_provenance:: 2.434046506881714
curr_diff: 0 tensor(1.5819e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5819e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03885698318481445
overhead3:: 0.10656332969665527
overhead4:: 2.3697290420532227
overhead5:: 0
time_provenance:: 3.117091655731201
curr_diff: 0 tensor(6.2244e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2244e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.277499
Train - Epoch 1, Batch: 0, Loss: 1.912073
Train - Epoch 2, Batch: 0, Loss: 1.644001
Train - Epoch 3, Batch: 0, Loss: 1.444886
Train - Epoch 4, Batch: 0, Loss: 1.294089
Train - Epoch 5, Batch: 0, Loss: 1.183390
Train - Epoch 6, Batch: 0, Loss: 1.098825
Train - Epoch 7, Batch: 0, Loss: 1.031250
Train - Epoch 8, Batch: 0, Loss: 0.969031
Train - Epoch 9, Batch: 0, Loss: 0.916164
Train - Epoch 10, Batch: 0, Loss: 0.884517
Train - Epoch 11, Batch: 0, Loss: 0.839937
Train - Epoch 12, Batch: 0, Loss: 0.821754
Train - Epoch 13, Batch: 0, Loss: 0.795278
Train - Epoch 14, Batch: 0, Loss: 0.764269
Train - Epoch 15, Batch: 0, Loss: 0.753948
Train - Epoch 16, Batch: 0, Loss: 0.728934
Train - Epoch 17, Batch: 0, Loss: 0.715115
Train - Epoch 18, Batch: 0, Loss: 0.689066
Train - Epoch 19, Batch: 0, Loss: 0.680586
Train - Epoch 20, Batch: 0, Loss: 0.663987
Train - Epoch 21, Batch: 0, Loss: 0.662826
Train - Epoch 22, Batch: 0, Loss: 0.644192
Train - Epoch 23, Batch: 0, Loss: 0.636628
Train - Epoch 24, Batch: 0, Loss: 0.622615
Train - Epoch 25, Batch: 0, Loss: 0.619543
Train - Epoch 26, Batch: 0, Loss: 0.608929
Train - Epoch 27, Batch: 0, Loss: 0.604394
Train - Epoch 28, Batch: 0, Loss: 0.591435
Train - Epoch 29, Batch: 0, Loss: 0.585009
Train - Epoch 30, Batch: 0, Loss: 0.576836
Train - Epoch 31, Batch: 0, Loss: 0.573167
training_time:: 3.3790431022644043
training time full:: 3.3790924549102783
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 30
training time is 2.5379316806793213
overhead:: 0
overhead2:: 0
time_baseline:: 2.5381901264190674
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.005159616470336914
overhead3:: 0.018668651580810547
overhead4:: 0.3374660015106201
overhead5:: 0
time_provenance:: 0.5577824115753174
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.00886225700378418
overhead3:: 0.027158021926879883
overhead4:: 0.5076079368591309
overhead5:: 0
time_provenance:: 0.7831456661224365
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011544466018676758
overhead3:: 0.033905029296875
overhead4:: 0.6793985366821289
overhead5:: 0
time_provenance:: 1.0102598667144775
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015903711318969727
overhead3:: 0.04376554489135742
overhead4:: 0.8671863079071045
overhead5:: 0
time_provenance:: 1.2375731468200684
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01700448989868164
overhead3:: 0.04955601692199707
overhead4:: 1.0385875701904297
overhead5:: 0
time_provenance:: 1.4572484493255615
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.0070896148681640625
overhead3:: 0.023075342178344727
overhead4:: 0.4536855220794678
overhead5:: 0
time_provenance:: 0.7123777866363525
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.00990438461303711
overhead3:: 0.030564546585083008
overhead4:: 0.6396818161010742
overhead5:: 0
time_provenance:: 0.9453330039978027
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012567281723022461
overhead3:: 0.03588461875915527
overhead4:: 0.7715237140655518
overhead5:: 0
time_provenance:: 1.119032859802246
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015056371688842773
overhead3:: 0.0437772274017334
overhead4:: 0.9397470951080322
overhead5:: 0
time_provenance:: 1.320807695388794
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01809406280517578
overhead3:: 0.05170893669128418
overhead4:: 1.1024479866027832
overhead5:: 0
time_provenance:: 1.535045862197876
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011969566345214844
overhead3:: 0.033403873443603516
overhead4:: 0.6756913661956787
overhead5:: 0
time_provenance:: 0.9804134368896484
curr_diff: 0 tensor(5.7308e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7308e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013810873031616211
overhead3:: 0.03860950469970703
overhead4:: 0.8295893669128418
overhead5:: 0
time_provenance:: 1.171579360961914
curr_diff: 0 tensor(5.2648e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2648e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015922069549560547
overhead3:: 0.04631304740905762
overhead4:: 0.9925382137298584
overhead5:: 0
time_provenance:: 1.3789381980895996
curr_diff: 0 tensor(4.6374e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6374e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.019027233123779297
overhead3:: 0.05177593231201172
overhead4:: 1.1127517223358154
overhead5:: 0
time_provenance:: 1.5339338779449463
curr_diff: 0 tensor(4.4229e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4229e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021358013153076172
overhead3:: 0.05828571319580078
overhead4:: 1.2654011249542236
overhead5:: 0
time_provenance:: 1.7312211990356445
curr_diff: 0 tensor(3.9791e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9791e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021436691284179688
overhead3:: 0.0569150447845459
overhead4:: 1.4359819889068604
overhead5:: 0
time_provenance:: 1.9604358673095703
curr_diff: 0 tensor(1.6349e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6349e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.023042678833007812
overhead3:: 0.06272602081298828
overhead4:: 1.5959787368774414
overhead5:: 0
time_provenance:: 2.144035816192627
curr_diff: 0 tensor(1.6038e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6038e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02414727210998535
overhead3:: 0.06749200820922852
overhead4:: 1.6464271545410156
overhead5:: 0
time_provenance:: 2.2213828563690186
curr_diff: 0 tensor(1.5467e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5467e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.027942180633544922
overhead3:: 0.07410073280334473
overhead4:: 1.7621536254882812
overhead5:: 0
time_provenance:: 2.3548362255096436
curr_diff: 0 tensor(1.5021e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5021e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02785015106201172
overhead3:: 0.07457613945007324
overhead4:: 1.7948665618896484
overhead5:: 0
time_provenance:: 2.4057085514068604
curr_diff: 0 tensor(1.4749e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4749e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03918647766113281
overhead3:: 0.10590863227844238
overhead4:: 2.3524694442749023
overhead5:: 0
time_provenance:: 3.1044843196868896
curr_diff: 0 tensor(6.1586e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1586e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.303856
Train - Epoch 1, Batch: 0, Loss: 1.926136
Train - Epoch 2, Batch: 0, Loss: 1.651638
Train - Epoch 3, Batch: 0, Loss: 1.452957
Train - Epoch 4, Batch: 0, Loss: 1.295052
Train - Epoch 5, Batch: 0, Loss: 1.182053
Train - Epoch 6, Batch: 0, Loss: 1.095470
Train - Epoch 7, Batch: 0, Loss: 1.031222
Train - Epoch 8, Batch: 0, Loss: 0.965695
Train - Epoch 9, Batch: 0, Loss: 0.922629
Train - Epoch 10, Batch: 0, Loss: 0.887421
Train - Epoch 11, Batch: 0, Loss: 0.843227
Train - Epoch 12, Batch: 0, Loss: 0.820274
Train - Epoch 13, Batch: 0, Loss: 0.789236
Train - Epoch 14, Batch: 0, Loss: 0.765936
Train - Epoch 15, Batch: 0, Loss: 0.744898
Train - Epoch 16, Batch: 0, Loss: 0.729181
Train - Epoch 17, Batch: 0, Loss: 0.701292
Train - Epoch 18, Batch: 0, Loss: 0.691171
Train - Epoch 19, Batch: 0, Loss: 0.690986
Train - Epoch 20, Batch: 0, Loss: 0.663736
Train - Epoch 21, Batch: 0, Loss: 0.649416
Train - Epoch 22, Batch: 0, Loss: 0.649951
Train - Epoch 23, Batch: 0, Loss: 0.641923
Train - Epoch 24, Batch: 0, Loss: 0.632043
Train - Epoch 25, Batch: 0, Loss: 0.617779
Train - Epoch 26, Batch: 0, Loss: 0.616102
Train - Epoch 27, Batch: 0, Loss: 0.604427
Train - Epoch 28, Batch: 0, Loss: 0.601358
Train - Epoch 29, Batch: 0, Loss: 0.595315
Train - Epoch 30, Batch: 0, Loss: 0.582665
Train - Epoch 31, Batch: 0, Loss: 0.579074
training_time:: 3.380244493484497
training time full:: 3.3802902698516846
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 30
training time is 2.5767385959625244
overhead:: 0
overhead2:: 0
time_baseline:: 2.576967239379883
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.00542140007019043
overhead3:: 0.0185089111328125
overhead4:: 0.32714343070983887
overhead5:: 0
time_provenance:: 0.5461552143096924
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008992195129394531
overhead3:: 0.025628328323364258
overhead4:: 0.5252530574798584
overhead5:: 0
time_provenance:: 0.8011641502380371
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.010756254196166992
overhead3:: 0.03359365463256836
overhead4:: 0.6688356399536133
overhead5:: 0
time_provenance:: 0.9938850402832031
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01416468620300293
overhead3:: 0.041127681732177734
overhead4:: 0.8650555610656738
overhead5:: 0
time_provenance:: 1.2458109855651855
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.017004728317260742
overhead3:: 0.04842519760131836
overhead4:: 1.0209946632385254
overhead5:: 0
time_provenance:: 1.4406778812408447
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.007157087326049805
overhead3:: 0.022763490676879883
overhead4:: 0.45987963676452637
overhead5:: 0
time_provenance:: 0.712425708770752
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.009768009185791016
overhead3:: 0.02933812141418457
overhead4:: 0.6083965301513672
overhead5:: 0
time_provenance:: 0.9121854305267334
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012914180755615234
overhead3:: 0.03733539581298828
overhead4:: 0.7939426898956299
overhead5:: 0
time_provenance:: 1.132702350616455
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01565098762512207
overhead3:: 0.04481649398803711
overhead4:: 0.9402191638946533
overhead5:: 0
time_provenance:: 1.3262627124786377
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01823735237121582
overhead3:: 0.05222177505493164
overhead4:: 1.1237237453460693
overhead5:: 0
time_provenance:: 1.5713953971862793
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012171268463134766
overhead3:: 0.03242087364196777
overhead4:: 0.6719584465026855
overhead5:: 0
time_provenance:: 0.9783234596252441
curr_diff: 0 tensor(6.8150e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8150e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012683868408203125
overhead3:: 0.03934597969055176
overhead4:: 0.8238484859466553
overhead5:: 0
time_provenance:: 1.17433500289917
curr_diff: 0 tensor(6.3947e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3947e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.016262292861938477
overhead3:: 0.04536914825439453
overhead4:: 0.9796802997589111
overhead5:: 0
time_provenance:: 1.3789117336273193
curr_diff: 0 tensor(5.8168e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8168e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.017391443252563477
overhead3:: 0.051221609115600586
overhead4:: 1.1194627285003662
overhead5:: 0
time_provenance:: 1.5473549365997314
curr_diff: 0 tensor(5.7620e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7620e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02041006088256836
overhead3:: 0.05868721008300781
overhead4:: 1.2595446109771729
overhead5:: 0
time_provenance:: 1.7319591045379639
curr_diff: 0 tensor(5.2908e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2908e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02210235595703125
overhead3:: 0.05911135673522949
overhead4:: 1.4841957092285156
overhead5:: 0
time_provenance:: 2.011836051940918
curr_diff: 0 tensor(1.7827e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7827e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.022278308868408203
overhead3:: 0.06368207931518555
overhead4:: 1.5656089782714844
overhead5:: 0
time_provenance:: 2.1168062686920166
curr_diff: 0 tensor(1.6781e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6781e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.023581981658935547
overhead3:: 0.06804609298706055
overhead4:: 1.6576449871063232
overhead5:: 0
time_provenance:: 2.225949287414551
curr_diff: 0 tensor(1.5779e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5779e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02507781982421875
overhead3:: 0.0699300765991211
overhead4:: 1.7257671356201172
overhead5:: 0
time_provenance:: 2.313559055328369
curr_diff: 0 tensor(1.5195e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5195e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.03014087677001953
overhead3:: 0.06969428062438965
overhead4:: 1.8216686248779297
overhead5:: 0
time_provenance:: 2.4281399250030518
curr_diff: 0 tensor(1.4915e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4915e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.04277300834655762
overhead3:: 0.10649418830871582
overhead4:: 2.404104471206665
overhead5:: 0
time_provenance:: 3.155665397644043
curr_diff: 0 tensor(6.1637e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1637e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.306465
Train - Epoch 1, Batch: 0, Loss: 1.923325
Train - Epoch 2, Batch: 0, Loss: 1.651370
Train - Epoch 3, Batch: 0, Loss: 1.457144
Train - Epoch 4, Batch: 0, Loss: 1.305403
Train - Epoch 5, Batch: 0, Loss: 1.197430
Train - Epoch 6, Batch: 0, Loss: 1.108373
Train - Epoch 7, Batch: 0, Loss: 1.031411
Train - Epoch 8, Batch: 0, Loss: 0.965271
Train - Epoch 9, Batch: 0, Loss: 0.922834
Train - Epoch 10, Batch: 0, Loss: 0.876694
Train - Epoch 11, Batch: 0, Loss: 0.851956
Train - Epoch 12, Batch: 0, Loss: 0.808888
Train - Epoch 13, Batch: 0, Loss: 0.793949
Train - Epoch 14, Batch: 0, Loss: 0.773388
Train - Epoch 15, Batch: 0, Loss: 0.746508
Train - Epoch 16, Batch: 0, Loss: 0.724075
Train - Epoch 17, Batch: 0, Loss: 0.711216
Train - Epoch 18, Batch: 0, Loss: 0.695509
Train - Epoch 19, Batch: 0, Loss: 0.682709
Train - Epoch 20, Batch: 0, Loss: 0.669811
Train - Epoch 21, Batch: 0, Loss: 0.661576
Train - Epoch 22, Batch: 0, Loss: 0.648604
Train - Epoch 23, Batch: 0, Loss: 0.644161
Train - Epoch 24, Batch: 0, Loss: 0.620652
Train - Epoch 25, Batch: 0, Loss: 0.626320
Train - Epoch 26, Batch: 0, Loss: 0.614867
Train - Epoch 27, Batch: 0, Loss: 0.610248
Train - Epoch 28, Batch: 0, Loss: 0.599212
Train - Epoch 29, Batch: 0, Loss: 0.594172
Train - Epoch 30, Batch: 0, Loss: 0.579538
Train - Epoch 31, Batch: 0, Loss: 0.575146
training_time:: 3.376664161682129
training time full:: 3.376707077026367
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 30
training time is 2.533400535583496
overhead:: 0
overhead2:: 0
time_baseline:: 2.533629894256592
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.005364656448364258
overhead3:: 0.01880335807800293
overhead4:: 0.3357722759246826
overhead5:: 0
time_provenance:: 0.557091474533081
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.008204221725463867
overhead3:: 0.026959657669067383
overhead4:: 0.5085022449493408
overhead5:: 0
time_provenance:: 0.7824294567108154
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011018753051757812
overhead3:: 0.03372621536254883
overhead4:: 0.6993811130523682
overhead5:: 0
time_provenance:: 1.024841547012329
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.014498472213745117
overhead3:: 0.04191184043884277
overhead4:: 0.8588876724243164
overhead5:: 0
time_provenance:: 1.2255311012268066
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01685476303100586
overhead3:: 0.04863619804382324
overhead4:: 1.0231037139892578
overhead5:: 0
time_provenance:: 1.439286231994629
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.006775856018066406
overhead3:: 0.022859811782836914
overhead4:: 0.45400261878967285
overhead5:: 0
time_provenance:: 0.7139172554016113
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.009519577026367188
overhead3:: 0.029837369918823242
overhead4:: 0.6233165264129639
overhead5:: 0
time_provenance:: 0.9142346382141113
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.012095212936401367
overhead3:: 0.037097930908203125
overhead4:: 0.7889373302459717
overhead5:: 0
time_provenance:: 1.131134033203125
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.014606475830078125
overhead3:: 0.04280829429626465
overhead4:: 0.946190595626831
overhead5:: 0
time_provenance:: 1.3344624042510986
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01863694190979004
overhead3:: 0.052581071853637695
overhead4:: 1.1083028316497803
overhead5:: 0
time_provenance:: 1.538203239440918
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0036, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0036, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.011607170104980469
overhead3:: 0.03173375129699707
overhead4:: 0.6654953956604004
overhead5:: 0
time_provenance:: 0.9756689071655273
curr_diff: 0 tensor(6.1181e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1181e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.013261079788208008
overhead3:: 0.03950786590576172
overhead4:: 0.8158450126647949
overhead5:: 0
time_provenance:: 1.165189266204834
curr_diff: 0 tensor(5.3527e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3527e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.015231609344482422
overhead3:: 0.04599308967590332
overhead4:: 0.9646165370941162
overhead5:: 0
time_provenance:: 1.3543379306793213
curr_diff: 0 tensor(4.6783e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6783e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.01880478858947754
overhead3:: 0.05181002616882324
overhead4:: 1.1110930442810059
overhead5:: 0
time_provenance:: 1.5427827835083008
curr_diff: 0 tensor(4.0284e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0284e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021740198135375977
overhead3:: 0.061157941818237305
overhead4:: 1.2577788829803467
overhead5:: 0
time_provenance:: 1.7228131294250488
curr_diff: 0 tensor(3.7687e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7687e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.022433757781982422
overhead3:: 0.05979800224304199
overhead4:: 1.5013511180877686
overhead5:: 0
time_provenance:: 2.033864974975586
curr_diff: 0 tensor(1.8786e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8786e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.021666288375854492
overhead3:: 0.06338930130004883
overhead4:: 1.5452346801757812
overhead5:: 0
time_provenance:: 2.0961060523986816
curr_diff: 0 tensor(1.8149e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8149e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02376532554626465
overhead3:: 0.06735968589782715
overhead4:: 1.6668989658355713
overhead5:: 0
time_provenance:: 2.235835313796997
curr_diff: 0 tensor(1.7470e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7470e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.02564239501953125
overhead3:: 0.07259392738342285
overhead4:: 1.735217809677124
overhead5:: 0
time_provenance:: 2.333308458328247
curr_diff: 0 tensor(1.6840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.030210494995117188
overhead3:: 0.07839655876159668
overhead4:: 1.8447973728179932
overhead5:: 0
time_provenance:: 2.450188636779785
curr_diff: 0 tensor(1.6283e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6283e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
overhead:: 0
overhead2:: 0.040235042572021484
overhead3:: 0.1075582504272461
overhead4:: 2.35166597366333
overhead5:: 0
time_provenance:: 3.111663341522217
curr_diff: 0 tensor(6.2196e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2196e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0034, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0034, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
deletion rate:: 0.001
python3 generate_rand_ids 0.001  MNIST5 0
tensor([ 2049, 42754, 44163, 46087, 44553, 28938, 43661, 25872, 42001,  3600,
        29207, 45465,  5403, 54174, 25759, 57504, 19233, 40355, 25000, 44201,
        32554, 39469, 44205, 52660, 39348, 38964, 46012, 42173, 16190,  2876,
        10559, 28349, 35913, 25162,  2510, 54608, 56146, 58837, 48597,  6742,
        36055, 40155, 19548, 21339, 31325,  5729, 14049, 57084, 15208,  9322,
         5355, 28652,  5617, 46325, 14070, 45688, 49529, 57723, 54780, 48510])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.298878
Train - Epoch 1, Batch: 0, Loss: 1.928049
Train - Epoch 2, Batch: 0, Loss: 1.649553
Train - Epoch 3, Batch: 0, Loss: 1.454281
Train - Epoch 4, Batch: 0, Loss: 1.305333
Train - Epoch 5, Batch: 0, Loss: 1.194050
Train - Epoch 6, Batch: 0, Loss: 1.098908
Train - Epoch 7, Batch: 0, Loss: 1.026163
Train - Epoch 8, Batch: 0, Loss: 0.969174
Train - Epoch 9, Batch: 0, Loss: 0.928411
Train - Epoch 10, Batch: 0, Loss: 0.880414
Train - Epoch 11, Batch: 0, Loss: 0.850640
Train - Epoch 12, Batch: 0, Loss: 0.809401
Train - Epoch 13, Batch: 0, Loss: 0.782868
Train - Epoch 14, Batch: 0, Loss: 0.767659
Train - Epoch 15, Batch: 0, Loss: 0.748006
Train - Epoch 16, Batch: 0, Loss: 0.727233
Train - Epoch 17, Batch: 0, Loss: 0.713013
Train - Epoch 18, Batch: 0, Loss: 0.697305
Train - Epoch 19, Batch: 0, Loss: 0.677529
Train - Epoch 20, Batch: 0, Loss: 0.666653
Train - Epoch 21, Batch: 0, Loss: 0.665294
Train - Epoch 22, Batch: 0, Loss: 0.648702
Train - Epoch 23, Batch: 0, Loss: 0.649069
Train - Epoch 24, Batch: 0, Loss: 0.626692
Train - Epoch 25, Batch: 0, Loss: 0.628544
Train - Epoch 26, Batch: 0, Loss: 0.615541
Train - Epoch 27, Batch: 0, Loss: 0.612788
Train - Epoch 28, Batch: 0, Loss: 0.594240
Train - Epoch 29, Batch: 0, Loss: 0.591102
Train - Epoch 30, Batch: 0, Loss: 0.578895
Train - Epoch 31, Batch: 0, Loss: 0.576723
training_time:: 3.377983808517456
training time full:: 3.3780324459075928
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 60
training time is 2.560654401779175
overhead:: 0
overhead2:: 0
time_baseline:: 2.5608577728271484
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.0053691864013671875
overhead3:: 0.019034385681152344
overhead4:: 0.3302888870239258
overhead5:: 0
time_provenance:: 0.5623886585235596
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.008072376251220703
overhead3:: 0.02704906463623047
overhead4:: 0.5167806148529053
overhead5:: 0
time_provenance:: 0.7987232208251953
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010788679122924805
overhead3:: 0.0343019962310791
overhead4:: 0.67156982421875
overhead5:: 0
time_provenance:: 1.0028979778289795
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014408588409423828
overhead3:: 0.0405733585357666
overhead4:: 0.8704085350036621
overhead5:: 0
time_provenance:: 1.247795581817627
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.017300128936767578
overhead3:: 0.04965567588806152
overhead4:: 1.0313029289245605
overhead5:: 0
time_provenance:: 1.4693853855133057
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007184267044067383
overhead3:: 0.023508548736572266
overhead4:: 0.45621347427368164
overhead5:: 0
time_provenance:: 0.7189474105834961
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.009640932083129883
overhead3:: 0.030193090438842773
overhead4:: 0.6184577941894531
overhead5:: 0
time_provenance:: 0.9459915161132812
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013247251510620117
overhead3:: 0.03717947006225586
overhead4:: 0.8059225082397461
overhead5:: 0
time_provenance:: 1.1549031734466553
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.015677690505981445
overhead3:: 0.04472041130065918
overhead4:: 0.9556479454040527
overhead5:: 0
time_provenance:: 1.3513119220733643
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01830291748046875
overhead3:: 0.051671743392944336
overhead4:: 1.1252069473266602
overhead5:: 0
time_provenance:: 1.5723071098327637
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01193690299987793
overhead3:: 0.03257918357849121
overhead4:: 0.6673762798309326
overhead5:: 0
time_provenance:: 0.9881653785705566
curr_diff: 0 tensor(6.9584e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9584e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014265060424804688
overhead3:: 0.038542747497558594
overhead4:: 0.8392505645751953
overhead5:: 0
time_provenance:: 1.1987791061401367
curr_diff: 0 tensor(6.2371e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2371e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.017235994338989258
overhead3:: 0.04677605628967285
overhead4:: 0.9922065734863281
overhead5:: 0
time_provenance:: 1.3974003791809082
curr_diff: 0 tensor(5.9999e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9999e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.018849611282348633
overhead3:: 0.05028700828552246
overhead4:: 1.1099278926849365
overhead5:: 0
time_provenance:: 1.547513723373413
curr_diff: 0 tensor(5.7745e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7745e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021359682083129883
overhead3:: 0.057253122329711914
overhead4:: 1.2535886764526367
overhead5:: 0
time_provenance:: 1.7383379936218262
curr_diff: 0 tensor(5.3748e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3748e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.020597219467163086
overhead3:: 0.05947303771972656
overhead4:: 1.4829132556915283
overhead5:: 0
time_provenance:: 2.0308456420898438
curr_diff: 0 tensor(2.7183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02461528778076172
overhead3:: 0.06278252601623535
overhead4:: 1.5583372116088867
overhead5:: 0
time_provenance:: 2.126565456390381
curr_diff: 0 tensor(2.5976e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5976e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.024944782257080078
overhead3:: 0.06818151473999023
overhead4:: 1.6456377506256104
overhead5:: 0
time_provenance:: 2.2433784008026123
curr_diff: 0 tensor(2.5264e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5264e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.026625394821166992
overhead3:: 0.07109594345092773
overhead4:: 1.7844629287719727
overhead5:: 0
time_provenance:: 2.3946237564086914
curr_diff: 0 tensor(2.4665e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4665e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.028974533081054688
overhead3:: 0.07102131843566895
overhead4:: 1.7728111743927002
overhead5:: 0
time_provenance:: 2.4036030769348145
curr_diff: 0 tensor(2.4380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.039519309997558594
overhead3:: 0.105621337890625
overhead4:: 2.3862712383270264
overhead5:: 0
time_provenance:: 3.165335178375244
curr_diff: 0 tensor(6.3175e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3175e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.309458
Train - Epoch 1, Batch: 0, Loss: 1.930664
Train - Epoch 2, Batch: 0, Loss: 1.658830
Train - Epoch 3, Batch: 0, Loss: 1.456506
Train - Epoch 4, Batch: 0, Loss: 1.310718
Train - Epoch 5, Batch: 0, Loss: 1.190742
Train - Epoch 6, Batch: 0, Loss: 1.097885
Train - Epoch 7, Batch: 0, Loss: 1.030927
Train - Epoch 8, Batch: 0, Loss: 0.970062
Train - Epoch 9, Batch: 0, Loss: 0.923143
Train - Epoch 10, Batch: 0, Loss: 0.882629
Train - Epoch 11, Batch: 0, Loss: 0.851028
Train - Epoch 12, Batch: 0, Loss: 0.817379
Train - Epoch 13, Batch: 0, Loss: 0.789297
Train - Epoch 14, Batch: 0, Loss: 0.773432
Train - Epoch 15, Batch: 0, Loss: 0.748819
Train - Epoch 16, Batch: 0, Loss: 0.727003
Train - Epoch 17, Batch: 0, Loss: 0.712757
Train - Epoch 18, Batch: 0, Loss: 0.696850
Train - Epoch 19, Batch: 0, Loss: 0.691725
Train - Epoch 20, Batch: 0, Loss: 0.672013
Train - Epoch 21, Batch: 0, Loss: 0.663101
Train - Epoch 22, Batch: 0, Loss: 0.646062
Train - Epoch 23, Batch: 0, Loss: 0.635749
Train - Epoch 24, Batch: 0, Loss: 0.625583
Train - Epoch 25, Batch: 0, Loss: 0.618282
Train - Epoch 26, Batch: 0, Loss: 0.610046
Train - Epoch 27, Batch: 0, Loss: 0.600810
Train - Epoch 28, Batch: 0, Loss: 0.599052
Train - Epoch 29, Batch: 0, Loss: 0.591848
Train - Epoch 30, Batch: 0, Loss: 0.588318
Train - Epoch 31, Batch: 0, Loss: 0.580478
training_time:: 3.4431629180908203
training time full:: 3.443208932876587
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.874400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 60
training time is 2.571073293685913
overhead:: 0
overhead2:: 0
time_baseline:: 2.571338653564453
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.005072593688964844
overhead3:: 0.018702268600463867
overhead4:: 0.32501959800720215
overhead5:: 0
time_provenance:: 0.5615792274475098
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.00848388671875
overhead3:: 0.026643753051757812
overhead4:: 0.509188175201416
overhead5:: 0
time_provenance:: 0.7948892116546631
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874400
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01129293441772461
overhead3:: 0.03349041938781738
overhead4:: 0.6844308376312256
overhead5:: 0
time_provenance:: 1.022491693496704
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014656543731689453
overhead3:: 0.04204106330871582
overhead4:: 0.8507206439971924
overhead5:: 0
time_provenance:: 1.2342243194580078
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.017096996307373047
overhead3:: 0.04816269874572754
overhead4:: 1.0166347026824951
overhead5:: 0
time_provenance:: 1.4444022178649902
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007479667663574219
overhead3:: 0.022778749465942383
overhead4:: 0.45244312286376953
overhead5:: 0
time_provenance:: 0.7211194038391113
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010138511657714844
overhead3:: 0.0298769474029541
overhead4:: 0.6323287487030029
overhead5:: 0
time_provenance:: 0.9411044120788574
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.012495756149291992
overhead3:: 0.03739333152770996
overhead4:: 0.7780559062957764
overhead5:: 0
time_provenance:: 1.1317107677459717
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.016131162643432617
overhead3:: 0.04445624351501465
overhead4:: 0.9463086128234863
overhead5:: 0
time_provenance:: 1.3448445796966553
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.018614768981933594
overhead3:: 0.05314016342163086
overhead4:: 1.1492033004760742
overhead5:: 0
time_provenance:: 1.6057255268096924
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01219940185546875
overhead3:: 0.03165245056152344
overhead4:: 0.6797864437103271
overhead5:: 0
time_provenance:: 0.9991793632507324
curr_diff: 0 tensor(8.6375e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6375e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013200044631958008
overhead3:: 0.03900718688964844
overhead4:: 0.8136246204376221
overhead5:: 0
time_provenance:: 1.1773080825805664
curr_diff: 0 tensor(7.8081e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8081e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01578807830810547
overhead3:: 0.04524064064025879
overhead4:: 0.9663872718811035
overhead5:: 0
time_provenance:: 1.376650333404541
curr_diff: 0 tensor(7.0836e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0836e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.018377065658569336
overhead3:: 0.05232357978820801
overhead4:: 1.1053318977355957
overhead5:: 0
time_provenance:: 1.5546998977661133
curr_diff: 0 tensor(6.4158e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4158e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02147221565246582
overhead3:: 0.05745506286621094
overhead4:: 1.278832197189331
overhead5:: 0
time_provenance:: 1.76021146774292
curr_diff: 0 tensor(5.7941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7941e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021230459213256836
overhead3:: 0.057607412338256836
overhead4:: 1.4936745166778564
overhead5:: 0
time_provenance:: 2.044414758682251
curr_diff: 0 tensor(3.1109e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1109e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.022580623626708984
overhead3:: 0.06087923049926758
overhead4:: 1.5524718761444092
overhead5:: 0
time_provenance:: 2.1220901012420654
curr_diff: 0 tensor(2.9679e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9679e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.024881839752197266
overhead3:: 0.06662917137145996
overhead4:: 1.6816635131835938
overhead5:: 0
time_provenance:: 2.2737717628479004
curr_diff: 0 tensor(2.8720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02588200569152832
overhead3:: 0.07164692878723145
overhead4:: 1.7413959503173828
overhead5:: 0
time_provenance:: 2.3465559482574463
curr_diff: 0 tensor(2.7773e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7773e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.027207136154174805
overhead3:: 0.07560348510742188
overhead4:: 1.8110804557800293
overhead5:: 0
time_provenance:: 2.439303398132324
curr_diff: 0 tensor(2.7466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.040056467056274414
overhead3:: 0.10664701461791992
overhead4:: 2.3751261234283447
overhead5:: 0
time_provenance:: 3.1615219116210938
curr_diff: 0 tensor(6.0605e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0605e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.299849
Train - Epoch 1, Batch: 0, Loss: 1.919851
Train - Epoch 2, Batch: 0, Loss: 1.652838
Train - Epoch 3, Batch: 0, Loss: 1.456453
Train - Epoch 4, Batch: 0, Loss: 1.308794
Train - Epoch 5, Batch: 0, Loss: 1.193238
Train - Epoch 6, Batch: 0, Loss: 1.099511
Train - Epoch 7, Batch: 0, Loss: 1.027714
Train - Epoch 8, Batch: 0, Loss: 0.982328
Train - Epoch 9, Batch: 0, Loss: 0.926972
Train - Epoch 10, Batch: 0, Loss: 0.882228
Train - Epoch 11, Batch: 0, Loss: 0.850803
Train - Epoch 12, Batch: 0, Loss: 0.821550
Train - Epoch 13, Batch: 0, Loss: 0.789170
Train - Epoch 14, Batch: 0, Loss: 0.769438
Train - Epoch 15, Batch: 0, Loss: 0.751566
Train - Epoch 16, Batch: 0, Loss: 0.725311
Train - Epoch 17, Batch: 0, Loss: 0.709631
Train - Epoch 18, Batch: 0, Loss: 0.695365
Train - Epoch 19, Batch: 0, Loss: 0.687935
Train - Epoch 20, Batch: 0, Loss: 0.673062
Train - Epoch 21, Batch: 0, Loss: 0.655788
Train - Epoch 22, Batch: 0, Loss: 0.643531
Train - Epoch 23, Batch: 0, Loss: 0.634664
Train - Epoch 24, Batch: 0, Loss: 0.624188
Train - Epoch 25, Batch: 0, Loss: 0.619921
Train - Epoch 26, Batch: 0, Loss: 0.617609
Train - Epoch 27, Batch: 0, Loss: 0.600638
Train - Epoch 28, Batch: 0, Loss: 0.598664
Train - Epoch 29, Batch: 0, Loss: 0.594997
Train - Epoch 30, Batch: 0, Loss: 0.588477
Train - Epoch 31, Batch: 0, Loss: 0.580281
training_time:: 3.3848137855529785
training time full:: 3.384859085083008
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 60
training time is 2.548840284347534
overhead:: 0
overhead2:: 0
time_baseline:: 2.5490548610687256
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.005411386489868164
overhead3:: 0.019464731216430664
overhead4:: 0.3265397548675537
overhead5:: 0
time_provenance:: 0.5627882480621338
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.008207559585571289
overhead3:: 0.026050806045532227
overhead4:: 0.5080418586730957
overhead5:: 0
time_provenance:: 0.7938475608825684
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011151313781738281
overhead3:: 0.03432941436767578
overhead4:: 0.6968941688537598
overhead5:: 0
time_provenance:: 1.0276222229003906
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014152050018310547
overhead3:: 0.04087066650390625
overhead4:: 0.8517513275146484
overhead5:: 0
time_provenance:: 1.2349498271942139
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01847386360168457
overhead3:: 0.04969906806945801
overhead4:: 1.058347225189209
overhead5:: 0
time_provenance:: 1.4868206977844238
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007481575012207031
overhead3:: 0.023131370544433594
overhead4:: 0.48004770278930664
overhead5:: 0
time_provenance:: 0.7482466697692871
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010133028030395508
overhead3:: 0.03075122833251953
overhead4:: 0.6317627429962158
overhead5:: 0
time_provenance:: 0.9454889297485352
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.012640237808227539
overhead3:: 0.03730010986328125
overhead4:: 0.7923202514648438
overhead5:: 0
time_provenance:: 1.1517632007598877
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.015092849731445312
overhead3:: 0.04242396354675293
overhead4:: 0.9517018795013428
overhead5:: 0
time_provenance:: 1.3447685241699219
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01832270622253418
overhead3:: 0.05106520652770996
overhead4:: 1.10085129737854
overhead5:: 0
time_provenance:: 1.5429589748382568
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010853052139282227
overhead3:: 0.03405499458312988
overhead4:: 0.6766650676727295
overhead5:: 0
time_provenance:: 0.9988844394683838
curr_diff: 0 tensor(9.5144e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5144e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014267206192016602
overhead3:: 0.038600921630859375
overhead4:: 0.8246219158172607
overhead5:: 0
time_provenance:: 1.1946051120758057
curr_diff: 0 tensor(8.8544e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8544e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.016247987747192383
overhead3:: 0.045168161392211914
overhead4:: 0.9793641567230225
overhead5:: 0
time_provenance:: 1.3875038623809814
curr_diff: 0 tensor(8.2653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2653e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01791095733642578
overhead3:: 0.052337646484375
overhead4:: 1.118030071258545
overhead5:: 0
time_provenance:: 1.5660457611083984
curr_diff: 0 tensor(7.8526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.020724058151245117
overhead3:: 0.057724952697753906
overhead4:: 1.254298210144043
overhead5:: 0
time_provenance:: 1.7367939949035645
curr_diff: 0 tensor(7.2509e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2509e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021675586700439453
overhead3:: 0.0590052604675293
overhead4:: 1.481919527053833
overhead5:: 0
time_provenance:: 2.0279831886291504
curr_diff: 0 tensor(4.3705e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3705e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.023778676986694336
overhead3:: 0.06352806091308594
overhead4:: 1.55104398727417
overhead5:: 0
time_provenance:: 2.1238203048706055
curr_diff: 0 tensor(4.2748e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2748e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02424907684326172
overhead3:: 0.06762218475341797
overhead4:: 1.6528515815734863
overhead5:: 0
time_provenance:: 2.243475914001465
curr_diff: 0 tensor(4.1831e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1831e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02577829360961914
overhead3:: 0.07154321670532227
overhead4:: 1.7432820796966553
overhead5:: 0
time_provenance:: 2.3536343574523926
curr_diff: 0 tensor(4.1164e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1164e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02766275405883789
overhead3:: 0.07399106025695801
overhead4:: 1.8273003101348877
overhead5:: 0
time_provenance:: 2.4599361419677734
curr_diff: 0 tensor(4.1094e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1094e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.03947329521179199
overhead3:: 0.10438060760498047
overhead4:: 2.33953595161438
overhead5:: 0
time_provenance:: 3.120025634765625
curr_diff: 0 tensor(6.3329e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3329e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.354430
Train - Epoch 1, Batch: 0, Loss: 1.963958
Train - Epoch 2, Batch: 0, Loss: 1.680328
Train - Epoch 3, Batch: 0, Loss: 1.475416
Train - Epoch 4, Batch: 0, Loss: 1.326796
Train - Epoch 5, Batch: 0, Loss: 1.208886
Train - Epoch 6, Batch: 0, Loss: 1.110186
Train - Epoch 7, Batch: 0, Loss: 1.038169
Train - Epoch 8, Batch: 0, Loss: 0.975697
Train - Epoch 9, Batch: 0, Loss: 0.933241
Train - Epoch 10, Batch: 0, Loss: 0.883980
Train - Epoch 11, Batch: 0, Loss: 0.859666
Train - Epoch 12, Batch: 0, Loss: 0.823573
Train - Epoch 13, Batch: 0, Loss: 0.789662
Train - Epoch 14, Batch: 0, Loss: 0.774311
Train - Epoch 15, Batch: 0, Loss: 0.748738
Train - Epoch 16, Batch: 0, Loss: 0.733007
Train - Epoch 17, Batch: 0, Loss: 0.712413
Train - Epoch 18, Batch: 0, Loss: 0.699754
Train - Epoch 19, Batch: 0, Loss: 0.686779
Train - Epoch 20, Batch: 0, Loss: 0.671216
Train - Epoch 21, Batch: 0, Loss: 0.662484
Train - Epoch 22, Batch: 0, Loss: 0.647737
Train - Epoch 23, Batch: 0, Loss: 0.643088
Train - Epoch 24, Batch: 0, Loss: 0.632873
Train - Epoch 25, Batch: 0, Loss: 0.613294
Train - Epoch 26, Batch: 0, Loss: 0.614461
Train - Epoch 27, Batch: 0, Loss: 0.611604
Train - Epoch 28, Batch: 0, Loss: 0.594408
Train - Epoch 29, Batch: 0, Loss: 0.594095
Train - Epoch 30, Batch: 0, Loss: 0.587148
Train - Epoch 31, Batch: 0, Loss: 0.579779
training_time:: 3.4150450229644775
training time full:: 3.4150900840759277
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 60
training time is 2.6353752613067627
overhead:: 0
overhead2:: 0
time_baseline:: 2.6356022357940674
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.005048036575317383
overhead3:: 0.01846480369567871
overhead4:: 0.3301682472229004
overhead5:: 0
time_provenance:: 0.5631928443908691
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.008052825927734375
overhead3:: 0.025990724563598633
overhead4:: 0.5183169841766357
overhead5:: 0
time_provenance:: 0.8069398403167725
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011478900909423828
overhead3:: 0.03445005416870117
overhead4:: 0.671684741973877
overhead5:: 0
time_provenance:: 1.0004830360412598
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01410222053527832
overhead3:: 0.04198789596557617
overhead4:: 0.8521509170532227
overhead5:: 0
time_provenance:: 1.2347157001495361
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.016397714614868164
overhead3:: 0.04790997505187988
overhead4:: 1.017354965209961
overhead5:: 0
time_provenance:: 1.4438831806182861
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.007670164108276367
overhead3:: 0.024009227752685547
overhead4:: 0.48354077339172363
overhead5:: 0
time_provenance:: 0.7553777694702148
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.009811639785766602
overhead3:: 0.030073881149291992
overhead4:: 0.6298317909240723
overhead5:: 0
time_provenance:: 0.9344892501831055
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01227569580078125
overhead3:: 0.036551475524902344
overhead4:: 0.7794735431671143
overhead5:: 0
time_provenance:: 1.1350083351135254
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.014943838119506836
overhead3:: 0.04388308525085449
overhead4:: 0.9429962635040283
overhead5:: 0
time_provenance:: 1.3466598987579346
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.018352270126342773
overhead3:: 0.05139422416687012
overhead4:: 1.1321923732757568
overhead5:: 0
time_provenance:: 1.5804569721221924
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.011750936508178711
overhead3:: 0.0335240364074707
overhead4:: 0.7148287296295166
overhead5:: 0
time_provenance:: 1.0277702808380127
curr_diff: 0 tensor(9.7009e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7009e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013036966323852539
overhead3:: 0.03814125061035156
overhead4:: 0.8109545707702637
overhead5:: 0
time_provenance:: 1.16679048538208
curr_diff: 0 tensor(9.1098e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1098e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01678919792175293
overhead3:: 0.04680132865905762
overhead4:: 0.9703693389892578
overhead5:: 0
time_provenance:: 1.3671131134033203
curr_diff: 0 tensor(8.0510e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0510e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021498918533325195
overhead3:: 0.04697155952453613
overhead4:: 1.1205987930297852
overhead5:: 0
time_provenance:: 1.5580077171325684
curr_diff: 0 tensor(7.8096e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8096e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.021552562713623047
overhead3:: 0.05851554870605469
overhead4:: 1.2842721939086914
overhead5:: 0
time_provenance:: 1.7757019996643066
curr_diff: 0 tensor(6.6018e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6018e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.022082090377807617
overhead3:: 0.05649614334106445
overhead4:: 1.495225429534912
overhead5:: 0
time_provenance:: 2.046674966812134
curr_diff: 0 tensor(3.3927e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3927e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02234482765197754
overhead3:: 0.06238722801208496
overhead4:: 1.5398576259613037
overhead5:: 0
time_provenance:: 2.1098735332489014
curr_diff: 0 tensor(3.2916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.024727582931518555
overhead3:: 0.06613755226135254
overhead4:: 1.6219446659088135
overhead5:: 0
time_provenance:: 2.2096333503723145
curr_diff: 0 tensor(3.1931e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1931e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02679753303527832
overhead3:: 0.07061123847961426
overhead4:: 1.7379133701324463
overhead5:: 0
time_provenance:: 2.3447375297546387
curr_diff: 0 tensor(3.0718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0718e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02820754051208496
overhead3:: 0.07461380958557129
overhead4:: 1.837956428527832
overhead5:: 0
time_provenance:: 2.461855888366699
curr_diff: 0 tensor(2.9989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.03882408142089844
overhead3:: 0.10475444793701172
overhead4:: 2.375715970993042
overhead5:: 0
time_provenance:: 3.1549012660980225
curr_diff: 0 tensor(6.0819e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0819e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.324785
Train - Epoch 1, Batch: 0, Loss: 1.937356
Train - Epoch 2, Batch: 0, Loss: 1.662773
Train - Epoch 3, Batch: 0, Loss: 1.459380
Train - Epoch 4, Batch: 0, Loss: 1.310623
Train - Epoch 5, Batch: 0, Loss: 1.194480
Train - Epoch 6, Batch: 0, Loss: 1.107236
Train - Epoch 7, Batch: 0, Loss: 1.026021
Train - Epoch 8, Batch: 0, Loss: 0.976439
Train - Epoch 9, Batch: 0, Loss: 0.923589
Train - Epoch 10, Batch: 0, Loss: 0.883148
Train - Epoch 11, Batch: 0, Loss: 0.849715
Train - Epoch 12, Batch: 0, Loss: 0.816592
Train - Epoch 13, Batch: 0, Loss: 0.790334
Train - Epoch 14, Batch: 0, Loss: 0.768948
Train - Epoch 15, Batch: 0, Loss: 0.747182
Train - Epoch 16, Batch: 0, Loss: 0.729672
Train - Epoch 17, Batch: 0, Loss: 0.709457
Train - Epoch 18, Batch: 0, Loss: 0.696617
Train - Epoch 19, Batch: 0, Loss: 0.678473
Train - Epoch 20, Batch: 0, Loss: 0.671085
Train - Epoch 21, Batch: 0, Loss: 0.657799
Train - Epoch 22, Batch: 0, Loss: 0.656532
Train - Epoch 23, Batch: 0, Loss: 0.639580
Train - Epoch 24, Batch: 0, Loss: 0.629422
Train - Epoch 25, Batch: 0, Loss: 0.609541
Train - Epoch 26, Batch: 0, Loss: 0.608678
Train - Epoch 27, Batch: 0, Loss: 0.613113
Train - Epoch 28, Batch: 0, Loss: 0.602783
Train - Epoch 29, Batch: 0, Loss: 0.587118
Train - Epoch 30, Batch: 0, Loss: 0.577623
Train - Epoch 31, Batch: 0, Loss: 0.573703
training_time:: 3.399725914001465
training time full:: 3.3997714519500732
provenance prepare time:: 1.430511474609375e-06
Test Avg. Loss: 0.000055, Accuracy: 0.874800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 60
training time is 2.553511381149292
overhead:: 0
overhead2:: 0
time_baseline:: 2.5537397861480713
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.005682468414306641
overhead3:: 0.019277334213256836
overhead4:: 0.3381030559539795
overhead5:: 0
time_provenance:: 0.5757801532745361
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.008261442184448242
overhead3:: 0.026314973831176758
overhead4:: 0.5042412281036377
overhead5:: 0
time_provenance:: 0.7985289096832275
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010796308517456055
overhead3:: 0.03349494934082031
overhead4:: 0.6829161643981934
overhead5:: 0
time_provenance:: 1.0155954360961914
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01466822624206543
overhead3:: 0.04207110404968262
overhead4:: 0.851367712020874
overhead5:: 0
time_provenance:: 1.2397384643554688
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01752471923828125
overhead3:: 0.04863715171813965
overhead4:: 1.0182280540466309
overhead5:: 0
time_provenance:: 1.4448421001434326
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.00865793228149414
overhead3:: 0.024228572845458984
overhead4:: 0.4795384407043457
overhead5:: 0
time_provenance:: 0.7600696086883545
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.0098724365234375
overhead3:: 0.030771255493164062
overhead4:: 0.6490769386291504
overhead5:: 0
time_provenance:: 0.9604272842407227
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.012594223022460938
overhead3:: 0.03661608695983887
overhead4:: 0.7983977794647217
overhead5:: 0
time_provenance:: 1.1509552001953125
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01563739776611328
overhead3:: 0.0436704158782959
overhead4:: 0.9447791576385498
overhead5:: 0
time_provenance:: 1.3413786888122559
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01797318458557129
overhead3:: 0.051256418228149414
overhead4:: 1.1116244792938232
overhead5:: 0
time_provenance:: 1.5543749332427979
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.010899066925048828
overhead3:: 0.03361225128173828
overhead4:: 0.666684627532959
overhead5:: 0
time_provenance:: 0.9935662746429443
curr_diff: 0 tensor(7.6893e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6893e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.013191699981689453
overhead3:: 0.04071640968322754
overhead4:: 0.8384644985198975
overhead5:: 0
time_provenance:: 1.195847988128662
curr_diff: 0 tensor(6.8637e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8637e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.016042470932006836
overhead3:: 0.04552435874938965
overhead4:: 0.9852423667907715
overhead5:: 0
time_provenance:: 1.3808097839355469
curr_diff: 0 tensor(6.3157e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3157e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.01888275146484375
overhead3:: 0.05374336242675781
overhead4:: 1.1456272602081299
overhead5:: 0
time_provenance:: 1.5924158096313477
curr_diff: 0 tensor(5.4294e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4294e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.022718191146850586
overhead3:: 0.05755949020385742
overhead4:: 1.3320116996765137
overhead5:: 0
time_provenance:: 1.8212711811065674
curr_diff: 0 tensor(5.0555e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0555e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02187633514404297
overhead3:: 0.05786418914794922
overhead4:: 1.4651734828948975
overhead5:: 0
time_provenance:: 2.0103347301483154
curr_diff: 0 tensor(2.4412e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4412e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.022095918655395508
overhead3:: 0.0621180534362793
overhead4:: 1.5888190269470215
overhead5:: 0
time_provenance:: 2.1541223526000977
curr_diff: 0 tensor(2.3340e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3340e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.023876190185546875
overhead3:: 0.06771564483642578
overhead4:: 1.6340625286102295
overhead5:: 0
time_provenance:: 2.2215969562530518
curr_diff: 0 tensor(2.2649e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2649e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.02610468864440918
overhead3:: 0.07088661193847656
overhead4:: 1.6877169609069824
overhead5:: 0
time_provenance:: 2.294904947280884
curr_diff: 0 tensor(2.2003e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2003e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.027167797088623047
overhead3:: 0.07550573348999023
overhead4:: 1.7818832397460938
overhead5:: 0
time_provenance:: 2.417076349258423
curr_diff: 0 tensor(2.1271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1271e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
overhead:: 0
overhead2:: 0.039774417877197266
overhead3:: 0.1071014404296875
overhead4:: 2.354917049407959
overhead5:: 0
time_provenance:: 3.1394009590148926
curr_diff: 0 tensor(6.0668e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0668e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
deletion rate:: 0.002
python3 generate_rand_ids 0.002  MNIST5 0
tensor([ 2049, 42754,  5123, 36100, 52997, 46087, 44553, 28938,  8714, 34571,
        31759, 25872, 42001,  3600, 32016, 18196, 29207,  5403, 19233,  5153,
        55588, 31524, 31268, 26665, 32554, 33322, 39469, 55602, 38964, 10548,
        58423,  8504, 15928,  2876, 55101, 16190, 10559,  6210, 33607, 56391,
        35913, 25162, 24905, 56397, 54608, 56146, 59986,  6742, 58714, 21339,
        19548, 31325,  8540, 53341,  5729, 25698,  4451, 48997, 15208,  9322,
        21098, 45688, 49529, 57723, 48510, 15232, 44163, 43661, 13457,  6802,
        45465, 54174, 25759, 57504, 47010, 40355, 48294, 25000, 44201, 12971,
        44205, 49326, 52656, 12979, 52660, 39348,  1204, 31159, 46012, 42173,
        28349, 19389,  8640,  6343, 38343,  1227, 16844,  2510, 24526,   974,
        58837, 48597, 36055, 15066, 40155, 15324, 50909, 26845,  2014, 59614,
        14049, 57084,  5355, 28652, 18412,  5617, 46325, 14070, 46331, 54780])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.321354
Train - Epoch 1, Batch: 0, Loss: 1.941815
Train - Epoch 2, Batch: 0, Loss: 1.664922
Train - Epoch 3, Batch: 0, Loss: 1.465955
Train - Epoch 4, Batch: 0, Loss: 1.309605
Train - Epoch 5, Batch: 0, Loss: 1.193282
Train - Epoch 6, Batch: 0, Loss: 1.094665
Train - Epoch 7, Batch: 0, Loss: 1.033806
Train - Epoch 8, Batch: 0, Loss: 0.978472
Train - Epoch 9, Batch: 0, Loss: 0.923077
Train - Epoch 10, Batch: 0, Loss: 0.882725
Train - Epoch 11, Batch: 0, Loss: 0.844882
Train - Epoch 12, Batch: 0, Loss: 0.822578
Train - Epoch 13, Batch: 0, Loss: 0.793394
Train - Epoch 14, Batch: 0, Loss: 0.768627
Train - Epoch 15, Batch: 0, Loss: 0.754638
Train - Epoch 16, Batch: 0, Loss: 0.728230
Train - Epoch 17, Batch: 0, Loss: 0.707653
Train - Epoch 18, Batch: 0, Loss: 0.696299
Train - Epoch 19, Batch: 0, Loss: 0.682851
Train - Epoch 20, Batch: 0, Loss: 0.675444
Train - Epoch 21, Batch: 0, Loss: 0.656824
Train - Epoch 22, Batch: 0, Loss: 0.647923
Train - Epoch 23, Batch: 0, Loss: 0.630129
Train - Epoch 24, Batch: 0, Loss: 0.623362
Train - Epoch 25, Batch: 0, Loss: 0.618903
Train - Epoch 26, Batch: 0, Loss: 0.608638
Train - Epoch 27, Batch: 0, Loss: 0.606957
Train - Epoch 28, Batch: 0, Loss: 0.601695
Train - Epoch 29, Batch: 0, Loss: 0.596655
Train - Epoch 30, Batch: 0, Loss: 0.581277
Train - Epoch 31, Batch: 0, Loss: 0.571591
training_time:: 3.383460283279419
training time full:: 3.383507251739502
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 120
training time is 2.5659358501434326
overhead:: 0
overhead2:: 0
time_baseline:: 2.566145658493042
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.005312681198120117
overhead3:: 0.01853179931640625
overhead4:: 0.3288004398345947
overhead5:: 0
time_provenance:: 0.5813231468200684
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.00892329216003418
overhead3:: 0.026826143264770508
overhead4:: 0.5157928466796875
overhead5:: 0
time_provenance:: 0.8298263549804688
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012211799621582031
overhead3:: 0.03418087959289551
overhead4:: 0.6797282695770264
overhead5:: 0
time_provenance:: 1.0341804027557373
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014364242553710938
overhead3:: 0.041258811950683594
overhead4:: 0.8722951412200928
overhead5:: 0
time_provenance:: 1.2917242050170898
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01810431480407715
overhead3:: 0.04894685745239258
overhead4:: 1.0132107734680176
overhead5:: 0
time_provenance:: 1.4747376441955566
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.007152080535888672
overhead3:: 0.023041963577270508
overhead4:: 0.46806955337524414
overhead5:: 0
time_provenance:: 0.757286548614502
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.009995222091674805
overhead3:: 0.029500961303710938
overhead4:: 0.6314258575439453
overhead5:: 0
time_provenance:: 0.9628965854644775
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012978315353393555
overhead3:: 0.037970542907714844
overhead4:: 0.8290600776672363
overhead5:: 0
time_provenance:: 1.213660717010498
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0176084041595459
overhead3:: 0.04454326629638672
overhead4:: 0.9744002819061279
overhead5:: 0
time_provenance:: 1.4061152935028076
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01885223388671875
overhead3:: 0.05230140686035156
overhead4:: 1.13563871383667
overhead5:: 0
time_provenance:: 1.6191234588623047
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012927770614624023
overhead3:: 0.0335996150970459
overhead4:: 0.6781630516052246
overhead5:: 0
time_provenance:: 1.0309743881225586
curr_diff: 0 tensor(8.9891e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9891e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.015625715255737305
overhead3:: 0.04105353355407715
overhead4:: 0.8180270195007324
overhead5:: 0
time_provenance:: 1.2069458961486816
curr_diff: 0 tensor(6.8842e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8842e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.019263267517089844
overhead3:: 0.0508270263671875
overhead4:: 1.0046496391296387
overhead5:: 0
time_provenance:: 1.4407105445861816
curr_diff: 0 tensor(6.3310e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3310e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.019430875778198242
overhead3:: 0.05258822441101074
overhead4:: 1.1082963943481445
overhead5:: 0
time_provenance:: 1.5879483222961426
curr_diff: 0 tensor(5.8735e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8735e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.021086692810058594
overhead3:: 0.05771517753601074
overhead4:: 1.2549426555633545
overhead5:: 0
time_provenance:: 1.7766633033752441
curr_diff: 0 tensor(5.5140e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5140e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.022017240524291992
overhead3:: 0.056975364685058594
overhead4:: 1.4546983242034912
overhead5:: 0
time_provenance:: 2.046931743621826
curr_diff: 0 tensor(3.7427e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7427e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.024156570434570312
overhead3:: 0.06212878227233887
overhead4:: 1.5857303142547607
overhead5:: 0
time_provenance:: 2.191945791244507
curr_diff: 0 tensor(3.5403e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5403e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02825021743774414
overhead3:: 0.07166171073913574
overhead4:: 1.662306785583496
overhead5:: 0
time_provenance:: 2.291163682937622
curr_diff: 0 tensor(3.4151e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4151e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.027692079544067383
overhead3:: 0.07211732864379883
overhead4:: 1.7626526355743408
overhead5:: 0
time_provenance:: 2.4195423126220703
curr_diff: 0 tensor(3.3140e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3140e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02846670150756836
overhead3:: 0.07375144958496094
overhead4:: 1.8011786937713623
overhead5:: 0
time_provenance:: 2.476832389831543
curr_diff: 0 tensor(3.2390e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2390e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.04963088035583496
overhead3:: 0.11244988441467285
overhead4:: 2.50262713432312
overhead5:: 0
time_provenance:: 3.3285505771636963
curr_diff: 0 tensor(6.3704e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3704e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.301520
Train - Epoch 1, Batch: 0, Loss: 1.922865
Train - Epoch 2, Batch: 0, Loss: 1.650213
Train - Epoch 3, Batch: 0, Loss: 1.451553
Train - Epoch 4, Batch: 0, Loss: 1.302651
Train - Epoch 5, Batch: 0, Loss: 1.186972
Train - Epoch 6, Batch: 0, Loss: 1.095591
Train - Epoch 7, Batch: 0, Loss: 1.018359
Train - Epoch 8, Batch: 0, Loss: 0.972353
Train - Epoch 9, Batch: 0, Loss: 0.924755
Train - Epoch 10, Batch: 0, Loss: 0.878029
Train - Epoch 11, Batch: 0, Loss: 0.845100
Train - Epoch 12, Batch: 0, Loss: 0.823954
Train - Epoch 13, Batch: 0, Loss: 0.786388
Train - Epoch 14, Batch: 0, Loss: 0.769331
Train - Epoch 15, Batch: 0, Loss: 0.746126
Train - Epoch 16, Batch: 0, Loss: 0.729337
Train - Epoch 17, Batch: 0, Loss: 0.707756
Train - Epoch 18, Batch: 0, Loss: 0.694206
Train - Epoch 19, Batch: 0, Loss: 0.688654
Train - Epoch 20, Batch: 0, Loss: 0.676507
Train - Epoch 21, Batch: 0, Loss: 0.664877
Train - Epoch 22, Batch: 0, Loss: 0.647812
Train - Epoch 23, Batch: 0, Loss: 0.633739
Train - Epoch 24, Batch: 0, Loss: 0.623885
Train - Epoch 25, Batch: 0, Loss: 0.614329
Train - Epoch 26, Batch: 0, Loss: 0.613746
Train - Epoch 27, Batch: 0, Loss: 0.599160
Train - Epoch 28, Batch: 0, Loss: 0.600224
Train - Epoch 29, Batch: 0, Loss: 0.590685
Train - Epoch 30, Batch: 0, Loss: 0.584423
Train - Epoch 31, Batch: 0, Loss: 0.571596
training_time:: 3.4470326900482178
training time full:: 3.4470787048339844
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 120
training time is 2.540118455886841
overhead:: 0
overhead2:: 0
time_baseline:: 2.5403285026550293
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.005589485168457031
overhead3:: 0.01906275749206543
overhead4:: 0.33571338653564453
overhead5:: 0
time_provenance:: 0.5867292881011963
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0087890625
overhead3:: 0.026909589767456055
overhead4:: 0.5093181133270264
overhead5:: 0
time_provenance:: 0.8201038837432861
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.011388778686523438
overhead3:: 0.033418893814086914
overhead4:: 0.6978683471679688
overhead5:: 0
time_provenance:: 1.064086675643921
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014555931091308594
overhead3:: 0.040906667709350586
overhead4:: 0.8497145175933838
overhead5:: 0
time_provenance:: 1.2970194816589355
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01789999008178711
overhead3:: 0.048885345458984375
overhead4:: 1.029418706893921
overhead5:: 0
time_provenance:: 1.5036239624023438
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.007483482360839844
overhead3:: 0.023636341094970703
overhead4:: 0.491926908493042
overhead5:: 0
time_provenance:: 0.784203290939331
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.010379791259765625
overhead3:: 0.03032398223876953
overhead4:: 0.6399242877960205
overhead5:: 0
time_provenance:: 0.9819049835205078
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.013826131820678711
overhead3:: 0.03735518455505371
overhead4:: 0.7956445217132568
overhead5:: 0
time_provenance:: 1.1785962581634521
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01578998565673828
overhead3:: 0.04424786567687988
overhead4:: 0.996863603591919
overhead5:: 0
time_provenance:: 1.438002347946167
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.019170761108398438
overhead3:: 0.05084991455078125
overhead4:: 1.1149837970733643
overhead5:: 0
time_provenance:: 1.5885496139526367
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012151956558227539
overhead3:: 0.03175520896911621
overhead4:: 0.7095870971679688
overhead5:: 0
time_provenance:: 1.0563342571258545
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014652729034423828
overhead3:: 0.0383455753326416
overhead4:: 0.8384253978729248
overhead5:: 0
time_provenance:: 1.2261755466461182
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.017360925674438477
overhead3:: 0.0448613166809082
overhead4:: 0.9641404151916504
overhead5:: 0
time_provenance:: 1.3987202644348145
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01920938491821289
overhead3:: 0.050872087478637695
overhead4:: 1.1376540660858154
overhead5:: 0
time_provenance:: 1.608412265777588
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.021065235137939453
overhead3:: 0.059066057205200195
overhead4:: 1.280663013458252
overhead5:: 0
time_provenance:: 1.8045554161071777
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02306342124938965
overhead3:: 0.0565946102142334
overhead4:: 1.5221691131591797
overhead5:: 0
time_provenance:: 2.1155600547790527
curr_diff: 0 tensor(5.4593e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4593e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.025554180145263672
overhead3:: 0.057420969009399414
overhead4:: 1.5503883361816406
overhead5:: 0
time_provenance:: 2.1538314819335938
curr_diff: 0 tensor(5.3548e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3548e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.024838924407958984
overhead3:: 0.06525993347167969
overhead4:: 1.6637706756591797
overhead5:: 0
time_provenance:: 2.2914438247680664
curr_diff: 0 tensor(5.3204e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3204e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.028070926666259766
overhead3:: 0.06753349304199219
overhead4:: 1.7277758121490479
overhead5:: 0
time_provenance:: 2.3781871795654297
curr_diff: 0 tensor(5.1974e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1974e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02799057960510254
overhead3:: 0.07313942909240723
overhead4:: 1.833547830581665
overhead5:: 0
time_provenance:: 2.5114986896514893
curr_diff: 0 tensor(5.1124e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1124e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.04165244102478027
overhead3:: 0.10644698143005371
overhead4:: 2.3918862342834473
overhead5:: 0
time_provenance:: 3.222904682159424
curr_diff: 0 tensor(6.1512e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1512e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.282898
Train - Epoch 1, Batch: 0, Loss: 1.916845
Train - Epoch 2, Batch: 0, Loss: 1.647879
Train - Epoch 3, Batch: 0, Loss: 1.454576
Train - Epoch 4, Batch: 0, Loss: 1.298471
Train - Epoch 5, Batch: 0, Loss: 1.185628
Train - Epoch 6, Batch: 0, Loss: 1.099359
Train - Epoch 7, Batch: 0, Loss: 1.029333
Train - Epoch 8, Batch: 0, Loss: 0.964101
Train - Epoch 9, Batch: 0, Loss: 0.922445
Train - Epoch 10, Batch: 0, Loss: 0.876698
Train - Epoch 11, Batch: 0, Loss: 0.839506
Train - Epoch 12, Batch: 0, Loss: 0.816623
Train - Epoch 13, Batch: 0, Loss: 0.784145
Train - Epoch 14, Batch: 0, Loss: 0.772143
Train - Epoch 15, Batch: 0, Loss: 0.746767
Train - Epoch 16, Batch: 0, Loss: 0.725100
Train - Epoch 17, Batch: 0, Loss: 0.708281
Train - Epoch 18, Batch: 0, Loss: 0.695890
Train - Epoch 19, Batch: 0, Loss: 0.687726
Train - Epoch 20, Batch: 0, Loss: 0.675979
Train - Epoch 21, Batch: 0, Loss: 0.659197
Train - Epoch 22, Batch: 0, Loss: 0.646229
Train - Epoch 23, Batch: 0, Loss: 0.650437
Train - Epoch 24, Batch: 0, Loss: 0.632695
Train - Epoch 25, Batch: 0, Loss: 0.618403
Train - Epoch 26, Batch: 0, Loss: 0.610453
Train - Epoch 27, Batch: 0, Loss: 0.605041
Train - Epoch 28, Batch: 0, Loss: 0.595467
Train - Epoch 29, Batch: 0, Loss: 0.593971
Train - Epoch 30, Batch: 0, Loss: 0.590150
Train - Epoch 31, Batch: 0, Loss: 0.567932
training_time:: 3.3834478855133057
training time full:: 3.3834919929504395
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 120
training time is 2.531099319458008
overhead:: 0
overhead2:: 0
time_baseline:: 2.5313148498535156
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.005858421325683594
overhead3:: 0.019323110580444336
overhead4:: 0.3825511932373047
overhead5:: 0
time_provenance:: 0.6630795001983643
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.008480072021484375
overhead3:: 0.026077985763549805
overhead4:: 0.5112490653991699
overhead5:: 0
time_provenance:: 0.8233754634857178
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.011699676513671875
overhead3:: 0.033270835876464844
overhead4:: 0.7095248699188232
overhead5:: 0
time_provenance:: 1.069577932357788
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01468658447265625
overhead3:: 0.04086494445800781
overhead4:: 0.8642740249633789
overhead5:: 0
time_provenance:: 1.2817528247833252
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.020452260971069336
overhead3:: 0.049652099609375
overhead4:: 1.1022145748138428
overhead5:: 0
time_provenance:: 1.5678210258483887
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0074329376220703125
overhead3:: 0.022925138473510742
overhead4:: 0.4955911636352539
overhead5:: 0
time_provenance:: 0.7850303649902344
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.010657072067260742
overhead3:: 0.02910780906677246
overhead4:: 0.6708846092224121
overhead5:: 0
time_provenance:: 1.0000584125518799
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012534379959106445
overhead3:: 0.03717303276062012
overhead4:: 0.7850642204284668
overhead5:: 0
time_provenance:: 1.164525032043457
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01582026481628418
overhead3:: 0.045330047607421875
overhead4:: 0.9833035469055176
overhead5:: 0
time_provenance:: 1.409255027770996
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.018340110778808594
overhead3:: 0.05062127113342285
overhead4:: 1.1389026641845703
overhead5:: 0
time_provenance:: 1.611609935760498
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012643814086914062
overhead3:: 0.033092498779296875
overhead4:: 0.6880178451538086
overhead5:: 0
time_provenance:: 1.0618505477905273
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014295101165771484
overhead3:: 0.039734601974487305
overhead4:: 0.8235704898834229
overhead5:: 0
time_provenance:: 1.207841396331787
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01773238182067871
overhead3:: 0.0452723503112793
overhead4:: 0.9780087471008301
overhead5:: 0
time_provenance:: 1.4122092723846436
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01919102668762207
overhead3:: 0.05082893371582031
overhead4:: 1.1047656536102295
overhead5:: 0
time_provenance:: 1.5810115337371826
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.021605491638183594
overhead3:: 0.05817890167236328
overhead4:: 1.252969741821289
overhead5:: 0
time_provenance:: 1.7748818397521973
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.025424718856811523
overhead3:: 0.055231332778930664
overhead4:: 1.5180253982543945
overhead5:: 0
time_provenance:: 2.103203535079956
curr_diff: 0 tensor(4.1724e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1724e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.023956775665283203
overhead3:: 0.06349515914916992
overhead4:: 1.5694169998168945
overhead5:: 0
time_provenance:: 2.1789755821228027
curr_diff: 0 tensor(4.0168e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0168e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.025084495544433594
overhead3:: 0.06729674339294434
overhead4:: 1.619570255279541
overhead5:: 0
time_provenance:: 2.2554802894592285
curr_diff: 0 tensor(3.9118e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9118e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.026607275009155273
overhead3:: 0.0716557502746582
overhead4:: 1.7283885478973389
overhead5:: 0
time_provenance:: 2.377586603164673
curr_diff: 0 tensor(3.8026e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8026e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02899909019470215
overhead3:: 0.07357311248779297
overhead4:: 1.7791938781738281
overhead5:: 0
time_provenance:: 2.4564194679260254
curr_diff: 0 tensor(3.7452e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7452e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.039612770080566406
overhead3:: 0.10377216339111328
overhead4:: 2.3651492595672607
overhead5:: 0
time_provenance:: 3.196803092956543
curr_diff: 0 tensor(6.3917e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3917e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.331575
Train - Epoch 1, Batch: 0, Loss: 1.949113
Train - Epoch 2, Batch: 0, Loss: 1.668675
Train - Epoch 3, Batch: 0, Loss: 1.463881
Train - Epoch 4, Batch: 0, Loss: 1.308117
Train - Epoch 5, Batch: 0, Loss: 1.191563
Train - Epoch 6, Batch: 0, Loss: 1.102173
Train - Epoch 7, Batch: 0, Loss: 1.027302
Train - Epoch 8, Batch: 0, Loss: 0.973461
Train - Epoch 9, Batch: 0, Loss: 0.919823
Train - Epoch 10, Batch: 0, Loss: 0.883403
Train - Epoch 11, Batch: 0, Loss: 0.849806
Train - Epoch 12, Batch: 0, Loss: 0.808109
Train - Epoch 13, Batch: 0, Loss: 0.787271
Train - Epoch 14, Batch: 0, Loss: 0.769853
Train - Epoch 15, Batch: 0, Loss: 0.748846
Train - Epoch 16, Batch: 0, Loss: 0.732160
Train - Epoch 17, Batch: 0, Loss: 0.701314
Train - Epoch 18, Batch: 0, Loss: 0.690018
Train - Epoch 19, Batch: 0, Loss: 0.680240
Train - Epoch 20, Batch: 0, Loss: 0.670140
Train - Epoch 21, Batch: 0, Loss: 0.651359
Train - Epoch 22, Batch: 0, Loss: 0.647770
Train - Epoch 23, Batch: 0, Loss: 0.637658
Train - Epoch 24, Batch: 0, Loss: 0.626876
Train - Epoch 25, Batch: 0, Loss: 0.613298
Train - Epoch 26, Batch: 0, Loss: 0.603992
Train - Epoch 27, Batch: 0, Loss: 0.604791
Train - Epoch 28, Batch: 0, Loss: 0.595352
Train - Epoch 29, Batch: 0, Loss: 0.580703
Train - Epoch 30, Batch: 0, Loss: 0.581204
Train - Epoch 31, Batch: 0, Loss: 0.569532
training_time:: 3.365931987762451
training time full:: 3.3659772872924805
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 120
training time is 2.5955233573913574
overhead:: 0
overhead2:: 0
time_baseline:: 2.5957629680633545
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.005540609359741211
overhead3:: 0.01910567283630371
overhead4:: 0.33928751945495605
overhead5:: 0
time_provenance:: 0.5975539684295654
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.008835554122924805
overhead3:: 0.025908470153808594
overhead4:: 0.5365254878997803
overhead5:: 0
time_provenance:: 0.8467459678649902
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01155543327331543
overhead3:: 0.03417348861694336
overhead4:: 0.6794981956481934
overhead5:: 0
time_provenance:: 1.0418531894683838
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014730453491210938
overhead3:: 0.04127359390258789
overhead4:: 0.852348804473877
overhead5:: 0
time_provenance:: 1.265974760055542
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.017579317092895508
overhead3:: 0.04859018325805664
overhead4:: 1.0174319744110107
overhead5:: 0
time_provenance:: 1.478973150253296
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0076732635498046875
overhead3:: 0.023311376571655273
overhead4:: 0.4785897731781006
overhead5:: 0
time_provenance:: 0.7722160816192627
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.010825395584106445
overhead3:: 0.03076028823852539
overhead4:: 0.6345927715301514
overhead5:: 0
time_provenance:: 0.9708929061889648
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012798786163330078
overhead3:: 0.03717517852783203
overhead4:: 0.785438060760498
overhead5:: 0
time_provenance:: 1.1669981479644775
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0164797306060791
overhead3:: 0.04492807388305664
overhead4:: 0.9682247638702393
overhead5:: 0
time_provenance:: 1.4041368961334229
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.018298864364624023
overhead3:: 0.05033588409423828
overhead4:: 1.1101429462432861
overhead5:: 0
time_provenance:: 1.5865943431854248
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01170492172241211
overhead3:: 0.03490805625915527
overhead4:: 0.7079541683197021
overhead5:: 0
time_provenance:: 1.0538303852081299
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014618158340454102
overhead3:: 0.03867840766906738
overhead4:: 0.8624348640441895
overhead5:: 0
time_provenance:: 1.256643533706665
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01611614227294922
overhead3:: 0.04726362228393555
overhead4:: 0.9782650470733643
overhead5:: 0
time_provenance:: 1.4114627838134766
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.019756793975830078
overhead3:: 0.05157589912414551
overhead4:: 1.1051454544067383
overhead5:: 0
time_provenance:: 1.5811307430267334
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.021292686462402344
overhead3:: 0.056111812591552734
overhead4:: 1.270042896270752
overhead5:: 0
time_provenance:: 1.7827086448669434
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.023206710815429688
overhead3:: 0.06035900115966797
overhead4:: 1.556826114654541
overhead5:: 0
time_provenance:: 2.1433827877044678
curr_diff: 0 tensor(3.5712e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5712e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.024479389190673828
overhead3:: 0.06456971168518066
overhead4:: 1.5636210441589355
overhead5:: 0
time_provenance:: 2.1695120334625244
curr_diff: 0 tensor(3.4156e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4156e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.026697635650634766
overhead3:: 0.06742691993713379
overhead4:: 1.6764037609100342
overhead5:: 0
time_provenance:: 2.307899236679077
curr_diff: 0 tensor(3.3296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3296e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.025965452194213867
overhead3:: 0.07073855400085449
overhead4:: 1.7316813468933105
overhead5:: 0
time_provenance:: 2.3830044269561768
curr_diff: 0 tensor(3.1655e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1655e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.027743816375732422
overhead3:: 0.074371337890625
overhead4:: 1.783174753189087
overhead5:: 0
time_provenance:: 2.454777479171753
curr_diff: 0 tensor(3.0099e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0099e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.04149746894836426
overhead3:: 0.10371255874633789
overhead4:: 2.375751495361328
overhead5:: 0
time_provenance:: 3.211648941040039
curr_diff: 0 tensor(6.3059e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3059e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.301863
Train - Epoch 1, Batch: 0, Loss: 1.925234
Train - Epoch 2, Batch: 0, Loss: 1.653915
Train - Epoch 3, Batch: 0, Loss: 1.458661
Train - Epoch 4, Batch: 0, Loss: 1.308231
Train - Epoch 5, Batch: 0, Loss: 1.197805
Train - Epoch 6, Batch: 0, Loss: 1.104034
Train - Epoch 7, Batch: 0, Loss: 1.032690
Train - Epoch 8, Batch: 0, Loss: 0.971812
Train - Epoch 9, Batch: 0, Loss: 0.920973
Train - Epoch 10, Batch: 0, Loss: 0.882344
Train - Epoch 11, Batch: 0, Loss: 0.853191
Train - Epoch 12, Batch: 0, Loss: 0.815700
Train - Epoch 13, Batch: 0, Loss: 0.796454
Train - Epoch 14, Batch: 0, Loss: 0.766258
Train - Epoch 15, Batch: 0, Loss: 0.743789
Train - Epoch 16, Batch: 0, Loss: 0.730405
Train - Epoch 17, Batch: 0, Loss: 0.715771
Train - Epoch 18, Batch: 0, Loss: 0.705468
Train - Epoch 19, Batch: 0, Loss: 0.687133
Train - Epoch 20, Batch: 0, Loss: 0.674314
Train - Epoch 21, Batch: 0, Loss: 0.654596
Train - Epoch 22, Batch: 0, Loss: 0.658617
Train - Epoch 23, Batch: 0, Loss: 0.635885
Train - Epoch 24, Batch: 0, Loss: 0.630456
Train - Epoch 25, Batch: 0, Loss: 0.623852
Train - Epoch 26, Batch: 0, Loss: 0.610329
Train - Epoch 27, Batch: 0, Loss: 0.604317
Train - Epoch 28, Batch: 0, Loss: 0.595774
Train - Epoch 29, Batch: 0, Loss: 0.593160
Train - Epoch 30, Batch: 0, Loss: 0.583363
Train - Epoch 31, Batch: 0, Loss: 0.578082
training_time:: 3.4394607543945312
training time full:: 3.4395065307617188
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.874900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 120
training time is 2.548461675643921
overhead:: 0
overhead2:: 0
time_baseline:: 2.548704147338867
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.005287885665893555
overhead3:: 0.01880502700805664
overhead4:: 0.3330216407775879
overhead5:: 0
time_provenance:: 0.5896246433258057
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.008672475814819336
overhead3:: 0.027155399322509766
overhead4:: 0.5051774978637695
overhead5:: 0
time_provenance:: 0.8200881481170654
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0074, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0074, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.011568307876586914
overhead3:: 0.0339818000793457
overhead4:: 0.7040524482727051
overhead5:: 0
time_provenance:: 1.0598392486572266
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014485836029052734
overhead3:: 0.04065370559692383
overhead4:: 0.8567869663238525
overhead5:: 0
time_provenance:: 1.2658629417419434
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.019821643829345703
overhead3:: 0.04524111747741699
overhead4:: 1.0171692371368408
overhead5:: 0
time_provenance:: 1.4818933010101318
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0072209835052490234
overhead3:: 0.0232393741607666
overhead4:: 0.4510612487792969
overhead5:: 0
time_provenance:: 0.736260175704956
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.010093450546264648
overhead3:: 0.030517578125
overhead4:: 0.637326717376709
overhead5:: 0
time_provenance:: 0.9733819961547852
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.012981414794921875
overhead3:: 0.037581682205200195
overhead4:: 0.7759683132171631
overhead5:: 0
time_provenance:: 1.1614513397216797
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874700
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.016644001007080078
overhead3:: 0.045232534408569336
overhead4:: 0.9610695838928223
overhead5:: 0
time_provenance:: 1.3988299369812012
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.01814723014831543
overhead3:: 0.05081033706665039
overhead4:: 1.1163132190704346
overhead5:: 0
time_provenance:: 1.6010186672210693
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.011714458465576172
overhead3:: 0.03511309623718262
overhead4:: 0.7198679447174072
overhead5:: 0
time_provenance:: 1.0770275592803955
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.014745473861694336
overhead3:: 0.04049873352050781
overhead4:: 0.8450632095336914
overhead5:: 0
time_provenance:: 1.24391770362854
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.016422510147094727
overhead3:: 0.04574871063232422
overhead4:: 1.000657558441162
overhead5:: 0
time_provenance:: 1.437767744064331
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.0198516845703125
overhead3:: 0.054024457931518555
overhead4:: 1.1258132457733154
overhead5:: 0
time_provenance:: 1.600327730178833
curr_diff: 0 tensor(8.4226e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4226e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02281665802001953
overhead3:: 0.06527924537658691
overhead4:: 1.3055014610290527
overhead5:: 0
time_provenance:: 1.8310396671295166
curr_diff: 0 tensor(8.0723e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0723e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02183675765991211
overhead3:: 0.05997180938720703
overhead4:: 1.502946138381958
overhead5:: 0
time_provenance:: 2.0911269187927246
curr_diff: 0 tensor(4.3590e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3590e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.022854328155517578
overhead3:: 0.06240653991699219
overhead4:: 1.608607292175293
overhead5:: 0
time_provenance:: 2.2162230014801025
curr_diff: 0 tensor(4.2282e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2282e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.02854633331298828
overhead3:: 0.06183362007141113
overhead4:: 1.655590295791626
overhead5:: 0
time_provenance:: 2.284475088119507
curr_diff: 0 tensor(4.0854e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0854e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.027102947235107422
overhead3:: 0.06948113441467285
overhead4:: 1.6891682147979736
overhead5:: 0
time_provenance:: 2.3456013202667236
curr_diff: 0 tensor(3.9854e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9854e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.028927326202392578
overhead3:: 0.07591891288757324
overhead4:: 1.8076837062835693
overhead5:: 0
time_provenance:: 2.4782676696777344
curr_diff: 0 tensor(3.9320e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9320e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
overhead:: 0
overhead2:: 0.04121088981628418
overhead3:: 0.10596394538879395
overhead4:: 2.35312819480896
overhead5:: 0
time_provenance:: 3.1860623359680176
curr_diff: 0 tensor(6.4820e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4820e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
deletion rate:: 0.01
python3 generate_rand_ids 0.01  MNIST5 0
tensor([ 2049, 40961, 47111, 18450, 59411, 22547, 20502, 57368, 45081,  8219,
        36904, 26665, 10281, 30761,  2089, 49192, 49201, 38964, 36916, 26681,
        28731, 18494,  6210, 14410, 22605, 47186, 57428, 30807,  2137, 49241,
        53341, 36958, 30822, 34919, 10352, 20597, 41081, 55418, 57470, 12415,
        18564, 30859,  2188, 20619, 10379, 18571, 30868, 22677, 32916, 57504,
        53408, 59558,  2220, 49326, 37042, 57528, 47301, 26821,  6343,  4304,
        49365, 26838, 12503, 57557,  8410, 26845, 59614, 39136, 43242, 59638,
        14584, 57595, 26880, 43267, 59654,  8455, 28938, 28940, 24848, 33045,
        24855, 24856,  6434, 55588, 53541, 18727, 55602, 10548,  8504,  2362,
        47419, 20796, 51517, 10559, 39234, 10563, 24905,   346,  8540, 47454,
        35168, 57697,  4451, 35187, 59763, 10614, 49529, 57723, 51579, 27014,
        22919, 14728, 29067, 18830, 47507, 39316, 22934, 45465, 10652, 24994,
        39331, 25000, 39348, 31159,   440, 12729,  8639,  8640, 51653, 10698,
        55755, 16844,  2510, 27086, 16848, 33241, 47577, 14816, 31203,  2538,
        43502, 43505, 25077, 12801, 23043, 20996, 57865,  8714, 57872, 29207,
        35353, 31262, 51745, 31268, 43559, 25129, 33322, 39469, 47662, 23097,
         4668,  2622, 21054, 53825, 10822, 25162,  4687, 59986, 41557,  6742,
        31325, 55901, 25182, 43619, 21094, 21098, 12908, 43628, 41582, 57970,
        45684, 19062, 47734, 45688, 49785, 37503,  4736, 53889, 53887, 25222,
        10891, 43661,  6802, 10901,  4760, 43676, 25247, 10919,   679, 12971,
        45740,  2732, 51884,   688, 31410, 12979, 41651, 21176, 10948, 37573,
        15049,  8914,   725, 15066, 39645,  2787, 56035, 45803, 31468, 47858,
        25332, 43765,   758, 29432, 21240, 37631, 17152, 37632, 31490, 21252,
        39684, 49936, 43797, 49948, 17182, 51999, 19233, 31524,  2852,  6949,
         9004, 37677, 56118, 35641, 13114, 43835,  2876,  6980,  9029, 17222,
        33607, 11078, 49994, 56146, 45909, 21335, 58199,   857, 21339,  2909,
        58207, 15208, 33643, 13177, 17277, 15232, 48009, 35723, 15245, 58254,
        41875, 54171, 54174, 31655, 27561, 15280, 52152, 11194, 29627, 46012,
        19389, 17340, 48061, 46010,  5050,   974, 39887,  9167, 50133, 52183,
        31704, 15324, 35810,  9186,  5101, 46062, 44022, 52216,  3065, 44026,
        23550,  5123, 46087, 56333, 31759, 48143, 42001,  1040, 29718,  5153,
        39970, 11298, 11302, 29734, 17450, 19503, 50226, 58423, 48184, 35900,
        50238, 42050, 25667, 37956, 56386, 56391, 35913, 50251, 40012, 56397,
         3151, 29777, 17494, 19548,  9310, 42080, 25698,  9322, 50285, 23662,
        58481, 52339, 33913, 58494, 44163, 56454, 35984, 13457, 58513, 54418,
        40083, 35994, 21662, 25759, 48294,  7334, 44201, 11433, 46250, 44205,
        48302,  1204, 42164, 50357, 44218, 29883, 42173,  3262,  9412, 15558,
         1227, 40142, 56528, 17617, 11474, 50387, 36055, 40155, 40159, 23776,
         3302,  5355, 44269, 46325, 46331, 36100, 48388,  1284, 50439, 25872,
        32016, 13584,  5403,  9499, 54563, 42276, 15653, 21803, 48427, 42290,
         3379, 23864,  9531,  5443, 58692, 13642,  3403, 32078, 54608, 56657,
        30040, 17752, 58714, 28000, 38240,  7526,  9576, 11624, 15720, 32108,
        48510, 50558, 56702, 34181,  5510, 40334, 36239, 23954, 21908, 46484,
        32158,  1441, 40355, 58789, 28071, 52656, 52660, 42420, 32193, 58819,
        42435, 38343, 34250, 46538, 11722, 38353, 58837, 48597, 26072, 32224,
        11750, 56811, 40430, 38383,  5617, 11761, 26098, 44530, 11762, 30201,
        30203, 54780, 42496, 48642, 24067, 42499, 44553, 24079,  3600,  9744,
        15892, 52757, 56855, 19992, 11801, 46621, 48671, 32291, 32293, 50725,
         7721, 54825, 15914,  7727, 52785, 15928, 13894, 17991,  9800, 20042,
        40523, 11853, 36430, 20049,  7762, 18006,  5729, 48737, 54891, 22125,
        13934,  5745, 24182, 15999, 36489, 54928, 42640, 52887, 54936, 54948,
        42663, 13991, 20141,  9903, 18102, 36536, 28349, 50881, 26305, 46791,
        48846, 52953, 38618, 52955, 22234, 50909, 14049, 22247, 20202, 48878,
        52974, 18159, 55025, 40690, 14070, 40694, 18168, 57084, 42750, 42754,
        52997, 34571, 59148, 18196, 10024, 57128, 32554, 14127, 34608, 22321,
         1844, 14137, 55101, 16190, 20286, 12096,  3910, 26458, 26462, 59231,
        40804, 48997, 22384,  1905, 32629, 49015, 18297, 36736, 38786, 53124,
        32646, 59271, 14218, 24461, 55194, 10144, 47010, 18340, 42918, 12214,
         6071,  4034, 34757, 22472, 10184, 16331,  1996, 10189, 24526, 40910,
        22480,  2006,  2014, 42978, 47077, 24554, 28652, 18412, 36850, 22515])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.308266
Train - Epoch 1, Batch: 0, Loss: 1.929175
Train - Epoch 2, Batch: 0, Loss: 1.654633
Train - Epoch 3, Batch: 0, Loss: 1.453599
Train - Epoch 4, Batch: 0, Loss: 1.298590
Train - Epoch 5, Batch: 0, Loss: 1.188202
Train - Epoch 6, Batch: 0, Loss: 1.100521
Train - Epoch 7, Batch: 0, Loss: 1.026722
Train - Epoch 8, Batch: 0, Loss: 0.970328
Train - Epoch 9, Batch: 0, Loss: 0.918604
Train - Epoch 10, Batch: 0, Loss: 0.879901
Train - Epoch 11, Batch: 0, Loss: 0.843700
Train - Epoch 12, Batch: 0, Loss: 0.818402
Train - Epoch 13, Batch: 0, Loss: 0.791148
Train - Epoch 14, Batch: 0, Loss: 0.767329
Train - Epoch 15, Batch: 0, Loss: 0.748708
Train - Epoch 16, Batch: 0, Loss: 0.727127
Train - Epoch 17, Batch: 0, Loss: 0.706531
Train - Epoch 18, Batch: 0, Loss: 0.703941
Train - Epoch 19, Batch: 0, Loss: 0.694055
Train - Epoch 20, Batch: 0, Loss: 0.670189
Train - Epoch 21, Batch: 0, Loss: 0.647114
Train - Epoch 22, Batch: 0, Loss: 0.644754
Train - Epoch 23, Batch: 0, Loss: 0.633722
Train - Epoch 24, Batch: 0, Loss: 0.626797
Train - Epoch 25, Batch: 0, Loss: 0.615817
Train - Epoch 26, Batch: 0, Loss: 0.603912
Train - Epoch 27, Batch: 0, Loss: 0.602526
Train - Epoch 28, Batch: 0, Loss: 0.595844
Train - Epoch 29, Batch: 0, Loss: 0.589298
Train - Epoch 30, Batch: 0, Loss: 0.579473
Train - Epoch 31, Batch: 0, Loss: 0.572403
training_time:: 3.391150712966919
training time full:: 3.3911960124969482
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 600
training time is 2.5728039741516113
overhead:: 0
overhead2:: 0
time_baseline:: 2.573016405105591
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.00581669807434082
overhead3:: 0.017686843872070312
overhead4:: 0.337205171585083
overhead5:: 0
time_provenance:: 0.6392860412597656
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.009333372116088867
overhead3:: 0.024556398391723633
overhead4:: 0.5302307605743408
overhead5:: 0
time_provenance:: 0.8830680847167969
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013121366500854492
overhead3:: 0.030157804489135742
overhead4:: 0.7314832210540771
overhead5:: 0
time_provenance:: 1.0916898250579834
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01522684097290039
overhead3:: 0.0405886173248291
overhead4:: 0.9272346496582031
overhead5:: 0
time_provenance:: 1.3538129329681396
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01831531524658203
overhead3:: 0.04537367820739746
overhead4:: 1.0412447452545166
overhead5:: 0
time_provenance:: 1.549455165863037
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007666349411010742
overhead3:: 0.02194976806640625
overhead4:: 0.4985511302947998
overhead5:: 0
time_provenance:: 0.8160099983215332
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011319398880004883
overhead3:: 0.028422832489013672
overhead4:: 0.6668117046356201
overhead5:: 0
time_provenance:: 1.0421183109283447
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01332235336303711
overhead3:: 0.03547859191894531
overhead4:: 0.8387331962585449
overhead5:: 0
time_provenance:: 1.2270081043243408
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017920970916748047
overhead3:: 0.04153037071228027
overhead4:: 1.0047428607940674
overhead5:: 0
time_provenance:: 1.4353134632110596
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021414756774902344
overhead3:: 0.04651069641113281
overhead4:: 1.1750521659851074
overhead5:: 0
time_provenance:: 1.6642124652862549
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011582136154174805
overhead3:: 0.033249616622924805
overhead4:: 0.733067512512207
overhead5:: 0
time_provenance:: 1.0916845798492432
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014510869979858398
overhead3:: 0.03917956352233887
overhead4:: 0.8779733180999756
overhead5:: 0
time_provenance:: 1.2791566848754883
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.018370628356933594
overhead3:: 0.04312419891357422
overhead4:: 1.0013484954833984
overhead5:: 0
time_provenance:: 1.444976806640625
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021424293518066406
overhead3:: 0.04615449905395508
overhead4:: 1.1785163879394531
overhead5:: 0
time_provenance:: 1.658217191696167
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024509906768798828
overhead3:: 0.0522608757019043
overhead4:: 1.35202956199646
overhead5:: 0
time_provenance:: 1.8867299556732178
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.022556304931640625
overhead3:: 0.060359954833984375
overhead4:: 1.519124984741211
overhead5:: 0
time_provenance:: 2.123807907104492
curr_diff: 0 tensor(8.8562e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8562e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02475762367248535
overhead3:: 0.060616493225097656
overhead4:: 1.5979464054107666
overhead5:: 0
time_provenance:: 2.240001678466797
curr_diff: 0 tensor(8.3072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02887701988220215
overhead3:: 0.06505799293518066
overhead4:: 1.6891131401062012
overhead5:: 0
time_provenance:: 2.3373239040374756
curr_diff: 0 tensor(7.7384e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7384e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02721571922302246
overhead3:: 0.06834530830383301
overhead4:: 1.702770709991455
overhead5:: 0
time_provenance:: 2.3692195415496826
curr_diff: 0 tensor(7.3781e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3781e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.03119349479675293
overhead3:: 0.06953644752502441
overhead4:: 1.8459296226501465
overhead5:: 0
time_provenance:: 2.5357937812805176
curr_diff: 0 tensor(7.2102e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2102e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04778313636779785
overhead3:: 0.09792661666870117
overhead4:: 2.43064284324646
overhead5:: 0
time_provenance:: 3.268501043319702
curr_diff: 0 tensor(6.2050e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2050e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875500
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.295099
Train - Epoch 1, Batch: 0, Loss: 1.923181
Train - Epoch 2, Batch: 0, Loss: 1.651189
Train - Epoch 3, Batch: 0, Loss: 1.453293
Train - Epoch 4, Batch: 0, Loss: 1.303116
Train - Epoch 5, Batch: 0, Loss: 1.187246
Train - Epoch 6, Batch: 0, Loss: 1.097282
Train - Epoch 7, Batch: 0, Loss: 1.030541
Train - Epoch 8, Batch: 0, Loss: 0.972392
Train - Epoch 9, Batch: 0, Loss: 0.926158
Train - Epoch 10, Batch: 0, Loss: 0.872919
Train - Epoch 11, Batch: 0, Loss: 0.848871
Train - Epoch 12, Batch: 0, Loss: 0.813681
Train - Epoch 13, Batch: 0, Loss: 0.790911
Train - Epoch 14, Batch: 0, Loss: 0.760483
Train - Epoch 15, Batch: 0, Loss: 0.746341
Train - Epoch 16, Batch: 0, Loss: 0.729604
Train - Epoch 17, Batch: 0, Loss: 0.711017
Train - Epoch 18, Batch: 0, Loss: 0.697185
Train - Epoch 19, Batch: 0, Loss: 0.681099
Train - Epoch 20, Batch: 0, Loss: 0.669799
Train - Epoch 21, Batch: 0, Loss: 0.649359
Train - Epoch 22, Batch: 0, Loss: 0.657199
Train - Epoch 23, Batch: 0, Loss: 0.635423
Train - Epoch 24, Batch: 0, Loss: 0.626262
Train - Epoch 25, Batch: 0, Loss: 0.621277
Train - Epoch 26, Batch: 0, Loss: 0.617234
Train - Epoch 27, Batch: 0, Loss: 0.609773
Train - Epoch 28, Batch: 0, Loss: 0.593500
Train - Epoch 29, Batch: 0, Loss: 0.590781
Train - Epoch 30, Batch: 0, Loss: 0.586634
Train - Epoch 31, Batch: 0, Loss: 0.583305
training_time:: 3.3681793212890625
training time full:: 3.368227243423462
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 600
training time is 2.571711540222168
overhead:: 0
overhead2:: 0
time_baseline:: 2.5719354152679443
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.0057294368743896484
overhead3:: 0.017965078353881836
overhead4:: 0.37045717239379883
overhead5:: 0
time_provenance:: 0.6354467868804932
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.009539604187011719
overhead3:: 0.02726006507873535
overhead4:: 0.54006028175354
overhead5:: 0
time_provenance:: 0.9136102199554443
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013471841812133789
overhead3:: 0.03090953826904297
overhead4:: 0.7256152629852295
overhead5:: 0
time_provenance:: 1.1019814014434814
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017344236373901367
overhead3:: 0.03823661804199219
overhead4:: 0.9240868091583252
overhead5:: 0
time_provenance:: 1.3449287414550781
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02030014991760254
overhead3:: 0.043508052825927734
overhead4:: 1.0463311672210693
overhead5:: 0
time_provenance:: 1.5683231353759766
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.00730133056640625
overhead3:: 0.023997783660888672
overhead4:: 0.5293772220611572
overhead5:: 0
time_provenance:: 0.8416881561279297
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011524438858032227
overhead3:: 0.02882838249206543
overhead4:: 0.670137882232666
overhead5:: 0
time_provenance:: 1.0498661994934082
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014517784118652344
overhead3:: 0.03391599655151367
overhead4:: 0.84090256690979
overhead5:: 0
time_provenance:: 1.2374815940856934
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01751995086669922
overhead3:: 0.040561676025390625
overhead4:: 1.001429557800293
overhead5:: 0
time_provenance:: 1.436873197555542
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.020780086517333984
overhead3:: 0.04563188552856445
overhead4:: 1.16721773147583
overhead5:: 0
time_provenance:: 1.692899465560913
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011206388473510742
overhead3:: 0.03237199783325195
overhead4:: 0.7422103881835938
overhead5:: 0
time_provenance:: 1.0971884727478027
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.015076398849487305
overhead3:: 0.03739452362060547
overhead4:: 0.8997621536254883
overhead5:: 0
time_provenance:: 1.3054347038269043
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.019109010696411133
overhead3:: 0.042217254638671875
overhead4:: 1.0176396369934082
overhead5:: 0
time_provenance:: 1.4616570472717285
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02105545997619629
overhead3:: 0.04740333557128906
overhead4:: 1.1888906955718994
overhead5:: 0
time_provenance:: 1.6772034168243408
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024715662002563477
overhead3:: 0.0537569522857666
overhead4:: 1.325770378112793
overhead5:: 0
time_provenance:: 1.868114709854126
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.022940635681152344
overhead3:: 0.05919456481933594
overhead4:: 1.516268253326416
overhead5:: 0
time_provenance:: 2.1417953968048096
curr_diff: 0 tensor(8.3234e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3234e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.025581836700439453
overhead3:: 0.06037306785583496
overhead4:: 1.618497371673584
overhead5:: 0
time_provenance:: 2.240133762359619
curr_diff: 0 tensor(7.8796e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8796e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02928948402404785
overhead3:: 0.06001758575439453
overhead4:: 1.6698219776153564
overhead5:: 0
time_provenance:: 2.3129866123199463
curr_diff: 0 tensor(7.4919e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4919e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02830791473388672
overhead3:: 0.0690605640411377
overhead4:: 1.7890760898590088
overhead5:: 0
time_provenance:: 2.454745292663574
curr_diff: 0 tensor(7.2459e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2459e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.032231807708740234
overhead3:: 0.07103919982910156
overhead4:: 1.8511030673980713
overhead5:: 0
time_provenance:: 2.539111375808716
curr_diff: 0 tensor(7.1715e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1715e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04274559020996094
overhead3:: 0.09957432746887207
overhead4:: 2.417515993118286
overhead5:: 0
time_provenance:: 3.2603862285614014
curr_diff: 0 tensor(6.2385e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2385e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.294473
Train - Epoch 1, Batch: 0, Loss: 1.930831
Train - Epoch 2, Batch: 0, Loss: 1.656338
Train - Epoch 3, Batch: 0, Loss: 1.457140
Train - Epoch 4, Batch: 0, Loss: 1.307586
Train - Epoch 5, Batch: 0, Loss: 1.194496
Train - Epoch 6, Batch: 0, Loss: 1.105076
Train - Epoch 7, Batch: 0, Loss: 1.038829
Train - Epoch 8, Batch: 0, Loss: 0.965642
Train - Epoch 9, Batch: 0, Loss: 0.928264
Train - Epoch 10, Batch: 0, Loss: 0.881298
Train - Epoch 11, Batch: 0, Loss: 0.844660
Train - Epoch 12, Batch: 0, Loss: 0.820356
Train - Epoch 13, Batch: 0, Loss: 0.796136
Train - Epoch 14, Batch: 0, Loss: 0.768559
Train - Epoch 15, Batch: 0, Loss: 0.745797
Train - Epoch 16, Batch: 0, Loss: 0.731391
Train - Epoch 17, Batch: 0, Loss: 0.714712
Train - Epoch 18, Batch: 0, Loss: 0.689002
Train - Epoch 19, Batch: 0, Loss: 0.685281
Train - Epoch 20, Batch: 0, Loss: 0.668936
Train - Epoch 21, Batch: 0, Loss: 0.663426
Train - Epoch 22, Batch: 0, Loss: 0.649912
Train - Epoch 23, Batch: 0, Loss: 0.635879
Train - Epoch 24, Batch: 0, Loss: 0.629610
Train - Epoch 25, Batch: 0, Loss: 0.612222
Train - Epoch 26, Batch: 0, Loss: 0.614780
Train - Epoch 27, Batch: 0, Loss: 0.606553
Train - Epoch 28, Batch: 0, Loss: 0.598276
Train - Epoch 29, Batch: 0, Loss: 0.581572
Train - Epoch 30, Batch: 0, Loss: 0.579209
Train - Epoch 31, Batch: 0, Loss: 0.583390
training_time:: 3.4164509773254395
training time full:: 3.416496753692627
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000055, Accuracy: 0.873900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 600
training time is 2.6021952629089355
overhead:: 0
overhead2:: 0
time_baseline:: 2.6024329662323
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.0058438777923583984
overhead3:: 0.018033504486083984
overhead4:: 0.34303855895996094
overhead5:: 0
time_provenance:: 0.6468567848205566
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873800
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.008474349975585938
overhead3:: 0.026904821395874023
overhead4:: 0.5525510311126709
overhead5:: 0
time_provenance:: 0.8872268199920654
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012839555740356445
overhead3:: 0.03057575225830078
overhead4:: 0.7208638191223145
overhead5:: 0
time_provenance:: 1.0941359996795654
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.016570091247558594
overhead3:: 0.0409235954284668
overhead4:: 0.926642894744873
overhead5:: 0
time_provenance:: 1.3422174453735352
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.020603179931640625
overhead3:: 0.04506349563598633
overhead4:: 1.0745892524719238
overhead5:: 0
time_provenance:: 1.5616843700408936
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007961750030517578
overhead3:: 0.02324199676513672
overhead4:: 0.5001914501190186
overhead5:: 0
time_provenance:: 0.825599193572998
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011110782623291016
overhead3:: 0.02830815315246582
overhead4:: 0.6622412204742432
overhead5:: 0
time_provenance:: 1.0035829544067383
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.013688802719116211
overhead3:: 0.038556575775146484
overhead4:: 0.9002993106842041
overhead5:: 0
time_provenance:: 1.2901232242584229
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874000
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.016080856323242188
overhead3:: 0.04143857955932617
overhead4:: 1.005399465560913
overhead5:: 0
time_provenance:: 1.4398939609527588
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021356582641601562
overhead3:: 0.046918392181396484
overhead4:: 1.1678638458251953
overhead5:: 0
time_provenance:: 1.6569266319274902
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012203216552734375
overhead3:: 0.03174281120300293
overhead4:: 0.7245771884918213
overhead5:: 0
time_provenance:: 1.0911304950714111
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.015319347381591797
overhead3:: 0.037410736083984375
overhead4:: 0.8789424896240234
overhead5:: 0
time_provenance:: 1.2832136154174805
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017696380615234375
overhead3:: 0.043372154235839844
overhead4:: 1.0263853073120117
overhead5:: 0
time_provenance:: 1.4847283363342285
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.019516706466674805
overhead3:: 0.05238842964172363
overhead4:: 1.17384934425354
overhead5:: 0
time_provenance:: 1.664424180984497
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02416253089904785
overhead3:: 0.0569155216217041
overhead4:: 1.334015130996704
overhead5:: 0
time_provenance:: 1.863149881362915
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02301812171936035
overhead3:: 0.05869650840759277
overhead4:: 1.544898271560669
overhead5:: 0
time_provenance:: 2.149008274078369
curr_diff: 0 tensor(8.4363e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4363e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.025768041610717773
overhead3:: 0.06185030937194824
overhead4:: 1.6237437725067139
overhead5:: 0
time_provenance:: 2.2508018016815186
curr_diff: 0 tensor(8.0413e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0413e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.026985645294189453
overhead3:: 0.06409168243408203
overhead4:: 1.6911125183105469
overhead5:: 0
time_provenance:: 2.3338892459869385
curr_diff: 0 tensor(7.7437e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7437e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.028888225555419922
overhead3:: 0.06811404228210449
overhead4:: 1.7847161293029785
overhead5:: 0
time_provenance:: 2.455897092819214
curr_diff: 0 tensor(7.3642e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3642e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.030710935592651367
overhead3:: 0.07239937782287598
overhead4:: 1.858792781829834
overhead5:: 0
time_provenance:: 2.5486578941345215
curr_diff: 0 tensor(7.0626e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0626e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.048998355865478516
overhead3:: 0.09585189819335938
overhead4:: 2.4262006282806396
overhead5:: 0
time_provenance:: 3.271101236343384
curr_diff: 0 tensor(6.2372e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2372e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.321372
Train - Epoch 1, Batch: 0, Loss: 1.934952
Train - Epoch 2, Batch: 0, Loss: 1.660051
Train - Epoch 3, Batch: 0, Loss: 1.456712
Train - Epoch 4, Batch: 0, Loss: 1.304606
Train - Epoch 5, Batch: 0, Loss: 1.192765
Train - Epoch 6, Batch: 0, Loss: 1.115340
Train - Epoch 7, Batch: 0, Loss: 1.036777
Train - Epoch 8, Batch: 0, Loss: 0.973997
Train - Epoch 9, Batch: 0, Loss: 0.925301
Train - Epoch 10, Batch: 0, Loss: 0.886760
Train - Epoch 11, Batch: 0, Loss: 0.854638
Train - Epoch 12, Batch: 0, Loss: 0.819307
Train - Epoch 13, Batch: 0, Loss: 0.784012
Train - Epoch 14, Batch: 0, Loss: 0.772068
Train - Epoch 15, Batch: 0, Loss: 0.754538
Train - Epoch 16, Batch: 0, Loss: 0.728969
Train - Epoch 17, Batch: 0, Loss: 0.706192
Train - Epoch 18, Batch: 0, Loss: 0.688992
Train - Epoch 19, Batch: 0, Loss: 0.673058
Train - Epoch 20, Batch: 0, Loss: 0.672048
Train - Epoch 21, Batch: 0, Loss: 0.655271
Train - Epoch 22, Batch: 0, Loss: 0.642617
Train - Epoch 23, Batch: 0, Loss: 0.638258
Train - Epoch 24, Batch: 0, Loss: 0.628492
Train - Epoch 25, Batch: 0, Loss: 0.633862
Train - Epoch 26, Batch: 0, Loss: 0.611247
Train - Epoch 27, Batch: 0, Loss: 0.601662
Train - Epoch 28, Batch: 0, Loss: 0.603224
Train - Epoch 29, Batch: 0, Loss: 0.589076
Train - Epoch 30, Batch: 0, Loss: 0.582013
Train - Epoch 31, Batch: 0, Loss: 0.574261
training_time:: 3.391744375228882
training time full:: 3.3917901515960693
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000055, Accuracy: 0.876000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 600
training time is 2.5896811485290527
overhead:: 0
overhead2:: 0
time_baseline:: 2.5898993015289307
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.005833148956298828
overhead3:: 0.017310142517089844
overhead4:: 0.3588552474975586
overhead5:: 0
time_provenance:: 0.6602084636688232
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010284900665283203
overhead3:: 0.027331829071044922
overhead4:: 0.5664420127868652
overhead5:: 0
time_provenance:: 0.9457666873931885
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012852668762207031
overhead3:: 0.0308229923248291
overhead4:: 0.7083790302276611
overhead5:: 0
time_provenance:: 1.1176438331604004
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01704692840576172
overhead3:: 0.03791022300720215
overhead4:: 0.9278204441070557
overhead5:: 0
time_provenance:: 1.359215497970581
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.019899845123291016
overhead3:: 0.04304051399230957
overhead4:: 1.0803046226501465
overhead5:: 0
time_provenance:: 1.549010992050171
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.008098602294921875
overhead3:: 0.02609705924987793
overhead4:: 0.5049262046813965
overhead5:: 0
time_provenance:: 0.8033726215362549
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0147, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0147, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.011518239974975586
overhead3:: 0.029605865478515625
overhead4:: 0.6839277744293213
overhead5:: 0
time_provenance:: 1.0366392135620117
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014750003814697266
overhead3:: 0.034596920013427734
overhead4:: 0.867344856262207
overhead5:: 0
time_provenance:: 1.268831729888916
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875700
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017630577087402344
overhead3:: 0.04110598564147949
overhead4:: 1.0096945762634277
overhead5:: 0
time_provenance:: 1.457068681716919
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021261215209960938
overhead3:: 0.04560351371765137
overhead4:: 1.1731867790222168
overhead5:: 0
time_provenance:: 1.656973123550415
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012030363082885742
overhead3:: 0.032765865325927734
overhead4:: 0.7183072566986084
overhead5:: 0
time_provenance:: 1.077033519744873
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014664888381958008
overhead3:: 0.03885817527770996
overhead4:: 0.8757035732269287
overhead5:: 0
time_provenance:: 1.2967462539672852
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017546415328979492
overhead3:: 0.05004620552062988
overhead4:: 1.0071706771850586
overhead5:: 0
time_provenance:: 1.4623456001281738
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.021020889282226562
overhead3:: 0.05266523361206055
overhead4:: 1.200009822845459
overhead5:: 0
time_provenance:: 1.7285630702972412
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024782419204711914
overhead3:: 0.05291867256164551
overhead4:: 1.3449139595031738
overhead5:: 0
time_provenance:: 1.8805932998657227
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.023675203323364258
overhead3:: 0.05608248710632324
overhead4:: 1.5600500106811523
overhead5:: 0
time_provenance:: 2.1672186851501465
curr_diff: 0 tensor(8.5829e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5829e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02482771873474121
overhead3:: 0.061829328536987305
overhead4:: 1.6063063144683838
overhead5:: 0
time_provenance:: 2.2352421283721924
curr_diff: 0 tensor(8.2601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.026433706283569336
overhead3:: 0.06609153747558594
overhead4:: 1.7222821712493896
overhead5:: 0
time_provenance:: 2.378551483154297
curr_diff: 0 tensor(8.0353e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0353e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.028947114944458008
overhead3:: 0.06611204147338867
overhead4:: 1.7218284606933594
overhead5:: 0
time_provenance:: 2.3963255882263184
curr_diff: 0 tensor(7.8253e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8253e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.029001235961914062
overhead3:: 0.07026791572570801
overhead4:: 1.8358004093170166
overhead5:: 0
time_provenance:: 2.5215680599212646
curr_diff: 0 tensor(7.6585e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6585e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.042272090911865234
overhead3:: 0.10538506507873535
overhead4:: 2.5123045444488525
overhead5:: 0
time_provenance:: 3.359954595565796
curr_diff: 0 tensor(6.2004e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2004e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0140, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0140, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.353821
Train - Epoch 1, Batch: 0, Loss: 1.967244
Train - Epoch 2, Batch: 0, Loss: 1.684722
Train - Epoch 3, Batch: 0, Loss: 1.477749
Train - Epoch 4, Batch: 0, Loss: 1.318898
Train - Epoch 5, Batch: 0, Loss: 1.205751
Train - Epoch 6, Batch: 0, Loss: 1.109336
Train - Epoch 7, Batch: 0, Loss: 1.029409
Train - Epoch 8, Batch: 0, Loss: 0.974977
Train - Epoch 9, Batch: 0, Loss: 0.930697
Train - Epoch 10, Batch: 0, Loss: 0.881644
Train - Epoch 11, Batch: 0, Loss: 0.852293
Train - Epoch 12, Batch: 0, Loss: 0.816674
Train - Epoch 13, Batch: 0, Loss: 0.787214
Train - Epoch 14, Batch: 0, Loss: 0.768984
Train - Epoch 15, Batch: 0, Loss: 0.741866
Train - Epoch 16, Batch: 0, Loss: 0.730406
Train - Epoch 17, Batch: 0, Loss: 0.715051
Train - Epoch 18, Batch: 0, Loss: 0.694298
Train - Epoch 19, Batch: 0, Loss: 0.684591
Train - Epoch 20, Batch: 0, Loss: 0.680948
Train - Epoch 21, Batch: 0, Loss: 0.660060
Train - Epoch 22, Batch: 0, Loss: 0.652914
Train - Epoch 23, Batch: 0, Loss: 0.630608
Train - Epoch 24, Batch: 0, Loss: 0.627903
Train - Epoch 25, Batch: 0, Loss: 0.615305
Train - Epoch 26, Batch: 0, Loss: 0.613551
Train - Epoch 27, Batch: 0, Loss: 0.603151
Train - Epoch 28, Batch: 0, Loss: 0.600706
Train - Epoch 29, Batch: 0, Loss: 0.579272
Train - Epoch 30, Batch: 0, Loss: 0.583989
Train - Epoch 31, Batch: 0, Loss: 0.576749
training_time:: 3.394486427307129
training time full:: 3.394534111022949
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000055, Accuracy: 0.875900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 600
training time is 2.551929473876953
overhead:: 0
overhead2:: 0
time_baseline:: 2.5521647930145264
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.005921840667724609
overhead3:: 0.018509387969970703
overhead4:: 0.36827683448791504
overhead5:: 0
time_provenance:: 0.6350529193878174
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.00911712646484375
overhead3:: 0.02694106101989746
overhead4:: 0.569443941116333
overhead5:: 0
time_provenance:: 0.8882315158843994
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01379847526550293
overhead3:: 0.032019853591918945
overhead4:: 0.7296922206878662
overhead5:: 0
time_provenance:: 1.091404914855957
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876300
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01622152328491211
overhead3:: 0.03764605522155762
overhead4:: 0.8737573623657227
overhead5:: 0
time_provenance:: 1.3334131240844727
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01769709587097168
overhead3:: 0.04662895202636719
overhead4:: 1.0547029972076416
overhead5:: 0
time_provenance:: 1.5930485725402832
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0155, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0155, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.007437705993652344
overhead3:: 0.022716045379638672
overhead4:: 0.511206865310669
overhead5:: 0
time_provenance:: 0.8015735149383545
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.010958194732666016
overhead3:: 0.029660940170288086
overhead4:: 0.6764428615570068
overhead5:: 0
time_provenance:: 1.0155258178710938
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01313161849975586
overhead3:: 0.03482246398925781
overhead4:: 0.8360629081726074
overhead5:: 0
time_provenance:: 1.232060432434082
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875900
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.017449617385864258
overhead3:: 0.04309582710266113
overhead4:: 1.0302391052246094
overhead5:: 0
time_provenance:: 1.4708704948425293
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.01801753044128418
overhead3:: 0.04937338829040527
overhead4:: 1.1530859470367432
overhead5:: 0
time_provenance:: 1.6666150093078613
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.012901544570922852
overhead3:: 0.03250622749328613
overhead4:: 0.7174139022827148
overhead5:: 0
time_provenance:: 1.0787405967712402
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.014856338500976562
overhead3:: 0.037805795669555664
overhead4:: 0.8778278827667236
overhead5:: 0
time_provenance:: 1.2736973762512207
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.018201589584350586
overhead3:: 0.0428929328918457
overhead4:: 1.0199329853057861
overhead5:: 0
time_provenance:: 1.4633655548095703
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.019062280654907227
overhead3:: 0.05141043663024902
overhead4:: 1.1681690216064453
overhead5:: 0
time_provenance:: 1.6560487747192383
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024277687072753906
overhead3:: 0.05345487594604492
overhead4:: 1.3409626483917236
overhead5:: 0
time_provenance:: 1.8770365715026855
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.023261070251464844
overhead3:: 0.05869936943054199
overhead4:: 1.5491816997528076
overhead5:: 0
time_provenance:: 2.1520748138427734
curr_diff: 0 tensor(8.0254e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0254e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.024562835693359375
overhead3:: 0.06057429313659668
overhead4:: 1.5987739562988281
overhead5:: 0
time_provenance:: 2.228384494781494
curr_diff: 0 tensor(7.6340e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6340e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.02707695960998535
overhead3:: 0.06557703018188477
overhead4:: 1.7195472717285156
overhead5:: 0
time_provenance:: 2.3690171241760254
curr_diff: 0 tensor(7.3656e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3656e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.029556751251220703
overhead3:: 0.06539177894592285
overhead4:: 1.7471137046813965
overhead5:: 0
time_provenance:: 2.420079231262207
curr_diff: 0 tensor(7.1549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1549e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.03165268898010254
overhead3:: 0.07306385040283203
overhead4:: 1.8684046268463135
overhead5:: 0
time_provenance:: 2.5602784156799316
curr_diff: 0 tensor(6.9692e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9692e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
overhead:: 0
overhead2:: 0.04808998107910156
overhead3:: 0.09772157669067383
overhead4:: 2.476611852645874
overhead5:: 0
time_provenance:: 3.3191521167755127
curr_diff: 0 tensor(6.1916e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1916e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876000
