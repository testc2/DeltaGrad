period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression higgs 16384 120 5
start loading data...
normalization start!!
torch.Size([10500000, 29])
torch.Size([500000, 29])
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  higgs 1
start loading data...
normalization start!!
torch.Size([10500000, 29])
torch.Size([500000, 29])
tensor([ 3730434,    74242,  3311621,  8202252, 10039821,  2069009, 10359314,
         9572375,  2436631,  3628058,  6262299,  9777692,  9156127,  4569124,
         4635686,  8799270,  4454440,  1526825, 10385960,  4471339,  3733033,
         9391147,  8778800,  5077553,  7488050,  2585139,  6965812,  6895157,
        10484790,  4363323,  3849787,   403527,  7219272,  1337928,  5345864,
         2451531,  5058124,  9994319,  7130705,  9792082,  1253969,   567380,
         6575189,  9056347,  6048860,  3513437,  9608283,  4627037,  7831644,
         6122080,  8812130,  6883430,  2956907,  6939244,  8494187,  2120818,
         4587123,  4227702,  4992633,  2238076,  3757182,  2318465,  3359877,
         2152079, 10166927,  2427024,  2524306,  1875094,  2293915,  8927388,
         8383135,   575140,  9378984,  3319976,  3149994,   394414,   982703,
         1056943,   348338,  6475444,  6766773,  4164789,  6832823,  6114490,
         9404605,   805565,  9131710,  1992386,  3537091, 10356936,  4863689,
         3056329,  3110602,  4033741,   262862,  7443661,  6866128,  8366797,
         2102488,  8002266,  8694498,  3224291,  1237220,  4581610,  6320363,
         6493420,  9136877,  2667756,  6796524,   749292,  2107114,  9514226,
         6475507,  5507828,  5032690,  7946998,  3375865,   232190,  7368448,
         2658052,  8476941,  9976592,  4024080,  2070297,  9178394,  1804059,
         9143586,  4671778,  6685476,  9878309, 10371879,  6110508,  2412334,
         9814321,  4665138,  7654709,  1285435,  8873794, 10116932,  6038340,
         5292872,   717132,  8675151,  9665359,  7185232,  6892368,  3753813,
         1144662,  8630104,  6562137,  5037916,  2902881,  4780901,  1181030,
         7513449,  5346153,  3894639,  3137906,  9031539,  8894836,  8397683,
         8591228,  6335356,  9535869,  9659774,  2246529,  9860485,  5157767,
         5674889,  5435786,  5441420,  7457167,  5664657,  4411793,  2849175,
         2629528,  9237915,  6413724, 10162076,   881567,  4937645,  8605101,
        10096564,  3750838,  9610172,  5946313,  2325962,  7776715,  6174668,
          433616,  6501332,  6224341,  8558038,  1401815,  3381206,  4171733,
         5747162,  7855579,  1679838,  9168354,  3404259,  1152996,  6572517,
         2885604,  7534057,  5751273,  6738921,  8272883,  3009528,  7800828])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.703983
Train - Epoch 0, Batch: 10, Loss: 0.699188
Train - Epoch 0, Batch: 20, Loss: 0.696574
Train - Epoch 0, Batch: 30, Loss: 0.694582
Train - Epoch 0, Batch: 40, Loss: 0.694148
Train - Epoch 0, Batch: 50, Loss: 0.692345
Train - Epoch 0, Batch: 60, Loss: 0.691946
Train - Epoch 0, Batch: 70, Loss: 0.691234
Train - Epoch 0, Batch: 80, Loss: 0.690735
Train - Epoch 0, Batch: 90, Loss: 0.690111
Train - Epoch 0, Batch: 100, Loss: 0.689445
Train - Epoch 0, Batch: 110, Loss: 0.689229
Train - Epoch 0, Batch: 120, Loss: 0.689883
Train - Epoch 0, Batch: 130, Loss: 0.689483
Train - Epoch 0, Batch: 140, Loss: 0.689318
Train - Epoch 0, Batch: 150, Loss: 0.688726
Train - Epoch 0, Batch: 160, Loss: 0.688535
Train - Epoch 0, Batch: 170, Loss: 0.689271
Train - Epoch 0, Batch: 180, Loss: 0.689193
Train - Epoch 0, Batch: 190, Loss: 0.688243
Train - Epoch 0, Batch: 200, Loss: 0.688885
Train - Epoch 0, Batch: 210, Loss: 0.688401
Train - Epoch 0, Batch: 220, Loss: 0.688563
Train - Epoch 0, Batch: 230, Loss: 0.688897
Train - Epoch 0, Batch: 240, Loss: 0.688464
Train - Epoch 0, Batch: 250, Loss: 0.687570
Train - Epoch 0, Batch: 260, Loss: 0.688548
Train - Epoch 0, Batch: 270, Loss: 0.687372
Train - Epoch 0, Batch: 280, Loss: 0.687684
Train - Epoch 0, Batch: 290, Loss: 0.687042
Train - Epoch 0, Batch: 300, Loss: 0.688289
Train - Epoch 0, Batch: 310, Loss: 0.689043
Train - Epoch 0, Batch: 320, Loss: 0.687531
Train - Epoch 0, Batch: 330, Loss: 0.687220
Train - Epoch 0, Batch: 340, Loss: 0.687268
Train - Epoch 0, Batch: 350, Loss: 0.688022
Train - Epoch 0, Batch: 360, Loss: 0.686864
Train - Epoch 0, Batch: 370, Loss: 0.687174
Train - Epoch 0, Batch: 380, Loss: 0.686866
Train - Epoch 0, Batch: 390, Loss: 0.686853
Train - Epoch 0, Batch: 400, Loss: 0.686157
Train - Epoch 0, Batch: 410, Loss: 0.687667
Train - Epoch 0, Batch: 420, Loss: 0.685920
Train - Epoch 0, Batch: 430, Loss: 0.688374
Train - Epoch 0, Batch: 440, Loss: 0.687062
Train - Epoch 0, Batch: 450, Loss: 0.686503
Train - Epoch 0, Batch: 460, Loss: 0.686884
Train - Epoch 0, Batch: 470, Loss: 0.686918
Train - Epoch 0, Batch: 480, Loss: 0.687302
Train - Epoch 0, Batch: 490, Loss: 0.686893
Train - Epoch 0, Batch: 500, Loss: 0.686337
Train - Epoch 0, Batch: 510, Loss: 0.687662
Train - Epoch 0, Batch: 520, Loss: 0.686371
Train - Epoch 0, Batch: 530, Loss: 0.686347
Train - Epoch 0, Batch: 540, Loss: 0.686461
Train - Epoch 0, Batch: 550, Loss: 0.686042
Train - Epoch 0, Batch: 560, Loss: 0.685968
Train - Epoch 0, Batch: 570, Loss: 0.687344
Train - Epoch 0, Batch: 580, Loss: 0.686210
Train - Epoch 0, Batch: 590, Loss: 0.686090
Train - Epoch 0, Batch: 600, Loss: 0.686679
Train - Epoch 0, Batch: 610, Loss: 0.686341
Train - Epoch 0, Batch: 620, Loss: 0.686575
Train - Epoch 0, Batch: 630, Loss: 0.686854
Train - Epoch 0, Batch: 640, Loss: 0.687869
Train - Epoch 1, Batch: 0, Loss: 0.686835
Train - Epoch 1, Batch: 10, Loss: 0.685515
Train - Epoch 1, Batch: 20, Loss: 0.686403
Train - Epoch 1, Batch: 30, Loss: 0.686388
Train - Epoch 1, Batch: 40, Loss: 0.686763
Train - Epoch 1, Batch: 50, Loss: 0.686104
Train - Epoch 1, Batch: 60, Loss: 0.685448
Train - Epoch 1, Batch: 70, Loss: 0.685150
Train - Epoch 1, Batch: 80, Loss: 0.687345
Train - Epoch 1, Batch: 90, Loss: 0.686768
Train - Epoch 1, Batch: 100, Loss: 0.685082
Train - Epoch 1, Batch: 110, Loss: 0.686841
Train - Epoch 1, Batch: 120, Loss: 0.686537
Train - Epoch 1, Batch: 130, Loss: 0.685370
Train - Epoch 1, Batch: 140, Loss: 0.684717
Train - Epoch 1, Batch: 150, Loss: 0.686543
Train - Epoch 1, Batch: 160, Loss: 0.685198
Train - Epoch 1, Batch: 170, Loss: 0.685783
Train - Epoch 1, Batch: 180, Loss: 0.685840
Train - Epoch 1, Batch: 190, Loss: 0.686241
Train - Epoch 1, Batch: 200, Loss: 0.685711
Train - Epoch 1, Batch: 210, Loss: 0.685169
Train - Epoch 1, Batch: 220, Loss: 0.686768
Train - Epoch 1, Batch: 230, Loss: 0.685362
Train - Epoch 1, Batch: 240, Loss: 0.684021
Train - Epoch 1, Batch: 250, Loss: 0.685317
Train - Epoch 1, Batch: 260, Loss: 0.685296
Train - Epoch 1, Batch: 270, Loss: 0.685449
Train - Epoch 1, Batch: 280, Loss: 0.685086
Train - Epoch 1, Batch: 290, Loss: 0.685354
Train - Epoch 1, Batch: 300, Loss: 0.685285
Train - Epoch 1, Batch: 310, Loss: 0.685708
Train - Epoch 1, Batch: 320, Loss: 0.685157
Train - Epoch 1, Batch: 330, Loss: 0.685451
Train - Epoch 1, Batch: 340, Loss: 0.684681
Train - Epoch 1, Batch: 350, Loss: 0.686375
Train - Epoch 1, Batch: 360, Loss: 0.686429
Train - Epoch 1, Batch: 370, Loss: 0.686378
Train - Epoch 1, Batch: 380, Loss: 0.685955
Train - Epoch 1, Batch: 390, Loss: 0.684961
Train - Epoch 1, Batch: 400, Loss: 0.685457
Train - Epoch 1, Batch: 410, Loss: 0.685329
Train - Epoch 1, Batch: 420, Loss: 0.684616
Train - Epoch 1, Batch: 430, Loss: 0.685812
Train - Epoch 1, Batch: 440, Loss: 0.684832
Train - Epoch 1, Batch: 450, Loss: 0.685510
Train - Epoch 1, Batch: 460, Loss: 0.683586
Train - Epoch 1, Batch: 470, Loss: 0.685307
Train - Epoch 1, Batch: 480, Loss: 0.684410
Train - Epoch 1, Batch: 490, Loss: 0.684537
Train - Epoch 1, Batch: 500, Loss: 0.683720
Train - Epoch 1, Batch: 510, Loss: 0.684729
Train - Epoch 1, Batch: 520, Loss: 0.685549
Train - Epoch 1, Batch: 530, Loss: 0.684887
Train - Epoch 1, Batch: 540, Loss: 0.685238
Train - Epoch 1, Batch: 550, Loss: 0.685380
Train - Epoch 1, Batch: 560, Loss: 0.684571
Train - Epoch 1, Batch: 570, Loss: 0.685065
Train - Epoch 1, Batch: 580, Loss: 0.685219
Train - Epoch 1, Batch: 590, Loss: 0.684841
Train - Epoch 1, Batch: 600, Loss: 0.684826
Train - Epoch 1, Batch: 610, Loss: 0.685172
Train - Epoch 1, Batch: 620, Loss: 0.684699
Train - Epoch 1, Batch: 630, Loss: 0.684951
Train - Epoch 1, Batch: 640, Loss: 0.684506
Train - Epoch 2, Batch: 0, Loss: 0.684803
Train - Epoch 2, Batch: 10, Loss: 0.684834
Train - Epoch 2, Batch: 20, Loss: 0.684779
Train - Epoch 2, Batch: 30, Loss: 0.686020
Train - Epoch 2, Batch: 40, Loss: 0.685045
Train - Epoch 2, Batch: 50, Loss: 0.684886
Train - Epoch 2, Batch: 60, Loss: 0.684113
Train - Epoch 2, Batch: 70, Loss: 0.684846
Train - Epoch 2, Batch: 80, Loss: 0.685121
Train - Epoch 2, Batch: 90, Loss: 0.685570
Train - Epoch 2, Batch: 100, Loss: 0.685147
Train - Epoch 2, Batch: 110, Loss: 0.684991
Train - Epoch 2, Batch: 120, Loss: 0.684066
Train - Epoch 2, Batch: 130, Loss: 0.684933
Train - Epoch 2, Batch: 140, Loss: 0.686347
Train - Epoch 2, Batch: 150, Loss: 0.684137
Train - Epoch 2, Batch: 160, Loss: 0.685016
Train - Epoch 2, Batch: 170, Loss: 0.685334
Train - Epoch 2, Batch: 180, Loss: 0.685741
Train - Epoch 2, Batch: 190, Loss: 0.684294
Train - Epoch 2, Batch: 200, Loss: 0.684853
Train - Epoch 2, Batch: 210, Loss: 0.684163
Train - Epoch 2, Batch: 220, Loss: 0.685636
Train - Epoch 2, Batch: 230, Loss: 0.684358
Train - Epoch 2, Batch: 240, Loss: 0.684609
Train - Epoch 2, Batch: 250, Loss: 0.685076
Train - Epoch 2, Batch: 260, Loss: 0.683317
Train - Epoch 2, Batch: 270, Loss: 0.683954
Train - Epoch 2, Batch: 280, Loss: 0.684533
Train - Epoch 2, Batch: 290, Loss: 0.684070
Train - Epoch 2, Batch: 300, Loss: 0.684520
Train - Epoch 2, Batch: 310, Loss: 0.683913
Train - Epoch 2, Batch: 320, Loss: 0.684246
Train - Epoch 2, Batch: 330, Loss: 0.684498
Train - Epoch 2, Batch: 340, Loss: 0.684759
Train - Epoch 2, Batch: 350, Loss: 0.683740
Train - Epoch 2, Batch: 360, Loss: 0.684266
Train - Epoch 2, Batch: 370, Loss: 0.684986
Train - Epoch 2, Batch: 380, Loss: 0.683949
Train - Epoch 2, Batch: 390, Loss: 0.683530
Train - Epoch 2, Batch: 400, Loss: 0.683313
Train - Epoch 2, Batch: 410, Loss: 0.684879
Train - Epoch 2, Batch: 420, Loss: 0.684374
Train - Epoch 2, Batch: 430, Loss: 0.683502
Train - Epoch 2, Batch: 440, Loss: 0.683635
Train - Epoch 2, Batch: 450, Loss: 0.683776
Train - Epoch 2, Batch: 460, Loss: 0.685359
Train - Epoch 2, Batch: 470, Loss: 0.684575
Train - Epoch 2, Batch: 480, Loss: 0.684278
Train - Epoch 2, Batch: 490, Loss: 0.683953
Train - Epoch 2, Batch: 500, Loss: 0.684830
Train - Epoch 2, Batch: 510, Loss: 0.684052
Train - Epoch 2, Batch: 520, Loss: 0.685122
Train - Epoch 2, Batch: 530, Loss: 0.686165
Train - Epoch 2, Batch: 540, Loss: 0.684934
Train - Epoch 2, Batch: 550, Loss: 0.683798/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684712
Train - Epoch 2, Batch: 570, Loss: 0.683961
Train - Epoch 2, Batch: 580, Loss: 0.683662
Train - Epoch 2, Batch: 590, Loss: 0.684741
Train - Epoch 2, Batch: 600, Loss: 0.682729
Train - Epoch 2, Batch: 610, Loss: 0.685051
Train - Epoch 2, Batch: 620, Loss: 0.683471
Train - Epoch 2, Batch: 630, Loss: 0.683987
Train - Epoch 2, Batch: 640, Loss: 0.683394
Train - Epoch 3, Batch: 0, Loss: 0.683964
Train - Epoch 3, Batch: 10, Loss: 0.684101
Train - Epoch 3, Batch: 20, Loss: 0.684837
Train - Epoch 3, Batch: 30, Loss: 0.683401
Train - Epoch 3, Batch: 40, Loss: 0.684793
Train - Epoch 3, Batch: 50, Loss: 0.683962
Train - Epoch 3, Batch: 60, Loss: 0.683953
Train - Epoch 3, Batch: 70, Loss: 0.685512
Train - Epoch 3, Batch: 80, Loss: 0.684553
Train - Epoch 3, Batch: 90, Loss: 0.683915
Train - Epoch 3, Batch: 100, Loss: 0.684373
Train - Epoch 3, Batch: 110, Loss: 0.684749
Train - Epoch 3, Batch: 120, Loss: 0.683885
Train - Epoch 3, Batch: 130, Loss: 0.684409
Train - Epoch 3, Batch: 140, Loss: 0.682639
Train - Epoch 3, Batch: 150, Loss: 0.684757
Train - Epoch 3, Batch: 160, Loss: 0.683876
Train - Epoch 3, Batch: 170, Loss: 0.684065
Train - Epoch 3, Batch: 180, Loss: 0.683901
Train - Epoch 3, Batch: 190, Loss: 0.682911
Train - Epoch 3, Batch: 200, Loss: 0.684194
Train - Epoch 3, Batch: 210, Loss: 0.682761
Train - Epoch 3, Batch: 220, Loss: 0.684176
Train - Epoch 3, Batch: 230, Loss: 0.684395
Train - Epoch 3, Batch: 240, Loss: 0.683663
Train - Epoch 3, Batch: 250, Loss: 0.683456
Train - Epoch 3, Batch: 260, Loss: 0.683767
Train - Epoch 3, Batch: 270, Loss: 0.685333
Train - Epoch 3, Batch: 280, Loss: 0.683959
Train - Epoch 3, Batch: 290, Loss: 0.683275
Train - Epoch 3, Batch: 300, Loss: 0.684897
Train - Epoch 3, Batch: 310, Loss: 0.682923
Train - Epoch 3, Batch: 320, Loss: 0.684729
Train - Epoch 3, Batch: 330, Loss: 0.684301
Train - Epoch 3, Batch: 340, Loss: 0.683791
Train - Epoch 3, Batch: 350, Loss: 0.682502
Train - Epoch 3, Batch: 360, Loss: 0.683812
Train - Epoch 3, Batch: 370, Loss: 0.684604
Train - Epoch 3, Batch: 380, Loss: 0.683503
Train - Epoch 3, Batch: 390, Loss: 0.683026
Train - Epoch 3, Batch: 400, Loss: 0.684589
Train - Epoch 3, Batch: 410, Loss: 0.683286
Train - Epoch 3, Batch: 420, Loss: 0.684096
Train - Epoch 3, Batch: 430, Loss: 0.684482
Train - Epoch 3, Batch: 440, Loss: 0.683861
Train - Epoch 3, Batch: 450, Loss: 0.684003
Train - Epoch 3, Batch: 460, Loss: 0.682812
Train - Epoch 3, Batch: 470, Loss: 0.683760
Train - Epoch 3, Batch: 480, Loss: 0.683485
Train - Epoch 3, Batch: 490, Loss: 0.683850
Train - Epoch 3, Batch: 500, Loss: 0.683605
Train - Epoch 3, Batch: 510, Loss: 0.684545
Train - Epoch 3, Batch: 520, Loss: 0.684937
Train - Epoch 3, Batch: 530, Loss: 0.683288
Train - Epoch 3, Batch: 540, Loss: 0.685068
Train - Epoch 3, Batch: 550, Loss: 0.685077
Train - Epoch 3, Batch: 560, Loss: 0.684117
Train - Epoch 3, Batch: 570, Loss: 0.683767
Train - Epoch 3, Batch: 580, Loss: 0.683762
Train - Epoch 3, Batch: 590, Loss: 0.682912
Train - Epoch 3, Batch: 600, Loss: 0.682540
Train - Epoch 3, Batch: 610, Loss: 0.683022
Train - Epoch 3, Batch: 620, Loss: 0.683773
Train - Epoch 3, Batch: 630, Loss: 0.685033
Train - Epoch 3, Batch: 640, Loss: 0.684347
training_time:: 7.838524103164673
training time full:: 7.8385655879974365
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554056
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 210
training time is 5.706590414047241
overhead:: 0
overhead2:: 0
time_baseline:: 5.709706544876099
curr_diff: 0 tensor(3.9794e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9794e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.042151689529418945
overhead3:: 0.196136474609375
overhead4:: 0.705035924911499
overhead5:: 0
time_provenance:: 1.883183479309082
curr_diff: 0 tensor(7.6035e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6035e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.6806e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6806e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554060
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.05217313766479492
overhead3:: 0.22865056991577148
overhead4:: 0.8810539245605469
overhead5:: 0
time_provenance:: 2.0557539463043213
curr_diff: 0 tensor(7.5366e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5366e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.6874e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6874e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554060
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06465554237365723
overhead3:: 0.2884206771850586
overhead4:: 1.0833277702331543
overhead5:: 0
time_provenance:: 2.376215696334839
curr_diff: 0 tensor(7.3755e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3755e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554060
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07610964775085449
overhead3:: 0.3486499786376953
overhead4:: 1.240309715270996
overhead5:: 0
time_provenance:: 2.7218101024627686
curr_diff: 0 tensor(7.1847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7279e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7279e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554060
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06181931495666504
overhead3:: 0.31576013565063477
overhead4:: 1.1156349182128906
overhead5:: 0
time_provenance:: 2.500084400177002
curr_diff: 0 tensor(4.2825e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2825e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9285e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9285e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07294368743896484
overhead3:: 0.38506555557250977
overhead4:: 1.4400672912597656
overhead5:: 0
time_provenance:: 2.9310052394866943
curr_diff: 0 tensor(4.2572e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2572e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9303e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9303e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.0800783634185791
overhead3:: 0.38477039337158203
overhead4:: 1.3317749500274658
overhead5:: 0
time_provenance:: 2.7425084114074707
curr_diff: 0 tensor(4.2436e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2436e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9314e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9314e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08457779884338379
overhead3:: 0.42699766159057617
overhead4:: 1.532344102859497
overhead5:: 0
time_provenance:: 3.1110501289367676
curr_diff: 0 tensor(4.0908e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0908e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10307002067565918
overhead3:: 0.5062122344970703
overhead4:: 1.7375304698944092
overhead5:: 0
time_provenance:: 3.591926097869873
curr_diff: 0 tensor(2.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9428e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554062
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.09569883346557617
overhead3:: 0.48704981803894043
overhead4:: 1.900754451751709
overhead5:: 0
time_provenance:: 3.5940234661102295
curr_diff: 0 tensor(2.0967e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0967e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554066
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.09652161598205566
overhead3:: 0.48287391662597656
overhead4:: 1.9509305953979492
overhead5:: 0
time_provenance:: 3.5850255489349365
curr_diff: 0 tensor(1.8194e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8194e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9941e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.12371468544006348
overhead3:: 0.6455774307250977
overhead4:: 2.168278455734253
overhead5:: 0
time_provenance:: 4.050400495529175
curr_diff: 0 tensor(2.1330e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1330e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9472e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9472e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554062
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1219789981842041
overhead3:: 0.5723843574523926
overhead4:: 2.3523268699645996
overhead5:: 0
time_provenance:: 4.257684230804443
curr_diff: 0 tensor(1.2756e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2756e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9699e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9699e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.14343762397766113
overhead3:: 0.6568324565887451
overhead4:: 2.6533045768737793
overhead5:: 0
time_provenance:: 4.9257025718688965
curr_diff: 0 tensor(1.2582e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2582e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9712e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9712e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.12972211837768555
overhead3:: 0.6096105575561523
overhead4:: 2.5900814533233643
overhead5:: 0
time_provenance:: 4.5472187995910645
curr_diff: 0 tensor(1.2249e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2249e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9737e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9737e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1502668857574463
overhead3:: 0.7045900821685791
overhead4:: 3.0416646003723145
overhead5:: 0
time_provenance:: 5.365846157073975
curr_diff: 0 tensor(1.1977e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1977e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9757e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9757e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.2267310619354248
overhead3:: 1.1248226165771484
overhead4:: 3.6708014011383057
overhead5:: 0
time_provenance:: 5.525073766708374
curr_diff: 0 tensor(2.1237e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1237e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9794e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9794e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554064
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.720149
Train - Epoch 0, Batch: 10, Loss: 0.697490
Train - Epoch 0, Batch: 20, Loss: 0.695255
Train - Epoch 0, Batch: 30, Loss: 0.695035
Train - Epoch 0, Batch: 40, Loss: 0.693280
Train - Epoch 0, Batch: 50, Loss: 0.692913
Train - Epoch 0, Batch: 60, Loss: 0.691885
Train - Epoch 0, Batch: 70, Loss: 0.691921
Train - Epoch 0, Batch: 80, Loss: 0.691000
Train - Epoch 0, Batch: 90, Loss: 0.690756
Train - Epoch 0, Batch: 100, Loss: 0.691377
Train - Epoch 0, Batch: 110, Loss: 0.690442
Train - Epoch 0, Batch: 120, Loss: 0.690440
Train - Epoch 0, Batch: 130, Loss: 0.690289
Train - Epoch 0, Batch: 140, Loss: 0.689376
Train - Epoch 0, Batch: 150, Loss: 0.689319
Train - Epoch 0, Batch: 160, Loss: 0.689567
Train - Epoch 0, Batch: 170, Loss: 0.688607
Train - Epoch 0, Batch: 180, Loss: 0.689532
Train - Epoch 0, Batch: 190, Loss: 0.688809
Train - Epoch 0, Batch: 200, Loss: 0.689314
Train - Epoch 0, Batch: 210, Loss: 0.689416
Train - Epoch 0, Batch: 220, Loss: 0.688680
Train - Epoch 0, Batch: 230, Loss: 0.689287
Train - Epoch 0, Batch: 240, Loss: 0.688623
Train - Epoch 0, Batch: 250, Loss: 0.688353
Train - Epoch 0, Batch: 260, Loss: 0.688657
Train - Epoch 0, Batch: 270, Loss: 0.689195
Train - Epoch 0, Batch: 280, Loss: 0.688277
Train - Epoch 0, Batch: 290, Loss: 0.687700
Train - Epoch 0, Batch: 300, Loss: 0.687888
Train - Epoch 0, Batch: 310, Loss: 0.688190
Train - Epoch 0, Batch: 320, Loss: 0.687733
Train - Epoch 0, Batch: 330, Loss: 0.686678
Train - Epoch 0, Batch: 340, Loss: 0.688942
Train - Epoch 0, Batch: 350, Loss: 0.687698
Train - Epoch 0, Batch: 360, Loss: 0.688085
Train - Epoch 0, Batch: 370, Loss: 0.687334
Train - Epoch 0, Batch: 380, Loss: 0.687133
Train - Epoch 0, Batch: 390, Loss: 0.688299
Train - Epoch 0, Batch: 400, Loss: 0.688243
Train - Epoch 0, Batch: 410, Loss: 0.688169
Train - Epoch 0, Batch: 420, Loss: 0.688542
Train - Epoch 0, Batch: 430, Loss: 0.686355
Train - Epoch 0, Batch: 440, Loss: 0.686382
Train - Epoch 0, Batch: 450, Loss: 0.687866
Train - Epoch 0, Batch: 460, Loss: 0.688092
Train - Epoch 0, Batch: 470, Loss: 0.688152
Train - Epoch 0, Batch: 480, Loss: 0.687376
Train - Epoch 0, Batch: 490, Loss: 0.686906
Train - Epoch 0, Batch: 500, Loss: 0.687086
Train - Epoch 0, Batch: 510, Loss: 0.686633
Train - Epoch 0, Batch: 520, Loss: 0.687751
Train - Epoch 0, Batch: 530, Loss: 0.687194
Train - Epoch 0, Batch: 540, Loss: 0.686592
Train - Epoch 0, Batch: 550, Loss: 0.686694
Train - Epoch 0, Batch: 560, Loss: 0.686766
Train - Epoch 0, Batch: 570, Loss: 0.685898
Train - Epoch 0, Batch: 580, Loss: 0.686436
Train - Epoch 0, Batch: 590, Loss: 0.686730
Train - Epoch 0, Batch: 600, Loss: 0.687453
Train - Epoch 0, Batch: 610, Loss: 0.687486
Train - Epoch 0, Batch: 620, Loss: 0.686150
Train - Epoch 0, Batch: 630, Loss: 0.686513
Train - Epoch 0, Batch: 640, Loss: 0.686158
Train - Epoch 1, Batch: 0, Loss: 0.687207
Train - Epoch 1, Batch: 10, Loss: 0.687374
Train - Epoch 1, Batch: 20, Loss: 0.686221
Train - Epoch 1, Batch: 30, Loss: 0.688012
Train - Epoch 1, Batch: 40, Loss: 0.686617
Train - Epoch 1, Batch: 50, Loss: 0.687179
Train - Epoch 1, Batch: 60, Loss: 0.685879
Train - Epoch 1, Batch: 70, Loss: 0.686557
Train - Epoch 1, Batch: 80, Loss: 0.685707
Train - Epoch 1, Batch: 90, Loss: 0.685880
Train - Epoch 1, Batch: 100, Loss: 0.686522
Train - Epoch 1, Batch: 110, Loss: 0.686652
Train - Epoch 1, Batch: 120, Loss: 0.686545
Train - Epoch 1, Batch: 130, Loss: 0.685856
Train - Epoch 1, Batch: 140, Loss: 0.686089
Train - Epoch 1, Batch: 150, Loss: 0.686475
Train - Epoch 1, Batch: 160, Loss: 0.687121
Train - Epoch 1, Batch: 170, Loss: 0.686264
Train - Epoch 1, Batch: 180, Loss: 0.686716
Train - Epoch 1, Batch: 190, Loss: 0.686201
Train - Epoch 1, Batch: 200, Loss: 0.685264
Train - Epoch 1, Batch: 210, Loss: 0.686774
Train - Epoch 1, Batch: 220, Loss: 0.686005
Train - Epoch 1, Batch: 230, Loss: 0.685584
Train - Epoch 1, Batch: 240, Loss: 0.685777
Train - Epoch 1, Batch: 250, Loss: 0.686670
Train - Epoch 1, Batch: 260, Loss: 0.687580
Train - Epoch 1, Batch: 270, Loss: 0.685392
Train - Epoch 1, Batch: 280, Loss: 0.685459
Train - Epoch 1, Batch: 290, Loss: 0.685637
Train - Epoch 1, Batch: 300, Loss: 0.685840
Train - Epoch 1, Batch: 310, Loss: 0.686354
Train - Epoch 1, Batch: 320, Loss: 0.685693
Train - Epoch 1, Batch: 330, Loss: 0.685836
Train - Epoch 1, Batch: 340, Loss: 0.686638
Train - Epoch 1, Batch: 350, Loss: 0.685314
Train - Epoch 1, Batch: 360, Loss: 0.685371
Train - Epoch 1, Batch: 370, Loss: 0.685157
Train - Epoch 1, Batch: 380, Loss: 0.686398
Train - Epoch 1, Batch: 390, Loss: 0.684741
Train - Epoch 1, Batch: 400, Loss: 0.684797
Train - Epoch 1, Batch: 410, Loss: 0.685636
Train - Epoch 1, Batch: 420, Loss: 0.686137
Train - Epoch 1, Batch: 430, Loss: 0.684392
Train - Epoch 1, Batch: 440, Loss: 0.684604
Train - Epoch 1, Batch: 450, Loss: 0.685298
Train - Epoch 1, Batch: 460, Loss: 0.685506
Train - Epoch 1, Batch: 470, Loss: 0.684866
Train - Epoch 1, Batch: 480, Loss: 0.684142
Train - Epoch 1, Batch: 490, Loss: 0.685624
Train - Epoch 1, Batch: 500, Loss: 0.685917
Train - Epoch 1, Batch: 510, Loss: 0.686195
Train - Epoch 1, Batch: 520, Loss: 0.685967
Train - Epoch 1, Batch: 530, Loss: 0.685533
Train - Epoch 1, Batch: 540, Loss: 0.685251
Train - Epoch 1, Batch: 550, Loss: 0.684735
Train - Epoch 1, Batch: 560, Loss: 0.686028
Train - Epoch 1, Batch: 570, Loss: 0.685956
Train - Epoch 1, Batch: 580, Loss: 0.684667
Train - Epoch 1, Batch: 590, Loss: 0.685171
Train - Epoch 1, Batch: 600, Loss: 0.685648
Train - Epoch 1, Batch: 610, Loss: 0.684743
Train - Epoch 1, Batch: 620, Loss: 0.685862
Train - Epoch 1, Batch: 630, Loss: 0.685093
Train - Epoch 1, Batch: 640, Loss: 0.686338
Train - Epoch 2, Batch: 0, Loss: 0.686288
Train - Epoch 2, Batch: 10, Loss: 0.684162
Train - Epoch 2, Batch: 20, Loss: 0.684619
Train - Epoch 2, Batch: 30, Loss: 0.685439
Train - Epoch 2, Batch: 40, Loss: 0.685803
Train - Epoch 2, Batch: 50, Loss: 0.683726
Train - Epoch 2, Batch: 60, Loss: 0.685962
Train - Epoch 2, Batch: 70, Loss: 0.685292
Train - Epoch 2, Batch: 80, Loss: 0.685373
Train - Epoch 2, Batch: 90, Loss: 0.684423
Train - Epoch 2, Batch: 100, Loss: 0.684389
Train - Epoch 2, Batch: 110, Loss: 0.685066
Train - Epoch 2, Batch: 120, Loss: 0.685999
Train - Epoch 2, Batch: 130, Loss: 0.685446
Train - Epoch 2, Batch: 140, Loss: 0.685078
Train - Epoch 2, Batch: 150, Loss: 0.685362
Train - Epoch 2, Batch: 160, Loss: 0.684921
Train - Epoch 2, Batch: 170, Loss: 0.684164
Train - Epoch 2, Batch: 180, Loss: 0.685189
Train - Epoch 2, Batch: 190, Loss: 0.683363
Train - Epoch 2, Batch: 200, Loss: 0.685362
Train - Epoch 2, Batch: 210, Loss: 0.684326
Train - Epoch 2, Batch: 220, Loss: 0.686137
Train - Epoch 2, Batch: 230, Loss: 0.685488
Train - Epoch 2, Batch: 240, Loss: 0.684447
Train - Epoch 2, Batch: 250, Loss: 0.684298
Train - Epoch 2, Batch: 260, Loss: 0.685606
Train - Epoch 2, Batch: 270, Loss: 0.685419
Train - Epoch 2, Batch: 280, Loss: 0.685413
Train - Epoch 2, Batch: 290, Loss: 0.685306
Train - Epoch 2, Batch: 300, Loss: 0.685832
Train - Epoch 2, Batch: 310, Loss: 0.685489
Train - Epoch 2, Batch: 320, Loss: 0.685130
Train - Epoch 2, Batch: 330, Loss: 0.684676
Train - Epoch 2, Batch: 340, Loss: 0.686013
Train - Epoch 2, Batch: 350, Loss: 0.683656
Train - Epoch 2, Batch: 360, Loss: 0.685506
Train - Epoch 2, Batch: 370, Loss: 0.684095
Train - Epoch 2, Batch: 380, Loss: 0.685754
Train - Epoch 2, Batch: 390, Loss: 0.685228
Train - Epoch 2, Batch: 400, Loss: 0.685361
Train - Epoch 2, Batch: 410, Loss: 0.684946
Train - Epoch 2, Batch: 420, Loss: 0.685243
Train - Epoch 2, Batch: 430, Loss: 0.684099
Train - Epoch 2, Batch: 440, Loss: 0.684996
Train - Epoch 2, Batch: 450, Loss: 0.685031
Train - Epoch 2, Batch: 460, Loss: 0.685326
Train - Epoch 2, Batch: 470, Loss: 0.684870
Train - Epoch 2, Batch: 480, Loss: 0.684481
Train - Epoch 2, Batch: 490, Loss: 0.683939
Train - Epoch 2, Batch: 500, Loss: 0.684199
Train - Epoch 2, Batch: 510, Loss: 0.684430
Train - Epoch 2, Batch: 520, Loss: 0.684760
Train - Epoch 2, Batch: 530, Loss: 0.685049
Train - Epoch 2, Batch: 540, Loss: 0.684452
Train - Epoch 2, Batch: 550, Loss: 0.683641/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684262
Train - Epoch 2, Batch: 570, Loss: 0.684635
Train - Epoch 2, Batch: 580, Loss: 0.682882
Train - Epoch 2, Batch: 590, Loss: 0.683408
Train - Epoch 2, Batch: 600, Loss: 0.685438
Train - Epoch 2, Batch: 610, Loss: 0.683962
Train - Epoch 2, Batch: 620, Loss: 0.683918
Train - Epoch 2, Batch: 630, Loss: 0.683291
Train - Epoch 2, Batch: 640, Loss: 0.683585
Train - Epoch 3, Batch: 0, Loss: 0.685473
Train - Epoch 3, Batch: 10, Loss: 0.683274
Train - Epoch 3, Batch: 20, Loss: 0.683337
Train - Epoch 3, Batch: 30, Loss: 0.683827
Train - Epoch 3, Batch: 40, Loss: 0.684284
Train - Epoch 3, Batch: 50, Loss: 0.683647
Train - Epoch 3, Batch: 60, Loss: 0.682276
Train - Epoch 3, Batch: 70, Loss: 0.684743
Train - Epoch 3, Batch: 80, Loss: 0.684283
Train - Epoch 3, Batch: 90, Loss: 0.683823
Train - Epoch 3, Batch: 100, Loss: 0.685313
Train - Epoch 3, Batch: 110, Loss: 0.684726
Train - Epoch 3, Batch: 120, Loss: 0.683675
Train - Epoch 3, Batch: 130, Loss: 0.685166
Train - Epoch 3, Batch: 140, Loss: 0.684427
Train - Epoch 3, Batch: 150, Loss: 0.682274
Train - Epoch 3, Batch: 160, Loss: 0.684344
Train - Epoch 3, Batch: 170, Loss: 0.683595
Train - Epoch 3, Batch: 180, Loss: 0.683868
Train - Epoch 3, Batch: 190, Loss: 0.685408
Train - Epoch 3, Batch: 200, Loss: 0.683062
Train - Epoch 3, Batch: 210, Loss: 0.684964
Train - Epoch 3, Batch: 220, Loss: 0.683844
Train - Epoch 3, Batch: 230, Loss: 0.684463
Train - Epoch 3, Batch: 240, Loss: 0.684589
Train - Epoch 3, Batch: 250, Loss: 0.684875
Train - Epoch 3, Batch: 260, Loss: 0.683582
Train - Epoch 3, Batch: 270, Loss: 0.683552
Train - Epoch 3, Batch: 280, Loss: 0.683231
Train - Epoch 3, Batch: 290, Loss: 0.684657
Train - Epoch 3, Batch: 300, Loss: 0.685830
Train - Epoch 3, Batch: 310, Loss: 0.684088
Train - Epoch 3, Batch: 320, Loss: 0.683449
Train - Epoch 3, Batch: 330, Loss: 0.684503
Train - Epoch 3, Batch: 340, Loss: 0.682957
Train - Epoch 3, Batch: 350, Loss: 0.683520
Train - Epoch 3, Batch: 360, Loss: 0.683848
Train - Epoch 3, Batch: 370, Loss: 0.683063
Train - Epoch 3, Batch: 380, Loss: 0.684258
Train - Epoch 3, Batch: 390, Loss: 0.684074
Train - Epoch 3, Batch: 400, Loss: 0.684348
Train - Epoch 3, Batch: 410, Loss: 0.684551
Train - Epoch 3, Batch: 420, Loss: 0.683043
Train - Epoch 3, Batch: 430, Loss: 0.685150
Train - Epoch 3, Batch: 440, Loss: 0.683774
Train - Epoch 3, Batch: 450, Loss: 0.684247
Train - Epoch 3, Batch: 460, Loss: 0.684396
Train - Epoch 3, Batch: 470, Loss: 0.684586
Train - Epoch 3, Batch: 480, Loss: 0.683324
Train - Epoch 3, Batch: 490, Loss: 0.684649
Train - Epoch 3, Batch: 500, Loss: 0.683650
Train - Epoch 3, Batch: 510, Loss: 0.683105
Train - Epoch 3, Batch: 520, Loss: 0.684087
Train - Epoch 3, Batch: 530, Loss: 0.683557
Train - Epoch 3, Batch: 540, Loss: 0.684233
Train - Epoch 3, Batch: 550, Loss: 0.685017
Train - Epoch 3, Batch: 560, Loss: 0.683625
Train - Epoch 3, Batch: 570, Loss: 0.684053
Train - Epoch 3, Batch: 580, Loss: 0.684366
Train - Epoch 3, Batch: 590, Loss: 0.684227
Train - Epoch 3, Batch: 600, Loss: 0.683875
Train - Epoch 3, Batch: 610, Loss: 0.685136
Train - Epoch 3, Batch: 620, Loss: 0.684623
Train - Epoch 3, Batch: 630, Loss: 0.683686
Train - Epoch 3, Batch: 640, Loss: 0.683652
training_time:: 7.757015705108643
training time full:: 7.757057428359985
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554784
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 210
training time is 5.465126276016235
overhead:: 0
overhead2:: 0
time_baseline:: 5.46970534324646
curr_diff: 0 tensor(3.7377e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7377e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.046889305114746094
overhead3:: 0.20832514762878418
overhead4:: 0.781883955001831
overhead5:: 0
time_provenance:: 1.9704344272613525
curr_diff: 0 tensor(9.2043e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2043e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7029e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7029e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.05738210678100586
overhead3:: 0.2718660831451416
overhead4:: 0.9990987777709961
overhead5:: 0
time_provenance:: 2.447674036026001
curr_diff: 0 tensor(9.2903e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2903e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.6932e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6932e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.053516387939453125
overhead3:: 0.25499558448791504
overhead4:: 1.0687761306762695
overhead5:: 0
time_provenance:: 2.316279649734497
curr_diff: 0 tensor(9.2711e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2711e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.6953e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6953e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.0688929557800293
overhead3:: 0.3212156295776367
overhead4:: 1.2382633686065674
overhead5:: 0
time_provenance:: 2.843275547027588
curr_diff: 0 tensor(8.4881e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4881e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7896e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7896e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554790
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06983184814453125
overhead3:: 0.329486608505249
overhead4:: 1.1888346672058105
overhead5:: 0
time_provenance:: 2.59011173248291
curr_diff: 0 tensor(5.3581e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3581e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7253e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7253e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554790
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06933140754699707
overhead3:: 0.35539865493774414
overhead4:: 1.3544418811798096
overhead5:: 0
time_provenance:: 2.7866933345794678
curr_diff: 0 tensor(5.3178e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3178e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7286e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7286e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554790
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08637189865112305
overhead3:: 0.4351193904876709
overhead4:: 1.578049898147583
overhead5:: 0
time_provenance:: 3.24574613571167
curr_diff: 0 tensor(5.3077e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3077e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7298e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7298e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554790
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08140158653259277
overhead3:: 0.40012359619140625
overhead4:: 1.521284580230713
overhead5:: 0
time_provenance:: 2.967219352722168
curr_diff: 0 tensor(5.1325e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1325e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7467e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7467e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554790
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08982610702514648
overhead3:: 0.46487903594970703
overhead4:: 1.7862389087677002
overhead5:: 0
time_provenance:: 3.5168800354003906
curr_diff: 0 tensor(4.0261e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0261e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554784
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08875679969787598
overhead3:: 0.41426682472229004
overhead4:: 1.894552230834961
overhead5:: 0
time_provenance:: 3.491297483444214
curr_diff: 0 tensor(2.8722e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8722e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.09701251983642578
overhead3:: 0.5379378795623779
overhead4:: 2.086501121520996
overhead5:: 0
time_provenance:: 3.826235055923462
curr_diff: 0 tensor(2.6641e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6641e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7063e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7063e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10573124885559082
overhead3:: 0.5728566646575928
overhead4:: 1.9979314804077148
overhead5:: 0
time_provenance:: 3.7033464908599854
curr_diff: 0 tensor(3.9324e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9324e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7760e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7760e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554784
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11864233016967773
overhead3:: 0.5907726287841797
overhead4:: 2.4334423542022705
overhead5:: 0
time_provenance:: 4.361456632614136
curr_diff: 0 tensor(1.5665e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5665e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11697006225585938
overhead3:: 0.5832695960998535
overhead4:: 2.4811809062957764
overhead5:: 0
time_provenance:: 4.417209625244141
curr_diff: 0 tensor(1.5627e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5627e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7382e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7382e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1371002197265625
overhead3:: 0.719820499420166
overhead4:: 2.621856927871704
overhead5:: 0
time_provenance:: 4.692385196685791
curr_diff: 0 tensor(1.5037e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5037e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.12594318389892578
overhead3:: 0.6553971767425537
overhead4:: 2.6324501037597656
overhead5:: 0
time_provenance:: 4.555434226989746
curr_diff: 0 tensor(1.4443e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4443e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.21419644355773926
overhead3:: 0.9570090770721436
overhead4:: 3.4138050079345703
overhead5:: 0
time_provenance:: 5.076148271560669
curr_diff: 0 tensor(2.1009e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1009e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7377e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7377e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554788
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.731750
Train - Epoch 0, Batch: 10, Loss: 0.695614
Train - Epoch 0, Batch: 20, Loss: 0.695224
Train - Epoch 0, Batch: 30, Loss: 0.694100
Train - Epoch 0, Batch: 40, Loss: 0.694406
Train - Epoch 0, Batch: 50, Loss: 0.692944
Train - Epoch 0, Batch: 60, Loss: 0.691178
Train - Epoch 0, Batch: 70, Loss: 0.691105
Train - Epoch 0, Batch: 80, Loss: 0.691490
Train - Epoch 0, Batch: 90, Loss: 0.692362
Train - Epoch 0, Batch: 100, Loss: 0.692330
Train - Epoch 0, Batch: 110, Loss: 0.691481
Train - Epoch 0, Batch: 120, Loss: 0.690400
Train - Epoch 0, Batch: 130, Loss: 0.690514
Train - Epoch 0, Batch: 140, Loss: 0.690297
Train - Epoch 0, Batch: 150, Loss: 0.689791
Train - Epoch 0, Batch: 160, Loss: 0.690267
Train - Epoch 0, Batch: 170, Loss: 0.689679
Train - Epoch 0, Batch: 180, Loss: 0.690418
Train - Epoch 0, Batch: 190, Loss: 0.689059
Train - Epoch 0, Batch: 200, Loss: 0.689592
Train - Epoch 0, Batch: 210, Loss: 0.689061
Train - Epoch 0, Batch: 220, Loss: 0.688988
Train - Epoch 0, Batch: 230, Loss: 0.687781
Train - Epoch 0, Batch: 240, Loss: 0.688353
Train - Epoch 0, Batch: 250, Loss: 0.688925
Train - Epoch 0, Batch: 260, Loss: 0.688484
Train - Epoch 0, Batch: 270, Loss: 0.689153
Train - Epoch 0, Batch: 280, Loss: 0.687861
Train - Epoch 0, Batch: 290, Loss: 0.688359
Train - Epoch 0, Batch: 300, Loss: 0.688716
Train - Epoch 0, Batch: 310, Loss: 0.688583
Train - Epoch 0, Batch: 320, Loss: 0.688662
Train - Epoch 0, Batch: 330, Loss: 0.688887
Train - Epoch 0, Batch: 340, Loss: 0.689187
Train - Epoch 0, Batch: 350, Loss: 0.687889
Train - Epoch 0, Batch: 360, Loss: 0.689100
Train - Epoch 0, Batch: 370, Loss: 0.687613
Train - Epoch 0, Batch: 380, Loss: 0.688457
Train - Epoch 0, Batch: 390, Loss: 0.688121
Train - Epoch 0, Batch: 400, Loss: 0.688588
Train - Epoch 0, Batch: 410, Loss: 0.688604
Train - Epoch 0, Batch: 420, Loss: 0.689279
Train - Epoch 0, Batch: 430, Loss: 0.687247
Train - Epoch 0, Batch: 440, Loss: 0.687903
Train - Epoch 0, Batch: 450, Loss: 0.688664
Train - Epoch 0, Batch: 460, Loss: 0.687188
Train - Epoch 0, Batch: 470, Loss: 0.688285
Train - Epoch 0, Batch: 480, Loss: 0.686801
Train - Epoch 0, Batch: 490, Loss: 0.687565
Train - Epoch 0, Batch: 500, Loss: 0.686949
Train - Epoch 0, Batch: 510, Loss: 0.686564
Train - Epoch 0, Batch: 520, Loss: 0.687264
Train - Epoch 0, Batch: 530, Loss: 0.686859
Train - Epoch 0, Batch: 540, Loss: 0.686860
Train - Epoch 0, Batch: 550, Loss: 0.687371
Train - Epoch 0, Batch: 560, Loss: 0.687240
Train - Epoch 0, Batch: 570, Loss: 0.686597
Train - Epoch 0, Batch: 580, Loss: 0.687082
Train - Epoch 0, Batch: 590, Loss: 0.687695
Train - Epoch 0, Batch: 600, Loss: 0.687100
Train - Epoch 0, Batch: 610, Loss: 0.685436
Train - Epoch 0, Batch: 620, Loss: 0.685903
Train - Epoch 0, Batch: 630, Loss: 0.687740
Train - Epoch 0, Batch: 640, Loss: 0.686809
Train - Epoch 1, Batch: 0, Loss: 0.687916
Train - Epoch 1, Batch: 10, Loss: 0.687013
Train - Epoch 1, Batch: 20, Loss: 0.687978
Train - Epoch 1, Batch: 30, Loss: 0.686510
Train - Epoch 1, Batch: 40, Loss: 0.686409
Train - Epoch 1, Batch: 50, Loss: 0.686539
Train - Epoch 1, Batch: 60, Loss: 0.687133
Train - Epoch 1, Batch: 70, Loss: 0.688237
Train - Epoch 1, Batch: 80, Loss: 0.686748
Train - Epoch 1, Batch: 90, Loss: 0.686035
Train - Epoch 1, Batch: 100, Loss: 0.686928
Train - Epoch 1, Batch: 110, Loss: 0.686641
Train - Epoch 1, Batch: 120, Loss: 0.686482
Train - Epoch 1, Batch: 130, Loss: 0.685686
Train - Epoch 1, Batch: 140, Loss: 0.685983
Train - Epoch 1, Batch: 150, Loss: 0.687299
Train - Epoch 1, Batch: 160, Loss: 0.686100
Train - Epoch 1, Batch: 170, Loss: 0.685804
Train - Epoch 1, Batch: 180, Loss: 0.685813
Train - Epoch 1, Batch: 190, Loss: 0.687475
Train - Epoch 1, Batch: 200, Loss: 0.687158
Train - Epoch 1, Batch: 210, Loss: 0.686108
Train - Epoch 1, Batch: 220, Loss: 0.686625
Train - Epoch 1, Batch: 230, Loss: 0.685361
Train - Epoch 1, Batch: 240, Loss: 0.686001
Train - Epoch 1, Batch: 250, Loss: 0.685567
Train - Epoch 1, Batch: 260, Loss: 0.686559
Train - Epoch 1, Batch: 270, Loss: 0.686138
Train - Epoch 1, Batch: 280, Loss: 0.686213
Train - Epoch 1, Batch: 290, Loss: 0.686887
Train - Epoch 1, Batch: 300, Loss: 0.685229
Train - Epoch 1, Batch: 310, Loss: 0.685471
Train - Epoch 1, Batch: 320, Loss: 0.686051
Train - Epoch 1, Batch: 330, Loss: 0.684384
Train - Epoch 1, Batch: 340, Loss: 0.686732
Train - Epoch 1, Batch: 350, Loss: 0.686136
Train - Epoch 1, Batch: 360, Loss: 0.686794
Train - Epoch 1, Batch: 370, Loss: 0.686582
Train - Epoch 1, Batch: 380, Loss: 0.686275
Train - Epoch 1, Batch: 390, Loss: 0.686486
Train - Epoch 1, Batch: 400, Loss: 0.686009
Train - Epoch 1, Batch: 410, Loss: 0.685635
Train - Epoch 1, Batch: 420, Loss: 0.687124
Train - Epoch 1, Batch: 430, Loss: 0.685045
Train - Epoch 1, Batch: 440, Loss: 0.685421
Train - Epoch 1, Batch: 450, Loss: 0.685792
Train - Epoch 1, Batch: 460, Loss: 0.685763
Train - Epoch 1, Batch: 470, Loss: 0.685619
Train - Epoch 1, Batch: 480, Loss: 0.686341
Train - Epoch 1, Batch: 490, Loss: 0.685527
Train - Epoch 1, Batch: 500, Loss: 0.686835
Train - Epoch 1, Batch: 510, Loss: 0.686446
Train - Epoch 1, Batch: 520, Loss: 0.685886
Train - Epoch 1, Batch: 530, Loss: 0.685460
Train - Epoch 1, Batch: 540, Loss: 0.686394
Train - Epoch 1, Batch: 550, Loss: 0.686409
Train - Epoch 1, Batch: 560, Loss: 0.685393
Train - Epoch 1, Batch: 570, Loss: 0.684657
Train - Epoch 1, Batch: 580, Loss: 0.685698
Train - Epoch 1, Batch: 590, Loss: 0.684420
Train - Epoch 1, Batch: 600, Loss: 0.685345
Train - Epoch 1, Batch: 610, Loss: 0.685313
Train - Epoch 1, Batch: 620, Loss: 0.685736
Train - Epoch 1, Batch: 630, Loss: 0.684869
Train - Epoch 1, Batch: 640, Loss: 0.685183
Train - Epoch 2, Batch: 0, Loss: 0.685755
Train - Epoch 2, Batch: 10, Loss: 0.684402
Train - Epoch 2, Batch: 20, Loss: 0.684173
Train - Epoch 2, Batch: 30, Loss: 0.684838
Train - Epoch 2, Batch: 40, Loss: 0.685374
Train - Epoch 2, Batch: 50, Loss: 0.685867
Train - Epoch 2, Batch: 60, Loss: 0.685291
Train - Epoch 2, Batch: 70, Loss: 0.686395
Train - Epoch 2, Batch: 80, Loss: 0.684679
Train - Epoch 2, Batch: 90, Loss: 0.685072
Train - Epoch 2, Batch: 100, Loss: 0.684354
Train - Epoch 2, Batch: 110, Loss: 0.686077
Train - Epoch 2, Batch: 120, Loss: 0.685186
Train - Epoch 2, Batch: 130, Loss: 0.685019
Train - Epoch 2, Batch: 140, Loss: 0.685785
Train - Epoch 2, Batch: 150, Loss: 0.684855
Train - Epoch 2, Batch: 160, Loss: 0.685658
Train - Epoch 2, Batch: 170, Loss: 0.684067
Train - Epoch 2, Batch: 180, Loss: 0.684621
Train - Epoch 2, Batch: 190, Loss: 0.684832
Train - Epoch 2, Batch: 200, Loss: 0.684419
Train - Epoch 2, Batch: 210, Loss: 0.685192
Train - Epoch 2, Batch: 220, Loss: 0.684317
Train - Epoch 2, Batch: 230, Loss: 0.685835
Train - Epoch 2, Batch: 240, Loss: 0.684656
Train - Epoch 2, Batch: 250, Loss: 0.686219
Train - Epoch 2, Batch: 260, Loss: 0.684480
Train - Epoch 2, Batch: 270, Loss: 0.684287
Train - Epoch 2, Batch: 280, Loss: 0.685588
Train - Epoch 2, Batch: 290, Loss: 0.684450
Train - Epoch 2, Batch: 300, Loss: 0.685278
Train - Epoch 2, Batch: 310, Loss: 0.684218
Train - Epoch 2, Batch: 320, Loss: 0.685601
Train - Epoch 2, Batch: 330, Loss: 0.683392
Train - Epoch 2, Batch: 340, Loss: 0.684889
Train - Epoch 2, Batch: 350, Loss: 0.685364
Train - Epoch 2, Batch: 360, Loss: 0.685069
Train - Epoch 2, Batch: 370, Loss: 0.684479
Train - Epoch 2, Batch: 380, Loss: 0.685436
Train - Epoch 2, Batch: 390, Loss: 0.684940
Train - Epoch 2, Batch: 400, Loss: 0.685984
Train - Epoch 2, Batch: 410, Loss: 0.684820
Train - Epoch 2, Batch: 420, Loss: 0.684837
Train - Epoch 2, Batch: 430, Loss: 0.684412
Train - Epoch 2, Batch: 440, Loss: 0.685189
Train - Epoch 2, Batch: 450, Loss: 0.685578
Train - Epoch 2, Batch: 460, Loss: 0.685317
Train - Epoch 2, Batch: 470, Loss: 0.684238
Train - Epoch 2, Batch: 480, Loss: 0.685469
Train - Epoch 2, Batch: 490, Loss: 0.684990
Train - Epoch 2, Batch: 500, Loss: 0.684824
Train - Epoch 2, Batch: 510, Loss: 0.686169
Train - Epoch 2, Batch: 520, Loss: 0.684614
Train - Epoch 2, Batch: 530, Loss: 0.685670
Train - Epoch 2, Batch: 540, Loss: 0.683922
Train - Epoch 2, Batch: 550, Loss: 0.684092/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685995
Train - Epoch 2, Batch: 570, Loss: 0.684595
Train - Epoch 2, Batch: 580, Loss: 0.684459
Train - Epoch 2, Batch: 590, Loss: 0.684206
Train - Epoch 2, Batch: 600, Loss: 0.683853
Train - Epoch 2, Batch: 610, Loss: 0.685218
Train - Epoch 2, Batch: 620, Loss: 0.684272
Train - Epoch 2, Batch: 630, Loss: 0.685779
Train - Epoch 2, Batch: 640, Loss: 0.684710
Train - Epoch 3, Batch: 0, Loss: 0.683928
Train - Epoch 3, Batch: 10, Loss: 0.683881
Train - Epoch 3, Batch: 20, Loss: 0.684215
Train - Epoch 3, Batch: 30, Loss: 0.684295
Train - Epoch 3, Batch: 40, Loss: 0.684521
Train - Epoch 3, Batch: 50, Loss: 0.683837
Train - Epoch 3, Batch: 60, Loss: 0.683378
Train - Epoch 3, Batch: 70, Loss: 0.683961
Train - Epoch 3, Batch: 80, Loss: 0.683183
Train - Epoch 3, Batch: 90, Loss: 0.685186
Train - Epoch 3, Batch: 100, Loss: 0.685343
Train - Epoch 3, Batch: 110, Loss: 0.684784
Train - Epoch 3, Batch: 120, Loss: 0.684203
Train - Epoch 3, Batch: 130, Loss: 0.684261
Train - Epoch 3, Batch: 140, Loss: 0.683506
Train - Epoch 3, Batch: 150, Loss: 0.683947
Train - Epoch 3, Batch: 160, Loss: 0.684333
Train - Epoch 3, Batch: 170, Loss: 0.685493
Train - Epoch 3, Batch: 180, Loss: 0.684735
Train - Epoch 3, Batch: 190, Loss: 0.683778
Train - Epoch 3, Batch: 200, Loss: 0.684382
Train - Epoch 3, Batch: 210, Loss: 0.685411
Train - Epoch 3, Batch: 220, Loss: 0.683653
Train - Epoch 3, Batch: 230, Loss: 0.684909
Train - Epoch 3, Batch: 240, Loss: 0.684590
Train - Epoch 3, Batch: 250, Loss: 0.684216
Train - Epoch 3, Batch: 260, Loss: 0.683937
Train - Epoch 3, Batch: 270, Loss: 0.683778
Train - Epoch 3, Batch: 280, Loss: 0.685319
Train - Epoch 3, Batch: 290, Loss: 0.684228
Train - Epoch 3, Batch: 300, Loss: 0.684329
Train - Epoch 3, Batch: 310, Loss: 0.684940
Train - Epoch 3, Batch: 320, Loss: 0.682906
Train - Epoch 3, Batch: 330, Loss: 0.683591
Train - Epoch 3, Batch: 340, Loss: 0.684104
Train - Epoch 3, Batch: 350, Loss: 0.685150
Train - Epoch 3, Batch: 360, Loss: 0.683997
Train - Epoch 3, Batch: 370, Loss: 0.683659
Train - Epoch 3, Batch: 380, Loss: 0.684237
Train - Epoch 3, Batch: 390, Loss: 0.684708
Train - Epoch 3, Batch: 400, Loss: 0.684404
Train - Epoch 3, Batch: 410, Loss: 0.684269
Train - Epoch 3, Batch: 420, Loss: 0.684542
Train - Epoch 3, Batch: 430, Loss: 0.683310
Train - Epoch 3, Batch: 440, Loss: 0.683545
Train - Epoch 3, Batch: 450, Loss: 0.684068
Train - Epoch 3, Batch: 460, Loss: 0.683759
Train - Epoch 3, Batch: 470, Loss: 0.683007
Train - Epoch 3, Batch: 480, Loss: 0.683449
Train - Epoch 3, Batch: 490, Loss: 0.684420
Train - Epoch 3, Batch: 500, Loss: 0.683351
Train - Epoch 3, Batch: 510, Loss: 0.684748
Train - Epoch 3, Batch: 520, Loss: 0.684587
Train - Epoch 3, Batch: 530, Loss: 0.683813
Train - Epoch 3, Batch: 540, Loss: 0.685420
Train - Epoch 3, Batch: 550, Loss: 0.684848
Train - Epoch 3, Batch: 560, Loss: 0.683735
Train - Epoch 3, Batch: 570, Loss: 0.684377
Train - Epoch 3, Batch: 580, Loss: 0.684876
Train - Epoch 3, Batch: 590, Loss: 0.684139
Train - Epoch 3, Batch: 600, Loss: 0.683014
Train - Epoch 3, Batch: 610, Loss: 0.684033
Train - Epoch 3, Batch: 620, Loss: 0.683724
Train - Epoch 3, Batch: 630, Loss: 0.684023
Train - Epoch 3, Batch: 640, Loss: 0.684022
training_time:: 8.03556203842163
training time full:: 8.035602569580078
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554386
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 210
training time is 4.832493782043457
overhead:: 0
overhead2:: 0
time_baseline:: 4.8354363441467285
curr_diff: 0 tensor(4.6201e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6201e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.039672136306762695
overhead3:: 0.22340083122253418
overhead4:: 0.8588759899139404
overhead5:: 0
time_provenance:: 2.103912830352783
curr_diff: 0 tensor(9.0229e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0229e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8282e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8282e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.050356149673461914
overhead3:: 0.2501847743988037
overhead4:: 0.9321362972259521
overhead5:: 0
time_provenance:: 2.1866238117218018
curr_diff: 0 tensor(8.9684e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9684e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8364e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8364e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.059420108795166016
overhead3:: 0.3012540340423584
overhead4:: 1.1182472705841064
overhead5:: 0
time_provenance:: 2.6191821098327637
curr_diff: 0 tensor(8.7233e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7233e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8640e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8640e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.060074567794799805
overhead3:: 0.3292214870452881
overhead4:: 1.1651992797851562
overhead5:: 0
time_provenance:: 2.618272304534912
curr_diff: 0 tensor(8.6560e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6560e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06628751754760742
overhead3:: 0.3821749687194824
overhead4:: 1.1710858345031738
overhead5:: 0
time_provenance:: 2.6411426067352295
curr_diff: 0 tensor(5.4898e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4898e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5867e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5867e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06515836715698242
overhead3:: 0.3169877529144287
overhead4:: 1.3203554153442383
overhead5:: 0
time_provenance:: 2.700441837310791
curr_diff: 0 tensor(5.4763e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4763e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5878e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5878e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07979297637939453
overhead3:: 0.39051175117492676
overhead4:: 1.5317468643188477
overhead5:: 0
time_provenance:: 3.323305130004883
curr_diff: 0 tensor(5.3317e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3317e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5973e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5973e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08984708786010742
overhead3:: 0.4782674312591553
overhead4:: 1.6390650272369385
overhead5:: 0
time_provenance:: 3.421191930770874
curr_diff: 0 tensor(5.2185e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2185e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.6048e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6048e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554414
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08166050910949707
overhead3:: 0.40876245498657227
overhead4:: 1.8056983947753906
overhead5:: 0
time_provenance:: 3.462318181991577
curr_diff: 0 tensor(3.2435e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2435e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5806e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5806e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08641886711120605
overhead3:: 0.42559146881103516
overhead4:: 1.7022864818572998
overhead5:: 0
time_provenance:: 3.256354331970215
curr_diff: 0 tensor(2.7727e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7727e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554410
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1034250259399414
overhead3:: 0.5101814270019531
overhead4:: 2.1307618618011475
overhead5:: 0
time_provenance:: 4.0005786418914795
curr_diff: 0 tensor(3.6045e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6045e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.6540e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6540e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554412
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10699772834777832
overhead3:: 0.5688972473144531
overhead4:: 2.1886074542999268
overhead5:: 0
time_provenance:: 4.045864105224609
curr_diff: 0 tensor(3.0365e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0365e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5923e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5923e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10903263092041016
overhead3:: 0.593104362487793
overhead4:: 2.4462740421295166
overhead5:: 0
time_provenance:: 4.36989164352417
curr_diff: 0 tensor(2.0627e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0627e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5641e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5641e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11265707015991211
overhead3:: 0.6001005172729492
overhead4:: 2.4719831943511963
overhead5:: 0
time_provenance:: 4.375939607620239
curr_diff: 0 tensor(2.0483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5650e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5650e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.12737107276916504
overhead3:: 0.6223502159118652
overhead4:: 2.491408348083496
overhead5:: 0
time_provenance:: 4.419393301010132
curr_diff: 0 tensor(2.0219e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0219e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5663e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11833810806274414
overhead3:: 0.5940399169921875
overhead4:: 2.5373730659484863
overhead5:: 0
time_provenance:: 4.353416442871094
curr_diff: 0 tensor(1.9506e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9506e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.5699e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5699e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.22869586944580078
overhead3:: 1.1951398849487305
overhead4:: 3.7923061847686768
overhead5:: 0
time_provenance:: 5.71979022026062
curr_diff: 0 tensor(2.5875e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5875e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.6201e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6201e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554408
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.719822
Train - Epoch 0, Batch: 10, Loss: 0.698527
Train - Epoch 0, Batch: 20, Loss: 0.697703
Train - Epoch 0, Batch: 30, Loss: 0.696260
Train - Epoch 0, Batch: 40, Loss: 0.695661
Train - Epoch 0, Batch: 50, Loss: 0.692872
Train - Epoch 0, Batch: 60, Loss: 0.693960
Train - Epoch 0, Batch: 70, Loss: 0.691910
Train - Epoch 0, Batch: 80, Loss: 0.693686
Train - Epoch 0, Batch: 90, Loss: 0.692379
Train - Epoch 0, Batch: 100, Loss: 0.692638
Train - Epoch 0, Batch: 110, Loss: 0.692738
Train - Epoch 0, Batch: 120, Loss: 0.691029
Train - Epoch 0, Batch: 130, Loss: 0.691106
Train - Epoch 0, Batch: 140, Loss: 0.690758
Train - Epoch 0, Batch: 150, Loss: 0.691802
Train - Epoch 0, Batch: 160, Loss: 0.691033
Train - Epoch 0, Batch: 170, Loss: 0.690227
Train - Epoch 0, Batch: 180, Loss: 0.689653
Train - Epoch 0, Batch: 190, Loss: 0.690173
Train - Epoch 0, Batch: 200, Loss: 0.689581
Train - Epoch 0, Batch: 210, Loss: 0.689256
Train - Epoch 0, Batch: 220, Loss: 0.688734
Train - Epoch 0, Batch: 230, Loss: 0.689586
Train - Epoch 0, Batch: 240, Loss: 0.688927
Train - Epoch 0, Batch: 250, Loss: 0.690333
Train - Epoch 0, Batch: 260, Loss: 0.689431
Train - Epoch 0, Batch: 270, Loss: 0.688473
Train - Epoch 0, Batch: 280, Loss: 0.689835
Train - Epoch 0, Batch: 290, Loss: 0.688278
Train - Epoch 0, Batch: 300, Loss: 0.688544
Train - Epoch 0, Batch: 310, Loss: 0.687371
Train - Epoch 0, Batch: 320, Loss: 0.689121
Train - Epoch 0, Batch: 330, Loss: 0.689143
Train - Epoch 0, Batch: 340, Loss: 0.688401
Train - Epoch 0, Batch: 350, Loss: 0.688226
Train - Epoch 0, Batch: 360, Loss: 0.687319
Train - Epoch 0, Batch: 370, Loss: 0.687539
Train - Epoch 0, Batch: 380, Loss: 0.688494
Train - Epoch 0, Batch: 390, Loss: 0.686934
Train - Epoch 0, Batch: 400, Loss: 0.686940
Train - Epoch 0, Batch: 410, Loss: 0.686639
Train - Epoch 0, Batch: 420, Loss: 0.687984
Train - Epoch 0, Batch: 430, Loss: 0.686598
Train - Epoch 0, Batch: 440, Loss: 0.685993
Train - Epoch 0, Batch: 450, Loss: 0.687024
Train - Epoch 0, Batch: 460, Loss: 0.687297
Train - Epoch 0, Batch: 470, Loss: 0.687783
Train - Epoch 0, Batch: 480, Loss: 0.687078
Train - Epoch 0, Batch: 490, Loss: 0.687268
Train - Epoch 0, Batch: 500, Loss: 0.686646
Train - Epoch 0, Batch: 510, Loss: 0.687800
Train - Epoch 0, Batch: 520, Loss: 0.685822
Train - Epoch 0, Batch: 530, Loss: 0.687642
Train - Epoch 0, Batch: 540, Loss: 0.687142
Train - Epoch 0, Batch: 550, Loss: 0.686670
Train - Epoch 0, Batch: 560, Loss: 0.687168
Train - Epoch 0, Batch: 570, Loss: 0.687099
Train - Epoch 0, Batch: 580, Loss: 0.686392
Train - Epoch 0, Batch: 590, Loss: 0.687062
Train - Epoch 0, Batch: 600, Loss: 0.686227
Train - Epoch 0, Batch: 610, Loss: 0.687210
Train - Epoch 0, Batch: 620, Loss: 0.686353
Train - Epoch 0, Batch: 630, Loss: 0.687731
Train - Epoch 0, Batch: 640, Loss: 0.687558
Train - Epoch 1, Batch: 0, Loss: 0.687113
Train - Epoch 1, Batch: 10, Loss: 0.686476
Train - Epoch 1, Batch: 20, Loss: 0.686367
Train - Epoch 1, Batch: 30, Loss: 0.685761
Train - Epoch 1, Batch: 40, Loss: 0.687320
Train - Epoch 1, Batch: 50, Loss: 0.686931
Train - Epoch 1, Batch: 60, Loss: 0.686654
Train - Epoch 1, Batch: 70, Loss: 0.686305
Train - Epoch 1, Batch: 80, Loss: 0.685722
Train - Epoch 1, Batch: 90, Loss: 0.685565
Train - Epoch 1, Batch: 100, Loss: 0.687030
Train - Epoch 1, Batch: 110, Loss: 0.686419
Train - Epoch 1, Batch: 120, Loss: 0.687561
Train - Epoch 1, Batch: 130, Loss: 0.686015
Train - Epoch 1, Batch: 140, Loss: 0.687408
Train - Epoch 1, Batch: 150, Loss: 0.685418
Train - Epoch 1, Batch: 160, Loss: 0.686389
Train - Epoch 1, Batch: 170, Loss: 0.685674
Train - Epoch 1, Batch: 180, Loss: 0.686553
Train - Epoch 1, Batch: 190, Loss: 0.686202
Train - Epoch 1, Batch: 200, Loss: 0.687593
Train - Epoch 1, Batch: 210, Loss: 0.686810
Train - Epoch 1, Batch: 220, Loss: 0.685309
Train - Epoch 1, Batch: 230, Loss: 0.686093
Train - Epoch 1, Batch: 240, Loss: 0.686070
Train - Epoch 1, Batch: 250, Loss: 0.685482
Train - Epoch 1, Batch: 260, Loss: 0.685749
Train - Epoch 1, Batch: 270, Loss: 0.687129
Train - Epoch 1, Batch: 280, Loss: 0.684551
Train - Epoch 1, Batch: 290, Loss: 0.685799
Train - Epoch 1, Batch: 300, Loss: 0.686720
Train - Epoch 1, Batch: 310, Loss: 0.685730
Train - Epoch 1, Batch: 320, Loss: 0.685732
Train - Epoch 1, Batch: 330, Loss: 0.685581
Train - Epoch 1, Batch: 340, Loss: 0.685372
Train - Epoch 1, Batch: 350, Loss: 0.685945
Train - Epoch 1, Batch: 360, Loss: 0.684651
Train - Epoch 1, Batch: 370, Loss: 0.684877
Train - Epoch 1, Batch: 380, Loss: 0.685807
Train - Epoch 1, Batch: 390, Loss: 0.686285
Train - Epoch 1, Batch: 400, Loss: 0.684858
Train - Epoch 1, Batch: 410, Loss: 0.686294
Train - Epoch 1, Batch: 420, Loss: 0.686928
Train - Epoch 1, Batch: 430, Loss: 0.685628
Train - Epoch 1, Batch: 440, Loss: 0.686048
Train - Epoch 1, Batch: 450, Loss: 0.684583
Train - Epoch 1, Batch: 460, Loss: 0.685366
Train - Epoch 1, Batch: 470, Loss: 0.686802
Train - Epoch 1, Batch: 480, Loss: 0.685040
Train - Epoch 1, Batch: 490, Loss: 0.684931
Train - Epoch 1, Batch: 500, Loss: 0.685707
Train - Epoch 1, Batch: 510, Loss: 0.685598
Train - Epoch 1, Batch: 520, Loss: 0.685524
Train - Epoch 1, Batch: 530, Loss: 0.685506
Train - Epoch 1, Batch: 540, Loss: 0.686122
Train - Epoch 1, Batch: 550, Loss: 0.685740
Train - Epoch 1, Batch: 560, Loss: 0.684867
Train - Epoch 1, Batch: 570, Loss: 0.685153
Train - Epoch 1, Batch: 580, Loss: 0.685798
Train - Epoch 1, Batch: 590, Loss: 0.684858
Train - Epoch 1, Batch: 600, Loss: 0.685823
Train - Epoch 1, Batch: 610, Loss: 0.685656
Train - Epoch 1, Batch: 620, Loss: 0.684780
Train - Epoch 1, Batch: 630, Loss: 0.684628
Train - Epoch 1, Batch: 640, Loss: 0.685075
Train - Epoch 2, Batch: 0, Loss: 0.686125
Train - Epoch 2, Batch: 10, Loss: 0.686416
Train - Epoch 2, Batch: 20, Loss: 0.686309
Train - Epoch 2, Batch: 30, Loss: 0.685113
Train - Epoch 2, Batch: 40, Loss: 0.685399
Train - Epoch 2, Batch: 50, Loss: 0.687075
Train - Epoch 2, Batch: 60, Loss: 0.685310
Train - Epoch 2, Batch: 70, Loss: 0.684832
Train - Epoch 2, Batch: 80, Loss: 0.685170
Train - Epoch 2, Batch: 90, Loss: 0.685055
Train - Epoch 2, Batch: 100, Loss: 0.684472
Train - Epoch 2, Batch: 110, Loss: 0.684564
Train - Epoch 2, Batch: 120, Loss: 0.685711
Train - Epoch 2, Batch: 130, Loss: 0.686397
Train - Epoch 2, Batch: 140, Loss: 0.686534
Train - Epoch 2, Batch: 150, Loss: 0.685345
Train - Epoch 2, Batch: 160, Loss: 0.684775
Train - Epoch 2, Batch: 170, Loss: 0.684882
Train - Epoch 2, Batch: 180, Loss: 0.684744
Train - Epoch 2, Batch: 190, Loss: 0.685671
Train - Epoch 2, Batch: 200, Loss: 0.684745
Train - Epoch 2, Batch: 210, Loss: 0.684874
Train - Epoch 2, Batch: 220, Loss: 0.685496
Train - Epoch 2, Batch: 230, Loss: 0.684043
Train - Epoch 2, Batch: 240, Loss: 0.683231
Train - Epoch 2, Batch: 250, Loss: 0.685885
Train - Epoch 2, Batch: 260, Loss: 0.684983
Train - Epoch 2, Batch: 270, Loss: 0.685061
Train - Epoch 2, Batch: 280, Loss: 0.684518
Train - Epoch 2, Batch: 290, Loss: 0.686169
Train - Epoch 2, Batch: 300, Loss: 0.685625
Train - Epoch 2, Batch: 310, Loss: 0.684725
Train - Epoch 2, Batch: 320, Loss: 0.685124
Train - Epoch 2, Batch: 330, Loss: 0.686047
Train - Epoch 2, Batch: 340, Loss: 0.684825
Train - Epoch 2, Batch: 350, Loss: 0.684365
Train - Epoch 2, Batch: 360, Loss: 0.683812
Train - Epoch 2, Batch: 370, Loss: 0.684432
Train - Epoch 2, Batch: 380, Loss: 0.686561
Train - Epoch 2, Batch: 390, Loss: 0.683622
Train - Epoch 2, Batch: 400, Loss: 0.683726
Train - Epoch 2, Batch: 410, Loss: 0.684882
Train - Epoch 2, Batch: 420, Loss: 0.684598
Train - Epoch 2, Batch: 430, Loss: 0.683896
Train - Epoch 2, Batch: 440, Loss: 0.684422
Train - Epoch 2, Batch: 450, Loss: 0.685920
Train - Epoch 2, Batch: 460, Loss: 0.686114
Train - Epoch 2, Batch: 470, Loss: 0.683982
Train - Epoch 2, Batch: 480, Loss: 0.683292
Train - Epoch 2, Batch: 490, Loss: 0.685844
Train - Epoch 2, Batch: 500, Loss: 0.684867
Train - Epoch 2, Batch: 510, Loss: 0.684663
Train - Epoch 2, Batch: 520, Loss: 0.683923
Train - Epoch 2, Batch: 530, Loss: 0.683989
Train - Epoch 2, Batch: 540, Loss: 0.683496
Train - Epoch 2, Batch: 550, Loss: 0.684893/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684584
Train - Epoch 2, Batch: 570, Loss: 0.684701
Train - Epoch 2, Batch: 580, Loss: 0.683704
Train - Epoch 2, Batch: 590, Loss: 0.684372
Train - Epoch 2, Batch: 600, Loss: 0.682431
Train - Epoch 2, Batch: 610, Loss: 0.684475
Train - Epoch 2, Batch: 620, Loss: 0.684308
Train - Epoch 2, Batch: 630, Loss: 0.685589
Train - Epoch 2, Batch: 640, Loss: 0.685695
Train - Epoch 3, Batch: 0, Loss: 0.684339
Train - Epoch 3, Batch: 10, Loss: 0.683923
Train - Epoch 3, Batch: 20, Loss: 0.685596
Train - Epoch 3, Batch: 30, Loss: 0.685043
Train - Epoch 3, Batch: 40, Loss: 0.684762
Train - Epoch 3, Batch: 50, Loss: 0.684972
Train - Epoch 3, Batch: 60, Loss: 0.683903
Train - Epoch 3, Batch: 70, Loss: 0.684724
Train - Epoch 3, Batch: 80, Loss: 0.684216
Train - Epoch 3, Batch: 90, Loss: 0.683324
Train - Epoch 3, Batch: 100, Loss: 0.683938
Train - Epoch 3, Batch: 110, Loss: 0.684305
Train - Epoch 3, Batch: 120, Loss: 0.685331
Train - Epoch 3, Batch: 130, Loss: 0.684752
Train - Epoch 3, Batch: 140, Loss: 0.682606
Train - Epoch 3, Batch: 150, Loss: 0.684450
Train - Epoch 3, Batch: 160, Loss: 0.684413
Train - Epoch 3, Batch: 170, Loss: 0.685114
Train - Epoch 3, Batch: 180, Loss: 0.684251
Train - Epoch 3, Batch: 190, Loss: 0.683532
Train - Epoch 3, Batch: 200, Loss: 0.684374
Train - Epoch 3, Batch: 210, Loss: 0.683875
Train - Epoch 3, Batch: 220, Loss: 0.684997
Train - Epoch 3, Batch: 230, Loss: 0.684008
Train - Epoch 3, Batch: 240, Loss: 0.684206
Train - Epoch 3, Batch: 250, Loss: 0.683521
Train - Epoch 3, Batch: 260, Loss: 0.684588
Train - Epoch 3, Batch: 270, Loss: 0.683597
Train - Epoch 3, Batch: 280, Loss: 0.685590
Train - Epoch 3, Batch: 290, Loss: 0.683787
Train - Epoch 3, Batch: 300, Loss: 0.683606
Train - Epoch 3, Batch: 310, Loss: 0.683907
Train - Epoch 3, Batch: 320, Loss: 0.683844
Train - Epoch 3, Batch: 330, Loss: 0.684135
Train - Epoch 3, Batch: 340, Loss: 0.685561
Train - Epoch 3, Batch: 350, Loss: 0.682187
Train - Epoch 3, Batch: 360, Loss: 0.685224
Train - Epoch 3, Batch: 370, Loss: 0.684297
Train - Epoch 3, Batch: 380, Loss: 0.684727
Train - Epoch 3, Batch: 390, Loss: 0.684050
Train - Epoch 3, Batch: 400, Loss: 0.683285
Train - Epoch 3, Batch: 410, Loss: 0.682918
Train - Epoch 3, Batch: 420, Loss: 0.683294
Train - Epoch 3, Batch: 430, Loss: 0.684345
Train - Epoch 3, Batch: 440, Loss: 0.683011
Train - Epoch 3, Batch: 450, Loss: 0.683790
Train - Epoch 3, Batch: 460, Loss: 0.684073
Train - Epoch 3, Batch: 470, Loss: 0.684637
Train - Epoch 3, Batch: 480, Loss: 0.683920
Train - Epoch 3, Batch: 490, Loss: 0.683809
Train - Epoch 3, Batch: 500, Loss: 0.683911
Train - Epoch 3, Batch: 510, Loss: 0.684259
Train - Epoch 3, Batch: 520, Loss: 0.684351
Train - Epoch 3, Batch: 530, Loss: 0.684384
Train - Epoch 3, Batch: 540, Loss: 0.683194
Train - Epoch 3, Batch: 550, Loss: 0.684826
Train - Epoch 3, Batch: 560, Loss: 0.684085
Train - Epoch 3, Batch: 570, Loss: 0.684620
Train - Epoch 3, Batch: 580, Loss: 0.684080
Train - Epoch 3, Batch: 590, Loss: 0.683702
Train - Epoch 3, Batch: 600, Loss: 0.683298
Train - Epoch 3, Batch: 610, Loss: 0.683180
Train - Epoch 3, Batch: 620, Loss: 0.684930
Train - Epoch 3, Batch: 630, Loss: 0.682499
Train - Epoch 3, Batch: 640, Loss: 0.683640
training_time:: 7.6646552085876465
training time full:: 7.664694786071777
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.555094
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 210
training time is 5.073190689086914
overhead:: 0
overhead2:: 0
time_baseline:: 5.0762107372283936
curr_diff: 0 tensor(3.9922e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9922e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555092
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.04876875877380371
overhead3:: 0.27757954597473145
overhead4:: 0.9296383857727051
overhead5:: 0
time_provenance:: 2.268683671951294
curr_diff: 0 tensor(7.7056e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7056e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7468e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7468e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.04999995231628418
overhead3:: 0.27930712699890137
overhead4:: 0.9554240703582764
overhead5:: 0
time_provenance:: 2.255746603012085
curr_diff: 0 tensor(7.5932e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5932e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7564e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7564e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.05653119087219238
overhead3:: 0.2985861301422119
overhead4:: 1.0450994968414307
overhead5:: 0
time_provenance:: 2.4073538780212402
curr_diff: 0 tensor(7.5126e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5126e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7612e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7612e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07477235794067383
overhead3:: 0.3928236961364746
overhead4:: 1.2615528106689453
overhead5:: 0
time_provenance:: 2.6703944206237793
curr_diff: 0 tensor(7.3028e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3028e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.7765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7765e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06400942802429199
overhead3:: 0.33729076385498047
overhead4:: 1.222200870513916
overhead5:: 0
time_provenance:: 2.776818037033081
curr_diff: 0 tensor(4.5470e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5470e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9755e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9755e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555084
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.0726006031036377
overhead3:: 0.43112921714782715
overhead4:: 1.4396114349365234
overhead5:: 0
time_provenance:: 3.0005743503570557
curr_diff: 0 tensor(4.4837e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4837e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9811e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9811e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555084
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07214117050170898
overhead3:: 0.36387014389038086
overhead4:: 1.3718242645263672
overhead5:: 0
time_provenance:: 2.752796173095703
curr_diff: 0 tensor(4.4573e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4573e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9824e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9824e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555084
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.09117650985717773
overhead3:: 0.5024139881134033
overhead4:: 1.5650181770324707
overhead5:: 0
time_provenance:: 3.0779097080230713
curr_diff: 0 tensor(4.3550e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3550e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9897e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9897e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555084
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07957649230957031
overhead3:: 0.4101846218109131
overhead4:: 1.7459611892700195
overhead5:: 0
time_provenance:: 3.3721344470977783
curr_diff: 0 tensor(2.3795e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3795e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0046e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0046e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555090
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08738207817077637
overhead3:: 0.4304695129394531
overhead4:: 1.7678792476654053
overhead5:: 0
time_provenance:: 3.3791489601135254
curr_diff: 0 tensor(2.6496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0224e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0224e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11295652389526367
overhead3:: 0.5561270713806152
overhead4:: 2.2053136825561523
overhead5:: 0
time_provenance:: 4.324454307556152
curr_diff: 0 tensor(3.6558e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6558e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0797e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0797e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10005569458007812
overhead3:: 0.54103684425354
overhead4:: 2.011939287185669
overhead5:: 0
time_provenance:: 3.6900551319122314
curr_diff: 0 tensor(2.2410e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2410e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555090
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.13930225372314453
overhead3:: 0.658022403717041
overhead4:: 2.8734383583068848
overhead5:: 0
time_provenance:: 5.328109979629517
curr_diff: 0 tensor(1.2544e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2544e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9869e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9869e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1117854118347168
overhead3:: 0.5795600414276123
overhead4:: 2.4323978424072266
overhead5:: 0
time_provenance:: 4.313491582870483
curr_diff: 0 tensor(1.2482e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2482e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9875e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9875e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11849498748779297
overhead3:: 0.5798685550689697
overhead4:: 2.530459403991699
overhead5:: 0
time_provenance:: 4.383403539657593
curr_diff: 0 tensor(1.2339e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2339e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9886e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9886e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.1267681121826172
overhead3:: 0.6479535102844238
overhead4:: 2.6417737007141113
overhead5:: 0
time_provenance:: 4.683720588684082
curr_diff: 0 tensor(1.2362e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2362e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9882e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9882e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555088
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.20386743545532227
overhead3:: 0.9736945629119873
overhead4:: 3.481325626373291
overhead5:: 0
time_provenance:: 5.157655239105225
curr_diff: 0 tensor(2.1838e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1838e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9922e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9922e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555092
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702112
Train - Epoch 0, Batch: 10, Loss: 0.692441
Train - Epoch 0, Batch: 20, Loss: 0.692691
Train - Epoch 0, Batch: 30, Loss: 0.691632
Train - Epoch 0, Batch: 40, Loss: 0.690724
Train - Epoch 0, Batch: 50, Loss: 0.690827
Train - Epoch 0, Batch: 60, Loss: 0.690067
Train - Epoch 0, Batch: 70, Loss: 0.690555
Train - Epoch 0, Batch: 80, Loss: 0.691415
Train - Epoch 0, Batch: 90, Loss: 0.689599
Train - Epoch 0, Batch: 100, Loss: 0.689668
Train - Epoch 0, Batch: 110, Loss: 0.690081
Train - Epoch 0, Batch: 120, Loss: 0.689271
Train - Epoch 0, Batch: 130, Loss: 0.690115
Train - Epoch 0, Batch: 140, Loss: 0.689270
Train - Epoch 0, Batch: 150, Loss: 0.689560
Train - Epoch 0, Batch: 160, Loss: 0.688456
Train - Epoch 0, Batch: 170, Loss: 0.689222
Train - Epoch 0, Batch: 180, Loss: 0.688280
Train - Epoch 0, Batch: 190, Loss: 0.689676
Train - Epoch 0, Batch: 200, Loss: 0.688261
Train - Epoch 0, Batch: 210, Loss: 0.688286
Train - Epoch 0, Batch: 220, Loss: 0.688627
Train - Epoch 0, Batch: 230, Loss: 0.687972
Train - Epoch 0, Batch: 240, Loss: 0.687864
Train - Epoch 0, Batch: 250, Loss: 0.688142
Train - Epoch 0, Batch: 260, Loss: 0.687922
Train - Epoch 0, Batch: 270, Loss: 0.687390
Train - Epoch 0, Batch: 280, Loss: 0.687730
Train - Epoch 0, Batch: 290, Loss: 0.686652
Train - Epoch 0, Batch: 300, Loss: 0.686597
Train - Epoch 0, Batch: 310, Loss: 0.687803
Train - Epoch 0, Batch: 320, Loss: 0.687815
Train - Epoch 0, Batch: 330, Loss: 0.687464
Train - Epoch 0, Batch: 340, Loss: 0.688003
Train - Epoch 0, Batch: 350, Loss: 0.687048
Train - Epoch 0, Batch: 360, Loss: 0.688065
Train - Epoch 0, Batch: 370, Loss: 0.687316
Train - Epoch 0, Batch: 380, Loss: 0.686609
Train - Epoch 0, Batch: 390, Loss: 0.687520
Train - Epoch 0, Batch: 400, Loss: 0.685942
Train - Epoch 0, Batch: 410, Loss: 0.686923
Train - Epoch 0, Batch: 420, Loss: 0.687009
Train - Epoch 0, Batch: 430, Loss: 0.686617
Train - Epoch 0, Batch: 440, Loss: 0.688937
Train - Epoch 0, Batch: 450, Loss: 0.685996
Train - Epoch 0, Batch: 460, Loss: 0.687183
Train - Epoch 0, Batch: 470, Loss: 0.686808
Train - Epoch 0, Batch: 480, Loss: 0.686859
Train - Epoch 0, Batch: 490, Loss: 0.687339
Train - Epoch 0, Batch: 500, Loss: 0.687775
Train - Epoch 0, Batch: 510, Loss: 0.687102
Train - Epoch 0, Batch: 520, Loss: 0.687672
Train - Epoch 0, Batch: 530, Loss: 0.687461
Train - Epoch 0, Batch: 540, Loss: 0.687031
Train - Epoch 0, Batch: 550, Loss: 0.687105
Train - Epoch 0, Batch: 560, Loss: 0.687373
Train - Epoch 0, Batch: 570, Loss: 0.686489
Train - Epoch 0, Batch: 580, Loss: 0.685480
Train - Epoch 0, Batch: 590, Loss: 0.686899
Train - Epoch 0, Batch: 600, Loss: 0.686370
Train - Epoch 0, Batch: 610, Loss: 0.687177
Train - Epoch 0, Batch: 620, Loss: 0.686839
Train - Epoch 0, Batch: 630, Loss: 0.686723
Train - Epoch 0, Batch: 640, Loss: 0.684980
Train - Epoch 1, Batch: 0, Loss: 0.685884
Train - Epoch 1, Batch: 10, Loss: 0.686609
Train - Epoch 1, Batch: 20, Loss: 0.687543
Train - Epoch 1, Batch: 30, Loss: 0.687095
Train - Epoch 1, Batch: 40, Loss: 0.686340
Train - Epoch 1, Batch: 50, Loss: 0.686421
Train - Epoch 1, Batch: 60, Loss: 0.685728
Train - Epoch 1, Batch: 70, Loss: 0.687170
Train - Epoch 1, Batch: 80, Loss: 0.686373
Train - Epoch 1, Batch: 90, Loss: 0.686365
Train - Epoch 1, Batch: 100, Loss: 0.687246
Train - Epoch 1, Batch: 110, Loss: 0.687105
Train - Epoch 1, Batch: 120, Loss: 0.686092
Train - Epoch 1, Batch: 130, Loss: 0.686333
Train - Epoch 1, Batch: 140, Loss: 0.685839
Train - Epoch 1, Batch: 150, Loss: 0.684929
Train - Epoch 1, Batch: 160, Loss: 0.686977
Train - Epoch 1, Batch: 170, Loss: 0.685903
Train - Epoch 1, Batch: 180, Loss: 0.687182
Train - Epoch 1, Batch: 190, Loss: 0.685867
Train - Epoch 1, Batch: 200, Loss: 0.686066
Train - Epoch 1, Batch: 210, Loss: 0.685656
Train - Epoch 1, Batch: 220, Loss: 0.686241
Train - Epoch 1, Batch: 230, Loss: 0.686150
Train - Epoch 1, Batch: 240, Loss: 0.684875
Train - Epoch 1, Batch: 250, Loss: 0.686624
Train - Epoch 1, Batch: 260, Loss: 0.686358
Train - Epoch 1, Batch: 270, Loss: 0.686521
Train - Epoch 1, Batch: 280, Loss: 0.684850
Train - Epoch 1, Batch: 290, Loss: 0.686376
Train - Epoch 1, Batch: 300, Loss: 0.685574
Train - Epoch 1, Batch: 310, Loss: 0.685098
Train - Epoch 1, Batch: 320, Loss: 0.685741
Train - Epoch 1, Batch: 330, Loss: 0.685512
Train - Epoch 1, Batch: 340, Loss: 0.685748
Train - Epoch 1, Batch: 350, Loss: 0.685412
Train - Epoch 1, Batch: 360, Loss: 0.685993
Train - Epoch 1, Batch: 370, Loss: 0.686397
Train - Epoch 1, Batch: 380, Loss: 0.686401
Train - Epoch 1, Batch: 390, Loss: 0.685406
Train - Epoch 1, Batch: 400, Loss: 0.685858
Train - Epoch 1, Batch: 410, Loss: 0.685889
Train - Epoch 1, Batch: 420, Loss: 0.684524
Train - Epoch 1, Batch: 430, Loss: 0.683932
Train - Epoch 1, Batch: 440, Loss: 0.685605
Train - Epoch 1, Batch: 450, Loss: 0.685462
Train - Epoch 1, Batch: 460, Loss: 0.685911
Train - Epoch 1, Batch: 470, Loss: 0.685285
Train - Epoch 1, Batch: 480, Loss: 0.685309
Train - Epoch 1, Batch: 490, Loss: 0.685201
Train - Epoch 1, Batch: 500, Loss: 0.684303
Train - Epoch 1, Batch: 510, Loss: 0.684927
Train - Epoch 1, Batch: 520, Loss: 0.685636
Train - Epoch 1, Batch: 530, Loss: 0.686426
Train - Epoch 1, Batch: 540, Loss: 0.684663
Train - Epoch 1, Batch: 550, Loss: 0.685684
Train - Epoch 1, Batch: 560, Loss: 0.686116
Train - Epoch 1, Batch: 570, Loss: 0.683754
Train - Epoch 1, Batch: 580, Loss: 0.686924
Train - Epoch 1, Batch: 590, Loss: 0.685843
Train - Epoch 1, Batch: 600, Loss: 0.683989
Train - Epoch 1, Batch: 610, Loss: 0.684764
Train - Epoch 1, Batch: 620, Loss: 0.684178
Train - Epoch 1, Batch: 630, Loss: 0.685028
Train - Epoch 1, Batch: 640, Loss: 0.685385
Train - Epoch 2, Batch: 0, Loss: 0.684839
Train - Epoch 2, Batch: 10, Loss: 0.684372
Train - Epoch 2, Batch: 20, Loss: 0.685729
Train - Epoch 2, Batch: 30, Loss: 0.684998
Train - Epoch 2, Batch: 40, Loss: 0.685427
Train - Epoch 2, Batch: 50, Loss: 0.684792
Train - Epoch 2, Batch: 60, Loss: 0.683571
Train - Epoch 2, Batch: 70, Loss: 0.685426
Train - Epoch 2, Batch: 80, Loss: 0.684624
Train - Epoch 2, Batch: 90, Loss: 0.684552
Train - Epoch 2, Batch: 100, Loss: 0.684540
Train - Epoch 2, Batch: 110, Loss: 0.685135
Train - Epoch 2, Batch: 120, Loss: 0.684158
Train - Epoch 2, Batch: 130, Loss: 0.684777
Train - Epoch 2, Batch: 140, Loss: 0.684340
Train - Epoch 2, Batch: 150, Loss: 0.684208
Train - Epoch 2, Batch: 160, Loss: 0.685956
Train - Epoch 2, Batch: 170, Loss: 0.684455
Train - Epoch 2, Batch: 180, Loss: 0.684034
Train - Epoch 2, Batch: 190, Loss: 0.685350
Train - Epoch 2, Batch: 200, Loss: 0.685735
Train - Epoch 2, Batch: 210, Loss: 0.683602
Train - Epoch 2, Batch: 220, Loss: 0.684995
Train - Epoch 2, Batch: 230, Loss: 0.684402
Train - Epoch 2, Batch: 240, Loss: 0.685391
Train - Epoch 2, Batch: 250, Loss: 0.684299
Train - Epoch 2, Batch: 260, Loss: 0.684761
Train - Epoch 2, Batch: 270, Loss: 0.684801
Train - Epoch 2, Batch: 280, Loss: 0.685340
Train - Epoch 2, Batch: 290, Loss: 0.684149
Train - Epoch 2, Batch: 300, Loss: 0.684700
Train - Epoch 2, Batch: 310, Loss: 0.683858
Train - Epoch 2, Batch: 320, Loss: 0.684530
Train - Epoch 2, Batch: 330, Loss: 0.685400
Train - Epoch 2, Batch: 340, Loss: 0.684507
Train - Epoch 2, Batch: 350, Loss: 0.684524
Train - Epoch 2, Batch: 360, Loss: 0.683954
Train - Epoch 2, Batch: 370, Loss: 0.684716
Train - Epoch 2, Batch: 380, Loss: 0.685611
Train - Epoch 2, Batch: 390, Loss: 0.685434
Train - Epoch 2, Batch: 400, Loss: 0.685357
Train - Epoch 2, Batch: 410, Loss: 0.683375
Train - Epoch 2, Batch: 420, Loss: 0.684742
Train - Epoch 2, Batch: 430, Loss: 0.684178
Train - Epoch 2, Batch: 440, Loss: 0.683454
Train - Epoch 2, Batch: 450, Loss: 0.684024
Train - Epoch 2, Batch: 460, Loss: 0.684235
Train - Epoch 2, Batch: 470, Loss: 0.683512
Train - Epoch 2, Batch: 480, Loss: 0.683965
Train - Epoch 2, Batch: 490, Loss: 0.684992
Train - Epoch 2, Batch: 500, Loss: 0.684777
Train - Epoch 2, Batch: 510, Loss: 0.684915
Train - Epoch 2, Batch: 520, Loss: 0.684821
Train - Epoch 2, Batch: 530, Loss: 0.684708
Train - Epoch 2, Batch: 540, Loss: 0.683971
Train - Epoch 2, Batch: 550, Loss: 0.683993/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684268
Train - Epoch 2, Batch: 570, Loss: 0.684651
Train - Epoch 2, Batch: 580, Loss: 0.683905
Train - Epoch 2, Batch: 590, Loss: 0.684005
Train - Epoch 2, Batch: 600, Loss: 0.685131
Train - Epoch 2, Batch: 610, Loss: 0.683560
Train - Epoch 2, Batch: 620, Loss: 0.685658
Train - Epoch 2, Batch: 630, Loss: 0.684226
Train - Epoch 2, Batch: 640, Loss: 0.683967
Train - Epoch 3, Batch: 0, Loss: 0.684595
Train - Epoch 3, Batch: 10, Loss: 0.683827
Train - Epoch 3, Batch: 20, Loss: 0.684212
Train - Epoch 3, Batch: 30, Loss: 0.683418
Train - Epoch 3, Batch: 40, Loss: 0.684700
Train - Epoch 3, Batch: 50, Loss: 0.684751
Train - Epoch 3, Batch: 60, Loss: 0.684825
Train - Epoch 3, Batch: 70, Loss: 0.685718
Train - Epoch 3, Batch: 80, Loss: 0.684185
Train - Epoch 3, Batch: 90, Loss: 0.683780
Train - Epoch 3, Batch: 100, Loss: 0.684264
Train - Epoch 3, Batch: 110, Loss: 0.684056
Train - Epoch 3, Batch: 120, Loss: 0.684407
Train - Epoch 3, Batch: 130, Loss: 0.685221
Train - Epoch 3, Batch: 140, Loss: 0.684802
Train - Epoch 3, Batch: 150, Loss: 0.684090
Train - Epoch 3, Batch: 160, Loss: 0.684680
Train - Epoch 3, Batch: 170, Loss: 0.684514
Train - Epoch 3, Batch: 180, Loss: 0.684386
Train - Epoch 3, Batch: 190, Loss: 0.683816
Train - Epoch 3, Batch: 200, Loss: 0.683915
Train - Epoch 3, Batch: 210, Loss: 0.684158
Train - Epoch 3, Batch: 220, Loss: 0.683601
Train - Epoch 3, Batch: 230, Loss: 0.684388
Train - Epoch 3, Batch: 240, Loss: 0.685135
Train - Epoch 3, Batch: 250, Loss: 0.684307
Train - Epoch 3, Batch: 260, Loss: 0.682655
Train - Epoch 3, Batch: 270, Loss: 0.684902
Train - Epoch 3, Batch: 280, Loss: 0.683539
Train - Epoch 3, Batch: 290, Loss: 0.682872
Train - Epoch 3, Batch: 300, Loss: 0.684300
Train - Epoch 3, Batch: 310, Loss: 0.684023
Train - Epoch 3, Batch: 320, Loss: 0.684731
Train - Epoch 3, Batch: 330, Loss: 0.683471
Train - Epoch 3, Batch: 340, Loss: 0.684177
Train - Epoch 3, Batch: 350, Loss: 0.684157
Train - Epoch 3, Batch: 360, Loss: 0.683721
Train - Epoch 3, Batch: 370, Loss: 0.684105
Train - Epoch 3, Batch: 380, Loss: 0.684243
Train - Epoch 3, Batch: 390, Loss: 0.683223
Train - Epoch 3, Batch: 400, Loss: 0.684797
Train - Epoch 3, Batch: 410, Loss: 0.684211
Train - Epoch 3, Batch: 420, Loss: 0.683485
Train - Epoch 3, Batch: 430, Loss: 0.683304
Train - Epoch 3, Batch: 440, Loss: 0.683371
Train - Epoch 3, Batch: 450, Loss: 0.682300
Train - Epoch 3, Batch: 460, Loss: 0.684068
Train - Epoch 3, Batch: 470, Loss: 0.685093
Train - Epoch 3, Batch: 480, Loss: 0.684785
Train - Epoch 3, Batch: 490, Loss: 0.684757
Train - Epoch 3, Batch: 500, Loss: 0.682778
Train - Epoch 3, Batch: 510, Loss: 0.684489
Train - Epoch 3, Batch: 520, Loss: 0.683972
Train - Epoch 3, Batch: 530, Loss: 0.684411
Train - Epoch 3, Batch: 540, Loss: 0.685426
Train - Epoch 3, Batch: 550, Loss: 0.683460
Train - Epoch 3, Batch: 560, Loss: 0.683913
Train - Epoch 3, Batch: 570, Loss: 0.683479
Train - Epoch 3, Batch: 580, Loss: 0.683518
Train - Epoch 3, Batch: 590, Loss: 0.683759
Train - Epoch 3, Batch: 600, Loss: 0.683645
Train - Epoch 3, Batch: 610, Loss: 0.684103
Train - Epoch 3, Batch: 620, Loss: 0.683559
Train - Epoch 3, Batch: 630, Loss: 0.682367
Train - Epoch 3, Batch: 640, Loss: 0.685383
training_time:: 7.74574089050293
training time full:: 7.7457780838012695
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.555578
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 210
training time is 4.75386381149292
overhead:: 0
overhead2:: 0
time_baseline:: 4.757897138595581
curr_diff: 0 tensor(4.2063e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2063e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.04807257652282715
overhead3:: 0.23687529563903809
overhead4:: 0.8572878837585449
overhead5:: 0
time_provenance:: 2.134828805923462
curr_diff: 0 tensor(8.9252e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9252e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0760e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0760e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555572
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.050766706466674805
overhead3:: 0.2770102024078369
overhead4:: 0.984774112701416
overhead5:: 0
time_provenance:: 2.2609403133392334
curr_diff: 0 tensor(8.6623e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6623e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1081e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1081e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555572
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.05719947814941406
overhead3:: 0.28591299057006836
overhead4:: 1.0717918872833252
overhead5:: 0
time_provenance:: 2.340716600418091
curr_diff: 0 tensor(8.5598e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5598e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1184e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1184e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555572
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.0680546760559082
overhead3:: 0.30427026748657227
overhead4:: 1.1542205810546875
overhead5:: 0
time_provenance:: 2.517575740814209
curr_diff: 0 tensor(8.1357e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1357e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1704e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1704e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06620192527770996
overhead3:: 0.33258652687072754
overhead4:: 1.305129051208496
overhead5:: 0
time_provenance:: 2.7336223125457764
curr_diff: 0 tensor(6.2304e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2304e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.8762e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8762e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555564
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.06616711616516113
overhead3:: 0.32588839530944824
overhead4:: 1.2672419548034668
overhead5:: 0
time_provenance:: 2.6663196086883545
curr_diff: 0 tensor(6.0235e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0235e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.8919e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8919e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555564
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07188582420349121
overhead3:: 0.3651919364929199
overhead4:: 1.4532077312469482
overhead5:: 0
time_provenance:: 2.8826637268066406
curr_diff: 0 tensor(5.8644e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8644e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9040e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9040e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555564
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.08754968643188477
overhead3:: 0.42514729499816895
overhead4:: 1.5273289680480957
overhead5:: 0
time_provenance:: 3.152005672454834
curr_diff: 0 tensor(5.8042e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8042e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.9084e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9084e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555564
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.07440519332885742
overhead3:: 0.3915691375732422
overhead4:: 1.7460126876831055
overhead5:: 0
time_provenance:: 3.3375649452209473
curr_diff: 0 tensor(3.1800e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1800e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1385e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1385e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10439276695251465
overhead3:: 0.5257143974304199
overhead4:: 2.1169674396514893
overhead5:: 0
time_provenance:: 4.151248216629028
curr_diff: 0 tensor(3.4416e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4416e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.09451007843017578
overhead3:: 0.44694066047668457
overhead4:: 1.9181952476501465
overhead5:: 0
time_provenance:: 3.510885000228882
curr_diff: 0 tensor(2.6750e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6750e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1633e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1633e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555566
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.10744929313659668
overhead3:: 0.5338799953460693
overhead4:: 2.057830333709717
overhead5:: 0
time_provenance:: 3.855384111404419
curr_diff: 0 tensor(3.0385e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0385e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1499e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1499e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.14025211334228516
overhead3:: 0.6911346912384033
overhead4:: 2.82761287689209
overhead5:: 0
time_provenance:: 5.260612726211548
curr_diff: 0 tensor(1.3504e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3504e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1975e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1975e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.12302184104919434
overhead3:: 0.6858253479003906
overhead4:: 2.5970442295074463
overhead5:: 0
time_provenance:: 4.6594226360321045
curr_diff: 0 tensor(1.3259e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3259e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.11727666854858398
overhead3:: 0.5609838962554932
overhead4:: 2.4692933559417725
overhead5:: 0
time_provenance:: 4.29871129989624
curr_diff: 0 tensor(1.2986e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2986e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2002e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2002e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.14904022216796875
overhead3:: 0.6915128231048584
overhead4:: 2.901606559753418
overhead5:: 0
time_provenance:: 5.20305323600769
curr_diff: 0 tensor(1.2614e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2614e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2025e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2025e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 210
max_epoch:: 4
overhead:: 0
overhead2:: 0.21128368377685547
overhead3:: 0.9566364288330078
overhead4:: 3.430981397628784
overhead5:: 0
time_provenance:: 5.097136735916138
curr_diff: 0 tensor(2.5427e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5427e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2063e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2063e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555570
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  higgs 0
tensor([ 3311621,  8554503,  5801994,  8202252,  1757200,  9572375,  4569124,
         4454440,  6531113,  1675311,  1652785,  3862588, 10104893,  3686463,
          403527,  7219272,  2451531,  1364044,  7608395,  9994319,  7180367,
          567380,   567384,  9056347,  7831644,  9099361,  6883430,  1017967,
         2568303,  7106679,  4331646,  2318465,  9449602,   444550,  5494924,
         9441423,  2427024,  9158802,  3979415,  2293915,  8927388,  3416228,
         6975655,  3319976,  3149994,   721066,  1056943,   348338,  6766773,
         8482997,  8554682,  9404605,  4593854, 10389696,  3537091,  8374470,
        10356936, 10453192,  1237220,  1589477,  9740519,  8718569,  4581610,
         6320363,  5316844,  5902571,  2300145,  8159476,  8681717,  5087486,
         1630463,  7815432,   585995,  5038348,  8476941,  3547406,  4671778,
         2887981,  9814321,  6826293,   717132, 10209615,  4589904,  1601879,
         7532888,  6562137,  2695511,  9998687,  2097504,  8972647,   129392,
         3137906,  8677746,  8894836,   545140,  2541941,  9535869,  1296772,
         9642374,  5435786,  7457167,  2482575,  4411793,  6396306,  8515989,
         2849175,  7350686,  6248871,   645545,  4919721,  5181869,  3477935,
          686517,  1651129,  4594110,  4159946,  7776715,  5585356,  6224341,
         5747162,  1679838,  6302174,  3404259,  6572517,  5751273,  9527786,
         9681387,  4415977,  3672560,  2793968,    74242, 10039821,  2069009,
        10359314,   991765,  9644566,  2593305,  9777692,   188959,  5632544,
         2744865,  9275937, 10385960,  4471339,   768555,  8114735,  9280048,
         5077553,  7488050,  2585139,  6965812,  9538109,  8776254,  6806080,
         1337928,  5345864,  1253969,  9792082,  1557075,  2912858,  6971996,
         4627037,   905822,  6122080,  6939244,  6064748,  4227702,  9808505,
         3414656,  8487558,  6867592,  9366152, 10166927,  4135567,  8546963,
         7840409,  7164571,  4586142,  8383135,  3003043,  6441636,  6615716,
         7600805,  7215784,  6709933,  2175669,  6832823,  6875831,   187066,
          805565,  3056329,  8366797,   262862,  7998166,  8002266,  8694498,
         3224291,  3125993,  5143273,  9136877,  9097965,   875249,  5032690,
        10148595,  5507828,  9071346,  7946998,  3664629,  3375865,  8921850,
          232190, 10255102,  1235712,  6042377,  9976592,  6992668,  6685476,
         9878309, 10371879,    95026,  4657972,  1141559,  2442049,  8088386,
         6038340,  5292872,  7252810,  9665359,  7185232,  6892368,  9933655,
         2902881,  8180580,  4780901,  3298151,  5346153,  1573741,  8397683,
         6703988,  6335356,  4520828,  4367228,  2218882,  5157767,  4719495,
         2098058, 10382221,  2208657,  6007703,   705433,  5368732,   881567,
         2210722,  7203752,  7216044,  6194100,  3750838,  7607222,  9610172,
          291778,  5946313,  1428430,  1770446,  6501332,  1401815,  6269914,
         9389020,  8846306, 10357731,  2411496,  6738921,  3023856,  8272883,
         3009528,   187384,  4113407,  2399232,  3730434,  6061058,   855047,
         8840200,  1096712,  5843980, 10419213,  3628058,  1471516,  4023332,
         4635686,  8799270,  9819174,  1526825,  9391147,  8778800,  7420977,
         6478896, 10484790,  1016887,  8182841,  4363323,  9608253,  3165249,
         4287568,  6575189,  9608283,  6048860,  3513437,  1594462,  4568161,
         4920422,  3183722,  8494187,  6825066,  8146031,  2483312,  2120818,
        10005619,  3757182,  4203651,  3359877, 10263688,  4338831,  2524306,
         4471957,  1875094,  5622946,  2831525, 10149031,  9378984,   394414,
         4164789,  3966137,  6114490,  2303165,  2847935,  4033741,  7443661,
         9053391,  6866128,  2436304,  2301139,  2102488,  6493420,  2667756,
         6796524, 10317036,  6417646,  9514226,  5031154,  1336563,  4259084,
         9596178,  9178394,  2352411,  6825246,  9143586,  6417707,  6110508,
         2624813,  8666418,  7654709,  1285435,  4232507,  3288388,   521543,
         8574285,  8637777,  2647385,   750942,  7601508,  1181030,  2674023,
         7513449,  3894639,  4769142,  9792892,  9659774,  7048578,  9860485,
         5637528,  1385881,  9237915,  6413724,  1025438,  8836512,  8605101,
         9389489,  8033719,  2325962,  7220683,   433616,  9115090,  8558038,
         5105112,  7855579,   800224,  9168354,  4957669,  7534057,  9958907,
         2436631,  5639706,  6262299,  9965085,  9156127,  1797663,  3733033,
         9530931,  7431732,  6895157,  3849787,  2338364,  5363265,  9561670,
         3262027,  5058124,  7130705,  4413012,   140887, 10403417,  9479770,
         8316513,  8812130,  1957474,   763496,  4148841,  2956907,  4587123,
         9129587,  7999095,  2913912,  4992633,  4142712,  3845752,  2238076,
         6948475,  3368576,  3405441, 10274446,  2152079,  1201817,  8285851,
          575140,  2115245,   982703,  6475444,  9213621,  2498231,  8670905,
         9131710,  8177343,   155328,  1992386,  3413703,  4863689,  3110602,
         5387977,   351946,  9801433,  5105370,  2107114,   749292,  1265388,
         3694321,  1584882,  6475507,   876286,  7368448,  2658052,  3452679,
         3254028,  4024080,  2070297,  1804059,   880411,  3565346,  4480805,
         3753768,  2412334,  5099310,  4665138,   231226,  8873794,  8257346,
        10116932,   950086,  2764616,  8675151,  3753813,  1144662,  8630104,
         5037916,  2072415,  4519780,  3456879,  9031539,  8591228,  2252669,
         2246529,  5674889,  5441420,  5664657,  1718165,  2629528, 10162076,
         3039132,  4937645,  4872115, 10096564,  6027191,  1087420,  1710021,
         2428873,  6174668,  5955539,  4745172,  4171733,  3381206,  9140182,
         5550045,  1152996,  2885604,  7833577,   249835,  9924598,  7800828])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.698084
Train - Epoch 0, Batch: 10, Loss: 0.696696
Train - Epoch 0, Batch: 20, Loss: 0.694346
Train - Epoch 0, Batch: 30, Loss: 0.692128
Train - Epoch 0, Batch: 40, Loss: 0.692161
Train - Epoch 0, Batch: 50, Loss: 0.691594
Train - Epoch 0, Batch: 60, Loss: 0.691316
Train - Epoch 0, Batch: 70, Loss: 0.691170
Train - Epoch 0, Batch: 80, Loss: 0.691201
Train - Epoch 0, Batch: 90, Loss: 0.689703
Train - Epoch 0, Batch: 100, Loss: 0.690118
Train - Epoch 0, Batch: 110, Loss: 0.689740
Train - Epoch 0, Batch: 120, Loss: 0.690258
Train - Epoch 0, Batch: 130, Loss: 0.689888
Train - Epoch 0, Batch: 140, Loss: 0.688884
Train - Epoch 0, Batch: 150, Loss: 0.690601
Train - Epoch 0, Batch: 160, Loss: 0.688743
Train - Epoch 0, Batch: 170, Loss: 0.688972
Train - Epoch 0, Batch: 180, Loss: 0.688653
Train - Epoch 0, Batch: 190, Loss: 0.689241
Train - Epoch 0, Batch: 200, Loss: 0.688854
Train - Epoch 0, Batch: 210, Loss: 0.688866
Train - Epoch 0, Batch: 220, Loss: 0.689158
Train - Epoch 0, Batch: 230, Loss: 0.687601
Train - Epoch 0, Batch: 240, Loss: 0.689072
Train - Epoch 0, Batch: 250, Loss: 0.687736
Train - Epoch 0, Batch: 260, Loss: 0.688222
Train - Epoch 0, Batch: 270, Loss: 0.687669
Train - Epoch 0, Batch: 280, Loss: 0.689172
Train - Epoch 0, Batch: 290, Loss: 0.688798
Train - Epoch 0, Batch: 300, Loss: 0.687614
Train - Epoch 0, Batch: 310, Loss: 0.687725
Train - Epoch 0, Batch: 320, Loss: 0.688464
Train - Epoch 0, Batch: 330, Loss: 0.687865
Train - Epoch 0, Batch: 340, Loss: 0.688141
Train - Epoch 0, Batch: 350, Loss: 0.687081
Train - Epoch 0, Batch: 360, Loss: 0.687450
Train - Epoch 0, Batch: 370, Loss: 0.687624
Train - Epoch 0, Batch: 380, Loss: 0.687567
Train - Epoch 0, Batch: 390, Loss: 0.687467
Train - Epoch 0, Batch: 400, Loss: 0.687998
Train - Epoch 0, Batch: 410, Loss: 0.688517
Train - Epoch 0, Batch: 420, Loss: 0.687417
Train - Epoch 0, Batch: 430, Loss: 0.687052
Train - Epoch 0, Batch: 440, Loss: 0.686194
Train - Epoch 0, Batch: 450, Loss: 0.688183
Train - Epoch 0, Batch: 460, Loss: 0.687278
Train - Epoch 0, Batch: 470, Loss: 0.688070
Train - Epoch 0, Batch: 480, Loss: 0.687151
Train - Epoch 0, Batch: 490, Loss: 0.688458
Train - Epoch 0, Batch: 500, Loss: 0.686076
Train - Epoch 0, Batch: 510, Loss: 0.687224
Train - Epoch 0, Batch: 520, Loss: 0.687647
Train - Epoch 0, Batch: 530, Loss: 0.685899
Train - Epoch 0, Batch: 540, Loss: 0.687369
Train - Epoch 0, Batch: 550, Loss: 0.686521
Train - Epoch 0, Batch: 560, Loss: 0.687094
Train - Epoch 0, Batch: 570, Loss: 0.687136
Train - Epoch 0, Batch: 580, Loss: 0.687522
Train - Epoch 0, Batch: 590, Loss: 0.687434
Train - Epoch 0, Batch: 600, Loss: 0.686841
Train - Epoch 0, Batch: 610, Loss: 0.686930
Train - Epoch 0, Batch: 620, Loss: 0.687691
Train - Epoch 0, Batch: 630, Loss: 0.685628
Train - Epoch 0, Batch: 640, Loss: 0.687033
Train - Epoch 1, Batch: 0, Loss: 0.685648
Train - Epoch 1, Batch: 10, Loss: 0.686125
Train - Epoch 1, Batch: 20, Loss: 0.687371
Train - Epoch 1, Batch: 30, Loss: 0.687249
Train - Epoch 1, Batch: 40, Loss: 0.686064
Train - Epoch 1, Batch: 50, Loss: 0.686527
Train - Epoch 1, Batch: 60, Loss: 0.685556
Train - Epoch 1, Batch: 70, Loss: 0.686303
Train - Epoch 1, Batch: 80, Loss: 0.685783
Train - Epoch 1, Batch: 90, Loss: 0.685638
Train - Epoch 1, Batch: 100, Loss: 0.686519
Train - Epoch 1, Batch: 110, Loss: 0.686125
Train - Epoch 1, Batch: 120, Loss: 0.687238
Train - Epoch 1, Batch: 130, Loss: 0.685585
Train - Epoch 1, Batch: 140, Loss: 0.687249
Train - Epoch 1, Batch: 150, Loss: 0.685577
Train - Epoch 1, Batch: 160, Loss: 0.685177
Train - Epoch 1, Batch: 170, Loss: 0.686479
Train - Epoch 1, Batch: 180, Loss: 0.686966
Train - Epoch 1, Batch: 190, Loss: 0.686258
Train - Epoch 1, Batch: 200, Loss: 0.686139
Train - Epoch 1, Batch: 210, Loss: 0.687051
Train - Epoch 1, Batch: 220, Loss: 0.685114
Train - Epoch 1, Batch: 230, Loss: 0.685660
Train - Epoch 1, Batch: 240, Loss: 0.686383
Train - Epoch 1, Batch: 250, Loss: 0.685387
Train - Epoch 1, Batch: 260, Loss: 0.685776
Train - Epoch 1, Batch: 270, Loss: 0.687052
Train - Epoch 1, Batch: 280, Loss: 0.685295
Train - Epoch 1, Batch: 290, Loss: 0.686862
Train - Epoch 1, Batch: 300, Loss: 0.685337
Train - Epoch 1, Batch: 310, Loss: 0.685191
Train - Epoch 1, Batch: 320, Loss: 0.686174
Train - Epoch 1, Batch: 330, Loss: 0.685966
Train - Epoch 1, Batch: 340, Loss: 0.685007
Train - Epoch 1, Batch: 350, Loss: 0.684852
Train - Epoch 1, Batch: 360, Loss: 0.684143
Train - Epoch 1, Batch: 370, Loss: 0.684350
Train - Epoch 1, Batch: 380, Loss: 0.684876
Train - Epoch 1, Batch: 390, Loss: 0.684957
Train - Epoch 1, Batch: 400, Loss: 0.686345
Train - Epoch 1, Batch: 410, Loss: 0.686425
Train - Epoch 1, Batch: 420, Loss: 0.686711
Train - Epoch 1, Batch: 430, Loss: 0.686274
Train - Epoch 1, Batch: 440, Loss: 0.686698
Train - Epoch 1, Batch: 450, Loss: 0.686370
Train - Epoch 1, Batch: 460, Loss: 0.686321
Train - Epoch 1, Batch: 470, Loss: 0.685590
Train - Epoch 1, Batch: 480, Loss: 0.685612
Train - Epoch 1, Batch: 490, Loss: 0.685596
Train - Epoch 1, Batch: 500, Loss: 0.686257
Train - Epoch 1, Batch: 510, Loss: 0.685568
Train - Epoch 1, Batch: 520, Loss: 0.686068
Train - Epoch 1, Batch: 530, Loss: 0.684071
Train - Epoch 1, Batch: 540, Loss: 0.685713
Train - Epoch 1, Batch: 550, Loss: 0.684437
Train - Epoch 1, Batch: 560, Loss: 0.684616
Train - Epoch 1, Batch: 570, Loss: 0.686098
Train - Epoch 1, Batch: 580, Loss: 0.685503
Train - Epoch 1, Batch: 590, Loss: 0.685842
Train - Epoch 1, Batch: 600, Loss: 0.685877
Train - Epoch 1, Batch: 610, Loss: 0.684853
Train - Epoch 1, Batch: 620, Loss: 0.684932
Train - Epoch 1, Batch: 630, Loss: 0.685365
Train - Epoch 1, Batch: 640, Loss: 0.685152
Train - Epoch 2, Batch: 0, Loss: 0.685435
Train - Epoch 2, Batch: 10, Loss: 0.683751
Train - Epoch 2, Batch: 20, Loss: 0.684689
Train - Epoch 2, Batch: 30, Loss: 0.685630
Train - Epoch 2, Batch: 40, Loss: 0.685673
Train - Epoch 2, Batch: 50, Loss: 0.684738
Train - Epoch 2, Batch: 60, Loss: 0.685265
Train - Epoch 2, Batch: 70, Loss: 0.684618
Train - Epoch 2, Batch: 80, Loss: 0.686108
Train - Epoch 2, Batch: 90, Loss: 0.685211
Train - Epoch 2, Batch: 100, Loss: 0.684812
Train - Epoch 2, Batch: 110, Loss: 0.685450
Train - Epoch 2, Batch: 120, Loss: 0.685535
Train - Epoch 2, Batch: 130, Loss: 0.684924
Train - Epoch 2, Batch: 140, Loss: 0.684761
Train - Epoch 2, Batch: 150, Loss: 0.684866
Train - Epoch 2, Batch: 160, Loss: 0.685729
Train - Epoch 2, Batch: 170, Loss: 0.686915
Train - Epoch 2, Batch: 180, Loss: 0.686363
Train - Epoch 2, Batch: 190, Loss: 0.685718
Train - Epoch 2, Batch: 200, Loss: 0.684864
Train - Epoch 2, Batch: 210, Loss: 0.685270
Train - Epoch 2, Batch: 220, Loss: 0.684957
Train - Epoch 2, Batch: 230, Loss: 0.685556
Train - Epoch 2, Batch: 240, Loss: 0.685263
Train - Epoch 2, Batch: 250, Loss: 0.685910
Train - Epoch 2, Batch: 260, Loss: 0.684551
Train - Epoch 2, Batch: 270, Loss: 0.684465
Train - Epoch 2, Batch: 280, Loss: 0.684850
Train - Epoch 2, Batch: 290, Loss: 0.684541
Train - Epoch 2, Batch: 300, Loss: 0.685531
Train - Epoch 2, Batch: 310, Loss: 0.684884
Train - Epoch 2, Batch: 320, Loss: 0.685132
Train - Epoch 2, Batch: 330, Loss: 0.685177
Train - Epoch 2, Batch: 340, Loss: 0.685240
Train - Epoch 2, Batch: 350, Loss: 0.685794
Train - Epoch 2, Batch: 360, Loss: 0.684776
Train - Epoch 2, Batch: 370, Loss: 0.684962
Train - Epoch 2, Batch: 380, Loss: 0.686025
Train - Epoch 2, Batch: 390, Loss: 0.684399
Train - Epoch 2, Batch: 400, Loss: 0.684110
Train - Epoch 2, Batch: 410, Loss: 0.684317
Train - Epoch 2, Batch: 420, Loss: 0.686128
Train - Epoch 2, Batch: 430, Loss: 0.685347
Train - Epoch 2, Batch: 440, Loss: 0.685417
Train - Epoch 2, Batch: 450, Loss: 0.685820
Train - Epoch 2, Batch: 460, Loss: 0.684766
Train - Epoch 2, Batch: 470, Loss: 0.684706
Train - Epoch 2, Batch: 480, Loss: 0.684999
Train - Epoch 2, Batch: 490, Loss: 0.684524
Train - Epoch 2, Batch: 500, Loss: 0.685222
Train - Epoch 2, Batch: 510, Loss: 0.684999
Train - Epoch 2, Batch: 520, Loss: 0.684720
Train - Epoch 2, Batch: 530, Loss: 0.685952
Train - Epoch 2, Batch: 540, Loss: 0.684604
Train - Epoch 2, Batch: 550, Loss: 0.684447/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684381
Train - Epoch 2, Batch: 570, Loss: 0.684439
Train - Epoch 2, Batch: 580, Loss: 0.684996
Train - Epoch 2, Batch: 590, Loss: 0.683851
Train - Epoch 2, Batch: 600, Loss: 0.684653
Train - Epoch 2, Batch: 610, Loss: 0.684037
Train - Epoch 2, Batch: 620, Loss: 0.685186
Train - Epoch 2, Batch: 630, Loss: 0.684073
Train - Epoch 2, Batch: 640, Loss: 0.683573
Train - Epoch 3, Batch: 0, Loss: 0.684295
Train - Epoch 3, Batch: 10, Loss: 0.683101
Train - Epoch 3, Batch: 20, Loss: 0.685445
Train - Epoch 3, Batch: 30, Loss: 0.685228
Train - Epoch 3, Batch: 40, Loss: 0.684914
Train - Epoch 3, Batch: 50, Loss: 0.684269
Train - Epoch 3, Batch: 60, Loss: 0.686008
Train - Epoch 3, Batch: 70, Loss: 0.685847
Train - Epoch 3, Batch: 80, Loss: 0.684844
Train - Epoch 3, Batch: 90, Loss: 0.684295
Train - Epoch 3, Batch: 100, Loss: 0.685100
Train - Epoch 3, Batch: 110, Loss: 0.685133
Train - Epoch 3, Batch: 120, Loss: 0.684231
Train - Epoch 3, Batch: 130, Loss: 0.684253
Train - Epoch 3, Batch: 140, Loss: 0.682547
Train - Epoch 3, Batch: 150, Loss: 0.683363
Train - Epoch 3, Batch: 160, Loss: 0.685478
Train - Epoch 3, Batch: 170, Loss: 0.682712
Train - Epoch 3, Batch: 180, Loss: 0.684083
Train - Epoch 3, Batch: 190, Loss: 0.684453
Train - Epoch 3, Batch: 200, Loss: 0.685941
Train - Epoch 3, Batch: 210, Loss: 0.684539
Train - Epoch 3, Batch: 220, Loss: 0.682888
Train - Epoch 3, Batch: 230, Loss: 0.684086
Train - Epoch 3, Batch: 240, Loss: 0.684138
Train - Epoch 3, Batch: 250, Loss: 0.684217
Train - Epoch 3, Batch: 260, Loss: 0.682628
Train - Epoch 3, Batch: 270, Loss: 0.684970
Train - Epoch 3, Batch: 280, Loss: 0.683188
Train - Epoch 3, Batch: 290, Loss: 0.684907
Train - Epoch 3, Batch: 300, Loss: 0.684003
Train - Epoch 3, Batch: 310, Loss: 0.683881
Train - Epoch 3, Batch: 320, Loss: 0.683385
Train - Epoch 3, Batch: 330, Loss: 0.684161
Train - Epoch 3, Batch: 340, Loss: 0.682430
Train - Epoch 3, Batch: 350, Loss: 0.683896
Train - Epoch 3, Batch: 360, Loss: 0.684500
Train - Epoch 3, Batch: 370, Loss: 0.683670
Train - Epoch 3, Batch: 380, Loss: 0.685029
Train - Epoch 3, Batch: 390, Loss: 0.684564
Train - Epoch 3, Batch: 400, Loss: 0.683784
Train - Epoch 3, Batch: 410, Loss: 0.682660
Train - Epoch 3, Batch: 420, Loss: 0.684398
Train - Epoch 3, Batch: 430, Loss: 0.684544
Train - Epoch 3, Batch: 440, Loss: 0.684866
Train - Epoch 3, Batch: 450, Loss: 0.685197
Train - Epoch 3, Batch: 460, Loss: 0.683039
Train - Epoch 3, Batch: 470, Loss: 0.683857
Train - Epoch 3, Batch: 480, Loss: 0.684610
Train - Epoch 3, Batch: 490, Loss: 0.682624
Train - Epoch 3, Batch: 500, Loss: 0.684840
Train - Epoch 3, Batch: 510, Loss: 0.685979
Train - Epoch 3, Batch: 520, Loss: 0.683223
Train - Epoch 3, Batch: 530, Loss: 0.683744
Train - Epoch 3, Batch: 540, Loss: 0.683895
Train - Epoch 3, Batch: 550, Loss: 0.684103
Train - Epoch 3, Batch: 560, Loss: 0.683677
Train - Epoch 3, Batch: 570, Loss: 0.684507
Train - Epoch 3, Batch: 580, Loss: 0.682609
Train - Epoch 3, Batch: 590, Loss: 0.684664
Train - Epoch 3, Batch: 600, Loss: 0.683663
Train - Epoch 3, Batch: 610, Loss: 0.683235
Train - Epoch 3, Batch: 620, Loss: 0.684255
Train - Epoch 3, Batch: 630, Loss: 0.683005
Train - Epoch 3, Batch: 640, Loss: 0.683307
training_time:: 7.506108522415161
training time full:: 7.506147146224976
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553292
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 525
training time is 4.932790994644165
overhead:: 0
overhead2:: 0
time_baseline:: 4.936479806900024
curr_diff: 0 tensor(5.9916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.07542729377746582
overhead3:: 0.19448208808898926
overhead4:: 0.8075628280639648
overhead5:: 0
time_provenance:: 2.3437960147857666
curr_diff: 0 tensor(2.5127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5127e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1703e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1703e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553294
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.11297750473022461
overhead3:: 0.3029654026031494
overhead4:: 1.016629934310913
overhead5:: 0
time_provenance:: 2.7715580463409424
curr_diff: 0 tensor(2.4676e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4676e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2097e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2097e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553294
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13241863250732422
overhead3:: 0.3731997013092041
overhead4:: 1.212756633758545
overhead5:: 0
time_provenance:: 3.2032392024993896
curr_diff: 0 tensor(2.4681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2091e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2091e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553294
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13822340965270996
overhead3:: 0.3490157127380371
overhead4:: 1.3083043098449707
overhead5:: 0
time_provenance:: 2.9568469524383545
curr_diff: 0 tensor(2.3247e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3247e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.3333e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3333e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553294
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.12042641639709473
overhead3:: 0.30597829818725586
overhead4:: 1.1686770915985107
overhead5:: 0
time_provenance:: 2.893320322036743
curr_diff: 0 tensor(1.0071e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0071e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5535e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5535e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13324904441833496
overhead3:: 0.3162832260131836
overhead4:: 1.2874808311462402
overhead5:: 0
time_provenance:: 3.0224862098693848
curr_diff: 0 tensor(9.8115e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8115e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5702e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5702e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16243481636047363
overhead3:: 0.3993344306945801
overhead4:: 1.6562974452972412
overhead5:: 0
time_provenance:: 3.8796911239624023
curr_diff: 0 tensor(9.8225e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8225e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5694e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5694e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.15840649604797363
overhead3:: 0.3927619457244873
overhead4:: 1.3663134574890137
overhead5:: 0
time_provenance:: 3.0633606910705566
curr_diff: 0 tensor(9.5718e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5718e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5860e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5860e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.17037296295166016
overhead3:: 0.42256855964660645
overhead4:: 1.7999870777130127
overhead5:: 0
time_provenance:: 3.729616641998291
curr_diff: 0 tensor(8.5892e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5892e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5310e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5310e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16374611854553223
overhead3:: 0.4125802516937256
overhead4:: 1.74528169631958
overhead5:: 0
time_provenance:: 3.5653951168060303
curr_diff: 0 tensor(5.2578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9957e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9957e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553294
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.22075343132019043
overhead3:: 0.5239980220794678
overhead4:: 2.2183661460876465
overhead5:: 0
time_provenance:: 4.674667596817017
curr_diff: 0 tensor(4.3322e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3322e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.7994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7994e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2230370044708252
overhead3:: 0.5540866851806641
overhead4:: 2.119723081588745
overhead5:: 0
time_provenance:: 4.302373886108398
curr_diff: 0 tensor(8.3315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2136087417602539
overhead3:: 0.53717041015625
overhead4:: 2.468648672103882
overhead5:: 0
time_provenance:: 4.576155662536621
curr_diff: 0 tensor(3.2944e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2944e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0025e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0025e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553292
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.22684979438781738
overhead3:: 0.5735208988189697
overhead4:: 2.4951560497283936
overhead5:: 0
time_provenance:: 4.600309371948242
curr_diff: 0 tensor(3.2606e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2606e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0047e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0047e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553292
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.25420546531677246
overhead3:: 0.6376416683197021
overhead4:: 2.550492763519287
overhead5:: 0
time_provenance:: 4.874204158782959
curr_diff: 0 tensor(3.2497e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2497e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0053e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0053e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553292
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2564830780029297
overhead3:: 0.638441801071167
overhead4:: 2.617879867553711
overhead5:: 0
time_provenance:: 4.768510580062866
curr_diff: 0 tensor(3.1988e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1988e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0093e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0093e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553292
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.42734742164611816
overhead3:: 1.0756916999816895
overhead4:: 3.6193082332611084
overhead5:: 0
time_provenance:: 5.674964189529419
curr_diff: 0 tensor(2.0168e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0168e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553290
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.694228
Train - Epoch 0, Batch: 10, Loss: 0.694019
Train - Epoch 0, Batch: 20, Loss: 0.695531
Train - Epoch 0, Batch: 30, Loss: 0.693641
Train - Epoch 0, Batch: 40, Loss: 0.692438
Train - Epoch 0, Batch: 50, Loss: 0.691164
Train - Epoch 0, Batch: 60, Loss: 0.691658
Train - Epoch 0, Batch: 70, Loss: 0.690186
Train - Epoch 0, Batch: 80, Loss: 0.689922
Train - Epoch 0, Batch: 90, Loss: 0.689420
Train - Epoch 0, Batch: 100, Loss: 0.689794
Train - Epoch 0, Batch: 110, Loss: 0.689409
Train - Epoch 0, Batch: 120, Loss: 0.690339
Train - Epoch 0, Batch: 130, Loss: 0.691113
Train - Epoch 0, Batch: 140, Loss: 0.688603
Train - Epoch 0, Batch: 150, Loss: 0.689502
Train - Epoch 0, Batch: 160, Loss: 0.689702
Train - Epoch 0, Batch: 170, Loss: 0.689033
Train - Epoch 0, Batch: 180, Loss: 0.688628
Train - Epoch 0, Batch: 190, Loss: 0.689049
Train - Epoch 0, Batch: 200, Loss: 0.688175
Train - Epoch 0, Batch: 210, Loss: 0.690426
Train - Epoch 0, Batch: 220, Loss: 0.687399
Train - Epoch 0, Batch: 230, Loss: 0.688209
Train - Epoch 0, Batch: 240, Loss: 0.688039
Train - Epoch 0, Batch: 250, Loss: 0.688236
Train - Epoch 0, Batch: 260, Loss: 0.688228
Train - Epoch 0, Batch: 270, Loss: 0.688802
Train - Epoch 0, Batch: 280, Loss: 0.687864
Train - Epoch 0, Batch: 290, Loss: 0.687485
Train - Epoch 0, Batch: 300, Loss: 0.688033
Train - Epoch 0, Batch: 310, Loss: 0.687325
Train - Epoch 0, Batch: 320, Loss: 0.688471
Train - Epoch 0, Batch: 330, Loss: 0.687928
Train - Epoch 0, Batch: 340, Loss: 0.688217
Train - Epoch 0, Batch: 350, Loss: 0.688251
Train - Epoch 0, Batch: 360, Loss: 0.687980
Train - Epoch 0, Batch: 370, Loss: 0.687437
Train - Epoch 0, Batch: 380, Loss: 0.687409
Train - Epoch 0, Batch: 390, Loss: 0.688132
Train - Epoch 0, Batch: 400, Loss: 0.688343
Train - Epoch 0, Batch: 410, Loss: 0.687806
Train - Epoch 0, Batch: 420, Loss: 0.686980
Train - Epoch 0, Batch: 430, Loss: 0.687965
Train - Epoch 0, Batch: 440, Loss: 0.687283
Train - Epoch 0, Batch: 450, Loss: 0.687423
Train - Epoch 0, Batch: 460, Loss: 0.686748
Train - Epoch 0, Batch: 470, Loss: 0.687226
Train - Epoch 0, Batch: 480, Loss: 0.687523
Train - Epoch 0, Batch: 490, Loss: 0.686925
Train - Epoch 0, Batch: 500, Loss: 0.687231
Train - Epoch 0, Batch: 510, Loss: 0.686232
Train - Epoch 0, Batch: 520, Loss: 0.687896
Train - Epoch 0, Batch: 530, Loss: 0.686237
Train - Epoch 0, Batch: 540, Loss: 0.686131
Train - Epoch 0, Batch: 550, Loss: 0.687392
Train - Epoch 0, Batch: 560, Loss: 0.686513
Train - Epoch 0, Batch: 570, Loss: 0.687535
Train - Epoch 0, Batch: 580, Loss: 0.687566
Train - Epoch 0, Batch: 590, Loss: 0.687173
Train - Epoch 0, Batch: 600, Loss: 0.686084
Train - Epoch 0, Batch: 610, Loss: 0.687609
Train - Epoch 0, Batch: 620, Loss: 0.687537
Train - Epoch 0, Batch: 630, Loss: 0.686581
Train - Epoch 0, Batch: 640, Loss: 0.688252
Train - Epoch 1, Batch: 0, Loss: 0.686523
Train - Epoch 1, Batch: 10, Loss: 0.686071
Train - Epoch 1, Batch: 20, Loss: 0.686048
Train - Epoch 1, Batch: 30, Loss: 0.686464
Train - Epoch 1, Batch: 40, Loss: 0.686086
Train - Epoch 1, Batch: 50, Loss: 0.686015
Train - Epoch 1, Batch: 60, Loss: 0.686611
Train - Epoch 1, Batch: 70, Loss: 0.686083
Train - Epoch 1, Batch: 80, Loss: 0.685587
Train - Epoch 1, Batch: 90, Loss: 0.685674
Train - Epoch 1, Batch: 100, Loss: 0.685526
Train - Epoch 1, Batch: 110, Loss: 0.687259
Train - Epoch 1, Batch: 120, Loss: 0.685277
Train - Epoch 1, Batch: 130, Loss: 0.686212
Train - Epoch 1, Batch: 140, Loss: 0.685353
Train - Epoch 1, Batch: 150, Loss: 0.686287
Train - Epoch 1, Batch: 160, Loss: 0.685784
Train - Epoch 1, Batch: 170, Loss: 0.686356
Train - Epoch 1, Batch: 180, Loss: 0.685375
Train - Epoch 1, Batch: 190, Loss: 0.686167
Train - Epoch 1, Batch: 200, Loss: 0.686247
Train - Epoch 1, Batch: 210, Loss: 0.685711
Train - Epoch 1, Batch: 220, Loss: 0.686775
Train - Epoch 1, Batch: 230, Loss: 0.685406
Train - Epoch 1, Batch: 240, Loss: 0.686832
Train - Epoch 1, Batch: 250, Loss: 0.686515
Train - Epoch 1, Batch: 260, Loss: 0.685505
Train - Epoch 1, Batch: 270, Loss: 0.686559
Train - Epoch 1, Batch: 280, Loss: 0.686067
Train - Epoch 1, Batch: 290, Loss: 0.685148
Train - Epoch 1, Batch: 300, Loss: 0.685688
Train - Epoch 1, Batch: 310, Loss: 0.685164
Train - Epoch 1, Batch: 320, Loss: 0.686245
Train - Epoch 1, Batch: 330, Loss: 0.686438
Train - Epoch 1, Batch: 340, Loss: 0.685841
Train - Epoch 1, Batch: 350, Loss: 0.685464
Train - Epoch 1, Batch: 360, Loss: 0.685949
Train - Epoch 1, Batch: 370, Loss: 0.686013
Train - Epoch 1, Batch: 380, Loss: 0.686246
Train - Epoch 1, Batch: 390, Loss: 0.685950
Train - Epoch 1, Batch: 400, Loss: 0.685552
Train - Epoch 1, Batch: 410, Loss: 0.685816
Train - Epoch 1, Batch: 420, Loss: 0.686860
Train - Epoch 1, Batch: 430, Loss: 0.685035
Train - Epoch 1, Batch: 440, Loss: 0.685983
Train - Epoch 1, Batch: 450, Loss: 0.686159
Train - Epoch 1, Batch: 460, Loss: 0.685461
Train - Epoch 1, Batch: 470, Loss: 0.685401
Train - Epoch 1, Batch: 480, Loss: 0.685609
Train - Epoch 1, Batch: 490, Loss: 0.684471
Train - Epoch 1, Batch: 500, Loss: 0.683711
Train - Epoch 1, Batch: 510, Loss: 0.684309
Train - Epoch 1, Batch: 520, Loss: 0.685376
Train - Epoch 1, Batch: 530, Loss: 0.685109
Train - Epoch 1, Batch: 540, Loss: 0.685766
Train - Epoch 1, Batch: 550, Loss: 0.686044
Train - Epoch 1, Batch: 560, Loss: 0.684766
Train - Epoch 1, Batch: 570, Loss: 0.684845
Train - Epoch 1, Batch: 580, Loss: 0.685337
Train - Epoch 1, Batch: 590, Loss: 0.686225
Train - Epoch 1, Batch: 600, Loss: 0.684030
Train - Epoch 1, Batch: 610, Loss: 0.684829
Train - Epoch 1, Batch: 620, Loss: 0.685785
Train - Epoch 1, Batch: 630, Loss: 0.685397
Train - Epoch 1, Batch: 640, Loss: 0.685054
Train - Epoch 2, Batch: 0, Loss: 0.686281
Train - Epoch 2, Batch: 10, Loss: 0.685498
Train - Epoch 2, Batch: 20, Loss: 0.683891
Train - Epoch 2, Batch: 30, Loss: 0.684860
Train - Epoch 2, Batch: 40, Loss: 0.685331
Train - Epoch 2, Batch: 50, Loss: 0.684853
Train - Epoch 2, Batch: 60, Loss: 0.685448
Train - Epoch 2, Batch: 70, Loss: 0.684441
Train - Epoch 2, Batch: 80, Loss: 0.684043
Train - Epoch 2, Batch: 90, Loss: 0.685019
Train - Epoch 2, Batch: 100, Loss: 0.685356
Train - Epoch 2, Batch: 110, Loss: 0.684769
Train - Epoch 2, Batch: 120, Loss: 0.686068
Train - Epoch 2, Batch: 130, Loss: 0.683941
Train - Epoch 2, Batch: 140, Loss: 0.684385
Train - Epoch 2, Batch: 150, Loss: 0.684147
Train - Epoch 2, Batch: 160, Loss: 0.686457
Train - Epoch 2, Batch: 170, Loss: 0.684768
Train - Epoch 2, Batch: 180, Loss: 0.684631
Train - Epoch 2, Batch: 190, Loss: 0.684073
Train - Epoch 2, Batch: 200, Loss: 0.683662
Train - Epoch 2, Batch: 210, Loss: 0.685783
Train - Epoch 2, Batch: 220, Loss: 0.684521
Train - Epoch 2, Batch: 230, Loss: 0.685232
Train - Epoch 2, Batch: 240, Loss: 0.684361
Train - Epoch 2, Batch: 250, Loss: 0.685818
Train - Epoch 2, Batch: 260, Loss: 0.685620
Train - Epoch 2, Batch: 270, Loss: 0.683817
Train - Epoch 2, Batch: 280, Loss: 0.685100
Train - Epoch 2, Batch: 290, Loss: 0.685270
Train - Epoch 2, Batch: 300, Loss: 0.684453
Train - Epoch 2, Batch: 310, Loss: 0.684972
Train - Epoch 2, Batch: 320, Loss: 0.684453
Train - Epoch 2, Batch: 330, Loss: 0.684656
Train - Epoch 2, Batch: 340, Loss: 0.684454
Train - Epoch 2, Batch: 350, Loss: 0.685793
Train - Epoch 2, Batch: 360, Loss: 0.685694
Train - Epoch 2, Batch: 370, Loss: 0.685225
Train - Epoch 2, Batch: 380, Loss: 0.684080
Train - Epoch 2, Batch: 390, Loss: 0.684345
Train - Epoch 2, Batch: 400, Loss: 0.684825
Train - Epoch 2, Batch: 410, Loss: 0.684406
Train - Epoch 2, Batch: 420, Loss: 0.685317
Train - Epoch 2, Batch: 430, Loss: 0.685133
Train - Epoch 2, Batch: 440, Loss: 0.684120
Train - Epoch 2, Batch: 450, Loss: 0.684913
Train - Epoch 2, Batch: 460, Loss: 0.685284
Train - Epoch 2, Batch: 470, Loss: 0.684766
Train - Epoch 2, Batch: 480, Loss: 0.684188
Train - Epoch 2, Batch: 490, Loss: 0.684258
Train - Epoch 2, Batch: 500, Loss: 0.683732
Train - Epoch 2, Batch: 510, Loss: 0.684744
Train - Epoch 2, Batch: 520, Loss: 0.685777
Train - Epoch 2, Batch: 530, Loss: 0.684244
Train - Epoch 2, Batch: 540, Loss: 0.684990
Train - Epoch 2, Batch: 550, Loss: 0.685218/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684931
Train - Epoch 2, Batch: 570, Loss: 0.684933
Train - Epoch 2, Batch: 580, Loss: 0.684277
Train - Epoch 2, Batch: 590, Loss: 0.684364
Train - Epoch 2, Batch: 600, Loss: 0.684736
Train - Epoch 2, Batch: 610, Loss: 0.683253
Train - Epoch 2, Batch: 620, Loss: 0.684417
Train - Epoch 2, Batch: 630, Loss: 0.684771
Train - Epoch 2, Batch: 640, Loss: 0.685220
Train - Epoch 3, Batch: 0, Loss: 0.682964
Train - Epoch 3, Batch: 10, Loss: 0.683939
Train - Epoch 3, Batch: 20, Loss: 0.684379
Train - Epoch 3, Batch: 30, Loss: 0.683801
Train - Epoch 3, Batch: 40, Loss: 0.686364
Train - Epoch 3, Batch: 50, Loss: 0.682935
Train - Epoch 3, Batch: 60, Loss: 0.685187
Train - Epoch 3, Batch: 70, Loss: 0.684520
Train - Epoch 3, Batch: 80, Loss: 0.684586
Train - Epoch 3, Batch: 90, Loss: 0.684159
Train - Epoch 3, Batch: 100, Loss: 0.684037
Train - Epoch 3, Batch: 110, Loss: 0.684619
Train - Epoch 3, Batch: 120, Loss: 0.684553
Train - Epoch 3, Batch: 130, Loss: 0.684338
Train - Epoch 3, Batch: 140, Loss: 0.684734
Train - Epoch 3, Batch: 150, Loss: 0.684692
Train - Epoch 3, Batch: 160, Loss: 0.684496
Train - Epoch 3, Batch: 170, Loss: 0.683524
Train - Epoch 3, Batch: 180, Loss: 0.684563
Train - Epoch 3, Batch: 190, Loss: 0.683993
Train - Epoch 3, Batch: 200, Loss: 0.684580
Train - Epoch 3, Batch: 210, Loss: 0.683400
Train - Epoch 3, Batch: 220, Loss: 0.684285
Train - Epoch 3, Batch: 230, Loss: 0.684282
Train - Epoch 3, Batch: 240, Loss: 0.684502
Train - Epoch 3, Batch: 250, Loss: 0.684187
Train - Epoch 3, Batch: 260, Loss: 0.685715
Train - Epoch 3, Batch: 270, Loss: 0.683436
Train - Epoch 3, Batch: 280, Loss: 0.684602
Train - Epoch 3, Batch: 290, Loss: 0.684956
Train - Epoch 3, Batch: 300, Loss: 0.683110
Train - Epoch 3, Batch: 310, Loss: 0.684244
Train - Epoch 3, Batch: 320, Loss: 0.684134
Train - Epoch 3, Batch: 330, Loss: 0.684071
Train - Epoch 3, Batch: 340, Loss: 0.684744
Train - Epoch 3, Batch: 350, Loss: 0.684490
Train - Epoch 3, Batch: 360, Loss: 0.685361
Train - Epoch 3, Batch: 370, Loss: 0.684343
Train - Epoch 3, Batch: 380, Loss: 0.684240
Train - Epoch 3, Batch: 390, Loss: 0.684374
Train - Epoch 3, Batch: 400, Loss: 0.683281
Train - Epoch 3, Batch: 410, Loss: 0.684379
Train - Epoch 3, Batch: 420, Loss: 0.684444
Train - Epoch 3, Batch: 430, Loss: 0.684312
Train - Epoch 3, Batch: 440, Loss: 0.683379
Train - Epoch 3, Batch: 450, Loss: 0.683358
Train - Epoch 3, Batch: 460, Loss: 0.683718
Train - Epoch 3, Batch: 470, Loss: 0.684306
Train - Epoch 3, Batch: 480, Loss: 0.683939
Train - Epoch 3, Batch: 490, Loss: 0.683702
Train - Epoch 3, Batch: 500, Loss: 0.683824
Train - Epoch 3, Batch: 510, Loss: 0.684340
Train - Epoch 3, Batch: 520, Loss: 0.683845
Train - Epoch 3, Batch: 530, Loss: 0.684723
Train - Epoch 3, Batch: 540, Loss: 0.685014
Train - Epoch 3, Batch: 550, Loss: 0.684969
Train - Epoch 3, Batch: 560, Loss: 0.684711
Train - Epoch 3, Batch: 570, Loss: 0.683613
Train - Epoch 3, Batch: 580, Loss: 0.684863
Train - Epoch 3, Batch: 590, Loss: 0.684801
Train - Epoch 3, Batch: 600, Loss: 0.683217
Train - Epoch 3, Batch: 610, Loss: 0.685031
Train - Epoch 3, Batch: 620, Loss: 0.684157
Train - Epoch 3, Batch: 630, Loss: 0.682687
Train - Epoch 3, Batch: 640, Loss: 0.685311
training_time:: 7.75744891166687
training time full:: 7.7574896812438965
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553590
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 525
training time is 6.1428062915802
overhead:: 0
overhead2:: 0
time_baseline:: 6.146036386489868
curr_diff: 0 tensor(5.0180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0180e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1025543212890625
overhead3:: 0.2843594551086426
overhead4:: 0.9185619354248047
overhead5:: 0
time_provenance:: 2.601276397705078
curr_diff: 0 tensor(2.2026e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2026e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.3825e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3825e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553592
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.11036849021911621
overhead3:: 0.27678823471069336
overhead4:: 1.0840981006622314
overhead5:: 0
time_provenance:: 3.1537587642669678
curr_diff: 0 tensor(2.1690e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1690e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.4084e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4084e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553596
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.12180519104003906
overhead3:: 0.29936885833740234
overhead4:: 1.1687712669372559
overhead5:: 0
time_provenance:: 3.1731295585632324
curr_diff: 0 tensor(2.1450e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1450e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.4269e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4269e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553596
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13580632209777832
overhead3:: 0.35503506660461426
overhead4:: 1.2864489555358887
overhead5:: 0
time_provenance:: 2.946392774581909
curr_diff: 0 tensor(2.0928e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0928e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(3.4676e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4676e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553596
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13554787635803223
overhead3:: 0.35056114196777344
overhead4:: 1.2965030670166016
overhead5:: 0
time_provenance:: 3.177856922149658
curr_diff: 0 tensor(1.0203e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0203e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.3793e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3793e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.13591456413269043
overhead3:: 0.36118173599243164
overhead4:: 1.387354850769043
overhead5:: 0
time_provenance:: 3.130042552947998
curr_diff: 0 tensor(1.0024e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0024e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.3925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.14736509323120117
overhead3:: 0.3600921630859375
overhead4:: 1.330979824066162
overhead5:: 0
time_provenance:: 3.0397496223449707
curr_diff: 0 tensor(9.9633e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9633e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.3971e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3971e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.15740084648132324
overhead3:: 0.41493844985961914
overhead4:: 1.5678398609161377
overhead5:: 0
time_provenance:: 3.284618616104126
curr_diff: 0 tensor(9.7784e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7784e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.4108e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4108e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.160261869430542
overhead3:: 0.39392876625061035
overhead4:: 1.5191693305969238
overhead5:: 0
time_provenance:: 3.3151042461395264
curr_diff: 0 tensor(5.1415e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1415e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8129e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8129e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553586
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16803336143493652
overhead3:: 0.43134045600891113
overhead4:: 1.8139877319335938
overhead5:: 0
time_provenance:: 3.6714577674865723
curr_diff: 0 tensor(4.6450e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6450e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8114e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8114e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.22682619094848633
overhead3:: 0.5166661739349365
overhead4:: 2.1088480949401855
overhead5:: 0
time_provenance:: 4.520520210266113
curr_diff: 0 tensor(5.5602e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5602e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.7776e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7776e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.19668889045715332
overhead3:: 0.5013535022735596
overhead4:: 1.927018642425537
overhead5:: 0
time_provenance:: 3.814075469970703
curr_diff: 0 tensor(4.9656e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9656e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8243e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8243e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553586
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2197566032409668
overhead3:: 0.5582082271575928
overhead4:: 2.3122639656066895
overhead5:: 0
time_provenance:: 4.440010070800781
curr_diff: 0 tensor(3.0791e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0791e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8633e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8633e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.23379993438720703
overhead3:: 0.5735301971435547
overhead4:: 2.580777168273926
overhead5:: 0
time_provenance:: 4.835972309112549
curr_diff: 0 tensor(3.0285e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0285e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.287158727645874
overhead3:: 0.7096724510192871
overhead4:: 2.9406771659851074
overhead5:: 0
time_provenance:: 5.694465160369873
curr_diff: 0 tensor(2.9999e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9999e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8678e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8678e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.25187087059020996
overhead3:: 0.6428649425506592
overhead4:: 2.6676483154296875
overhead5:: 0
time_provenance:: 4.903520584106445
curr_diff: 0 tensor(2.9738e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9738e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8696e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8696e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.435107946395874
overhead3:: 1.1465239524841309
overhead4:: 3.750253438949585
overhead5:: 0
time_provenance:: 5.889783620834351
curr_diff: 0 tensor(2.1742e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1742e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.0180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0180e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553598
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.704702
Train - Epoch 0, Batch: 10, Loss: 0.695955
Train - Epoch 0, Batch: 20, Loss: 0.694680
Train - Epoch 0, Batch: 30, Loss: 0.692853
Train - Epoch 0, Batch: 40, Loss: 0.692759
Train - Epoch 0, Batch: 50, Loss: 0.691849
Train - Epoch 0, Batch: 60, Loss: 0.692250
Train - Epoch 0, Batch: 70, Loss: 0.690570
Train - Epoch 0, Batch: 80, Loss: 0.690830
Train - Epoch 0, Batch: 90, Loss: 0.690685
Train - Epoch 0, Batch: 100, Loss: 0.690147
Train - Epoch 0, Batch: 110, Loss: 0.689672
Train - Epoch 0, Batch: 120, Loss: 0.689517
Train - Epoch 0, Batch: 130, Loss: 0.689427
Train - Epoch 0, Batch: 140, Loss: 0.688401
Train - Epoch 0, Batch: 150, Loss: 0.689293
Train - Epoch 0, Batch: 160, Loss: 0.689211
Train - Epoch 0, Batch: 170, Loss: 0.687821
Train - Epoch 0, Batch: 180, Loss: 0.686999
Train - Epoch 0, Batch: 190, Loss: 0.688667
Train - Epoch 0, Batch: 200, Loss: 0.689499
Train - Epoch 0, Batch: 210, Loss: 0.688143
Train - Epoch 0, Batch: 220, Loss: 0.689505
Train - Epoch 0, Batch: 230, Loss: 0.688583
Train - Epoch 0, Batch: 240, Loss: 0.688023
Train - Epoch 0, Batch: 250, Loss: 0.688828
Train - Epoch 0, Batch: 260, Loss: 0.687892
Train - Epoch 0, Batch: 270, Loss: 0.687068
Train - Epoch 0, Batch: 280, Loss: 0.688299
Train - Epoch 0, Batch: 290, Loss: 0.688525
Train - Epoch 0, Batch: 300, Loss: 0.687572
Train - Epoch 0, Batch: 310, Loss: 0.688191
Train - Epoch 0, Batch: 320, Loss: 0.687602
Train - Epoch 0, Batch: 330, Loss: 0.687398
Train - Epoch 0, Batch: 340, Loss: 0.687587
Train - Epoch 0, Batch: 350, Loss: 0.687842
Train - Epoch 0, Batch: 360, Loss: 0.687389
Train - Epoch 0, Batch: 370, Loss: 0.688207
Train - Epoch 0, Batch: 380, Loss: 0.687244
Train - Epoch 0, Batch: 390, Loss: 0.687395
Train - Epoch 0, Batch: 400, Loss: 0.686728
Train - Epoch 0, Batch: 410, Loss: 0.689025
Train - Epoch 0, Batch: 420, Loss: 0.686826
Train - Epoch 0, Batch: 430, Loss: 0.687014
Train - Epoch 0, Batch: 440, Loss: 0.688518
Train - Epoch 0, Batch: 450, Loss: 0.687808
Train - Epoch 0, Batch: 460, Loss: 0.688236
Train - Epoch 0, Batch: 470, Loss: 0.687143
Train - Epoch 0, Batch: 480, Loss: 0.686528
Train - Epoch 0, Batch: 490, Loss: 0.687379
Train - Epoch 0, Batch: 500, Loss: 0.686836
Train - Epoch 0, Batch: 510, Loss: 0.686964
Train - Epoch 0, Batch: 520, Loss: 0.686436
Train - Epoch 0, Batch: 530, Loss: 0.686198
Train - Epoch 0, Batch: 540, Loss: 0.687093
Train - Epoch 0, Batch: 550, Loss: 0.687193
Train - Epoch 0, Batch: 560, Loss: 0.688227
Train - Epoch 0, Batch: 570, Loss: 0.686760
Train - Epoch 0, Batch: 580, Loss: 0.687155
Train - Epoch 0, Batch: 590, Loss: 0.687249
Train - Epoch 0, Batch: 600, Loss: 0.686525
Train - Epoch 0, Batch: 610, Loss: 0.686834
Train - Epoch 0, Batch: 620, Loss: 0.686762
Train - Epoch 0, Batch: 630, Loss: 0.686452
Train - Epoch 0, Batch: 640, Loss: 0.686867
Train - Epoch 1, Batch: 0, Loss: 0.687811
Train - Epoch 1, Batch: 10, Loss: 0.686018
Train - Epoch 1, Batch: 20, Loss: 0.685757
Train - Epoch 1, Batch: 30, Loss: 0.686716
Train - Epoch 1, Batch: 40, Loss: 0.686375
Train - Epoch 1, Batch: 50, Loss: 0.686696
Train - Epoch 1, Batch: 60, Loss: 0.685976
Train - Epoch 1, Batch: 70, Loss: 0.686064
Train - Epoch 1, Batch: 80, Loss: 0.686516
Train - Epoch 1, Batch: 90, Loss: 0.685936
Train - Epoch 1, Batch: 100, Loss: 0.685851
Train - Epoch 1, Batch: 110, Loss: 0.686478
Train - Epoch 1, Batch: 120, Loss: 0.686357
Train - Epoch 1, Batch: 130, Loss: 0.686304
Train - Epoch 1, Batch: 140, Loss: 0.686641
Train - Epoch 1, Batch: 150, Loss: 0.684882
Train - Epoch 1, Batch: 160, Loss: 0.685548
Train - Epoch 1, Batch: 170, Loss: 0.685411
Train - Epoch 1, Batch: 180, Loss: 0.685932
Train - Epoch 1, Batch: 190, Loss: 0.686701
Train - Epoch 1, Batch: 200, Loss: 0.685940
Train - Epoch 1, Batch: 210, Loss: 0.686410
Train - Epoch 1, Batch: 220, Loss: 0.686627
Train - Epoch 1, Batch: 230, Loss: 0.685952
Train - Epoch 1, Batch: 240, Loss: 0.686277
Train - Epoch 1, Batch: 250, Loss: 0.686792
Train - Epoch 1, Batch: 260, Loss: 0.686122
Train - Epoch 1, Batch: 270, Loss: 0.684717
Train - Epoch 1, Batch: 280, Loss: 0.685532
Train - Epoch 1, Batch: 290, Loss: 0.686325
Train - Epoch 1, Batch: 300, Loss: 0.686475
Train - Epoch 1, Batch: 310, Loss: 0.684738
Train - Epoch 1, Batch: 320, Loss: 0.685749
Train - Epoch 1, Batch: 330, Loss: 0.684499
Train - Epoch 1, Batch: 340, Loss: 0.684462
Train - Epoch 1, Batch: 350, Loss: 0.685554
Train - Epoch 1, Batch: 360, Loss: 0.685754
Train - Epoch 1, Batch: 370, Loss: 0.685334
Train - Epoch 1, Batch: 380, Loss: 0.685266
Train - Epoch 1, Batch: 390, Loss: 0.684748
Train - Epoch 1, Batch: 400, Loss: 0.686988
Train - Epoch 1, Batch: 410, Loss: 0.685900
Train - Epoch 1, Batch: 420, Loss: 0.685843
Train - Epoch 1, Batch: 430, Loss: 0.685974
Train - Epoch 1, Batch: 440, Loss: 0.686007
Train - Epoch 1, Batch: 450, Loss: 0.686411
Train - Epoch 1, Batch: 460, Loss: 0.684772
Train - Epoch 1, Batch: 470, Loss: 0.686520
Train - Epoch 1, Batch: 480, Loss: 0.686802
Train - Epoch 1, Batch: 490, Loss: 0.684525
Train - Epoch 1, Batch: 500, Loss: 0.684762
Train - Epoch 1, Batch: 510, Loss: 0.685653
Train - Epoch 1, Batch: 520, Loss: 0.684015
Train - Epoch 1, Batch: 530, Loss: 0.686228
Train - Epoch 1, Batch: 540, Loss: 0.684740
Train - Epoch 1, Batch: 550, Loss: 0.683981
Train - Epoch 1, Batch: 560, Loss: 0.685855
Train - Epoch 1, Batch: 570, Loss: 0.684807
Train - Epoch 1, Batch: 580, Loss: 0.686146
Train - Epoch 1, Batch: 590, Loss: 0.685356
Train - Epoch 1, Batch: 600, Loss: 0.685877
Train - Epoch 1, Batch: 610, Loss: 0.685037
Train - Epoch 1, Batch: 620, Loss: 0.684811
Train - Epoch 1, Batch: 630, Loss: 0.685686
Train - Epoch 1, Batch: 640, Loss: 0.684887
Train - Epoch 2, Batch: 0, Loss: 0.685356
Train - Epoch 2, Batch: 10, Loss: 0.686474
Train - Epoch 2, Batch: 20, Loss: 0.685758
Train - Epoch 2, Batch: 30, Loss: 0.686045
Train - Epoch 2, Batch: 40, Loss: 0.684981
Train - Epoch 2, Batch: 50, Loss: 0.685042
Train - Epoch 2, Batch: 60, Loss: 0.684741
Train - Epoch 2, Batch: 70, Loss: 0.683594
Train - Epoch 2, Batch: 80, Loss: 0.685595
Train - Epoch 2, Batch: 90, Loss: 0.685353
Train - Epoch 2, Batch: 100, Loss: 0.686607
Train - Epoch 2, Batch: 110, Loss: 0.684706
Train - Epoch 2, Batch: 120, Loss: 0.685211
Train - Epoch 2, Batch: 130, Loss: 0.684660
Train - Epoch 2, Batch: 140, Loss: 0.685460
Train - Epoch 2, Batch: 150, Loss: 0.684346
Train - Epoch 2, Batch: 160, Loss: 0.684499
Train - Epoch 2, Batch: 170, Loss: 0.685198
Train - Epoch 2, Batch: 180, Loss: 0.684329
Train - Epoch 2, Batch: 190, Loss: 0.684326
Train - Epoch 2, Batch: 200, Loss: 0.683805
Train - Epoch 2, Batch: 210, Loss: 0.685079
Train - Epoch 2, Batch: 220, Loss: 0.684129
Train - Epoch 2, Batch: 230, Loss: 0.684599
Train - Epoch 2, Batch: 240, Loss: 0.685147
Train - Epoch 2, Batch: 250, Loss: 0.685492
Train - Epoch 2, Batch: 260, Loss: 0.684420
Train - Epoch 2, Batch: 270, Loss: 0.685310
Train - Epoch 2, Batch: 280, Loss: 0.683039
Train - Epoch 2, Batch: 290, Loss: 0.685861
Train - Epoch 2, Batch: 300, Loss: 0.685300
Train - Epoch 2, Batch: 310, Loss: 0.683947
Train - Epoch 2, Batch: 320, Loss: 0.685131
Train - Epoch 2, Batch: 330, Loss: 0.684914
Train - Epoch 2, Batch: 340, Loss: 0.684200
Train - Epoch 2, Batch: 350, Loss: 0.685197
Train - Epoch 2, Batch: 360, Loss: 0.682437
Train - Epoch 2, Batch: 370, Loss: 0.685272
Train - Epoch 2, Batch: 380, Loss: 0.684300
Train - Epoch 2, Batch: 390, Loss: 0.684883
Train - Epoch 2, Batch: 400, Loss: 0.684667
Train - Epoch 2, Batch: 410, Loss: 0.685287
Train - Epoch 2, Batch: 420, Loss: 0.685061
Train - Epoch 2, Batch: 430, Loss: 0.683970
Train - Epoch 2, Batch: 440, Loss: 0.684846
Train - Epoch 2, Batch: 450, Loss: 0.684336
Train - Epoch 2, Batch: 460, Loss: 0.685652
Train - Epoch 2, Batch: 470, Loss: 0.684350
Train - Epoch 2, Batch: 480, Loss: 0.684117
Train - Epoch 2, Batch: 490, Loss: 0.684486
Train - Epoch 2, Batch: 500, Loss: 0.685508
Train - Epoch 2, Batch: 510, Loss: 0.684161
Train - Epoch 2, Batch: 520, Loss: 0.684770
Train - Epoch 2, Batch: 530, Loss: 0.684926
Train - Epoch 2, Batch: 540, Loss: 0.684927
Train - Epoch 2, Batch: 550, Loss: 0.685154/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684639
Train - Epoch 2, Batch: 570, Loss: 0.684316
Train - Epoch 2, Batch: 580, Loss: 0.684595
Train - Epoch 2, Batch: 590, Loss: 0.683646
Train - Epoch 2, Batch: 600, Loss: 0.683777
Train - Epoch 2, Batch: 610, Loss: 0.684879
Train - Epoch 2, Batch: 620, Loss: 0.684995
Train - Epoch 2, Batch: 630, Loss: 0.682972
Train - Epoch 2, Batch: 640, Loss: 0.684769
Train - Epoch 3, Batch: 0, Loss: 0.683836
Train - Epoch 3, Batch: 10, Loss: 0.684318
Train - Epoch 3, Batch: 20, Loss: 0.685941
Train - Epoch 3, Batch: 30, Loss: 0.684520
Train - Epoch 3, Batch: 40, Loss: 0.684584
Train - Epoch 3, Batch: 50, Loss: 0.684055
Train - Epoch 3, Batch: 60, Loss: 0.684170
Train - Epoch 3, Batch: 70, Loss: 0.683992
Train - Epoch 3, Batch: 80, Loss: 0.683558
Train - Epoch 3, Batch: 90, Loss: 0.684107
Train - Epoch 3, Batch: 100, Loss: 0.684722
Train - Epoch 3, Batch: 110, Loss: 0.683979
Train - Epoch 3, Batch: 120, Loss: 0.685596
Train - Epoch 3, Batch: 130, Loss: 0.684605
Train - Epoch 3, Batch: 140, Loss: 0.684467
Train - Epoch 3, Batch: 150, Loss: 0.684659
Train - Epoch 3, Batch: 160, Loss: 0.684904
Train - Epoch 3, Batch: 170, Loss: 0.684539
Train - Epoch 3, Batch: 180, Loss: 0.684122
Train - Epoch 3, Batch: 190, Loss: 0.684619
Train - Epoch 3, Batch: 200, Loss: 0.684314
Train - Epoch 3, Batch: 210, Loss: 0.683762
Train - Epoch 3, Batch: 220, Loss: 0.683116
Train - Epoch 3, Batch: 230, Loss: 0.683556
Train - Epoch 3, Batch: 240, Loss: 0.684152
Train - Epoch 3, Batch: 250, Loss: 0.683702
Train - Epoch 3, Batch: 260, Loss: 0.684479
Train - Epoch 3, Batch: 270, Loss: 0.682686
Train - Epoch 3, Batch: 280, Loss: 0.683194
Train - Epoch 3, Batch: 290, Loss: 0.683231
Train - Epoch 3, Batch: 300, Loss: 0.684551
Train - Epoch 3, Batch: 310, Loss: 0.683649
Train - Epoch 3, Batch: 320, Loss: 0.683451
Train - Epoch 3, Batch: 330, Loss: 0.684940
Train - Epoch 3, Batch: 340, Loss: 0.683246
Train - Epoch 3, Batch: 350, Loss: 0.684673
Train - Epoch 3, Batch: 360, Loss: 0.683162
Train - Epoch 3, Batch: 370, Loss: 0.684742
Train - Epoch 3, Batch: 380, Loss: 0.684320
Train - Epoch 3, Batch: 390, Loss: 0.684931
Train - Epoch 3, Batch: 400, Loss: 0.682819
Train - Epoch 3, Batch: 410, Loss: 0.683110
Train - Epoch 3, Batch: 420, Loss: 0.682739
Train - Epoch 3, Batch: 430, Loss: 0.684659
Train - Epoch 3, Batch: 440, Loss: 0.684219
Train - Epoch 3, Batch: 450, Loss: 0.682936
Train - Epoch 3, Batch: 460, Loss: 0.683363
Train - Epoch 3, Batch: 470, Loss: 0.683315
Train - Epoch 3, Batch: 480, Loss: 0.683349
Train - Epoch 3, Batch: 490, Loss: 0.684598
Train - Epoch 3, Batch: 500, Loss: 0.682975
Train - Epoch 3, Batch: 510, Loss: 0.684558
Train - Epoch 3, Batch: 520, Loss: 0.683905
Train - Epoch 3, Batch: 530, Loss: 0.684642
Train - Epoch 3, Batch: 540, Loss: 0.683804
Train - Epoch 3, Batch: 550, Loss: 0.684235
Train - Epoch 3, Batch: 560, Loss: 0.683352
Train - Epoch 3, Batch: 570, Loss: 0.684406
Train - Epoch 3, Batch: 580, Loss: 0.685307
Train - Epoch 3, Batch: 590, Loss: 0.683955
Train - Epoch 3, Batch: 600, Loss: 0.683759
Train - Epoch 3, Batch: 610, Loss: 0.683644
Train - Epoch 3, Batch: 620, Loss: 0.683572
Train - Epoch 3, Batch: 630, Loss: 0.685534
Train - Epoch 3, Batch: 640, Loss: 0.683610
training_time:: 8.230313062667847
training time full:: 8.230353116989136
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554480
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 525
training time is 5.307509183883667
overhead:: 0
overhead2:: 0
time_baseline:: 5.310554027557373
curr_diff: 0 tensor(6.5076e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5076e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.08424639701843262
overhead3:: 0.20429778099060059
overhead4:: 0.9144935607910156
overhead5:: 0
time_provenance:: 2.4510040283203125
curr_diff: 0 tensor(2.3771e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3771e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2127e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.10460257530212402
overhead3:: 0.2487049102783203
overhead4:: 1.0067884922027588
overhead5:: 0
time_provenance:: 2.8322086334228516
curr_diff: 0 tensor(2.3753e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3753e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2142e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2142e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1149287223815918
overhead3:: 0.26710009574890137
overhead4:: 1.130122184753418
overhead5:: 0
time_provenance:: 3.0297975540161133
curr_diff: 0 tensor(2.3617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2259e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2259e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1265888214111328
overhead3:: 0.312166690826416
overhead4:: 1.1922976970672607
overhead5:: 0
time_provenance:: 2.78743577003479
curr_diff: 0 tensor(2.3602e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3602e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2268e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2268e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.12426042556762695
overhead3:: 0.29134654998779297
overhead4:: 1.2198841571807861
overhead5:: 0
time_provenance:: 2.925375461578369
curr_diff: 0 tensor(9.7605e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7605e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2204e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2204e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554474
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.15106987953186035
overhead3:: 0.3754570484161377
overhead4:: 1.2453010082244873
overhead5:: 0
time_provenance:: 3.046199321746826
curr_diff: 0 tensor(9.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2206e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2206e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554474
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.17833256721496582
overhead3:: 0.4025840759277344
overhead4:: 1.5659599304199219
overhead5:: 0
time_provenance:: 3.847822427749634
curr_diff: 0 tensor(9.6999e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6999e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2246e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2246e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554474
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16686439514160156
overhead3:: 0.40901923179626465
overhead4:: 1.4916179180145264
overhead5:: 0
time_provenance:: 3.311958074569702
curr_diff: 0 tensor(9.6535e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6535e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2271e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554474
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.17003583908081055
overhead3:: 0.40256190299987793
overhead4:: 1.7623186111450195
overhead5:: 0
time_provenance:: 3.6474335193634033
curr_diff: 0 tensor(5.5763e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5763e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.17830157279968262
overhead3:: 0.4204990863800049
overhead4:: 1.8400309085845947
overhead5:: 0
time_provenance:: 3.7037057876586914
curr_diff: 0 tensor(6.6070e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6070e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2884e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2884e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554484
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.18784379959106445
overhead3:: 0.4624309539794922
overhead4:: 1.8791751861572266
overhead5:: 0
time_provenance:: 3.922988176345825
curr_diff: 0 tensor(5.8390e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8390e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3527e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3527e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.21572113037109375
overhead3:: 0.5682291984558105
overhead4:: 2.140855550765991
overhead5:: 0
time_provenance:: 4.144517660140991
curr_diff: 0 tensor(5.4510e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4510e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3657e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3657e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554478
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2944924831390381
overhead3:: 0.6997237205505371
overhead4:: 2.7940094470977783
overhead5:: 0
time_provenance:: 5.656177282333374
curr_diff: 0 tensor(3.8736e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8736e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3881e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3881e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.24068713188171387
overhead3:: 0.5732944011688232
overhead4:: 2.5774612426757812
overhead5:: 0
time_provenance:: 4.804946422576904
curr_diff: 0 tensor(3.8567e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8567e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.23177003860473633
overhead3:: 0.5873193740844727
overhead4:: 2.5278751850128174
overhead5:: 0
time_provenance:: 4.633428335189819
curr_diff: 0 tensor(3.8326e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8326e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3902e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3902e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2584800720214844
overhead3:: 0.6499893665313721
overhead4:: 2.6471054553985596
overhead5:: 0
time_provenance:: 4.962857246398926
curr_diff: 0 tensor(3.8026e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8026e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3916e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3916e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.4141075611114502
overhead3:: 0.993811845779419
overhead4:: 3.4981350898742676
overhead5:: 0
time_provenance:: 5.449931621551514
curr_diff: 0 tensor(2.7383e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7383e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.5076e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5076e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554482
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.725579
Train - Epoch 0, Batch: 10, Loss: 0.697420
Train - Epoch 0, Batch: 20, Loss: 0.695527
Train - Epoch 0, Batch: 30, Loss: 0.695145
Train - Epoch 0, Batch: 40, Loss: 0.694241
Train - Epoch 0, Batch: 50, Loss: 0.693384
Train - Epoch 0, Batch: 60, Loss: 0.691977
Train - Epoch 0, Batch: 70, Loss: 0.693054
Train - Epoch 0, Batch: 80, Loss: 0.691885
Train - Epoch 0, Batch: 90, Loss: 0.692501
Train - Epoch 0, Batch: 100, Loss: 0.691918
Train - Epoch 0, Batch: 110, Loss: 0.690417
Train - Epoch 0, Batch: 120, Loss: 0.690905
Train - Epoch 0, Batch: 130, Loss: 0.690443
Train - Epoch 0, Batch: 140, Loss: 0.690211
Train - Epoch 0, Batch: 150, Loss: 0.690273
Train - Epoch 0, Batch: 160, Loss: 0.689933
Train - Epoch 0, Batch: 170, Loss: 0.689735
Train - Epoch 0, Batch: 180, Loss: 0.689473
Train - Epoch 0, Batch: 190, Loss: 0.689840
Train - Epoch 0, Batch: 200, Loss: 0.690400
Train - Epoch 0, Batch: 210, Loss: 0.689002
Train - Epoch 0, Batch: 220, Loss: 0.689617
Train - Epoch 0, Batch: 230, Loss: 0.690267
Train - Epoch 0, Batch: 240, Loss: 0.689314
Train - Epoch 0, Batch: 250, Loss: 0.688943
Train - Epoch 0, Batch: 260, Loss: 0.689810
Train - Epoch 0, Batch: 270, Loss: 0.688683
Train - Epoch 0, Batch: 280, Loss: 0.688595
Train - Epoch 0, Batch: 290, Loss: 0.688447
Train - Epoch 0, Batch: 300, Loss: 0.688453
Train - Epoch 0, Batch: 310, Loss: 0.687126
Train - Epoch 0, Batch: 320, Loss: 0.688171
Train - Epoch 0, Batch: 330, Loss: 0.687962
Train - Epoch 0, Batch: 340, Loss: 0.688162
Train - Epoch 0, Batch: 350, Loss: 0.688631
Train - Epoch 0, Batch: 360, Loss: 0.688875
Train - Epoch 0, Batch: 370, Loss: 0.688233
Train - Epoch 0, Batch: 380, Loss: 0.687225
Train - Epoch 0, Batch: 390, Loss: 0.687836
Train - Epoch 0, Batch: 400, Loss: 0.689110
Train - Epoch 0, Batch: 410, Loss: 0.688259
Train - Epoch 0, Batch: 420, Loss: 0.687782
Train - Epoch 0, Batch: 430, Loss: 0.688294
Train - Epoch 0, Batch: 440, Loss: 0.687882
Train - Epoch 0, Batch: 450, Loss: 0.687723
Train - Epoch 0, Batch: 460, Loss: 0.687807
Train - Epoch 0, Batch: 470, Loss: 0.687153
Train - Epoch 0, Batch: 480, Loss: 0.687505
Train - Epoch 0, Batch: 490, Loss: 0.687820
Train - Epoch 0, Batch: 500, Loss: 0.688659
Train - Epoch 0, Batch: 510, Loss: 0.687163
Train - Epoch 0, Batch: 520, Loss: 0.686716
Train - Epoch 0, Batch: 530, Loss: 0.688093
Train - Epoch 0, Batch: 540, Loss: 0.687308
Train - Epoch 0, Batch: 550, Loss: 0.688506
Train - Epoch 0, Batch: 560, Loss: 0.687482
Train - Epoch 0, Batch: 570, Loss: 0.688014
Train - Epoch 0, Batch: 580, Loss: 0.687145
Train - Epoch 0, Batch: 590, Loss: 0.688264
Train - Epoch 0, Batch: 600, Loss: 0.687279
Train - Epoch 0, Batch: 610, Loss: 0.685560
Train - Epoch 0, Batch: 620, Loss: 0.686855
Train - Epoch 0, Batch: 630, Loss: 0.687035
Train - Epoch 0, Batch: 640, Loss: 0.686393
Train - Epoch 1, Batch: 0, Loss: 0.686636
Train - Epoch 1, Batch: 10, Loss: 0.686878
Train - Epoch 1, Batch: 20, Loss: 0.687975
Train - Epoch 1, Batch: 30, Loss: 0.687733
Train - Epoch 1, Batch: 40, Loss: 0.687293
Train - Epoch 1, Batch: 50, Loss: 0.685868
Train - Epoch 1, Batch: 60, Loss: 0.687334
Train - Epoch 1, Batch: 70, Loss: 0.687354
Train - Epoch 1, Batch: 80, Loss: 0.686098
Train - Epoch 1, Batch: 90, Loss: 0.686438
Train - Epoch 1, Batch: 100, Loss: 0.687269
Train - Epoch 1, Batch: 110, Loss: 0.686507
Train - Epoch 1, Batch: 120, Loss: 0.686458
Train - Epoch 1, Batch: 130, Loss: 0.686585
Train - Epoch 1, Batch: 140, Loss: 0.687632
Train - Epoch 1, Batch: 150, Loss: 0.686024
Train - Epoch 1, Batch: 160, Loss: 0.686577
Train - Epoch 1, Batch: 170, Loss: 0.686892
Train - Epoch 1, Batch: 180, Loss: 0.687763
Train - Epoch 1, Batch: 190, Loss: 0.686360
Train - Epoch 1, Batch: 200, Loss: 0.687786
Train - Epoch 1, Batch: 210, Loss: 0.686172
Train - Epoch 1, Batch: 220, Loss: 0.686272
Train - Epoch 1, Batch: 230, Loss: 0.687877
Train - Epoch 1, Batch: 240, Loss: 0.684956
Train - Epoch 1, Batch: 250, Loss: 0.686737
Train - Epoch 1, Batch: 260, Loss: 0.687066
Train - Epoch 1, Batch: 270, Loss: 0.687068
Train - Epoch 1, Batch: 280, Loss: 0.686206
Train - Epoch 1, Batch: 290, Loss: 0.684993
Train - Epoch 1, Batch: 300, Loss: 0.686353
Train - Epoch 1, Batch: 310, Loss: 0.685240
Train - Epoch 1, Batch: 320, Loss: 0.686988
Train - Epoch 1, Batch: 330, Loss: 0.686438
Train - Epoch 1, Batch: 340, Loss: 0.686480
Train - Epoch 1, Batch: 350, Loss: 0.686268
Train - Epoch 1, Batch: 360, Loss: 0.686377
Train - Epoch 1, Batch: 370, Loss: 0.685127
Train - Epoch 1, Batch: 380, Loss: 0.685136
Train - Epoch 1, Batch: 390, Loss: 0.686033
Train - Epoch 1, Batch: 400, Loss: 0.685376
Train - Epoch 1, Batch: 410, Loss: 0.685855
Train - Epoch 1, Batch: 420, Loss: 0.685660
Train - Epoch 1, Batch: 430, Loss: 0.686557
Train - Epoch 1, Batch: 440, Loss: 0.686089
Train - Epoch 1, Batch: 450, Loss: 0.686180
Train - Epoch 1, Batch: 460, Loss: 0.685736
Train - Epoch 1, Batch: 470, Loss: 0.685561
Train - Epoch 1, Batch: 480, Loss: 0.685875
Train - Epoch 1, Batch: 490, Loss: 0.686328
Train - Epoch 1, Batch: 500, Loss: 0.685631
Train - Epoch 1, Batch: 510, Loss: 0.684900
Train - Epoch 1, Batch: 520, Loss: 0.686170
Train - Epoch 1, Batch: 530, Loss: 0.687007
Train - Epoch 1, Batch: 540, Loss: 0.686528
Train - Epoch 1, Batch: 550, Loss: 0.684380
Train - Epoch 1, Batch: 560, Loss: 0.686253
Train - Epoch 1, Batch: 570, Loss: 0.685210
Train - Epoch 1, Batch: 580, Loss: 0.684646
Train - Epoch 1, Batch: 590, Loss: 0.685822
Train - Epoch 1, Batch: 600, Loss: 0.685112
Train - Epoch 1, Batch: 610, Loss: 0.685903
Train - Epoch 1, Batch: 620, Loss: 0.684563
Train - Epoch 1, Batch: 630, Loss: 0.686168
Train - Epoch 1, Batch: 640, Loss: 0.685190
Train - Epoch 2, Batch: 0, Loss: 0.686412
Train - Epoch 2, Batch: 10, Loss: 0.685282
Train - Epoch 2, Batch: 20, Loss: 0.687427
Train - Epoch 2, Batch: 30, Loss: 0.685414
Train - Epoch 2, Batch: 40, Loss: 0.685767
Train - Epoch 2, Batch: 50, Loss: 0.686054
Train - Epoch 2, Batch: 60, Loss: 0.684633
Train - Epoch 2, Batch: 70, Loss: 0.685255
Train - Epoch 2, Batch: 80, Loss: 0.685324
Train - Epoch 2, Batch: 90, Loss: 0.685267
Train - Epoch 2, Batch: 100, Loss: 0.685433
Train - Epoch 2, Batch: 110, Loss: 0.684769
Train - Epoch 2, Batch: 120, Loss: 0.685782
Train - Epoch 2, Batch: 130, Loss: 0.685663
Train - Epoch 2, Batch: 140, Loss: 0.685992
Train - Epoch 2, Batch: 150, Loss: 0.685188
Train - Epoch 2, Batch: 160, Loss: 0.685271
Train - Epoch 2, Batch: 170, Loss: 0.685938
Train - Epoch 2, Batch: 180, Loss: 0.685847
Train - Epoch 2, Batch: 190, Loss: 0.685800
Train - Epoch 2, Batch: 200, Loss: 0.686280
Train - Epoch 2, Batch: 210, Loss: 0.685196
Train - Epoch 2, Batch: 220, Loss: 0.685190
Train - Epoch 2, Batch: 230, Loss: 0.685438
Train - Epoch 2, Batch: 240, Loss: 0.685171
Train - Epoch 2, Batch: 250, Loss: 0.685070
Train - Epoch 2, Batch: 260, Loss: 0.685620
Train - Epoch 2, Batch: 270, Loss: 0.684437
Train - Epoch 2, Batch: 280, Loss: 0.684303
Train - Epoch 2, Batch: 290, Loss: 0.686315
Train - Epoch 2, Batch: 300, Loss: 0.685000
Train - Epoch 2, Batch: 310, Loss: 0.685321
Train - Epoch 2, Batch: 320, Loss: 0.685897
Train - Epoch 2, Batch: 330, Loss: 0.686162
Train - Epoch 2, Batch: 340, Loss: 0.685467
Train - Epoch 2, Batch: 350, Loss: 0.685723
Train - Epoch 2, Batch: 360, Loss: 0.684742
Train - Epoch 2, Batch: 370, Loss: 0.684675
Train - Epoch 2, Batch: 380, Loss: 0.685889
Train - Epoch 2, Batch: 390, Loss: 0.685422
Train - Epoch 2, Batch: 400, Loss: 0.684153
Train - Epoch 2, Batch: 410, Loss: 0.685245
Train - Epoch 2, Batch: 420, Loss: 0.685036
Train - Epoch 2, Batch: 430, Loss: 0.685397
Train - Epoch 2, Batch: 440, Loss: 0.684144
Train - Epoch 2, Batch: 450, Loss: 0.684412
Train - Epoch 2, Batch: 460, Loss: 0.684951
Train - Epoch 2, Batch: 470, Loss: 0.684717
Train - Epoch 2, Batch: 480, Loss: 0.684573
Train - Epoch 2, Batch: 490, Loss: 0.684451
Train - Epoch 2, Batch: 500, Loss: 0.684569
Train - Epoch 2, Batch: 510, Loss: 0.684757
Train - Epoch 2, Batch: 520, Loss: 0.685020
Train - Epoch 2, Batch: 530, Loss: 0.684702
Train - Epoch 2, Batch: 540, Loss: 0.685177
Train - Epoch 2, Batch: 550, Loss: 0.684518/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684220
Train - Epoch 2, Batch: 570, Loss: 0.684143
Train - Epoch 2, Batch: 580, Loss: 0.683719
Train - Epoch 2, Batch: 590, Loss: 0.684073
Train - Epoch 2, Batch: 600, Loss: 0.683751
Train - Epoch 2, Batch: 610, Loss: 0.685173
Train - Epoch 2, Batch: 620, Loss: 0.684434
Train - Epoch 2, Batch: 630, Loss: 0.685356
Train - Epoch 2, Batch: 640, Loss: 0.684057
Train - Epoch 3, Batch: 0, Loss: 0.683978
Train - Epoch 3, Batch: 10, Loss: 0.683745
Train - Epoch 3, Batch: 20, Loss: 0.684692
Train - Epoch 3, Batch: 30, Loss: 0.685084
Train - Epoch 3, Batch: 40, Loss: 0.685017
Train - Epoch 3, Batch: 50, Loss: 0.683794
Train - Epoch 3, Batch: 60, Loss: 0.684652
Train - Epoch 3, Batch: 70, Loss: 0.684103
Train - Epoch 3, Batch: 80, Loss: 0.684814
Train - Epoch 3, Batch: 90, Loss: 0.684292
Train - Epoch 3, Batch: 100, Loss: 0.684767
Train - Epoch 3, Batch: 110, Loss: 0.685265
Train - Epoch 3, Batch: 120, Loss: 0.685005
Train - Epoch 3, Batch: 130, Loss: 0.683628
Train - Epoch 3, Batch: 140, Loss: 0.685027
Train - Epoch 3, Batch: 150, Loss: 0.684063
Train - Epoch 3, Batch: 160, Loss: 0.684430
Train - Epoch 3, Batch: 170, Loss: 0.684039
Train - Epoch 3, Batch: 180, Loss: 0.685582
Train - Epoch 3, Batch: 190, Loss: 0.684851
Train - Epoch 3, Batch: 200, Loss: 0.682616
Train - Epoch 3, Batch: 210, Loss: 0.684285
Train - Epoch 3, Batch: 220, Loss: 0.683738
Train - Epoch 3, Batch: 230, Loss: 0.684094
Train - Epoch 3, Batch: 240, Loss: 0.684445
Train - Epoch 3, Batch: 250, Loss: 0.684542
Train - Epoch 3, Batch: 260, Loss: 0.684180
Train - Epoch 3, Batch: 270, Loss: 0.684753
Train - Epoch 3, Batch: 280, Loss: 0.685233
Train - Epoch 3, Batch: 290, Loss: 0.684698
Train - Epoch 3, Batch: 300, Loss: 0.684268
Train - Epoch 3, Batch: 310, Loss: 0.685292
Train - Epoch 3, Batch: 320, Loss: 0.684007
Train - Epoch 3, Batch: 330, Loss: 0.683764
Train - Epoch 3, Batch: 340, Loss: 0.684386
Train - Epoch 3, Batch: 350, Loss: 0.683933
Train - Epoch 3, Batch: 360, Loss: 0.685218
Train - Epoch 3, Batch: 370, Loss: 0.683993
Train - Epoch 3, Batch: 380, Loss: 0.682677
Train - Epoch 3, Batch: 390, Loss: 0.684192
Train - Epoch 3, Batch: 400, Loss: 0.683012
Train - Epoch 3, Batch: 410, Loss: 0.684174
Train - Epoch 3, Batch: 420, Loss: 0.684039
Train - Epoch 3, Batch: 430, Loss: 0.684253
Train - Epoch 3, Batch: 440, Loss: 0.684149
Train - Epoch 3, Batch: 450, Loss: 0.682589
Train - Epoch 3, Batch: 460, Loss: 0.683914
Train - Epoch 3, Batch: 470, Loss: 0.683018
Train - Epoch 3, Batch: 480, Loss: 0.684005
Train - Epoch 3, Batch: 490, Loss: 0.682753
Train - Epoch 3, Batch: 500, Loss: 0.684534
Train - Epoch 3, Batch: 510, Loss: 0.682904
Train - Epoch 3, Batch: 520, Loss: 0.684910
Train - Epoch 3, Batch: 530, Loss: 0.683677
Train - Epoch 3, Batch: 540, Loss: 0.682365
Train - Epoch 3, Batch: 550, Loss: 0.684583
Train - Epoch 3, Batch: 560, Loss: 0.683126
Train - Epoch 3, Batch: 570, Loss: 0.684849
Train - Epoch 3, Batch: 580, Loss: 0.683290
Train - Epoch 3, Batch: 590, Loss: 0.684524
Train - Epoch 3, Batch: 600, Loss: 0.684750
Train - Epoch 3, Batch: 610, Loss: 0.684239
Train - Epoch 3, Batch: 620, Loss: 0.683676
Train - Epoch 3, Batch: 630, Loss: 0.684602
Train - Epoch 3, Batch: 640, Loss: 0.684172
training_time:: 7.864739418029785
training time full:: 7.864778518676758
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553116
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 525
training time is 5.745520830154419
overhead:: 0
overhead2:: 0
time_baseline:: 5.749088525772095
curr_diff: 0 tensor(5.6477e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6477e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.10946226119995117
overhead3:: 0.2965664863586426
overhead4:: 0.8923699855804443
overhead5:: 0
time_provenance:: 2.556729316711426
curr_diff: 0 tensor(1.4884e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4884e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2100e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2100e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1174764633178711
overhead3:: 0.30884838104248047
overhead4:: 1.0638296604156494
overhead5:: 0
time_provenance:: 2.9438509941101074
curr_diff: 0 tensor(1.4774e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4774e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2167e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2167e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1056222915649414
overhead3:: 0.24834513664245605
overhead4:: 1.0577356815338135
overhead5:: 0
time_provenance:: 2.5849430561065674
curr_diff: 0 tensor(1.4608e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4608e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2283e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2283e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1686244010925293
overhead3:: 0.47368741035461426
overhead4:: 1.4473798274993896
overhead5:: 0
time_provenance:: 3.5513200759887695
curr_diff: 0 tensor(1.4617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2258e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2258e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1244964599609375
overhead3:: 0.31340575218200684
overhead4:: 1.21187162399292
overhead5:: 0
time_provenance:: 2.9352822303771973
curr_diff: 0 tensor(9.3098e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3098e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2364e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2364e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1642773151397705
overhead3:: 0.39966273307800293
overhead4:: 1.584923505783081
overhead5:: 0
time_provenance:: 3.872184991836548
curr_diff: 0 tensor(9.2544e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2544e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2391e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2391e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1454167366027832
overhead3:: 0.3836996555328369
overhead4:: 1.4314241409301758
overhead5:: 0
time_provenance:: 3.187763214111328
curr_diff: 0 tensor(9.2184e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2184e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2411e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2411e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1564621925354004
overhead3:: 0.3689708709716797
overhead4:: 1.5123472213745117
overhead5:: 0
time_provenance:: 3.2275619506835938
curr_diff: 0 tensor(9.1760e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1760e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16192936897277832
overhead3:: 0.39150357246398926
overhead4:: 1.6854677200317383
overhead5:: 0
time_provenance:: 3.5293760299682617
curr_diff: 0 tensor(6.3516e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3516e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.4736e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4736e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.21348357200622559
overhead3:: 0.5161049365997314
overhead4:: 2.108941078186035
overhead5:: 0
time_provenance:: 4.5428431034088135
curr_diff: 0 tensor(6.1740e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1740e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.4941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4941e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.19891786575317383
overhead3:: 0.46703219413757324
overhead4:: 1.9675335884094238
overhead5:: 0
time_provenance:: 4.101833820343018
curr_diff: 0 tensor(4.0319e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0319e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5697e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5697e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.26010727882385254
overhead3:: 0.6846318244934082
overhead4:: 2.4657142162323
overhead5:: 0
time_provenance:: 5.051244497299194
curr_diff: 0 tensor(6.1742e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1742e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.4821e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4821e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553064
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2849910259246826
overhead3:: 0.7094345092773438
overhead4:: 2.8804855346679688
overhead5:: 0
time_provenance:: 5.7400665283203125
curr_diff: 0 tensor(2.8773e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8773e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5574e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5574e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.28546595573425293
overhead3:: 0.7160272598266602
overhead4:: 2.8876428604125977
overhead5:: 0
time_provenance:: 5.681440353393555
curr_diff: 0 tensor(2.8422e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8422e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5591e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5591e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.28478550910949707
overhead3:: 0.6991186141967773
overhead4:: 2.9440200328826904
overhead5:: 0
time_provenance:: 5.725727081298828
curr_diff: 0 tensor(2.8018e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8018e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5610e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5610e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2317032814025879
overhead3:: 0.5767779350280762
overhead4:: 2.4878389835357666
overhead5:: 0
time_provenance:: 4.545088052749634
curr_diff: 0 tensor(2.7538e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7538e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.5636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.48969578742980957
overhead3:: 1.2763123512268066
overhead4:: 3.8613855838775635
overhead5:: 0
time_provenance:: 6.18181586265564
curr_diff: 0 tensor(2.6037e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6037e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.6477e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6477e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.703478
Train - Epoch 0, Batch: 10, Loss: 0.694831
Train - Epoch 0, Batch: 20, Loss: 0.695026
Train - Epoch 0, Batch: 30, Loss: 0.692559
Train - Epoch 0, Batch: 40, Loss: 0.691867
Train - Epoch 0, Batch: 50, Loss: 0.691511
Train - Epoch 0, Batch: 60, Loss: 0.692222
Train - Epoch 0, Batch: 70, Loss: 0.691363
Train - Epoch 0, Batch: 80, Loss: 0.690369
Train - Epoch 0, Batch: 90, Loss: 0.690806
Train - Epoch 0, Batch: 100, Loss: 0.689585
Train - Epoch 0, Batch: 110, Loss: 0.690955
Train - Epoch 0, Batch: 120, Loss: 0.689763
Train - Epoch 0, Batch: 130, Loss: 0.689903
Train - Epoch 0, Batch: 140, Loss: 0.688868
Train - Epoch 0, Batch: 150, Loss: 0.689808
Train - Epoch 0, Batch: 160, Loss: 0.689240
Train - Epoch 0, Batch: 170, Loss: 0.690006
Train - Epoch 0, Batch: 180, Loss: 0.688477
Train - Epoch 0, Batch: 190, Loss: 0.689643
Train - Epoch 0, Batch: 200, Loss: 0.688668
Train - Epoch 0, Batch: 210, Loss: 0.689180
Train - Epoch 0, Batch: 220, Loss: 0.687989
Train - Epoch 0, Batch: 230, Loss: 0.688904
Train - Epoch 0, Batch: 240, Loss: 0.687350
Train - Epoch 0, Batch: 250, Loss: 0.688855
Train - Epoch 0, Batch: 260, Loss: 0.688760
Train - Epoch 0, Batch: 270, Loss: 0.688010
Train - Epoch 0, Batch: 280, Loss: 0.688538
Train - Epoch 0, Batch: 290, Loss: 0.688785
Train - Epoch 0, Batch: 300, Loss: 0.687447
Train - Epoch 0, Batch: 310, Loss: 0.688487
Train - Epoch 0, Batch: 320, Loss: 0.686862
Train - Epoch 0, Batch: 330, Loss: 0.688690
Train - Epoch 0, Batch: 340, Loss: 0.688239
Train - Epoch 0, Batch: 350, Loss: 0.687749
Train - Epoch 0, Batch: 360, Loss: 0.687463
Train - Epoch 0, Batch: 370, Loss: 0.688445
Train - Epoch 0, Batch: 380, Loss: 0.686542
Train - Epoch 0, Batch: 390, Loss: 0.687884
Train - Epoch 0, Batch: 400, Loss: 0.687109
Train - Epoch 0, Batch: 410, Loss: 0.687396
Train - Epoch 0, Batch: 420, Loss: 0.687506
Train - Epoch 0, Batch: 430, Loss: 0.686680
Train - Epoch 0, Batch: 440, Loss: 0.686434
Train - Epoch 0, Batch: 450, Loss: 0.686681
Train - Epoch 0, Batch: 460, Loss: 0.685477
Train - Epoch 0, Batch: 470, Loss: 0.686189
Train - Epoch 0, Batch: 480, Loss: 0.686453
Train - Epoch 0, Batch: 490, Loss: 0.684654
Train - Epoch 0, Batch: 500, Loss: 0.687011
Train - Epoch 0, Batch: 510, Loss: 0.687033
Train - Epoch 0, Batch: 520, Loss: 0.686778
Train - Epoch 0, Batch: 530, Loss: 0.686554
Train - Epoch 0, Batch: 540, Loss: 0.687195
Train - Epoch 0, Batch: 550, Loss: 0.685508
Train - Epoch 0, Batch: 560, Loss: 0.686532
Train - Epoch 0, Batch: 570, Loss: 0.686417
Train - Epoch 0, Batch: 580, Loss: 0.686804
Train - Epoch 0, Batch: 590, Loss: 0.686756
Train - Epoch 0, Batch: 600, Loss: 0.685919
Train - Epoch 0, Batch: 610, Loss: 0.686615
Train - Epoch 0, Batch: 620, Loss: 0.686023
Train - Epoch 0, Batch: 630, Loss: 0.686944
Train - Epoch 0, Batch: 640, Loss: 0.684870
Train - Epoch 1, Batch: 0, Loss: 0.685932
Train - Epoch 1, Batch: 10, Loss: 0.686563
Train - Epoch 1, Batch: 20, Loss: 0.684900
Train - Epoch 1, Batch: 30, Loss: 0.685995
Train - Epoch 1, Batch: 40, Loss: 0.685016
Train - Epoch 1, Batch: 50, Loss: 0.686032
Train - Epoch 1, Batch: 60, Loss: 0.685244
Train - Epoch 1, Batch: 70, Loss: 0.685836
Train - Epoch 1, Batch: 80, Loss: 0.686055
Train - Epoch 1, Batch: 90, Loss: 0.685483
Train - Epoch 1, Batch: 100, Loss: 0.686255
Train - Epoch 1, Batch: 110, Loss: 0.685450
Train - Epoch 1, Batch: 120, Loss: 0.686028
Train - Epoch 1, Batch: 130, Loss: 0.686293
Train - Epoch 1, Batch: 140, Loss: 0.686715
Train - Epoch 1, Batch: 150, Loss: 0.684290
Train - Epoch 1, Batch: 160, Loss: 0.686192
Train - Epoch 1, Batch: 170, Loss: 0.687052
Train - Epoch 1, Batch: 180, Loss: 0.686243
Train - Epoch 1, Batch: 190, Loss: 0.686408
Train - Epoch 1, Batch: 200, Loss: 0.685287
Train - Epoch 1, Batch: 210, Loss: 0.684853
Train - Epoch 1, Batch: 220, Loss: 0.685368
Train - Epoch 1, Batch: 230, Loss: 0.686989
Train - Epoch 1, Batch: 240, Loss: 0.685306
Train - Epoch 1, Batch: 250, Loss: 0.685507
Train - Epoch 1, Batch: 260, Loss: 0.685848
Train - Epoch 1, Batch: 270, Loss: 0.685017
Train - Epoch 1, Batch: 280, Loss: 0.684959
Train - Epoch 1, Batch: 290, Loss: 0.686124
Train - Epoch 1, Batch: 300, Loss: 0.685809
Train - Epoch 1, Batch: 310, Loss: 0.685253
Train - Epoch 1, Batch: 320, Loss: 0.685830
Train - Epoch 1, Batch: 330, Loss: 0.684620
Train - Epoch 1, Batch: 340, Loss: 0.685650
Train - Epoch 1, Batch: 350, Loss: 0.686150
Train - Epoch 1, Batch: 360, Loss: 0.685095
Train - Epoch 1, Batch: 370, Loss: 0.684924
Train - Epoch 1, Batch: 380, Loss: 0.685059
Train - Epoch 1, Batch: 390, Loss: 0.683983
Train - Epoch 1, Batch: 400, Loss: 0.684372
Train - Epoch 1, Batch: 410, Loss: 0.685885
Train - Epoch 1, Batch: 420, Loss: 0.685254
Train - Epoch 1, Batch: 430, Loss: 0.685326
Train - Epoch 1, Batch: 440, Loss: 0.685522
Train - Epoch 1, Batch: 450, Loss: 0.685349
Train - Epoch 1, Batch: 460, Loss: 0.686570
Train - Epoch 1, Batch: 470, Loss: 0.686039
Train - Epoch 1, Batch: 480, Loss: 0.684761
Train - Epoch 1, Batch: 490, Loss: 0.685221
Train - Epoch 1, Batch: 500, Loss: 0.685026
Train - Epoch 1, Batch: 510, Loss: 0.685341
Train - Epoch 1, Batch: 520, Loss: 0.684988
Train - Epoch 1, Batch: 530, Loss: 0.684897
Train - Epoch 1, Batch: 540, Loss: 0.682955
Train - Epoch 1, Batch: 550, Loss: 0.685871
Train - Epoch 1, Batch: 560, Loss: 0.685670
Train - Epoch 1, Batch: 570, Loss: 0.683754
Train - Epoch 1, Batch: 580, Loss: 0.685323
Train - Epoch 1, Batch: 590, Loss: 0.684616
Train - Epoch 1, Batch: 600, Loss: 0.685222
Train - Epoch 1, Batch: 610, Loss: 0.683980
Train - Epoch 1, Batch: 620, Loss: 0.684567
Train - Epoch 1, Batch: 630, Loss: 0.683549
Train - Epoch 1, Batch: 640, Loss: 0.684145
Train - Epoch 2, Batch: 0, Loss: 0.684150
Train - Epoch 2, Batch: 10, Loss: 0.684882
Train - Epoch 2, Batch: 20, Loss: 0.684921
Train - Epoch 2, Batch: 30, Loss: 0.685489
Train - Epoch 2, Batch: 40, Loss: 0.685429
Train - Epoch 2, Batch: 50, Loss: 0.684762
Train - Epoch 2, Batch: 60, Loss: 0.683992
Train - Epoch 2, Batch: 70, Loss: 0.685610
Train - Epoch 2, Batch: 80, Loss: 0.684105
Train - Epoch 2, Batch: 90, Loss: 0.685450
Train - Epoch 2, Batch: 100, Loss: 0.685933
Train - Epoch 2, Batch: 110, Loss: 0.684962
Train - Epoch 2, Batch: 120, Loss: 0.685133
Train - Epoch 2, Batch: 130, Loss: 0.686194
Train - Epoch 2, Batch: 140, Loss: 0.685300
Train - Epoch 2, Batch: 150, Loss: 0.685517
Train - Epoch 2, Batch: 160, Loss: 0.684694
Train - Epoch 2, Batch: 170, Loss: 0.684821
Train - Epoch 2, Batch: 180, Loss: 0.683233
Train - Epoch 2, Batch: 190, Loss: 0.685717
Train - Epoch 2, Batch: 200, Loss: 0.683803
Train - Epoch 2, Batch: 210, Loss: 0.685387
Train - Epoch 2, Batch: 220, Loss: 0.684252
Train - Epoch 2, Batch: 230, Loss: 0.685309
Train - Epoch 2, Batch: 240, Loss: 0.684593
Train - Epoch 2, Batch: 250, Loss: 0.686512
Train - Epoch 2, Batch: 260, Loss: 0.683722
Train - Epoch 2, Batch: 270, Loss: 0.685719
Train - Epoch 2, Batch: 280, Loss: 0.684400
Train - Epoch 2, Batch: 290, Loss: 0.685523
Train - Epoch 2, Batch: 300, Loss: 0.684278
Train - Epoch 2, Batch: 310, Loss: 0.684176
Train - Epoch 2, Batch: 320, Loss: 0.684591
Train - Epoch 2, Batch: 330, Loss: 0.684088
Train - Epoch 2, Batch: 340, Loss: 0.683606
Train - Epoch 2, Batch: 350, Loss: 0.686041
Train - Epoch 2, Batch: 360, Loss: 0.684733
Train - Epoch 2, Batch: 370, Loss: 0.684609
Train - Epoch 2, Batch: 380, Loss: 0.683088
Train - Epoch 2, Batch: 390, Loss: 0.683795
Train - Epoch 2, Batch: 400, Loss: 0.687041
Train - Epoch 2, Batch: 410, Loss: 0.684487
Train - Epoch 2, Batch: 420, Loss: 0.683795
Train - Epoch 2, Batch: 430, Loss: 0.683980
Train - Epoch 2, Batch: 440, Loss: 0.684684
Train - Epoch 2, Batch: 450, Loss: 0.684980
Train - Epoch 2, Batch: 460, Loss: 0.684209
Train - Epoch 2, Batch: 470, Loss: 0.684843
Train - Epoch 2, Batch: 480, Loss: 0.684854
Train - Epoch 2, Batch: 490, Loss: 0.683769
Train - Epoch 2, Batch: 500, Loss: 0.683018
Train - Epoch 2, Batch: 510, Loss: 0.684535
Train - Epoch 2, Batch: 520, Loss: 0.683832
Train - Epoch 2, Batch: 530, Loss: 0.683391
Train - Epoch 2, Batch: 540, Loss: 0.683507
Train - Epoch 2, Batch: 550, Loss: 0.683933/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684163
Train - Epoch 2, Batch: 570, Loss: 0.684400
Train - Epoch 2, Batch: 580, Loss: 0.684291
Train - Epoch 2, Batch: 590, Loss: 0.684463
Train - Epoch 2, Batch: 600, Loss: 0.684386
Train - Epoch 2, Batch: 610, Loss: 0.684928
Train - Epoch 2, Batch: 620, Loss: 0.684146
Train - Epoch 2, Batch: 630, Loss: 0.683612
Train - Epoch 2, Batch: 640, Loss: 0.684065
Train - Epoch 3, Batch: 0, Loss: 0.684728
Train - Epoch 3, Batch: 10, Loss: 0.684357
Train - Epoch 3, Batch: 20, Loss: 0.683836
Train - Epoch 3, Batch: 30, Loss: 0.684956
Train - Epoch 3, Batch: 40, Loss: 0.684402
Train - Epoch 3, Batch: 50, Loss: 0.683839
Train - Epoch 3, Batch: 60, Loss: 0.685333
Train - Epoch 3, Batch: 70, Loss: 0.683901
Train - Epoch 3, Batch: 80, Loss: 0.685177
Train - Epoch 3, Batch: 90, Loss: 0.684071
Train - Epoch 3, Batch: 100, Loss: 0.684684
Train - Epoch 3, Batch: 110, Loss: 0.683893
Train - Epoch 3, Batch: 120, Loss: 0.684316
Train - Epoch 3, Batch: 130, Loss: 0.685195
Train - Epoch 3, Batch: 140, Loss: 0.684347
Train - Epoch 3, Batch: 150, Loss: 0.683934
Train - Epoch 3, Batch: 160, Loss: 0.684046
Train - Epoch 3, Batch: 170, Loss: 0.684126
Train - Epoch 3, Batch: 180, Loss: 0.684016
Train - Epoch 3, Batch: 190, Loss: 0.684657
Train - Epoch 3, Batch: 200, Loss: 0.683999
Train - Epoch 3, Batch: 210, Loss: 0.685058
Train - Epoch 3, Batch: 220, Loss: 0.683754
Train - Epoch 3, Batch: 230, Loss: 0.684507
Train - Epoch 3, Batch: 240, Loss: 0.684410
Train - Epoch 3, Batch: 250, Loss: 0.683788
Train - Epoch 3, Batch: 260, Loss: 0.682959
Train - Epoch 3, Batch: 270, Loss: 0.684583
Train - Epoch 3, Batch: 280, Loss: 0.682922
Train - Epoch 3, Batch: 290, Loss: 0.682769
Train - Epoch 3, Batch: 300, Loss: 0.684418
Train - Epoch 3, Batch: 310, Loss: 0.683453
Train - Epoch 3, Batch: 320, Loss: 0.684225
Train - Epoch 3, Batch: 330, Loss: 0.683794
Train - Epoch 3, Batch: 340, Loss: 0.685637
Train - Epoch 3, Batch: 350, Loss: 0.683475
Train - Epoch 3, Batch: 360, Loss: 0.683613
Train - Epoch 3, Batch: 370, Loss: 0.683106
Train - Epoch 3, Batch: 380, Loss: 0.682889
Train - Epoch 3, Batch: 390, Loss: 0.683625
Train - Epoch 3, Batch: 400, Loss: 0.683560
Train - Epoch 3, Batch: 410, Loss: 0.684064
Train - Epoch 3, Batch: 420, Loss: 0.684134
Train - Epoch 3, Batch: 430, Loss: 0.683032
Train - Epoch 3, Batch: 440, Loss: 0.684028
Train - Epoch 3, Batch: 450, Loss: 0.683494
Train - Epoch 3, Batch: 460, Loss: 0.683350
Train - Epoch 3, Batch: 470, Loss: 0.684747
Train - Epoch 3, Batch: 480, Loss: 0.683864
Train - Epoch 3, Batch: 490, Loss: 0.683729
Train - Epoch 3, Batch: 500, Loss: 0.683885
Train - Epoch 3, Batch: 510, Loss: 0.683690
Train - Epoch 3, Batch: 520, Loss: 0.684067
Train - Epoch 3, Batch: 530, Loss: 0.684499
Train - Epoch 3, Batch: 540, Loss: 0.684501
Train - Epoch 3, Batch: 550, Loss: 0.684036
Train - Epoch 3, Batch: 560, Loss: 0.683597
Train - Epoch 3, Batch: 570, Loss: 0.684892
Train - Epoch 3, Batch: 580, Loss: 0.683791
Train - Epoch 3, Batch: 590, Loss: 0.683227
Train - Epoch 3, Batch: 600, Loss: 0.684039
Train - Epoch 3, Batch: 610, Loss: 0.683591
Train - Epoch 3, Batch: 620, Loss: 0.684062
Train - Epoch 3, Batch: 630, Loss: 0.683500
Train - Epoch 3, Batch: 640, Loss: 0.682800
training_time:: 7.606756925582886
training time full:: 7.606795787811279
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554196
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 525
training time is 4.438936710357666
overhead:: 0
overhead2:: 0
time_baseline:: 4.442045211791992
curr_diff: 0 tensor(5.2667e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2667e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554178
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.08736991882324219
overhead3:: 0.21749448776245117
overhead4:: 0.8163201808929443
overhead5:: 0
time_provenance:: 2.346820592880249
curr_diff: 0 tensor(2.0223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0871e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0871e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.11694884300231934
overhead3:: 0.32206034660339355
overhead4:: 0.9840388298034668
overhead5:: 0
time_provenance:: 2.5392463207244873
curr_diff: 0 tensor(2.0122e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0122e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.0938e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0938e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1224830150604248
overhead3:: 0.2992682456970215
overhead4:: 1.1415126323699951
overhead5:: 0
time_provenance:: 3.113581657409668
curr_diff: 0 tensor(1.9884e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9884e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1094e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1094e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.15944290161132812
overhead3:: 0.415696382522583
overhead4:: 1.3517792224884033
overhead5:: 0
time_provenance:: 3.3680286407470703
curr_diff: 0 tensor(1.9863e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9863e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.1106e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1106e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.12182927131652832
overhead3:: 0.2960186004638672
overhead4:: 1.2362771034240723
overhead5:: 0
time_provenance:: 2.9128053188323975
curr_diff: 0 tensor(1.4179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2680e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2680e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554174
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16040349006652832
overhead3:: 0.429462194442749
overhead4:: 1.5573318004608154
overhead5:: 0
time_provenance:: 3.694059133529663
curr_diff: 0 tensor(1.4183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2675e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2675e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554174
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1403956413269043
overhead3:: 0.33977794647216797
overhead4:: 1.4955220222473145
overhead5:: 0
time_provenance:: 3.1831178665161133
curr_diff: 0 tensor(1.4117e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4117e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2714e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2714e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554174
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16601204872131348
overhead3:: 0.39302873611450195
overhead4:: 1.4231657981872559
overhead5:: 0
time_provenance:: 3.1728320121765137
curr_diff: 0 tensor(1.4123e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4123e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2711e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2711e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554174
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.16205501556396484
overhead3:: 0.4134690761566162
overhead4:: 1.7430529594421387
overhead5:: 0
time_provenance:: 3.6273694038391113
curr_diff: 0 tensor(5.1406e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1406e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.0613e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0613e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.205169677734375
overhead3:: 0.5392096042633057
overhead4:: 2.0413644313812256
overhead5:: 0
time_provenance:: 4.267682790756226
curr_diff: 0 tensor(7.4154e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4154e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.8101e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8101e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.1819453239440918
overhead3:: 0.4547462463378906
overhead4:: 1.946993112564087
overhead5:: 0
time_provenance:: 3.831063747406006
curr_diff: 0 tensor(5.5020e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5020e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.9969e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9969e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.22722220420837402
overhead3:: 0.6527142524719238
overhead4:: 2.1484127044677734
overhead5:: 0
time_provenance:: 4.254504203796387
curr_diff: 0 tensor(5.0486e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0486e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.0664e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0664e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2287278175354004
overhead3:: 0.5491702556610107
overhead4:: 2.3494694232940674
overhead5:: 0
time_provenance:: 4.494837522506714
curr_diff: 0 tensor(2.7311e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7311e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.1808e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1808e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.24297499656677246
overhead3:: 0.585064172744751
overhead4:: 2.5087738037109375
overhead5:: 0
time_provenance:: 4.663838863372803
curr_diff: 0 tensor(2.7122e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7122e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.1819e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1819e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.2644176483154297
overhead3:: 0.7194850444793701
overhead4:: 2.6707897186279297
overhead5:: 0
time_provenance:: 4.987315654754639
curr_diff: 0 tensor(2.6847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.1830e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1830e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.26999425888061523
overhead3:: 0.714946985244751
overhead4:: 2.756148338317871
overhead5:: 0
time_provenance:: 5.017009019851685
curr_diff: 0 tensor(2.6729e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6729e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.1836e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1836e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554176
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 525
max_epoch:: 4
overhead:: 0
overhead2:: 0.43353748321533203
overhead3:: 1.122525691986084
overhead4:: 3.750805377960205
overhead5:: 0
time_provenance:: 5.8554065227508545
curr_diff: 0 tensor(2.0682e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0682e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.2667e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2667e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554178
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  higgs 0
tensor([3311621, 8554503, 5801994,  ..., 9924598, 3346426, 7800828])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702466
Train - Epoch 0, Batch: 10, Loss: 0.691140
Train - Epoch 0, Batch: 20, Loss: 0.691381
Train - Epoch 0, Batch: 30, Loss: 0.691275
Train - Epoch 0, Batch: 40, Loss: 0.691087
Train - Epoch 0, Batch: 50, Loss: 0.691696
Train - Epoch 0, Batch: 60, Loss: 0.690390
Train - Epoch 0, Batch: 70, Loss: 0.689726
Train - Epoch 0, Batch: 80, Loss: 0.690941
Train - Epoch 0, Batch: 90, Loss: 0.689412
Train - Epoch 0, Batch: 100, Loss: 0.689166
Train - Epoch 0, Batch: 110, Loss: 0.688812
Train - Epoch 0, Batch: 120, Loss: 0.688870
Train - Epoch 0, Batch: 130, Loss: 0.688907
Train - Epoch 0, Batch: 140, Loss: 0.689671
Train - Epoch 0, Batch: 150, Loss: 0.689284
Train - Epoch 0, Batch: 160, Loss: 0.688321
Train - Epoch 0, Batch: 170, Loss: 0.689384
Train - Epoch 0, Batch: 180, Loss: 0.688250
Train - Epoch 0, Batch: 190, Loss: 0.688620
Train - Epoch 0, Batch: 200, Loss: 0.688854
Train - Epoch 0, Batch: 210, Loss: 0.688689
Train - Epoch 0, Batch: 220, Loss: 0.687637
Train - Epoch 0, Batch: 230, Loss: 0.688501
Train - Epoch 0, Batch: 240, Loss: 0.686914
Train - Epoch 0, Batch: 250, Loss: 0.687444
Train - Epoch 0, Batch: 260, Loss: 0.687847
Train - Epoch 0, Batch: 270, Loss: 0.687871
Train - Epoch 0, Batch: 280, Loss: 0.687582
Train - Epoch 0, Batch: 290, Loss: 0.687834
Train - Epoch 0, Batch: 300, Loss: 0.687590
Train - Epoch 0, Batch: 310, Loss: 0.686978
Train - Epoch 0, Batch: 320, Loss: 0.686861
Train - Epoch 0, Batch: 330, Loss: 0.687264
Train - Epoch 0, Batch: 340, Loss: 0.686694
Train - Epoch 0, Batch: 350, Loss: 0.688003
Train - Epoch 0, Batch: 360, Loss: 0.687975
Train - Epoch 0, Batch: 370, Loss: 0.686330
Train - Epoch 0, Batch: 380, Loss: 0.687328
Train - Epoch 0, Batch: 390, Loss: 0.686508
Train - Epoch 0, Batch: 400, Loss: 0.687541
Train - Epoch 0, Batch: 410, Loss: 0.686933
Train - Epoch 0, Batch: 420, Loss: 0.686233
Train - Epoch 0, Batch: 430, Loss: 0.687244
Train - Epoch 0, Batch: 440, Loss: 0.686206
Train - Epoch 0, Batch: 450, Loss: 0.687108
Train - Epoch 0, Batch: 460, Loss: 0.686683
Train - Epoch 0, Batch: 470, Loss: 0.687101
Train - Epoch 0, Batch: 480, Loss: 0.686421
Train - Epoch 0, Batch: 490, Loss: 0.686220
Train - Epoch 0, Batch: 500, Loss: 0.687631
Train - Epoch 0, Batch: 510, Loss: 0.687195
Train - Epoch 0, Batch: 520, Loss: 0.686388
Train - Epoch 0, Batch: 530, Loss: 0.685757
Train - Epoch 0, Batch: 540, Loss: 0.685474
Train - Epoch 0, Batch: 550, Loss: 0.687843
Train - Epoch 0, Batch: 560, Loss: 0.685875
Train - Epoch 0, Batch: 570, Loss: 0.686726
Train - Epoch 0, Batch: 580, Loss: 0.685449
Train - Epoch 0, Batch: 590, Loss: 0.686978
Train - Epoch 0, Batch: 600, Loss: 0.686657
Train - Epoch 0, Batch: 610, Loss: 0.686156
Train - Epoch 0, Batch: 620, Loss: 0.687213
Train - Epoch 0, Batch: 630, Loss: 0.685520
Train - Epoch 0, Batch: 640, Loss: 0.685520
Train - Epoch 1, Batch: 0, Loss: 0.687538
Train - Epoch 1, Batch: 10, Loss: 0.685982
Train - Epoch 1, Batch: 20, Loss: 0.687030
Train - Epoch 1, Batch: 30, Loss: 0.685630
Train - Epoch 1, Batch: 40, Loss: 0.686640
Train - Epoch 1, Batch: 50, Loss: 0.686540
Train - Epoch 1, Batch: 60, Loss: 0.686198
Train - Epoch 1, Batch: 70, Loss: 0.686233
Train - Epoch 1, Batch: 80, Loss: 0.687142
Train - Epoch 1, Batch: 90, Loss: 0.685568
Train - Epoch 1, Batch: 100, Loss: 0.685492
Train - Epoch 1, Batch: 110, Loss: 0.685123
Train - Epoch 1, Batch: 120, Loss: 0.686010
Train - Epoch 1, Batch: 130, Loss: 0.686624
Train - Epoch 1, Batch: 140, Loss: 0.686111
Train - Epoch 1, Batch: 150, Loss: 0.685724
Train - Epoch 1, Batch: 160, Loss: 0.686387
Train - Epoch 1, Batch: 170, Loss: 0.686217
Train - Epoch 1, Batch: 180, Loss: 0.684790
Train - Epoch 1, Batch: 190, Loss: 0.686669
Train - Epoch 1, Batch: 200, Loss: 0.686591
Train - Epoch 1, Batch: 210, Loss: 0.685288
Train - Epoch 1, Batch: 220, Loss: 0.685618
Train - Epoch 1, Batch: 230, Loss: 0.686236
Train - Epoch 1, Batch: 240, Loss: 0.685098
Train - Epoch 1, Batch: 250, Loss: 0.685533
Train - Epoch 1, Batch: 260, Loss: 0.683972
Train - Epoch 1, Batch: 270, Loss: 0.686583
Train - Epoch 1, Batch: 280, Loss: 0.686036
Train - Epoch 1, Batch: 290, Loss: 0.686069
Train - Epoch 1, Batch: 300, Loss: 0.686156
Train - Epoch 1, Batch: 310, Loss: 0.685684
Train - Epoch 1, Batch: 320, Loss: 0.684775
Train - Epoch 1, Batch: 330, Loss: 0.685054
Train - Epoch 1, Batch: 340, Loss: 0.685966
Train - Epoch 1, Batch: 350, Loss: 0.685500
Train - Epoch 1, Batch: 360, Loss: 0.684762
Train - Epoch 1, Batch: 370, Loss: 0.685195
Train - Epoch 1, Batch: 380, Loss: 0.686353
Train - Epoch 1, Batch: 390, Loss: 0.685002
Train - Epoch 1, Batch: 400, Loss: 0.685238
Train - Epoch 1, Batch: 410, Loss: 0.684819
Train - Epoch 1, Batch: 420, Loss: 0.685038
Train - Epoch 1, Batch: 430, Loss: 0.685727
Train - Epoch 1, Batch: 440, Loss: 0.684806
Train - Epoch 1, Batch: 450, Loss: 0.684714
Train - Epoch 1, Batch: 460, Loss: 0.686484
Train - Epoch 1, Batch: 470, Loss: 0.684680
Train - Epoch 1, Batch: 480, Loss: 0.685503
Train - Epoch 1, Batch: 490, Loss: 0.683714
Train - Epoch 1, Batch: 500, Loss: 0.685230
Train - Epoch 1, Batch: 510, Loss: 0.686309
Train - Epoch 1, Batch: 520, Loss: 0.684958
Train - Epoch 1, Batch: 530, Loss: 0.684993
Train - Epoch 1, Batch: 540, Loss: 0.685228
Train - Epoch 1, Batch: 550, Loss: 0.685643
Train - Epoch 1, Batch: 560, Loss: 0.685670
Train - Epoch 1, Batch: 570, Loss: 0.685375
Train - Epoch 1, Batch: 580, Loss: 0.686555
Train - Epoch 1, Batch: 590, Loss: 0.684918
Train - Epoch 1, Batch: 600, Loss: 0.685354
Train - Epoch 1, Batch: 610, Loss: 0.684893
Train - Epoch 1, Batch: 620, Loss: 0.684975
Train - Epoch 1, Batch: 630, Loss: 0.684990
Train - Epoch 1, Batch: 640, Loss: 0.683415
Train - Epoch 2, Batch: 0, Loss: 0.685234
Train - Epoch 2, Batch: 10, Loss: 0.685097
Train - Epoch 2, Batch: 20, Loss: 0.684666
Train - Epoch 2, Batch: 30, Loss: 0.684906
Train - Epoch 2, Batch: 40, Loss: 0.684653
Train - Epoch 2, Batch: 50, Loss: 0.685070
Train - Epoch 2, Batch: 60, Loss: 0.684633
Train - Epoch 2, Batch: 70, Loss: 0.684121
Train - Epoch 2, Batch: 80, Loss: 0.683096
Train - Epoch 2, Batch: 90, Loss: 0.683616
Train - Epoch 2, Batch: 100, Loss: 0.683958
Train - Epoch 2, Batch: 110, Loss: 0.684226
Train - Epoch 2, Batch: 120, Loss: 0.684607
Train - Epoch 2, Batch: 130, Loss: 0.684243
Train - Epoch 2, Batch: 140, Loss: 0.685268
Train - Epoch 2, Batch: 150, Loss: 0.683912
Train - Epoch 2, Batch: 160, Loss: 0.683573
Train - Epoch 2, Batch: 170, Loss: 0.684534
Train - Epoch 2, Batch: 180, Loss: 0.685255
Train - Epoch 2, Batch: 190, Loss: 0.684537
Train - Epoch 2, Batch: 200, Loss: 0.685212
Train - Epoch 2, Batch: 210, Loss: 0.683910
Train - Epoch 2, Batch: 220, Loss: 0.684766
Train - Epoch 2, Batch: 230, Loss: 0.684606
Train - Epoch 2, Batch: 240, Loss: 0.684830
Train - Epoch 2, Batch: 250, Loss: 0.685708
Train - Epoch 2, Batch: 260, Loss: 0.684186
Train - Epoch 2, Batch: 270, Loss: 0.684325
Train - Epoch 2, Batch: 280, Loss: 0.684720
Train - Epoch 2, Batch: 290, Loss: 0.684488
Train - Epoch 2, Batch: 300, Loss: 0.684310
Train - Epoch 2, Batch: 310, Loss: 0.683864
Train - Epoch 2, Batch: 320, Loss: 0.684429
Train - Epoch 2, Batch: 330, Loss: 0.682671
Train - Epoch 2, Batch: 340, Loss: 0.685111
Train - Epoch 2, Batch: 350, Loss: 0.684277
Train - Epoch 2, Batch: 360, Loss: 0.685684
Train - Epoch 2, Batch: 370, Loss: 0.684944
Train - Epoch 2, Batch: 380, Loss: 0.683833
Train - Epoch 2, Batch: 390, Loss: 0.683533
Train - Epoch 2, Batch: 400, Loss: 0.683919
Train - Epoch 2, Batch: 410, Loss: 0.684713
Train - Epoch 2, Batch: 420, Loss: 0.684967
Train - Epoch 2, Batch: 430, Loss: 0.685896
Train - Epoch 2, Batch: 440, Loss: 0.683737
Train - Epoch 2, Batch: 450, Loss: 0.684288
Train - Epoch 2, Batch: 460, Loss: 0.684597
Train - Epoch 2, Batch: 470, Loss: 0.685441
Train - Epoch 2, Batch: 480, Loss: 0.684760
Train - Epoch 2, Batch: 490, Loss: 0.684836
Train - Epoch 2, Batch: 500, Loss: 0.684433
Train - Epoch 2, Batch: 510, Loss: 0.684382
Train - Epoch 2, Batch: 520, Loss: 0.684291
Train - Epoch 2, Batch: 530, Loss: 0.682833
Train - Epoch 2, Batch: 540, Loss: 0.683321
Train - Epoch 2, Batch: 550, Loss: 0.683857/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.682625
Train - Epoch 2, Batch: 570, Loss: 0.684091
Train - Epoch 2, Batch: 580, Loss: 0.683978
Train - Epoch 2, Batch: 590, Loss: 0.685073
Train - Epoch 2, Batch: 600, Loss: 0.683669
Train - Epoch 2, Batch: 610, Loss: 0.683182
Train - Epoch 2, Batch: 620, Loss: 0.684438
Train - Epoch 2, Batch: 630, Loss: 0.684139
Train - Epoch 2, Batch: 640, Loss: 0.684293
Train - Epoch 3, Batch: 0, Loss: 0.683673
Train - Epoch 3, Batch: 10, Loss: 0.683636
Train - Epoch 3, Batch: 20, Loss: 0.685139
Train - Epoch 3, Batch: 30, Loss: 0.684318
Train - Epoch 3, Batch: 40, Loss: 0.683350
Train - Epoch 3, Batch: 50, Loss: 0.683672
Train - Epoch 3, Batch: 60, Loss: 0.684905
Train - Epoch 3, Batch: 70, Loss: 0.683728
Train - Epoch 3, Batch: 80, Loss: 0.684119
Train - Epoch 3, Batch: 90, Loss: 0.683433
Train - Epoch 3, Batch: 100, Loss: 0.684480
Train - Epoch 3, Batch: 110, Loss: 0.684640
Train - Epoch 3, Batch: 120, Loss: 0.684058
Train - Epoch 3, Batch: 130, Loss: 0.683839
Train - Epoch 3, Batch: 140, Loss: 0.684261
Train - Epoch 3, Batch: 150, Loss: 0.684132
Train - Epoch 3, Batch: 160, Loss: 0.683366
Train - Epoch 3, Batch: 170, Loss: 0.684488
Train - Epoch 3, Batch: 180, Loss: 0.685461
Train - Epoch 3, Batch: 190, Loss: 0.683706
Train - Epoch 3, Batch: 200, Loss: 0.683226
Train - Epoch 3, Batch: 210, Loss: 0.684011
Train - Epoch 3, Batch: 220, Loss: 0.683835
Train - Epoch 3, Batch: 230, Loss: 0.682908
Train - Epoch 3, Batch: 240, Loss: 0.684543
Train - Epoch 3, Batch: 250, Loss: 0.683786
Train - Epoch 3, Batch: 260, Loss: 0.683320
Train - Epoch 3, Batch: 270, Loss: 0.683937
Train - Epoch 3, Batch: 280, Loss: 0.684174
Train - Epoch 3, Batch: 290, Loss: 0.684432
Train - Epoch 3, Batch: 300, Loss: 0.683526
Train - Epoch 3, Batch: 310, Loss: 0.684143
Train - Epoch 3, Batch: 320, Loss: 0.683050
Train - Epoch 3, Batch: 330, Loss: 0.684167
Train - Epoch 3, Batch: 340, Loss: 0.685713
Train - Epoch 3, Batch: 350, Loss: 0.685090
Train - Epoch 3, Batch: 360, Loss: 0.684070
Train - Epoch 3, Batch: 370, Loss: 0.685781
Train - Epoch 3, Batch: 380, Loss: 0.684040
Train - Epoch 3, Batch: 390, Loss: 0.683304
Train - Epoch 3, Batch: 400, Loss: 0.683202
Train - Epoch 3, Batch: 410, Loss: 0.683717
Train - Epoch 3, Batch: 420, Loss: 0.683422
Train - Epoch 3, Batch: 430, Loss: 0.683551
Train - Epoch 3, Batch: 440, Loss: 0.683489
Train - Epoch 3, Batch: 450, Loss: 0.683125
Train - Epoch 3, Batch: 460, Loss: 0.684483
Train - Epoch 3, Batch: 470, Loss: 0.684188
Train - Epoch 3, Batch: 480, Loss: 0.683859
Train - Epoch 3, Batch: 490, Loss: 0.683367
Train - Epoch 3, Batch: 500, Loss: 0.683538
Train - Epoch 3, Batch: 510, Loss: 0.684013
Train - Epoch 3, Batch: 520, Loss: 0.684054
Train - Epoch 3, Batch: 530, Loss: 0.683241
Train - Epoch 3, Batch: 540, Loss: 0.681708
Train - Epoch 3, Batch: 550, Loss: 0.683128
Train - Epoch 3, Batch: 560, Loss: 0.684182
Train - Epoch 3, Batch: 570, Loss: 0.683588
Train - Epoch 3, Batch: 580, Loss: 0.683080
Train - Epoch 3, Batch: 590, Loss: 0.683549
Train - Epoch 3, Batch: 600, Loss: 0.683668
Train - Epoch 3, Batch: 610, Loss: 0.682935
Train - Epoch 3, Batch: 620, Loss: 0.683299
Train - Epoch 3, Batch: 630, Loss: 0.683543
Train - Epoch 3, Batch: 640, Loss: 0.683626
training_time:: 7.970487356185913
training time full:: 7.970526695251465
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553458
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 1050
training time is 4.932698488235474
overhead:: 0
overhead2:: 0
time_baseline:: 4.935577392578125
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553464
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.12333440780639648
overhead3:: 0.2136390209197998
overhead4:: 0.8091340065002441
overhead5:: 0
time_provenance:: 2.592916250228882
curr_diff: 0 tensor(1.9512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.3983e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3983e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553476
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.15515375137329102
overhead3:: 0.2884359359741211
overhead4:: 1.0028011798858643
overhead5:: 0
time_provenance:: 2.8788881301879883
curr_diff: 0 tensor(1.9152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.4281e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4281e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553476
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.17156434059143066
overhead3:: 0.30950069427490234
overhead4:: 1.099181890487671
overhead5:: 0
time_provenance:: 3.0902509689331055
curr_diff: 0 tensor(1.9101e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9101e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.4323e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4323e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553476
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.22384166717529297
overhead3:: 0.41837167739868164
overhead4:: 1.3511242866516113
overhead5:: 0
time_provenance:: 3.3246920108795166
curr_diff: 0 tensor(1.8670e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8670e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.4669e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4669e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553476
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.16671204566955566
overhead3:: 0.2919175624847412
overhead4:: 1.2170743942260742
overhead5:: 0
time_provenance:: 3.118109703063965
curr_diff: 0 tensor(1.2430e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2430e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8744e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8744e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553468
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.19889616966247559
overhead3:: 0.34804201126098633
overhead4:: 1.3519110679626465
overhead5:: 0
time_provenance:: 3.538346529006958
curr_diff: 0 tensor(1.2254e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2254e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8896e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8896e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553468
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.23143506050109863
overhead3:: 0.4062628746032715
overhead4:: 1.5405521392822266
overhead5:: 0
time_provenance:: 3.550940990447998
curr_diff: 0 tensor(1.2259e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2259e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8887e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8887e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553468
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.25364065170288086
overhead3:: 0.43308472633361816
overhead4:: 1.746537446975708
overhead5:: 0
time_provenance:: 4.297106742858887
curr_diff: 0 tensor(1.2136e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2136e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553468
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.23122644424438477
overhead3:: 0.39621996879577637
overhead4:: 1.7505865097045898
overhead5:: 0
time_provenance:: 3.868906021118164
curr_diff: 0 tensor(7.5728e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5728e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.9710e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9710e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553478
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.26004958152770996
overhead3:: 0.44266843795776367
overhead4:: 1.8461956977844238
overhead5:: 0
time_provenance:: 3.977916717529297
curr_diff: 0 tensor(7.2153e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2153e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1299e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1299e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553478
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.32845640182495117
overhead3:: 0.5847668647766113
overhead4:: 2.1542279720306396
overhead5:: 0
time_provenance:: 4.846515893936157
curr_diff: 0 tensor(1.0650e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0650e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.7763e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7763e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553470
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.34603238105773926
overhead3:: 0.6259329319000244
overhead4:: 2.2227563858032227
overhead5:: 0
time_provenance:: 4.8369300365448
curr_diff: 0 tensor(7.2212e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2212e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.9984e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9984e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553478
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3709568977355957
overhead3:: 0.6169354915618896
overhead4:: 2.6202564239501953
overhead5:: 0
time_provenance:: 5.349927663803101
curr_diff: 0 tensor(4.7384e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7384e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1306e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1306e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553466
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3353888988494873
overhead3:: 0.5504157543182373
overhead4:: 2.4929392337799072
overhead5:: 0
time_provenance:: 4.827666759490967
curr_diff: 0 tensor(4.7100e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7100e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1328e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1328e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553466
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.35939979553222656
overhead3:: 0.6477413177490234
overhead4:: 2.5352742671966553
overhead5:: 0
time_provenance:: 4.9594032764434814
curr_diff: 0 tensor(4.6852e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6852e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1349e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1349e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553466
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.36780643463134766
overhead3:: 0.6557385921478271
overhead4:: 2.5939226150512695
overhead5:: 0
time_provenance:: 4.969587564468384
curr_diff: 0 tensor(4.5635e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5635e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553466
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.6204733848571777
overhead3:: 1.0499930381774902
overhead4:: 3.5721914768218994
overhead5:: 0
time_provenance:: 5.823608636856079
curr_diff: 0 tensor(2.8342e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8342e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553464
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702795
Train - Epoch 0, Batch: 10, Loss: 0.694020
Train - Epoch 0, Batch: 20, Loss: 0.693278
Train - Epoch 0, Batch: 30, Loss: 0.693804
Train - Epoch 0, Batch: 40, Loss: 0.692566
Train - Epoch 0, Batch: 50, Loss: 0.690940
Train - Epoch 0, Batch: 60, Loss: 0.690762
Train - Epoch 0, Batch: 70, Loss: 0.691584
Train - Epoch 0, Batch: 80, Loss: 0.690722
Train - Epoch 0, Batch: 90, Loss: 0.689395
Train - Epoch 0, Batch: 100, Loss: 0.690476
Train - Epoch 0, Batch: 110, Loss: 0.690558
Train - Epoch 0, Batch: 120, Loss: 0.689723
Train - Epoch 0, Batch: 130, Loss: 0.689153
Train - Epoch 0, Batch: 140, Loss: 0.689969
Train - Epoch 0, Batch: 150, Loss: 0.689844
Train - Epoch 0, Batch: 160, Loss: 0.689100
Train - Epoch 0, Batch: 170, Loss: 0.690235
Train - Epoch 0, Batch: 180, Loss: 0.688854
Train - Epoch 0, Batch: 190, Loss: 0.689651
Train - Epoch 0, Batch: 200, Loss: 0.689237
Train - Epoch 0, Batch: 210, Loss: 0.688126
Train - Epoch 0, Batch: 220, Loss: 0.687838
Train - Epoch 0, Batch: 230, Loss: 0.688046
Train - Epoch 0, Batch: 240, Loss: 0.687919
Train - Epoch 0, Batch: 250, Loss: 0.687875
Train - Epoch 0, Batch: 260, Loss: 0.688799
Train - Epoch 0, Batch: 270, Loss: 0.687171
Train - Epoch 0, Batch: 280, Loss: 0.687932
Train - Epoch 0, Batch: 290, Loss: 0.687013
Train - Epoch 0, Batch: 300, Loss: 0.687769
Train - Epoch 0, Batch: 310, Loss: 0.688480
Train - Epoch 0, Batch: 320, Loss: 0.688077
Train - Epoch 0, Batch: 330, Loss: 0.687732
Train - Epoch 0, Batch: 340, Loss: 0.688271
Train - Epoch 0, Batch: 350, Loss: 0.686203
Train - Epoch 0, Batch: 360, Loss: 0.687517
Train - Epoch 0, Batch: 370, Loss: 0.686312
Train - Epoch 0, Batch: 380, Loss: 0.688098
Train - Epoch 0, Batch: 390, Loss: 0.686589
Train - Epoch 0, Batch: 400, Loss: 0.687124
Train - Epoch 0, Batch: 410, Loss: 0.688409
Train - Epoch 0, Batch: 420, Loss: 0.686718
Train - Epoch 0, Batch: 430, Loss: 0.686460
Train - Epoch 0, Batch: 440, Loss: 0.685961
Train - Epoch 0, Batch: 450, Loss: 0.686417
Train - Epoch 0, Batch: 460, Loss: 0.685527
Train - Epoch 0, Batch: 470, Loss: 0.686642
Train - Epoch 0, Batch: 480, Loss: 0.687388
Train - Epoch 0, Batch: 490, Loss: 0.686812
Train - Epoch 0, Batch: 500, Loss: 0.685658
Train - Epoch 0, Batch: 510, Loss: 0.686696
Train - Epoch 0, Batch: 520, Loss: 0.685130
Train - Epoch 0, Batch: 530, Loss: 0.687458
Train - Epoch 0, Batch: 540, Loss: 0.685856
Train - Epoch 0, Batch: 550, Loss: 0.686802
Train - Epoch 0, Batch: 560, Loss: 0.685678
Train - Epoch 0, Batch: 570, Loss: 0.687076
Train - Epoch 0, Batch: 580, Loss: 0.686584
Train - Epoch 0, Batch: 590, Loss: 0.687061
Train - Epoch 0, Batch: 600, Loss: 0.686292
Train - Epoch 0, Batch: 610, Loss: 0.686306
Train - Epoch 0, Batch: 620, Loss: 0.686363
Train - Epoch 0, Batch: 630, Loss: 0.685662
Train - Epoch 0, Batch: 640, Loss: 0.685397
Train - Epoch 1, Batch: 0, Loss: 0.685321
Train - Epoch 1, Batch: 10, Loss: 0.686322
Train - Epoch 1, Batch: 20, Loss: 0.685594
Train - Epoch 1, Batch: 30, Loss: 0.686918
Train - Epoch 1, Batch: 40, Loss: 0.684937
Train - Epoch 1, Batch: 50, Loss: 0.686224
Train - Epoch 1, Batch: 60, Loss: 0.685141
Train - Epoch 1, Batch: 70, Loss: 0.686207
Train - Epoch 1, Batch: 80, Loss: 0.685017
Train - Epoch 1, Batch: 90, Loss: 0.684757
Train - Epoch 1, Batch: 100, Loss: 0.685455
Train - Epoch 1, Batch: 110, Loss: 0.686121
Train - Epoch 1, Batch: 120, Loss: 0.684744
Train - Epoch 1, Batch: 130, Loss: 0.686229
Train - Epoch 1, Batch: 140, Loss: 0.685490
Train - Epoch 1, Batch: 150, Loss: 0.686603
Train - Epoch 1, Batch: 160, Loss: 0.685524
Train - Epoch 1, Batch: 170, Loss: 0.686706
Train - Epoch 1, Batch: 180, Loss: 0.687045
Train - Epoch 1, Batch: 190, Loss: 0.685555
Train - Epoch 1, Batch: 200, Loss: 0.685941
Train - Epoch 1, Batch: 210, Loss: 0.686236
Train - Epoch 1, Batch: 220, Loss: 0.684869
Train - Epoch 1, Batch: 230, Loss: 0.686672
Train - Epoch 1, Batch: 240, Loss: 0.686214
Train - Epoch 1, Batch: 250, Loss: 0.686511
Train - Epoch 1, Batch: 260, Loss: 0.686997
Train - Epoch 1, Batch: 270, Loss: 0.686335
Train - Epoch 1, Batch: 280, Loss: 0.684977
Train - Epoch 1, Batch: 290, Loss: 0.685837
Train - Epoch 1, Batch: 300, Loss: 0.684805
Train - Epoch 1, Batch: 310, Loss: 0.685409
Train - Epoch 1, Batch: 320, Loss: 0.684462
Train - Epoch 1, Batch: 330, Loss: 0.685336
Train - Epoch 1, Batch: 340, Loss: 0.685777
Train - Epoch 1, Batch: 350, Loss: 0.685958
Train - Epoch 1, Batch: 360, Loss: 0.684309
Train - Epoch 1, Batch: 370, Loss: 0.685724
Train - Epoch 1, Batch: 380, Loss: 0.685101
Train - Epoch 1, Batch: 390, Loss: 0.684777
Train - Epoch 1, Batch: 400, Loss: 0.685542
Train - Epoch 1, Batch: 410, Loss: 0.685855
Train - Epoch 1, Batch: 420, Loss: 0.686212
Train - Epoch 1, Batch: 430, Loss: 0.685285
Train - Epoch 1, Batch: 440, Loss: 0.684931
Train - Epoch 1, Batch: 450, Loss: 0.685300
Train - Epoch 1, Batch: 460, Loss: 0.684468
Train - Epoch 1, Batch: 470, Loss: 0.685071
Train - Epoch 1, Batch: 480, Loss: 0.684482
Train - Epoch 1, Batch: 490, Loss: 0.684886
Train - Epoch 1, Batch: 500, Loss: 0.685247
Train - Epoch 1, Batch: 510, Loss: 0.685953
Train - Epoch 1, Batch: 520, Loss: 0.685053
Train - Epoch 1, Batch: 530, Loss: 0.684414
Train - Epoch 1, Batch: 540, Loss: 0.684869
Train - Epoch 1, Batch: 550, Loss: 0.685701
Train - Epoch 1, Batch: 560, Loss: 0.684781
Train - Epoch 1, Batch: 570, Loss: 0.686219
Train - Epoch 1, Batch: 580, Loss: 0.684233
Train - Epoch 1, Batch: 590, Loss: 0.684997
Train - Epoch 1, Batch: 600, Loss: 0.684427
Train - Epoch 1, Batch: 610, Loss: 0.685531
Train - Epoch 1, Batch: 620, Loss: 0.684456
Train - Epoch 1, Batch: 630, Loss: 0.686398
Train - Epoch 1, Batch: 640, Loss: 0.684499
Train - Epoch 2, Batch: 0, Loss: 0.685534
Train - Epoch 2, Batch: 10, Loss: 0.685299
Train - Epoch 2, Batch: 20, Loss: 0.686178
Train - Epoch 2, Batch: 30, Loss: 0.684278
Train - Epoch 2, Batch: 40, Loss: 0.684672
Train - Epoch 2, Batch: 50, Loss: 0.684818
Train - Epoch 2, Batch: 60, Loss: 0.684374
Train - Epoch 2, Batch: 70, Loss: 0.685525
Train - Epoch 2, Batch: 80, Loss: 0.684845
Train - Epoch 2, Batch: 90, Loss: 0.684847
Train - Epoch 2, Batch: 100, Loss: 0.685573
Train - Epoch 2, Batch: 110, Loss: 0.685170
Train - Epoch 2, Batch: 120, Loss: 0.683732
Train - Epoch 2, Batch: 130, Loss: 0.684989
Train - Epoch 2, Batch: 140, Loss: 0.684402
Train - Epoch 2, Batch: 150, Loss: 0.684162
Train - Epoch 2, Batch: 160, Loss: 0.684894
Train - Epoch 2, Batch: 170, Loss: 0.685947
Train - Epoch 2, Batch: 180, Loss: 0.684761
Train - Epoch 2, Batch: 190, Loss: 0.685681
Train - Epoch 2, Batch: 200, Loss: 0.684170
Train - Epoch 2, Batch: 210, Loss: 0.683817
Train - Epoch 2, Batch: 220, Loss: 0.684354
Train - Epoch 2, Batch: 230, Loss: 0.683283
Train - Epoch 2, Batch: 240, Loss: 0.684589
Train - Epoch 2, Batch: 250, Loss: 0.684734
Train - Epoch 2, Batch: 260, Loss: 0.684232
Train - Epoch 2, Batch: 270, Loss: 0.685577
Train - Epoch 2, Batch: 280, Loss: 0.683819
Train - Epoch 2, Batch: 290, Loss: 0.684812
Train - Epoch 2, Batch: 300, Loss: 0.684605
Train - Epoch 2, Batch: 310, Loss: 0.684946
Train - Epoch 2, Batch: 320, Loss: 0.686090
Train - Epoch 2, Batch: 330, Loss: 0.683781
Train - Epoch 2, Batch: 340, Loss: 0.683747
Train - Epoch 2, Batch: 350, Loss: 0.685870
Train - Epoch 2, Batch: 360, Loss: 0.684431
Train - Epoch 2, Batch: 370, Loss: 0.685237
Train - Epoch 2, Batch: 380, Loss: 0.683991
Train - Epoch 2, Batch: 390, Loss: 0.684586
Train - Epoch 2, Batch: 400, Loss: 0.683710
Train - Epoch 2, Batch: 410, Loss: 0.684567
Train - Epoch 2, Batch: 420, Loss: 0.683106
Train - Epoch 2, Batch: 430, Loss: 0.684669
Train - Epoch 2, Batch: 440, Loss: 0.684116
Train - Epoch 2, Batch: 450, Loss: 0.685127
Train - Epoch 2, Batch: 460, Loss: 0.684075
Train - Epoch 2, Batch: 470, Loss: 0.683498
Train - Epoch 2, Batch: 480, Loss: 0.684811
Train - Epoch 2, Batch: 490, Loss: 0.685137
Train - Epoch 2, Batch: 500, Loss: 0.684015
Train - Epoch 2, Batch: 510, Loss: 0.684726
Train - Epoch 2, Batch: 520, Loss: 0.684512
Train - Epoch 2, Batch: 530, Loss: 0.683835
Train - Epoch 2, Batch: 540, Loss: 0.683427
Train - Epoch 2, Batch: 550, Loss: 0.683879/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684716
Train - Epoch 2, Batch: 570, Loss: 0.684678
Train - Epoch 2, Batch: 580, Loss: 0.684175
Train - Epoch 2, Batch: 590, Loss: 0.684406
Train - Epoch 2, Batch: 600, Loss: 0.685079
Train - Epoch 2, Batch: 610, Loss: 0.684887
Train - Epoch 2, Batch: 620, Loss: 0.683982
Train - Epoch 2, Batch: 630, Loss: 0.684434
Train - Epoch 2, Batch: 640, Loss: 0.684389
Train - Epoch 3, Batch: 0, Loss: 0.684045
Train - Epoch 3, Batch: 10, Loss: 0.684287
Train - Epoch 3, Batch: 20, Loss: 0.683972
Train - Epoch 3, Batch: 30, Loss: 0.685195
Train - Epoch 3, Batch: 40, Loss: 0.684266
Train - Epoch 3, Batch: 50, Loss: 0.684271
Train - Epoch 3, Batch: 60, Loss: 0.683070
Train - Epoch 3, Batch: 70, Loss: 0.684458
Train - Epoch 3, Batch: 80, Loss: 0.684264
Train - Epoch 3, Batch: 90, Loss: 0.684723
Train - Epoch 3, Batch: 100, Loss: 0.684781
Train - Epoch 3, Batch: 110, Loss: 0.682067
Train - Epoch 3, Batch: 120, Loss: 0.684171
Train - Epoch 3, Batch: 130, Loss: 0.684045
Train - Epoch 3, Batch: 140, Loss: 0.683035
Train - Epoch 3, Batch: 150, Loss: 0.684817
Train - Epoch 3, Batch: 160, Loss: 0.682608
Train - Epoch 3, Batch: 170, Loss: 0.683345
Train - Epoch 3, Batch: 180, Loss: 0.683777
Train - Epoch 3, Batch: 190, Loss: 0.683463
Train - Epoch 3, Batch: 200, Loss: 0.683955
Train - Epoch 3, Batch: 210, Loss: 0.684718
Train - Epoch 3, Batch: 220, Loss: 0.683742
Train - Epoch 3, Batch: 230, Loss: 0.683301
Train - Epoch 3, Batch: 240, Loss: 0.686188
Train - Epoch 3, Batch: 250, Loss: 0.683531
Train - Epoch 3, Batch: 260, Loss: 0.684744
Train - Epoch 3, Batch: 270, Loss: 0.684326
Train - Epoch 3, Batch: 280, Loss: 0.682914
Train - Epoch 3, Batch: 290, Loss: 0.683661
Train - Epoch 3, Batch: 300, Loss: 0.683794
Train - Epoch 3, Batch: 310, Loss: 0.684173
Train - Epoch 3, Batch: 320, Loss: 0.682578
Train - Epoch 3, Batch: 330, Loss: 0.682443
Train - Epoch 3, Batch: 340, Loss: 0.684448
Train - Epoch 3, Batch: 350, Loss: 0.683999
Train - Epoch 3, Batch: 360, Loss: 0.685118
Train - Epoch 3, Batch: 370, Loss: 0.684247
Train - Epoch 3, Batch: 380, Loss: 0.683657
Train - Epoch 3, Batch: 390, Loss: 0.683571
Train - Epoch 3, Batch: 400, Loss: 0.683754
Train - Epoch 3, Batch: 410, Loss: 0.683782
Train - Epoch 3, Batch: 420, Loss: 0.684717
Train - Epoch 3, Batch: 430, Loss: 0.683633
Train - Epoch 3, Batch: 440, Loss: 0.682637
Train - Epoch 3, Batch: 450, Loss: 0.683424
Train - Epoch 3, Batch: 460, Loss: 0.683980
Train - Epoch 3, Batch: 470, Loss: 0.684373
Train - Epoch 3, Batch: 480, Loss: 0.683604
Train - Epoch 3, Batch: 490, Loss: 0.683695
Train - Epoch 3, Batch: 500, Loss: 0.684097
Train - Epoch 3, Batch: 510, Loss: 0.683604
Train - Epoch 3, Batch: 520, Loss: 0.683175
Train - Epoch 3, Batch: 530, Loss: 0.684766
Train - Epoch 3, Batch: 540, Loss: 0.684921
Train - Epoch 3, Batch: 550, Loss: 0.684432
Train - Epoch 3, Batch: 560, Loss: 0.684374
Train - Epoch 3, Batch: 570, Loss: 0.683035
Train - Epoch 3, Batch: 580, Loss: 0.683915
Train - Epoch 3, Batch: 590, Loss: 0.683921
Train - Epoch 3, Batch: 600, Loss: 0.683727
Train - Epoch 3, Batch: 610, Loss: 0.683766
Train - Epoch 3, Batch: 620, Loss: 0.683311
Train - Epoch 3, Batch: 630, Loss: 0.682552
Train - Epoch 3, Batch: 640, Loss: 0.682773
training_time:: 7.578648090362549
training time full:: 7.578689336776733
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554826
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 1050
training time is 4.426083564758301
overhead:: 0
overhead2:: 0
time_baseline:: 4.430144548416138
curr_diff: 0 tensor(8.7890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.11997032165527344
overhead3:: 0.2079172134399414
overhead4:: 0.8241016864776611
overhead5:: 0
time_provenance:: 2.6795220375061035
curr_diff: 0 tensor(2.5516e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5516e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9718e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.13682103157043457
overhead3:: 0.22892212867736816
overhead4:: 0.9230375289916992
overhead5:: 0
time_provenance:: 2.7156617641448975
curr_diff: 0 tensor(2.5455e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5455e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9759e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9759e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.1534409523010254
overhead3:: 0.26865482330322266
overhead4:: 1.040937900543213
overhead5:: 0
time_provenance:: 2.8343050479888916
curr_diff: 0 tensor(2.5539e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5539e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9694e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9694e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.18337392807006836
overhead3:: 0.29687976837158203
overhead4:: 1.1524803638458252
overhead5:: 0
time_provenance:: 3.004948139190674
curr_diff: 0 tensor(2.5019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.0087e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0087e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.17236328125
overhead3:: 0.29006147384643555
overhead4:: 1.2489709854125977
overhead5:: 0
time_provenance:: 3.189166784286499
curr_diff: 0 tensor(1.6765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6765e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.3448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2509026527404785
overhead3:: 0.44800758361816406
overhead4:: 1.5989022254943848
overhead5:: 0
time_provenance:: 4.222883224487305
curr_diff: 0 tensor(1.6740e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6740e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.3463e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3463e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.21943354606628418
overhead3:: 0.3671293258666992
overhead4:: 1.3882110118865967
overhead5:: 0
time_provenance:: 3.491060256958008
curr_diff: 0 tensor(1.6795e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6795e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.3420e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3420e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.25034523010253906
overhead3:: 0.4558138847351074
overhead4:: 1.5441718101501465
overhead5:: 0
time_provenance:: 3.618048906326294
curr_diff: 0 tensor(1.6565e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6565e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.3579e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3579e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.26261019706726074
overhead3:: 0.44727253913879395
overhead4:: 1.878889799118042
overhead5:: 0
time_provenance:: 4.358858108520508
curr_diff: 0 tensor(9.5119e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5119e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7245e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7245e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2512853145599365
overhead3:: 0.45520639419555664
overhead4:: 1.8541412353515625
overhead5:: 0
time_provenance:: 3.9899301528930664
curr_diff: 0 tensor(9.7040e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7040e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.4773e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4773e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2552461624145508
overhead3:: 0.4312899112701416
overhead4:: 1.8857262134552002
overhead5:: 0
time_provenance:: 3.9785661697387695
curr_diff: 0 tensor(6.7150e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7150e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.4263e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4263e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3171718120574951
overhead3:: 0.5147826671600342
overhead4:: 2.0456202030181885
overhead5:: 0
time_provenance:: 4.404911041259766
curr_diff: 0 tensor(9.3272e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3272e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554826
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.32416653633117676
overhead3:: 0.5803298950195312
overhead4:: 2.461355209350586
overhead5:: 0
time_provenance:: 4.840463161468506
curr_diff: 0 tensor(5.2114e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2114e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6965e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6965e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3460702896118164
overhead3:: 0.6203353404998779
overhead4:: 2.5494492053985596
overhead5:: 0
time_provenance:: 5.05301570892334
curr_diff: 0 tensor(5.1953e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1953e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6974e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6974e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.38129305839538574
overhead3:: 0.6728994846343994
overhead4:: 2.582972526550293
overhead5:: 0
time_provenance:: 5.0902650356292725
curr_diff: 0 tensor(5.1610e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1610e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6992e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6992e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.34459471702575684
overhead3:: 0.5907349586486816
overhead4:: 2.4993350505828857
overhead5:: 0
time_provenance:: 4.7828123569488525
curr_diff: 0 tensor(5.0815e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0815e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7046e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7046e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.5922307968139648
overhead3:: 0.9811031818389893
overhead4:: 3.48567533493042
overhead5:: 0
time_provenance:: 5.628977537155151
curr_diff: 0 tensor(2.8920e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8920e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554830
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.701444
Train - Epoch 0, Batch: 10, Loss: 0.693429
Train - Epoch 0, Batch: 20, Loss: 0.693004
Train - Epoch 0, Batch: 30, Loss: 0.691854
Train - Epoch 0, Batch: 40, Loss: 0.691012
Train - Epoch 0, Batch: 50, Loss: 0.691827
Train - Epoch 0, Batch: 60, Loss: 0.689669
Train - Epoch 0, Batch: 70, Loss: 0.690272
Train - Epoch 0, Batch: 80, Loss: 0.690959
Train - Epoch 0, Batch: 90, Loss: 0.689114
Train - Epoch 0, Batch: 100, Loss: 0.689590
Train - Epoch 0, Batch: 110, Loss: 0.689517
Train - Epoch 0, Batch: 120, Loss: 0.690224
Train - Epoch 0, Batch: 130, Loss: 0.689283
Train - Epoch 0, Batch: 140, Loss: 0.689987
Train - Epoch 0, Batch: 150, Loss: 0.689503
Train - Epoch 0, Batch: 160, Loss: 0.690170
Train - Epoch 0, Batch: 170, Loss: 0.687922
Train - Epoch 0, Batch: 180, Loss: 0.688939
Train - Epoch 0, Batch: 190, Loss: 0.688493
Train - Epoch 0, Batch: 200, Loss: 0.687638
Train - Epoch 0, Batch: 210, Loss: 0.688712
Train - Epoch 0, Batch: 220, Loss: 0.689094
Train - Epoch 0, Batch: 230, Loss: 0.688405
Train - Epoch 0, Batch: 240, Loss: 0.687658
Train - Epoch 0, Batch: 250, Loss: 0.688526
Train - Epoch 0, Batch: 260, Loss: 0.688891
Train - Epoch 0, Batch: 270, Loss: 0.687748
Train - Epoch 0, Batch: 280, Loss: 0.686800
Train - Epoch 0, Batch: 290, Loss: 0.687414
Train - Epoch 0, Batch: 300, Loss: 0.688777
Train - Epoch 0, Batch: 310, Loss: 0.687503
Train - Epoch 0, Batch: 320, Loss: 0.687571
Train - Epoch 0, Batch: 330, Loss: 0.688990
Train - Epoch 0, Batch: 340, Loss: 0.687798
Train - Epoch 0, Batch: 350, Loss: 0.688626
Train - Epoch 0, Batch: 360, Loss: 0.686634
Train - Epoch 0, Batch: 370, Loss: 0.686430
Train - Epoch 0, Batch: 380, Loss: 0.687993
Train - Epoch 0, Batch: 390, Loss: 0.687457
Train - Epoch 0, Batch: 400, Loss: 0.686289
Train - Epoch 0, Batch: 410, Loss: 0.688278
Train - Epoch 0, Batch: 420, Loss: 0.687349
Train - Epoch 0, Batch: 430, Loss: 0.686919
Train - Epoch 0, Batch: 440, Loss: 0.688155
Train - Epoch 0, Batch: 450, Loss: 0.687035
Train - Epoch 0, Batch: 460, Loss: 0.687116
Train - Epoch 0, Batch: 470, Loss: 0.686722
Train - Epoch 0, Batch: 480, Loss: 0.688237
Train - Epoch 0, Batch: 490, Loss: 0.687543
Train - Epoch 0, Batch: 500, Loss: 0.686490
Train - Epoch 0, Batch: 510, Loss: 0.685779
Train - Epoch 0, Batch: 520, Loss: 0.687357
Train - Epoch 0, Batch: 530, Loss: 0.685621
Train - Epoch 0, Batch: 540, Loss: 0.687110
Train - Epoch 0, Batch: 550, Loss: 0.685504
Train - Epoch 0, Batch: 560, Loss: 0.686863
Train - Epoch 0, Batch: 570, Loss: 0.687042
Train - Epoch 0, Batch: 580, Loss: 0.687208
Train - Epoch 0, Batch: 590, Loss: 0.686943
Train - Epoch 0, Batch: 600, Loss: 0.686185
Train - Epoch 0, Batch: 610, Loss: 0.686126
Train - Epoch 0, Batch: 620, Loss: 0.687010
Train - Epoch 0, Batch: 630, Loss: 0.686557
Train - Epoch 0, Batch: 640, Loss: 0.685537
Train - Epoch 1, Batch: 0, Loss: 0.686973
Train - Epoch 1, Batch: 10, Loss: 0.685187
Train - Epoch 1, Batch: 20, Loss: 0.685044
Train - Epoch 1, Batch: 30, Loss: 0.685984
Train - Epoch 1, Batch: 40, Loss: 0.687098
Train - Epoch 1, Batch: 50, Loss: 0.686234
Train - Epoch 1, Batch: 60, Loss: 0.685668
Train - Epoch 1, Batch: 70, Loss: 0.686569
Train - Epoch 1, Batch: 80, Loss: 0.685783
Train - Epoch 1, Batch: 90, Loss: 0.686689
Train - Epoch 1, Batch: 100, Loss: 0.686646
Train - Epoch 1, Batch: 110, Loss: 0.686772
Train - Epoch 1, Batch: 120, Loss: 0.686573
Train - Epoch 1, Batch: 130, Loss: 0.686485
Train - Epoch 1, Batch: 140, Loss: 0.686965
Train - Epoch 1, Batch: 150, Loss: 0.686176
Train - Epoch 1, Batch: 160, Loss: 0.686352
Train - Epoch 1, Batch: 170, Loss: 0.686521
Train - Epoch 1, Batch: 180, Loss: 0.685425
Train - Epoch 1, Batch: 190, Loss: 0.685828
Train - Epoch 1, Batch: 200, Loss: 0.685905
Train - Epoch 1, Batch: 210, Loss: 0.686400
Train - Epoch 1, Batch: 220, Loss: 0.687162
Train - Epoch 1, Batch: 230, Loss: 0.684988
Train - Epoch 1, Batch: 240, Loss: 0.686818
Train - Epoch 1, Batch: 250, Loss: 0.684846
Train - Epoch 1, Batch: 260, Loss: 0.685316
Train - Epoch 1, Batch: 270, Loss: 0.686219
Train - Epoch 1, Batch: 280, Loss: 0.685002
Train - Epoch 1, Batch: 290, Loss: 0.685380
Train - Epoch 1, Batch: 300, Loss: 0.686222
Train - Epoch 1, Batch: 310, Loss: 0.684668
Train - Epoch 1, Batch: 320, Loss: 0.686238
Train - Epoch 1, Batch: 330, Loss: 0.686070
Train - Epoch 1, Batch: 340, Loss: 0.686237
Train - Epoch 1, Batch: 350, Loss: 0.685197
Train - Epoch 1, Batch: 360, Loss: 0.685155
Train - Epoch 1, Batch: 370, Loss: 0.686260
Train - Epoch 1, Batch: 380, Loss: 0.685829
Train - Epoch 1, Batch: 390, Loss: 0.684929
Train - Epoch 1, Batch: 400, Loss: 0.684739
Train - Epoch 1, Batch: 410, Loss: 0.684826
Train - Epoch 1, Batch: 420, Loss: 0.685525
Train - Epoch 1, Batch: 430, Loss: 0.684510
Train - Epoch 1, Batch: 440, Loss: 0.685756
Train - Epoch 1, Batch: 450, Loss: 0.685146
Train - Epoch 1, Batch: 460, Loss: 0.686704
Train - Epoch 1, Batch: 470, Loss: 0.684390
Train - Epoch 1, Batch: 480, Loss: 0.685746
Train - Epoch 1, Batch: 490, Loss: 0.685149
Train - Epoch 1, Batch: 500, Loss: 0.685806
Train - Epoch 1, Batch: 510, Loss: 0.684689
Train - Epoch 1, Batch: 520, Loss: 0.685122
Train - Epoch 1, Batch: 530, Loss: 0.685551
Train - Epoch 1, Batch: 540, Loss: 0.685024
Train - Epoch 1, Batch: 550, Loss: 0.684071
Train - Epoch 1, Batch: 560, Loss: 0.684460
Train - Epoch 1, Batch: 570, Loss: 0.685756
Train - Epoch 1, Batch: 580, Loss: 0.684842
Train - Epoch 1, Batch: 590, Loss: 0.686092
Train - Epoch 1, Batch: 600, Loss: 0.685320
Train - Epoch 1, Batch: 610, Loss: 0.684968
Train - Epoch 1, Batch: 620, Loss: 0.685252
Train - Epoch 1, Batch: 630, Loss: 0.684835
Train - Epoch 1, Batch: 640, Loss: 0.684403
Train - Epoch 2, Batch: 0, Loss: 0.684231
Train - Epoch 2, Batch: 10, Loss: 0.684246
Train - Epoch 2, Batch: 20, Loss: 0.685108
Train - Epoch 2, Batch: 30, Loss: 0.685605
Train - Epoch 2, Batch: 40, Loss: 0.684469
Train - Epoch 2, Batch: 50, Loss: 0.684110
Train - Epoch 2, Batch: 60, Loss: 0.684713
Train - Epoch 2, Batch: 70, Loss: 0.685244
Train - Epoch 2, Batch: 80, Loss: 0.685593
Train - Epoch 2, Batch: 90, Loss: 0.683883
Train - Epoch 2, Batch: 100, Loss: 0.685229
Train - Epoch 2, Batch: 110, Loss: 0.684392
Train - Epoch 2, Batch: 120, Loss: 0.685724
Train - Epoch 2, Batch: 130, Loss: 0.683927
Train - Epoch 2, Batch: 140, Loss: 0.684427
Train - Epoch 2, Batch: 150, Loss: 0.684224
Train - Epoch 2, Batch: 160, Loss: 0.685570
Train - Epoch 2, Batch: 170, Loss: 0.685390
Train - Epoch 2, Batch: 180, Loss: 0.683896
Train - Epoch 2, Batch: 190, Loss: 0.684223
Train - Epoch 2, Batch: 200, Loss: 0.685059
Train - Epoch 2, Batch: 210, Loss: 0.686054
Train - Epoch 2, Batch: 220, Loss: 0.685101
Train - Epoch 2, Batch: 230, Loss: 0.683552
Train - Epoch 2, Batch: 240, Loss: 0.685174
Train - Epoch 2, Batch: 250, Loss: 0.685515
Train - Epoch 2, Batch: 260, Loss: 0.683045
Train - Epoch 2, Batch: 270, Loss: 0.683816
Train - Epoch 2, Batch: 280, Loss: 0.685282
Train - Epoch 2, Batch: 290, Loss: 0.685534
Train - Epoch 2, Batch: 300, Loss: 0.685097
Train - Epoch 2, Batch: 310, Loss: 0.684003
Train - Epoch 2, Batch: 320, Loss: 0.684421
Train - Epoch 2, Batch: 330, Loss: 0.682817
Train - Epoch 2, Batch: 340, Loss: 0.684321
Train - Epoch 2, Batch: 350, Loss: 0.685489
Train - Epoch 2, Batch: 360, Loss: 0.684606
Train - Epoch 2, Batch: 370, Loss: 0.683948
Train - Epoch 2, Batch: 380, Loss: 0.684456
Train - Epoch 2, Batch: 390, Loss: 0.684483
Train - Epoch 2, Batch: 400, Loss: 0.683810
Train - Epoch 2, Batch: 410, Loss: 0.685704
Train - Epoch 2, Batch: 420, Loss: 0.683202
Train - Epoch 2, Batch: 430, Loss: 0.684885
Train - Epoch 2, Batch: 440, Loss: 0.685734
Train - Epoch 2, Batch: 450, Loss: 0.684162
Train - Epoch 2, Batch: 460, Loss: 0.684363
Train - Epoch 2, Batch: 470, Loss: 0.683354
Train - Epoch 2, Batch: 480, Loss: 0.684526
Train - Epoch 2, Batch: 490, Loss: 0.684661
Train - Epoch 2, Batch: 500, Loss: 0.683908
Train - Epoch 2, Batch: 510, Loss: 0.683757
Train - Epoch 2, Batch: 520, Loss: 0.683796
Train - Epoch 2, Batch: 530, Loss: 0.685322
Train - Epoch 2, Batch: 540, Loss: 0.685579
Train - Epoch 2, Batch: 550, Loss: 0.684790/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685065
Train - Epoch 2, Batch: 570, Loss: 0.683618
Train - Epoch 2, Batch: 580, Loss: 0.684442
Train - Epoch 2, Batch: 590, Loss: 0.685028
Train - Epoch 2, Batch: 600, Loss: 0.684324
Train - Epoch 2, Batch: 610, Loss: 0.683521
Train - Epoch 2, Batch: 620, Loss: 0.684028
Train - Epoch 2, Batch: 630, Loss: 0.684142
Train - Epoch 2, Batch: 640, Loss: 0.683773
Train - Epoch 3, Batch: 0, Loss: 0.685031
Train - Epoch 3, Batch: 10, Loss: 0.684093
Train - Epoch 3, Batch: 20, Loss: 0.684124
Train - Epoch 3, Batch: 30, Loss: 0.684505
Train - Epoch 3, Batch: 40, Loss: 0.683650
Train - Epoch 3, Batch: 50, Loss: 0.683012
Train - Epoch 3, Batch: 60, Loss: 0.684695
Train - Epoch 3, Batch: 70, Loss: 0.683218
Train - Epoch 3, Batch: 80, Loss: 0.684578
Train - Epoch 3, Batch: 90, Loss: 0.684398
Train - Epoch 3, Batch: 100, Loss: 0.683707
Train - Epoch 3, Batch: 110, Loss: 0.684922
Train - Epoch 3, Batch: 120, Loss: 0.683132
Train - Epoch 3, Batch: 130, Loss: 0.684446
Train - Epoch 3, Batch: 140, Loss: 0.684942
Train - Epoch 3, Batch: 150, Loss: 0.683999
Train - Epoch 3, Batch: 160, Loss: 0.684993
Train - Epoch 3, Batch: 170, Loss: 0.684487
Train - Epoch 3, Batch: 180, Loss: 0.684224
Train - Epoch 3, Batch: 190, Loss: 0.684351
Train - Epoch 3, Batch: 200, Loss: 0.684145
Train - Epoch 3, Batch: 210, Loss: 0.684799
Train - Epoch 3, Batch: 220, Loss: 0.684556
Train - Epoch 3, Batch: 230, Loss: 0.683259
Train - Epoch 3, Batch: 240, Loss: 0.684036
Train - Epoch 3, Batch: 250, Loss: 0.684206
Train - Epoch 3, Batch: 260, Loss: 0.683296
Train - Epoch 3, Batch: 270, Loss: 0.684769
Train - Epoch 3, Batch: 280, Loss: 0.683906
Train - Epoch 3, Batch: 290, Loss: 0.684274
Train - Epoch 3, Batch: 300, Loss: 0.683713
Train - Epoch 3, Batch: 310, Loss: 0.684407
Train - Epoch 3, Batch: 320, Loss: 0.684163
Train - Epoch 3, Batch: 330, Loss: 0.685350
Train - Epoch 3, Batch: 340, Loss: 0.683748
Train - Epoch 3, Batch: 350, Loss: 0.683779
Train - Epoch 3, Batch: 360, Loss: 0.684096
Train - Epoch 3, Batch: 370, Loss: 0.685244
Train - Epoch 3, Batch: 380, Loss: 0.684753
Train - Epoch 3, Batch: 390, Loss: 0.683193
Train - Epoch 3, Batch: 400, Loss: 0.683983
Train - Epoch 3, Batch: 410, Loss: 0.684564
Train - Epoch 3, Batch: 420, Loss: 0.684582
Train - Epoch 3, Batch: 430, Loss: 0.684806
Train - Epoch 3, Batch: 440, Loss: 0.684237
Train - Epoch 3, Batch: 450, Loss: 0.682796
Train - Epoch 3, Batch: 460, Loss: 0.684026
Train - Epoch 3, Batch: 470, Loss: 0.682500
Train - Epoch 3, Batch: 480, Loss: 0.683148
Train - Epoch 3, Batch: 490, Loss: 0.683442
Train - Epoch 3, Batch: 500, Loss: 0.684199
Train - Epoch 3, Batch: 510, Loss: 0.684086
Train - Epoch 3, Batch: 520, Loss: 0.684985
Train - Epoch 3, Batch: 530, Loss: 0.684261
Train - Epoch 3, Batch: 540, Loss: 0.684118
Train - Epoch 3, Batch: 550, Loss: 0.683352
Train - Epoch 3, Batch: 560, Loss: 0.683526
Train - Epoch 3, Batch: 570, Loss: 0.683895
Train - Epoch 3, Batch: 580, Loss: 0.684802
Train - Epoch 3, Batch: 590, Loss: 0.683857
Train - Epoch 3, Batch: 600, Loss: 0.683720
Train - Epoch 3, Batch: 610, Loss: 0.682925
Train - Epoch 3, Batch: 620, Loss: 0.683074
Train - Epoch 3, Batch: 630, Loss: 0.683866
Train - Epoch 3, Batch: 640, Loss: 0.684347
training_time:: 7.800788640975952
training time full:: 7.800825834274292
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554166
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 1050
training time is 4.682502269744873
overhead:: 0
overhead2:: 0
time_baseline:: 4.685464382171631
curr_diff: 0 tensor(8.6052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6052e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554164
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.12868666648864746
overhead3:: 0.22026920318603516
overhead4:: 0.8007583618164062
overhead5:: 0
time_provenance:: 2.681797504425049
curr_diff: 0 tensor(2.5132e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5132e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.16125273704528809
overhead3:: 0.2842237949371338
overhead4:: 1.0274834632873535
overhead5:: 0
time_provenance:: 3.0819060802459717
curr_diff: 0 tensor(2.4996e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4996e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9272e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9272e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.19041037559509277
overhead3:: 0.36773228645324707
overhead4:: 1.2014594078063965
overhead5:: 0
time_provenance:: 3.157365322113037
curr_diff: 0 tensor(2.4121e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4121e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.9893e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9893e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.20400357246398926
overhead3:: 0.3866276741027832
overhead4:: 1.2739906311035156
overhead5:: 0
time_provenance:: 3.240044593811035
curr_diff: 0 tensor(2.3695e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3695e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.0233e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0233e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.19837737083435059
overhead3:: 0.34467220306396484
overhead4:: 1.2742033004760742
overhead5:: 0
time_provenance:: 3.355607032775879
curr_diff: 0 tensor(1.2261e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2261e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5884e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5884e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554170
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.20890235900878906
overhead3:: 0.37133193016052246
overhead4:: 1.4584934711456299
overhead5:: 0
time_provenance:: 3.481847047805786
curr_diff: 0 tensor(1.2030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554170
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2304213047027588
overhead3:: 0.4074411392211914
overhead4:: 1.373105525970459
overhead5:: 0
time_provenance:: 3.6222195625305176
curr_diff: 0 tensor(1.1897e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1897e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6122e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6122e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554170
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.24618196487426758
overhead3:: 0.4296081066131592
overhead4:: 1.5834639072418213
overhead5:: 0
time_provenance:: 3.5752570629119873
curr_diff: 0 tensor(1.1421e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1421e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6468e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6468e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554170
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.23515892028808594
overhead3:: 0.4100475311279297
overhead4:: 1.8090713024139404
overhead5:: 0
time_provenance:: 3.9270434379577637
curr_diff: 0 tensor(7.9605e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9605e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5391e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5391e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.24742889404296875
overhead3:: 0.4273955821990967
overhead4:: 1.7475841045379639
overhead5:: 0
time_provenance:: 3.8177599906921387
curr_diff: 0 tensor(8.2951e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2951e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.4641e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4641e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2683753967285156
overhead3:: 0.47051548957824707
overhead4:: 1.9099485874176025
overhead5:: 0
time_provenance:: 4.048984527587891
curr_diff: 0 tensor(8.0299e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0299e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.4474e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4474e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554168
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2936532497406006
overhead3:: 0.5237243175506592
overhead4:: 2.047452211380005
overhead5:: 0
time_provenance:: 4.221867561340332
curr_diff: 0 tensor(7.3950e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3950e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554172
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.31659555435180664
overhead3:: 0.5657165050506592
overhead4:: 2.495659112930298
overhead5:: 0
time_provenance:: 4.8420729637146
curr_diff: 0 tensor(4.2964e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2964e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5025e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5025e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554166
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.34249281883239746
overhead3:: 0.6392478942871094
overhead4:: 2.5286037921905518
overhead5:: 0
time_provenance:: 4.936753034591675
curr_diff: 0 tensor(4.2150e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2150e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554166
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3402707576751709
overhead3:: 0.606177568435669
overhead4:: 2.5147016048431396
overhead5:: 0
time_provenance:: 4.85508131980896
curr_diff: 0 tensor(4.1515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1515e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5100e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5100e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554166
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.42425966262817383
overhead3:: 0.7151434421539307
overhead4:: 3.0085866451263428
overhead5:: 0
time_provenance:: 5.99827766418457
curr_diff: 0 tensor(4.0050e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0050e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5188e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5188e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554166
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.61026930809021
overhead3:: 0.9820356369018555
overhead4:: 3.4397401809692383
overhead5:: 0
time_provenance:: 5.592915058135986
curr_diff: 0 tensor(2.7362e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7362e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6052e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554164
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.696944
Train - Epoch 0, Batch: 10, Loss: 0.690205
Train - Epoch 0, Batch: 20, Loss: 0.688442
Train - Epoch 0, Batch: 30, Loss: 0.688701
Train - Epoch 0, Batch: 40, Loss: 0.689873
Train - Epoch 0, Batch: 50, Loss: 0.687199
Train - Epoch 0, Batch: 60, Loss: 0.688467
Train - Epoch 0, Batch: 70, Loss: 0.686883
Train - Epoch 0, Batch: 80, Loss: 0.689267
Train - Epoch 0, Batch: 90, Loss: 0.688661
Train - Epoch 0, Batch: 100, Loss: 0.687258
Train - Epoch 0, Batch: 110, Loss: 0.687567
Train - Epoch 0, Batch: 120, Loss: 0.688711
Train - Epoch 0, Batch: 130, Loss: 0.688200
Train - Epoch 0, Batch: 140, Loss: 0.687933
Train - Epoch 0, Batch: 150, Loss: 0.686890
Train - Epoch 0, Batch: 160, Loss: 0.688183
Train - Epoch 0, Batch: 170, Loss: 0.687416
Train - Epoch 0, Batch: 180, Loss: 0.687706
Train - Epoch 0, Batch: 190, Loss: 0.688059
Train - Epoch 0, Batch: 200, Loss: 0.687635
Train - Epoch 0, Batch: 210, Loss: 0.687840
Train - Epoch 0, Batch: 220, Loss: 0.687876
Train - Epoch 0, Batch: 230, Loss: 0.686742
Train - Epoch 0, Batch: 240, Loss: 0.687779
Train - Epoch 0, Batch: 250, Loss: 0.687401
Train - Epoch 0, Batch: 260, Loss: 0.687078
Train - Epoch 0, Batch: 270, Loss: 0.686951
Train - Epoch 0, Batch: 280, Loss: 0.688212
Train - Epoch 0, Batch: 290, Loss: 0.687294
Train - Epoch 0, Batch: 300, Loss: 0.686695
Train - Epoch 0, Batch: 310, Loss: 0.688248
Train - Epoch 0, Batch: 320, Loss: 0.685531
Train - Epoch 0, Batch: 330, Loss: 0.688347
Train - Epoch 0, Batch: 340, Loss: 0.686640
Train - Epoch 0, Batch: 350, Loss: 0.687793
Train - Epoch 0, Batch: 360, Loss: 0.686982
Train - Epoch 0, Batch: 370, Loss: 0.686133
Train - Epoch 0, Batch: 380, Loss: 0.687570
Train - Epoch 0, Batch: 390, Loss: 0.686910
Train - Epoch 0, Batch: 400, Loss: 0.686299
Train - Epoch 0, Batch: 410, Loss: 0.686700
Train - Epoch 0, Batch: 420, Loss: 0.686826
Train - Epoch 0, Batch: 430, Loss: 0.687485
Train - Epoch 0, Batch: 440, Loss: 0.687157
Train - Epoch 0, Batch: 450, Loss: 0.687663
Train - Epoch 0, Batch: 460, Loss: 0.686895
Train - Epoch 0, Batch: 470, Loss: 0.687003
Train - Epoch 0, Batch: 480, Loss: 0.686728
Train - Epoch 0, Batch: 490, Loss: 0.687569
Train - Epoch 0, Batch: 500, Loss: 0.686482
Train - Epoch 0, Batch: 510, Loss: 0.686891
Train - Epoch 0, Batch: 520, Loss: 0.686422
Train - Epoch 0, Batch: 530, Loss: 0.684614
Train - Epoch 0, Batch: 540, Loss: 0.685999
Train - Epoch 0, Batch: 550, Loss: 0.686491
Train - Epoch 0, Batch: 560, Loss: 0.687311
Train - Epoch 0, Batch: 570, Loss: 0.685651
Train - Epoch 0, Batch: 580, Loss: 0.686035
Train - Epoch 0, Batch: 590, Loss: 0.685422
Train - Epoch 0, Batch: 600, Loss: 0.686135
Train - Epoch 0, Batch: 610, Loss: 0.687201
Train - Epoch 0, Batch: 620, Loss: 0.687271
Train - Epoch 0, Batch: 630, Loss: 0.686082
Train - Epoch 0, Batch: 640, Loss: 0.686766
Train - Epoch 1, Batch: 0, Loss: 0.686928
Train - Epoch 1, Batch: 10, Loss: 0.686872
Train - Epoch 1, Batch: 20, Loss: 0.687184
Train - Epoch 1, Batch: 30, Loss: 0.685474
Train - Epoch 1, Batch: 40, Loss: 0.685968
Train - Epoch 1, Batch: 50, Loss: 0.685888
Train - Epoch 1, Batch: 60, Loss: 0.686342
Train - Epoch 1, Batch: 70, Loss: 0.687250
Train - Epoch 1, Batch: 80, Loss: 0.686179
Train - Epoch 1, Batch: 90, Loss: 0.685504
Train - Epoch 1, Batch: 100, Loss: 0.686145
Train - Epoch 1, Batch: 110, Loss: 0.685165
Train - Epoch 1, Batch: 120, Loss: 0.685722
Train - Epoch 1, Batch: 130, Loss: 0.685542
Train - Epoch 1, Batch: 140, Loss: 0.684969
Train - Epoch 1, Batch: 150, Loss: 0.684809
Train - Epoch 1, Batch: 160, Loss: 0.686106
Train - Epoch 1, Batch: 170, Loss: 0.685516
Train - Epoch 1, Batch: 180, Loss: 0.684597
Train - Epoch 1, Batch: 190, Loss: 0.686214
Train - Epoch 1, Batch: 200, Loss: 0.685688
Train - Epoch 1, Batch: 210, Loss: 0.684988
Train - Epoch 1, Batch: 220, Loss: 0.685845
Train - Epoch 1, Batch: 230, Loss: 0.684463
Train - Epoch 1, Batch: 240, Loss: 0.686111
Train - Epoch 1, Batch: 250, Loss: 0.685553
Train - Epoch 1, Batch: 260, Loss: 0.685166
Train - Epoch 1, Batch: 270, Loss: 0.686166
Train - Epoch 1, Batch: 280, Loss: 0.686072
Train - Epoch 1, Batch: 290, Loss: 0.685616
Train - Epoch 1, Batch: 300, Loss: 0.685150
Train - Epoch 1, Batch: 310, Loss: 0.686071
Train - Epoch 1, Batch: 320, Loss: 0.686313
Train - Epoch 1, Batch: 330, Loss: 0.685477
Train - Epoch 1, Batch: 340, Loss: 0.685693
Train - Epoch 1, Batch: 350, Loss: 0.684157
Train - Epoch 1, Batch: 360, Loss: 0.685193
Train - Epoch 1, Batch: 370, Loss: 0.686124
Train - Epoch 1, Batch: 380, Loss: 0.687212
Train - Epoch 1, Batch: 390, Loss: 0.685147
Train - Epoch 1, Batch: 400, Loss: 0.685843
Train - Epoch 1, Batch: 410, Loss: 0.685217
Train - Epoch 1, Batch: 420, Loss: 0.685992
Train - Epoch 1, Batch: 430, Loss: 0.686383
Train - Epoch 1, Batch: 440, Loss: 0.684669
Train - Epoch 1, Batch: 450, Loss: 0.685060
Train - Epoch 1, Batch: 460, Loss: 0.685245
Train - Epoch 1, Batch: 470, Loss: 0.686290
Train - Epoch 1, Batch: 480, Loss: 0.684926
Train - Epoch 1, Batch: 490, Loss: 0.685401
Train - Epoch 1, Batch: 500, Loss: 0.684838
Train - Epoch 1, Batch: 510, Loss: 0.684474
Train - Epoch 1, Batch: 520, Loss: 0.685217
Train - Epoch 1, Batch: 530, Loss: 0.685020
Train - Epoch 1, Batch: 540, Loss: 0.685500
Train - Epoch 1, Batch: 550, Loss: 0.685625
Train - Epoch 1, Batch: 560, Loss: 0.684202
Train - Epoch 1, Batch: 570, Loss: 0.684847
Train - Epoch 1, Batch: 580, Loss: 0.685704
Train - Epoch 1, Batch: 590, Loss: 0.684924
Train - Epoch 1, Batch: 600, Loss: 0.683889
Train - Epoch 1, Batch: 610, Loss: 0.685282
Train - Epoch 1, Batch: 620, Loss: 0.685069
Train - Epoch 1, Batch: 630, Loss: 0.685699
Train - Epoch 1, Batch: 640, Loss: 0.685304
Train - Epoch 2, Batch: 0, Loss: 0.684974
Train - Epoch 2, Batch: 10, Loss: 0.685663
Train - Epoch 2, Batch: 20, Loss: 0.685805
Train - Epoch 2, Batch: 30, Loss: 0.684894
Train - Epoch 2, Batch: 40, Loss: 0.684398
Train - Epoch 2, Batch: 50, Loss: 0.685332
Train - Epoch 2, Batch: 60, Loss: 0.685177
Train - Epoch 2, Batch: 70, Loss: 0.685221
Train - Epoch 2, Batch: 80, Loss: 0.685453
Train - Epoch 2, Batch: 90, Loss: 0.684631
Train - Epoch 2, Batch: 100, Loss: 0.685339
Train - Epoch 2, Batch: 110, Loss: 0.686319
Train - Epoch 2, Batch: 120, Loss: 0.685507
Train - Epoch 2, Batch: 130, Loss: 0.685945
Train - Epoch 2, Batch: 140, Loss: 0.685079
Train - Epoch 2, Batch: 150, Loss: 0.684119
Train - Epoch 2, Batch: 160, Loss: 0.685728
Train - Epoch 2, Batch: 170, Loss: 0.684758
Train - Epoch 2, Batch: 180, Loss: 0.684486
Train - Epoch 2, Batch: 190, Loss: 0.684764
Train - Epoch 2, Batch: 200, Loss: 0.684911
Train - Epoch 2, Batch: 210, Loss: 0.685387
Train - Epoch 2, Batch: 220, Loss: 0.685499
Train - Epoch 2, Batch: 230, Loss: 0.683641
Train - Epoch 2, Batch: 240, Loss: 0.685106
Train - Epoch 2, Batch: 250, Loss: 0.684898
Train - Epoch 2, Batch: 260, Loss: 0.685359
Train - Epoch 2, Batch: 270, Loss: 0.684676
Train - Epoch 2, Batch: 280, Loss: 0.684580
Train - Epoch 2, Batch: 290, Loss: 0.685450
Train - Epoch 2, Batch: 300, Loss: 0.685821
Train - Epoch 2, Batch: 310, Loss: 0.684283
Train - Epoch 2, Batch: 320, Loss: 0.684798
Train - Epoch 2, Batch: 330, Loss: 0.684802
Train - Epoch 2, Batch: 340, Loss: 0.684748
Train - Epoch 2, Batch: 350, Loss: 0.685627
Train - Epoch 2, Batch: 360, Loss: 0.684391
Train - Epoch 2, Batch: 370, Loss: 0.685171
Train - Epoch 2, Batch: 380, Loss: 0.684524
Train - Epoch 2, Batch: 390, Loss: 0.685175
Train - Epoch 2, Batch: 400, Loss: 0.683994
Train - Epoch 2, Batch: 410, Loss: 0.684895
Train - Epoch 2, Batch: 420, Loss: 0.684725
Train - Epoch 2, Batch: 430, Loss: 0.683633
Train - Epoch 2, Batch: 440, Loss: 0.684650
Train - Epoch 2, Batch: 450, Loss: 0.684163
Train - Epoch 2, Batch: 460, Loss: 0.683517
Train - Epoch 2, Batch: 470, Loss: 0.685123
Train - Epoch 2, Batch: 480, Loss: 0.683811
Train - Epoch 2, Batch: 490, Loss: 0.683466
Train - Epoch 2, Batch: 500, Loss: 0.683780
Train - Epoch 2, Batch: 510, Loss: 0.683842
Train - Epoch 2, Batch: 520, Loss: 0.684031
Train - Epoch 2, Batch: 530, Loss: 0.685277
Train - Epoch 2, Batch: 540, Loss: 0.683951
Train - Epoch 2, Batch: 550, Loss: 0.685941/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683725
Train - Epoch 2, Batch: 570, Loss: 0.684531
Train - Epoch 2, Batch: 580, Loss: 0.684222
Train - Epoch 2, Batch: 590, Loss: 0.684864
Train - Epoch 2, Batch: 600, Loss: 0.682856
Train - Epoch 2, Batch: 610, Loss: 0.684744
Train - Epoch 2, Batch: 620, Loss: 0.684847
Train - Epoch 2, Batch: 630, Loss: 0.684065
Train - Epoch 2, Batch: 640, Loss: 0.685144
Train - Epoch 3, Batch: 0, Loss: 0.683129
Train - Epoch 3, Batch: 10, Loss: 0.684293
Train - Epoch 3, Batch: 20, Loss: 0.683802
Train - Epoch 3, Batch: 30, Loss: 0.682552
Train - Epoch 3, Batch: 40, Loss: 0.684028
Train - Epoch 3, Batch: 50, Loss: 0.684171
Train - Epoch 3, Batch: 60, Loss: 0.684839
Train - Epoch 3, Batch: 70, Loss: 0.685479
Train - Epoch 3, Batch: 80, Loss: 0.683956
Train - Epoch 3, Batch: 90, Loss: 0.684035
Train - Epoch 3, Batch: 100, Loss: 0.684681
Train - Epoch 3, Batch: 110, Loss: 0.682922
Train - Epoch 3, Batch: 120, Loss: 0.684279
Train - Epoch 3, Batch: 130, Loss: 0.684276
Train - Epoch 3, Batch: 140, Loss: 0.684479
Train - Epoch 3, Batch: 150, Loss: 0.684567
Train - Epoch 3, Batch: 160, Loss: 0.683892
Train - Epoch 3, Batch: 170, Loss: 0.685098
Train - Epoch 3, Batch: 180, Loss: 0.683961
Train - Epoch 3, Batch: 190, Loss: 0.683109
Train - Epoch 3, Batch: 200, Loss: 0.683198
Train - Epoch 3, Batch: 210, Loss: 0.683314
Train - Epoch 3, Batch: 220, Loss: 0.684048
Train - Epoch 3, Batch: 230, Loss: 0.684822
Train - Epoch 3, Batch: 240, Loss: 0.683929
Train - Epoch 3, Batch: 250, Loss: 0.684259
Train - Epoch 3, Batch: 260, Loss: 0.685064
Train - Epoch 3, Batch: 270, Loss: 0.683575
Train - Epoch 3, Batch: 280, Loss: 0.683296
Train - Epoch 3, Batch: 290, Loss: 0.684457
Train - Epoch 3, Batch: 300, Loss: 0.683900
Train - Epoch 3, Batch: 310, Loss: 0.684757
Train - Epoch 3, Batch: 320, Loss: 0.685288
Train - Epoch 3, Batch: 330, Loss: 0.684161
Train - Epoch 3, Batch: 340, Loss: 0.683668
Train - Epoch 3, Batch: 350, Loss: 0.685188
Train - Epoch 3, Batch: 360, Loss: 0.683576
Train - Epoch 3, Batch: 370, Loss: 0.684576
Train - Epoch 3, Batch: 380, Loss: 0.684801
Train - Epoch 3, Batch: 390, Loss: 0.682967
Train - Epoch 3, Batch: 400, Loss: 0.684236
Train - Epoch 3, Batch: 410, Loss: 0.683804
Train - Epoch 3, Batch: 420, Loss: 0.684176
Train - Epoch 3, Batch: 430, Loss: 0.685524
Train - Epoch 3, Batch: 440, Loss: 0.683965
Train - Epoch 3, Batch: 450, Loss: 0.684602
Train - Epoch 3, Batch: 460, Loss: 0.685307
Train - Epoch 3, Batch: 470, Loss: 0.684184
Train - Epoch 3, Batch: 480, Loss: 0.681957
Train - Epoch 3, Batch: 490, Loss: 0.683262
Train - Epoch 3, Batch: 500, Loss: 0.684949
Train - Epoch 3, Batch: 510, Loss: 0.684126
Train - Epoch 3, Batch: 520, Loss: 0.683294
Train - Epoch 3, Batch: 530, Loss: 0.683721
Train - Epoch 3, Batch: 540, Loss: 0.683475
Train - Epoch 3, Batch: 550, Loss: 0.683469
Train - Epoch 3, Batch: 560, Loss: 0.684683
Train - Epoch 3, Batch: 570, Loss: 0.685009
Train - Epoch 3, Batch: 580, Loss: 0.683142
Train - Epoch 3, Batch: 590, Loss: 0.683958
Train - Epoch 3, Batch: 600, Loss: 0.684191
Train - Epoch 3, Batch: 610, Loss: 0.683302
Train - Epoch 3, Batch: 620, Loss: 0.683397
Train - Epoch 3, Batch: 630, Loss: 0.682589
Train - Epoch 3, Batch: 640, Loss: 0.683834
training_time:: 8.122092008590698
training time full:: 8.122129917144775
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554774
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 1050
training time is 5.753327131271362
overhead:: 0
overhead2:: 0
time_baseline:: 5.756537199020386
curr_diff: 0 tensor(8.9284e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9284e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554750
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.12205839157104492
overhead3:: 0.19582033157348633
overhead4:: 0.8196930885314941
overhead5:: 0
time_provenance:: 2.5868611335754395
curr_diff: 0 tensor(2.2907e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2907e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2169e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2169e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554742
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.15358877182006836
overhead3:: 0.2786519527435303
overhead4:: 0.9117052555084229
overhead5:: 0
time_provenance:: 2.7404444217681885
curr_diff: 0 tensor(2.2925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2159e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2159e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554742
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.16633820533752441
overhead3:: 0.28708910942077637
overhead4:: 1.0616579055786133
overhead5:: 0
time_provenance:: 2.9119911193847656
curr_diff: 0 tensor(2.2662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2307e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2307e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554742
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.1989431381225586
overhead3:: 0.33977174758911133
overhead4:: 1.1861002445220947
overhead5:: 0
time_provenance:: 3.107602596282959
curr_diff: 0 tensor(2.2218e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2218e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2544e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2544e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554742
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.19005966186523438
overhead3:: 0.3461446762084961
overhead4:: 1.2930705547332764
overhead5:: 0
time_provenance:: 3.291156768798828
curr_diff: 0 tensor(1.3395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3395e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9576e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9576e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554740
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.20194602012634277
overhead3:: 0.35625457763671875
overhead4:: 1.3598401546478271
overhead5:: 0
time_provenance:: 3.3452324867248535
curr_diff: 0 tensor(1.3331e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3331e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9621e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9621e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554740
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.22400426864624023
overhead3:: 0.40136289596557617
overhead4:: 1.4579839706420898
overhead5:: 0
time_provenance:: 3.4551708698272705
curr_diff: 0 tensor(1.3244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9687e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9687e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554740
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.24834752082824707
overhead3:: 0.45620226860046387
overhead4:: 1.6391189098358154
overhead5:: 0
time_provenance:: 3.6897408962249756
curr_diff: 0 tensor(1.3046e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3046e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9821e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9821e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554740
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2567727565765381
overhead3:: 0.44959092140197754
overhead4:: 1.5937843322753906
overhead5:: 0
time_provenance:: 3.7666895389556885
curr_diff: 0 tensor(9.2310e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2310e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.26618170738220215
overhead3:: 0.4635202884674072
overhead4:: 1.8606038093566895
overhead5:: 0
time_provenance:: 4.011218786239624
curr_diff: 0 tensor(7.8220e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8220e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7348e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7348e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.27837347984313965
overhead3:: 0.4999501705169678
overhead4:: 1.7579410076141357
overhead5:: 0
time_provenance:: 3.8349974155426025
curr_diff: 0 tensor(5.1828e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1828e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9490e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9490e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.30763840675354004
overhead3:: 0.5371658802032471
overhead4:: 1.9864556789398193
overhead5:: 0
time_provenance:: 4.208446025848389
curr_diff: 0 tensor(9.0258e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0258e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7583e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7583e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.33495330810546875
overhead3:: 0.54018235206604
overhead4:: 2.4315621852874756
overhead5:: 0
time_provenance:: 4.821336507797241
curr_diff: 0 tensor(4.2241e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2241e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.8646e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8646e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3352839946746826
overhead3:: 0.5733668804168701
overhead4:: 2.4101130962371826
overhead5:: 0
time_provenance:: 4.736567974090576
curr_diff: 0 tensor(4.2202e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2202e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.8646e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8646e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3315460681915283
overhead3:: 0.5649325847625732
overhead4:: 2.5224814414978027
overhead5:: 0
time_provenance:: 4.792890787124634
curr_diff: 0 tensor(4.1939e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1939e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.8662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.4129929542541504
overhead3:: 0.7764337062835693
overhead4:: 2.7637431621551514
overhead5:: 0
time_provenance:: 5.367297172546387
curr_diff: 0 tensor(4.1127e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1127e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.8707e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8707e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554746
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.6062660217285156
overhead3:: 1.063673734664917
overhead4:: 3.653407335281372
overhead5:: 0
time_provenance:: 5.903284549713135
curr_diff: 0 tensor(2.6714e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6714e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9284e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9284e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554750
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.713461
Train - Epoch 0, Batch: 10, Loss: 0.695441
Train - Epoch 0, Batch: 20, Loss: 0.695096
Train - Epoch 0, Batch: 30, Loss: 0.693420
Train - Epoch 0, Batch: 40, Loss: 0.693868
Train - Epoch 0, Batch: 50, Loss: 0.693252
Train - Epoch 0, Batch: 60, Loss: 0.692893
Train - Epoch 0, Batch: 70, Loss: 0.691966
Train - Epoch 0, Batch: 80, Loss: 0.691628
Train - Epoch 0, Batch: 90, Loss: 0.691721
Train - Epoch 0, Batch: 100, Loss: 0.692285
Train - Epoch 0, Batch: 110, Loss: 0.690716
Train - Epoch 0, Batch: 120, Loss: 0.689582
Train - Epoch 0, Batch: 130, Loss: 0.689047
Train - Epoch 0, Batch: 140, Loss: 0.689580
Train - Epoch 0, Batch: 150, Loss: 0.689682
Train - Epoch 0, Batch: 160, Loss: 0.690020
Train - Epoch 0, Batch: 170, Loss: 0.689406
Train - Epoch 0, Batch: 180, Loss: 0.689557
Train - Epoch 0, Batch: 190, Loss: 0.688897
Train - Epoch 0, Batch: 200, Loss: 0.689616
Train - Epoch 0, Batch: 210, Loss: 0.688556
Train - Epoch 0, Batch: 220, Loss: 0.688868
Train - Epoch 0, Batch: 230, Loss: 0.689862
Train - Epoch 0, Batch: 240, Loss: 0.689648
Train - Epoch 0, Batch: 250, Loss: 0.688199
Train - Epoch 0, Batch: 260, Loss: 0.688758
Train - Epoch 0, Batch: 270, Loss: 0.688618
Train - Epoch 0, Batch: 280, Loss: 0.688095
Train - Epoch 0, Batch: 290, Loss: 0.688484
Train - Epoch 0, Batch: 300, Loss: 0.688469
Train - Epoch 0, Batch: 310, Loss: 0.687457
Train - Epoch 0, Batch: 320, Loss: 0.688164
Train - Epoch 0, Batch: 330, Loss: 0.688317
Train - Epoch 0, Batch: 340, Loss: 0.688153
Train - Epoch 0, Batch: 350, Loss: 0.687546
Train - Epoch 0, Batch: 360, Loss: 0.687078
Train - Epoch 0, Batch: 370, Loss: 0.687113
Train - Epoch 0, Batch: 380, Loss: 0.687403
Train - Epoch 0, Batch: 390, Loss: 0.688143
Train - Epoch 0, Batch: 400, Loss: 0.687989
Train - Epoch 0, Batch: 410, Loss: 0.687283
Train - Epoch 0, Batch: 420, Loss: 0.687032
Train - Epoch 0, Batch: 430, Loss: 0.687436
Train - Epoch 0, Batch: 440, Loss: 0.686458
Train - Epoch 0, Batch: 450, Loss: 0.688256
Train - Epoch 0, Batch: 460, Loss: 0.687675
Train - Epoch 0, Batch: 470, Loss: 0.686101
Train - Epoch 0, Batch: 480, Loss: 0.687583
Train - Epoch 0, Batch: 490, Loss: 0.687772
Train - Epoch 0, Batch: 500, Loss: 0.687441
Train - Epoch 0, Batch: 510, Loss: 0.687257
Train - Epoch 0, Batch: 520, Loss: 0.687070
Train - Epoch 0, Batch: 530, Loss: 0.686444
Train - Epoch 0, Batch: 540, Loss: 0.687356
Train - Epoch 0, Batch: 550, Loss: 0.687000
Train - Epoch 0, Batch: 560, Loss: 0.688064
Train - Epoch 0, Batch: 570, Loss: 0.687132
Train - Epoch 0, Batch: 580, Loss: 0.685749
Train - Epoch 0, Batch: 590, Loss: 0.686693
Train - Epoch 0, Batch: 600, Loss: 0.687260
Train - Epoch 0, Batch: 610, Loss: 0.686773
Train - Epoch 0, Batch: 620, Loss: 0.685993
Train - Epoch 0, Batch: 630, Loss: 0.687118
Train - Epoch 0, Batch: 640, Loss: 0.687182
Train - Epoch 1, Batch: 0, Loss: 0.686494
Train - Epoch 1, Batch: 10, Loss: 0.686944
Train - Epoch 1, Batch: 20, Loss: 0.686837
Train - Epoch 1, Batch: 30, Loss: 0.685438
Train - Epoch 1, Batch: 40, Loss: 0.686132
Train - Epoch 1, Batch: 50, Loss: 0.686627
Train - Epoch 1, Batch: 60, Loss: 0.686458
Train - Epoch 1, Batch: 70, Loss: 0.686489
Train - Epoch 1, Batch: 80, Loss: 0.686573
Train - Epoch 1, Batch: 90, Loss: 0.686964
Train - Epoch 1, Batch: 100, Loss: 0.686572
Train - Epoch 1, Batch: 110, Loss: 0.685756
Train - Epoch 1, Batch: 120, Loss: 0.687403
Train - Epoch 1, Batch: 130, Loss: 0.687515
Train - Epoch 1, Batch: 140, Loss: 0.686758
Train - Epoch 1, Batch: 150, Loss: 0.686724
Train - Epoch 1, Batch: 160, Loss: 0.686281
Train - Epoch 1, Batch: 170, Loss: 0.686126
Train - Epoch 1, Batch: 180, Loss: 0.686498
Train - Epoch 1, Batch: 190, Loss: 0.686180
Train - Epoch 1, Batch: 200, Loss: 0.685762
Train - Epoch 1, Batch: 210, Loss: 0.685711
Train - Epoch 1, Batch: 220, Loss: 0.685846
Train - Epoch 1, Batch: 230, Loss: 0.685151
Train - Epoch 1, Batch: 240, Loss: 0.686551
Train - Epoch 1, Batch: 250, Loss: 0.686127
Train - Epoch 1, Batch: 260, Loss: 0.685670
Train - Epoch 1, Batch: 270, Loss: 0.686764
Train - Epoch 1, Batch: 280, Loss: 0.686162
Train - Epoch 1, Batch: 290, Loss: 0.685423
Train - Epoch 1, Batch: 300, Loss: 0.686097
Train - Epoch 1, Batch: 310, Loss: 0.685074
Train - Epoch 1, Batch: 320, Loss: 0.686049
Train - Epoch 1, Batch: 330, Loss: 0.686018
Train - Epoch 1, Batch: 340, Loss: 0.686152
Train - Epoch 1, Batch: 350, Loss: 0.686192
Train - Epoch 1, Batch: 360, Loss: 0.685255
Train - Epoch 1, Batch: 370, Loss: 0.685796
Train - Epoch 1, Batch: 380, Loss: 0.684809
Train - Epoch 1, Batch: 390, Loss: 0.685509
Train - Epoch 1, Batch: 400, Loss: 0.685993
Train - Epoch 1, Batch: 410, Loss: 0.686056
Train - Epoch 1, Batch: 420, Loss: 0.686369
Train - Epoch 1, Batch: 430, Loss: 0.685977
Train - Epoch 1, Batch: 440, Loss: 0.685509
Train - Epoch 1, Batch: 450, Loss: 0.684860
Train - Epoch 1, Batch: 460, Loss: 0.684820
Train - Epoch 1, Batch: 470, Loss: 0.685144
Train - Epoch 1, Batch: 480, Loss: 0.685088
Train - Epoch 1, Batch: 490, Loss: 0.686251
Train - Epoch 1, Batch: 500, Loss: 0.685134
Train - Epoch 1, Batch: 510, Loss: 0.685437
Train - Epoch 1, Batch: 520, Loss: 0.685858
Train - Epoch 1, Batch: 530, Loss: 0.685921
Train - Epoch 1, Batch: 540, Loss: 0.686537
Train - Epoch 1, Batch: 550, Loss: 0.684465
Train - Epoch 1, Batch: 560, Loss: 0.684373
Train - Epoch 1, Batch: 570, Loss: 0.685296
Train - Epoch 1, Batch: 580, Loss: 0.684494
Train - Epoch 1, Batch: 590, Loss: 0.684776
Train - Epoch 1, Batch: 600, Loss: 0.684883
Train - Epoch 1, Batch: 610, Loss: 0.685883
Train - Epoch 1, Batch: 620, Loss: 0.686183
Train - Epoch 1, Batch: 630, Loss: 0.684128
Train - Epoch 1, Batch: 640, Loss: 0.687000
Train - Epoch 2, Batch: 0, Loss: 0.685000
Train - Epoch 2, Batch: 10, Loss: 0.686487
Train - Epoch 2, Batch: 20, Loss: 0.686213
Train - Epoch 2, Batch: 30, Loss: 0.685120
Train - Epoch 2, Batch: 40, Loss: 0.684984
Train - Epoch 2, Batch: 50, Loss: 0.684467
Train - Epoch 2, Batch: 60, Loss: 0.684834
Train - Epoch 2, Batch: 70, Loss: 0.684299
Train - Epoch 2, Batch: 80, Loss: 0.685480
Train - Epoch 2, Batch: 90, Loss: 0.684937
Train - Epoch 2, Batch: 100, Loss: 0.684338
Train - Epoch 2, Batch: 110, Loss: 0.685820
Train - Epoch 2, Batch: 120, Loss: 0.685415
Train - Epoch 2, Batch: 130, Loss: 0.684651
Train - Epoch 2, Batch: 140, Loss: 0.684371
Train - Epoch 2, Batch: 150, Loss: 0.686315
Train - Epoch 2, Batch: 160, Loss: 0.685005
Train - Epoch 2, Batch: 170, Loss: 0.685793
Train - Epoch 2, Batch: 180, Loss: 0.683820
Train - Epoch 2, Batch: 190, Loss: 0.685587
Train - Epoch 2, Batch: 200, Loss: 0.684988
Train - Epoch 2, Batch: 210, Loss: 0.685399
Train - Epoch 2, Batch: 220, Loss: 0.685366
Train - Epoch 2, Batch: 230, Loss: 0.684443
Train - Epoch 2, Batch: 240, Loss: 0.684487
Train - Epoch 2, Batch: 250, Loss: 0.686513
Train - Epoch 2, Batch: 260, Loss: 0.685267
Train - Epoch 2, Batch: 270, Loss: 0.685224
Train - Epoch 2, Batch: 280, Loss: 0.684840
Train - Epoch 2, Batch: 290, Loss: 0.685320
Train - Epoch 2, Batch: 300, Loss: 0.684856
Train - Epoch 2, Batch: 310, Loss: 0.685888
Train - Epoch 2, Batch: 320, Loss: 0.684978
Train - Epoch 2, Batch: 330, Loss: 0.683541
Train - Epoch 2, Batch: 340, Loss: 0.684801
Train - Epoch 2, Batch: 350, Loss: 0.683691
Train - Epoch 2, Batch: 360, Loss: 0.685562
Train - Epoch 2, Batch: 370, Loss: 0.685136
Train - Epoch 2, Batch: 380, Loss: 0.684014
Train - Epoch 2, Batch: 390, Loss: 0.684412
Train - Epoch 2, Batch: 400, Loss: 0.684563
Train - Epoch 2, Batch: 410, Loss: 0.685340
Train - Epoch 2, Batch: 420, Loss: 0.685123
Train - Epoch 2, Batch: 430, Loss: 0.684487
Train - Epoch 2, Batch: 440, Loss: 0.683763
Train - Epoch 2, Batch: 450, Loss: 0.685560
Train - Epoch 2, Batch: 460, Loss: 0.684654
Train - Epoch 2, Batch: 470, Loss: 0.683791
Train - Epoch 2, Batch: 480, Loss: 0.684586
Train - Epoch 2, Batch: 490, Loss: 0.685288
Train - Epoch 2, Batch: 500, Loss: 0.683352
Train - Epoch 2, Batch: 510, Loss: 0.684396
Train - Epoch 2, Batch: 520, Loss: 0.684484
Train - Epoch 2, Batch: 530, Loss: 0.683909
Train - Epoch 2, Batch: 540, Loss: 0.684814
Train - Epoch 2, Batch: 550, Loss: 0.683998/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685804
Train - Epoch 2, Batch: 570, Loss: 0.684531
Train - Epoch 2, Batch: 580, Loss: 0.684530
Train - Epoch 2, Batch: 590, Loss: 0.684240
Train - Epoch 2, Batch: 600, Loss: 0.684015
Train - Epoch 2, Batch: 610, Loss: 0.685211
Train - Epoch 2, Batch: 620, Loss: 0.683393
Train - Epoch 2, Batch: 630, Loss: 0.683909
Train - Epoch 2, Batch: 640, Loss: 0.684789
Train - Epoch 3, Batch: 0, Loss: 0.683980
Train - Epoch 3, Batch: 10, Loss: 0.685511
Train - Epoch 3, Batch: 20, Loss: 0.683378
Train - Epoch 3, Batch: 30, Loss: 0.684354
Train - Epoch 3, Batch: 40, Loss: 0.684935
Train - Epoch 3, Batch: 50, Loss: 0.684490
Train - Epoch 3, Batch: 60, Loss: 0.684264
Train - Epoch 3, Batch: 70, Loss: 0.684768
Train - Epoch 3, Batch: 80, Loss: 0.684851
Train - Epoch 3, Batch: 90, Loss: 0.684382
Train - Epoch 3, Batch: 100, Loss: 0.683601
Train - Epoch 3, Batch: 110, Loss: 0.685236
Train - Epoch 3, Batch: 120, Loss: 0.684845
Train - Epoch 3, Batch: 130, Loss: 0.684016
Train - Epoch 3, Batch: 140, Loss: 0.685600
Train - Epoch 3, Batch: 150, Loss: 0.684525
Train - Epoch 3, Batch: 160, Loss: 0.684412
Train - Epoch 3, Batch: 170, Loss: 0.685044
Train - Epoch 3, Batch: 180, Loss: 0.683112
Train - Epoch 3, Batch: 190, Loss: 0.684434
Train - Epoch 3, Batch: 200, Loss: 0.683962
Train - Epoch 3, Batch: 210, Loss: 0.684045
Train - Epoch 3, Batch: 220, Loss: 0.683859
Train - Epoch 3, Batch: 230, Loss: 0.683929
Train - Epoch 3, Batch: 240, Loss: 0.683160
Train - Epoch 3, Batch: 250, Loss: 0.682226
Train - Epoch 3, Batch: 260, Loss: 0.684312
Train - Epoch 3, Batch: 270, Loss: 0.684477
Train - Epoch 3, Batch: 280, Loss: 0.683978
Train - Epoch 3, Batch: 290, Loss: 0.684724
Train - Epoch 3, Batch: 300, Loss: 0.684347
Train - Epoch 3, Batch: 310, Loss: 0.683774
Train - Epoch 3, Batch: 320, Loss: 0.683765
Train - Epoch 3, Batch: 330, Loss: 0.684155
Train - Epoch 3, Batch: 340, Loss: 0.684963
Train - Epoch 3, Batch: 350, Loss: 0.683997
Train - Epoch 3, Batch: 360, Loss: 0.683881
Train - Epoch 3, Batch: 370, Loss: 0.685662
Train - Epoch 3, Batch: 380, Loss: 0.684077
Train - Epoch 3, Batch: 390, Loss: 0.683423
Train - Epoch 3, Batch: 400, Loss: 0.683370
Train - Epoch 3, Batch: 410, Loss: 0.684612
Train - Epoch 3, Batch: 420, Loss: 0.684045
Train - Epoch 3, Batch: 430, Loss: 0.684676
Train - Epoch 3, Batch: 440, Loss: 0.683519
Train - Epoch 3, Batch: 450, Loss: 0.682831
Train - Epoch 3, Batch: 460, Loss: 0.683324
Train - Epoch 3, Batch: 470, Loss: 0.684061
Train - Epoch 3, Batch: 480, Loss: 0.684867
Train - Epoch 3, Batch: 490, Loss: 0.683275
Train - Epoch 3, Batch: 500, Loss: 0.683754
Train - Epoch 3, Batch: 510, Loss: 0.684628
Train - Epoch 3, Batch: 520, Loss: 0.682968
Train - Epoch 3, Batch: 530, Loss: 0.683466
Train - Epoch 3, Batch: 540, Loss: 0.682744
Train - Epoch 3, Batch: 550, Loss: 0.685435
Train - Epoch 3, Batch: 560, Loss: 0.683566
Train - Epoch 3, Batch: 570, Loss: 0.684660
Train - Epoch 3, Batch: 580, Loss: 0.683733
Train - Epoch 3, Batch: 590, Loss: 0.684664
Train - Epoch 3, Batch: 600, Loss: 0.684801
Train - Epoch 3, Batch: 610, Loss: 0.682805
Train - Epoch 3, Batch: 620, Loss: 0.682788
Train - Epoch 3, Batch: 630, Loss: 0.682471
Train - Epoch 3, Batch: 640, Loss: 0.685174
training_time:: 7.4407641887664795
training time full:: 7.440804481506348
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553794
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 1050
training time is 5.3160624504089355
overhead:: 0
overhead2:: 0
time_baseline:: 5.31968355178833
curr_diff: 0 tensor(7.2772e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2772e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553776
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.12385869026184082
overhead3:: 0.20067548751831055
overhead4:: 0.8468260765075684
overhead5:: 0
time_provenance:: 2.706428289413452
curr_diff: 0 tensor(2.4448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9915e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9915e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.1471254825592041
overhead3:: 0.2539675235748291
overhead4:: 0.9471786022186279
overhead5:: 0
time_provenance:: 2.844294309616089
curr_diff: 0 tensor(2.4236e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4236e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.1764683723449707
overhead3:: 0.3285689353942871
overhead4:: 1.1303353309631348
overhead5:: 0
time_provenance:: 3.0214686393737793
curr_diff: 0 tensor(2.3882e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3882e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.0307e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0307e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.22340798377990723
overhead3:: 0.40699028968811035
overhead4:: 1.271493911743164
overhead5:: 0
time_provenance:: 3.2024941444396973
curr_diff: 0 tensor(2.1370e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1370e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.2095e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2095e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.17036962509155273
overhead3:: 0.30724573135375977
overhead4:: 1.255002737045288
overhead5:: 0
time_provenance:: 3.1729001998901367
curr_diff: 0 tensor(1.2122e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2122e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.7944e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7944e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553770
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.199110746383667
overhead3:: 0.35604000091552734
overhead4:: 1.306748390197754
overhead5:: 0
time_provenance:: 3.284636974334717
curr_diff: 0 tensor(1.2108e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2108e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.7957e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7957e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553770
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.21407270431518555
overhead3:: 0.3619651794433594
overhead4:: 1.4453885555267334
overhead5:: 0
time_provenance:: 3.402789354324341
curr_diff: 0 tensor(1.1744e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1744e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8232e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8232e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553770
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.2451004981994629
overhead3:: 0.4366586208343506
overhead4:: 1.6266283988952637
overhead5:: 0
time_provenance:: 3.804084300994873
curr_diff: 0 tensor(1.0978e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0978e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8870e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8870e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553770
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.23395800590515137
overhead3:: 0.4112224578857422
overhead4:: 1.7171120643615723
overhead5:: 0
time_provenance:: 3.8084914684295654
curr_diff: 0 tensor(8.9272e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9272e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.8965e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8965e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.26736879348754883
overhead3:: 0.4624335765838623
overhead4:: 1.714134931564331
overhead5:: 0
time_provenance:: 3.91217041015625
curr_diff: 0 tensor(8.2433e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2433e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1342e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1342e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553766
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.30423808097839355
overhead3:: 0.5647749900817871
overhead4:: 2.0370593070983887
overhead5:: 0
time_provenance:: 4.322585344314575
curr_diff: 0 tensor(6.3517e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3517e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.0066e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0066e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553776
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.34156131744384766
overhead3:: 0.5554399490356445
overhead4:: 2.1905181407928467
overhead5:: 0
time_provenance:: 4.9102394580841064
curr_diff: 0 tensor(8.2678e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2678e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.9429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553768
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.30698251724243164
overhead3:: 0.5277867317199707
overhead4:: 2.41178560256958
overhead5:: 0
time_provenance:: 4.703956604003906
curr_diff: 0 tensor(4.2743e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2743e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553774
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.35490965843200684
overhead3:: 0.5761539936065674
overhead4:: 2.5378503799438477
overhead5:: 0
time_provenance:: 4.987326383590698
curr_diff: 0 tensor(4.2710e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2710e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.0891e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0891e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553774
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3703923225402832
overhead3:: 0.5976142883300781
overhead4:: 2.546290159225464
overhead5:: 0
time_provenance:: 5.012619256973267
curr_diff: 0 tensor(4.1062e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1062e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.0994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0994e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553774
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.3761734962463379
overhead3:: 0.6896030902862549
overhead4:: 2.662736177444458
overhead5:: 0
time_provenance:: 5.094130277633667
curr_diff: 0 tensor(3.9159e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9159e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1126e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1126e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553774
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 1050
max_epoch:: 4
overhead:: 0
overhead2:: 0.5637023448944092
overhead3:: 0.9318113327026367
overhead4:: 3.459568977355957
overhead5:: 0
time_provenance:: 5.5242509841918945
curr_diff: 0 tensor(1.9843e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9843e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.2772e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2772e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553776
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  higgs 0
tensor([2879488, 6295554, 2256898,  ..., 9924598, 3346426,  393215])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.755128
Train - Epoch 0, Batch: 10, Loss: 0.696862
Train - Epoch 0, Batch: 20, Loss: 0.694882
Train - Epoch 0, Batch: 30, Loss: 0.694140
Train - Epoch 0, Batch: 40, Loss: 0.694001
Train - Epoch 0, Batch: 50, Loss: 0.692468
Train - Epoch 0, Batch: 60, Loss: 0.692962
Train - Epoch 0, Batch: 70, Loss: 0.692912
Train - Epoch 0, Batch: 80, Loss: 0.690768
Train - Epoch 0, Batch: 90, Loss: 0.691396
Train - Epoch 0, Batch: 100, Loss: 0.691190
Train - Epoch 0, Batch: 110, Loss: 0.691168
Train - Epoch 0, Batch: 120, Loss: 0.691646
Train - Epoch 0, Batch: 130, Loss: 0.690829
Train - Epoch 0, Batch: 140, Loss: 0.690085
Train - Epoch 0, Batch: 150, Loss: 0.689317
Train - Epoch 0, Batch: 160, Loss: 0.689861
Train - Epoch 0, Batch: 170, Loss: 0.691307
Train - Epoch 0, Batch: 180, Loss: 0.689784
Train - Epoch 0, Batch: 190, Loss: 0.690263
Train - Epoch 0, Batch: 200, Loss: 0.689509
Train - Epoch 0, Batch: 210, Loss: 0.689965
Train - Epoch 0, Batch: 220, Loss: 0.689701
Train - Epoch 0, Batch: 230, Loss: 0.689233
Train - Epoch 0, Batch: 240, Loss: 0.689158
Train - Epoch 0, Batch: 250, Loss: 0.688997
Train - Epoch 0, Batch: 260, Loss: 0.689251
Train - Epoch 0, Batch: 270, Loss: 0.689186
Train - Epoch 0, Batch: 280, Loss: 0.689273
Train - Epoch 0, Batch: 290, Loss: 0.689145
Train - Epoch 0, Batch: 300, Loss: 0.689218
Train - Epoch 0, Batch: 310, Loss: 0.688585
Train - Epoch 0, Batch: 320, Loss: 0.688969
Train - Epoch 0, Batch: 330, Loss: 0.688790
Train - Epoch 0, Batch: 340, Loss: 0.688192
Train - Epoch 0, Batch: 350, Loss: 0.687812
Train - Epoch 0, Batch: 360, Loss: 0.688827
Train - Epoch 0, Batch: 370, Loss: 0.687546
Train - Epoch 0, Batch: 380, Loss: 0.688207
Train - Epoch 0, Batch: 390, Loss: 0.687627
Train - Epoch 0, Batch: 400, Loss: 0.688429
Train - Epoch 0, Batch: 410, Loss: 0.688352
Train - Epoch 0, Batch: 420, Loss: 0.687466
Train - Epoch 0, Batch: 430, Loss: 0.689358
Train - Epoch 0, Batch: 440, Loss: 0.687849
Train - Epoch 0, Batch: 450, Loss: 0.688089
Train - Epoch 0, Batch: 460, Loss: 0.687265
Train - Epoch 0, Batch: 470, Loss: 0.687827
Train - Epoch 0, Batch: 480, Loss: 0.687872
Train - Epoch 0, Batch: 490, Loss: 0.687961
Train - Epoch 0, Batch: 500, Loss: 0.687582
Train - Epoch 0, Batch: 510, Loss: 0.688359
Train - Epoch 0, Batch: 520, Loss: 0.687389
Train - Epoch 0, Batch: 530, Loss: 0.687642
Train - Epoch 0, Batch: 540, Loss: 0.688009
Train - Epoch 0, Batch: 550, Loss: 0.687508
Train - Epoch 0, Batch: 560, Loss: 0.687354
Train - Epoch 0, Batch: 570, Loss: 0.686651
Train - Epoch 0, Batch: 580, Loss: 0.687977
Train - Epoch 0, Batch: 590, Loss: 0.687231
Train - Epoch 0, Batch: 600, Loss: 0.687707
Train - Epoch 0, Batch: 610, Loss: 0.688274
Train - Epoch 0, Batch: 620, Loss: 0.687713
Train - Epoch 0, Batch: 630, Loss: 0.687556
Train - Epoch 0, Batch: 640, Loss: 0.687569
Train - Epoch 1, Batch: 0, Loss: 0.687196
Train - Epoch 1, Batch: 10, Loss: 0.687210
Train - Epoch 1, Batch: 20, Loss: 0.687741
Train - Epoch 1, Batch: 30, Loss: 0.687914
Train - Epoch 1, Batch: 40, Loss: 0.687502
Train - Epoch 1, Batch: 50, Loss: 0.687569
Train - Epoch 1, Batch: 60, Loss: 0.687175
Train - Epoch 1, Batch: 70, Loss: 0.687680
Train - Epoch 1, Batch: 80, Loss: 0.687732
Train - Epoch 1, Batch: 90, Loss: 0.687899
Train - Epoch 1, Batch: 100, Loss: 0.686853
Train - Epoch 1, Batch: 110, Loss: 0.687685
Train - Epoch 1, Batch: 120, Loss: 0.687026
Train - Epoch 1, Batch: 130, Loss: 0.687308
Train - Epoch 1, Batch: 140, Loss: 0.686917
Train - Epoch 1, Batch: 150, Loss: 0.686595
Train - Epoch 1, Batch: 160, Loss: 0.685569
Train - Epoch 1, Batch: 170, Loss: 0.686616
Train - Epoch 1, Batch: 180, Loss: 0.686394
Train - Epoch 1, Batch: 190, Loss: 0.686501
Train - Epoch 1, Batch: 200, Loss: 0.686723
Train - Epoch 1, Batch: 210, Loss: 0.687466
Train - Epoch 1, Batch: 220, Loss: 0.685477
Train - Epoch 1, Batch: 230, Loss: 0.685869
Train - Epoch 1, Batch: 240, Loss: 0.686166
Train - Epoch 1, Batch: 250, Loss: 0.686427
Train - Epoch 1, Batch: 260, Loss: 0.687427
Train - Epoch 1, Batch: 270, Loss: 0.686683
Train - Epoch 1, Batch: 280, Loss: 0.685716
Train - Epoch 1, Batch: 290, Loss: 0.686906
Train - Epoch 1, Batch: 300, Loss: 0.686621
Train - Epoch 1, Batch: 310, Loss: 0.686991
Train - Epoch 1, Batch: 320, Loss: 0.686748
Train - Epoch 1, Batch: 330, Loss: 0.686917
Train - Epoch 1, Batch: 340, Loss: 0.687598
Train - Epoch 1, Batch: 350, Loss: 0.685891
Train - Epoch 1, Batch: 360, Loss: 0.686220
Train - Epoch 1, Batch: 370, Loss: 0.685818
Train - Epoch 1, Batch: 380, Loss: 0.685035
Train - Epoch 1, Batch: 390, Loss: 0.686363
Train - Epoch 1, Batch: 400, Loss: 0.685619
Train - Epoch 1, Batch: 410, Loss: 0.687032
Train - Epoch 1, Batch: 420, Loss: 0.686839
Train - Epoch 1, Batch: 430, Loss: 0.686251
Train - Epoch 1, Batch: 440, Loss: 0.685674
Train - Epoch 1, Batch: 450, Loss: 0.686686
Train - Epoch 1, Batch: 460, Loss: 0.687599
Train - Epoch 1, Batch: 470, Loss: 0.685438
Train - Epoch 1, Batch: 480, Loss: 0.686517
Train - Epoch 1, Batch: 490, Loss: 0.686473
Train - Epoch 1, Batch: 500, Loss: 0.685822
Train - Epoch 1, Batch: 510, Loss: 0.685493
Train - Epoch 1, Batch: 520, Loss: 0.684299
Train - Epoch 1, Batch: 530, Loss: 0.685731
Train - Epoch 1, Batch: 540, Loss: 0.685845
Train - Epoch 1, Batch: 550, Loss: 0.685606
Train - Epoch 1, Batch: 560, Loss: 0.687102
Train - Epoch 1, Batch: 570, Loss: 0.685361
Train - Epoch 1, Batch: 580, Loss: 0.686535
Train - Epoch 1, Batch: 590, Loss: 0.685257
Train - Epoch 1, Batch: 600, Loss: 0.686751
Train - Epoch 1, Batch: 610, Loss: 0.684509
Train - Epoch 1, Batch: 620, Loss: 0.685320
Train - Epoch 1, Batch: 630, Loss: 0.684631
Train - Epoch 1, Batch: 640, Loss: 0.685139
Train - Epoch 2, Batch: 0, Loss: 0.685901
Train - Epoch 2, Batch: 10, Loss: 0.685478
Train - Epoch 2, Batch: 20, Loss: 0.685807
Train - Epoch 2, Batch: 30, Loss: 0.685341
Train - Epoch 2, Batch: 40, Loss: 0.685074
Train - Epoch 2, Batch: 50, Loss: 0.686311
Train - Epoch 2, Batch: 60, Loss: 0.685239
Train - Epoch 2, Batch: 70, Loss: 0.685182
Train - Epoch 2, Batch: 80, Loss: 0.685220
Train - Epoch 2, Batch: 90, Loss: 0.686160
Train - Epoch 2, Batch: 100, Loss: 0.685644
Train - Epoch 2, Batch: 110, Loss: 0.685843
Train - Epoch 2, Batch: 120, Loss: 0.685856
Train - Epoch 2, Batch: 130, Loss: 0.684622
Train - Epoch 2, Batch: 140, Loss: 0.686376
Train - Epoch 2, Batch: 150, Loss: 0.686033
Train - Epoch 2, Batch: 160, Loss: 0.685424
Train - Epoch 2, Batch: 170, Loss: 0.684467
Train - Epoch 2, Batch: 180, Loss: 0.686029
Train - Epoch 2, Batch: 190, Loss: 0.683883
Train - Epoch 2, Batch: 200, Loss: 0.683829
Train - Epoch 2, Batch: 210, Loss: 0.685445
Train - Epoch 2, Batch: 220, Loss: 0.683944
Train - Epoch 2, Batch: 230, Loss: 0.685126
Train - Epoch 2, Batch: 240, Loss: 0.685763
Train - Epoch 2, Batch: 250, Loss: 0.685735
Train - Epoch 2, Batch: 260, Loss: 0.686104
Train - Epoch 2, Batch: 270, Loss: 0.684427
Train - Epoch 2, Batch: 280, Loss: 0.685091
Train - Epoch 2, Batch: 290, Loss: 0.686081
Train - Epoch 2, Batch: 300, Loss: 0.685570
Train - Epoch 2, Batch: 310, Loss: 0.684674
Train - Epoch 2, Batch: 320, Loss: 0.684907
Train - Epoch 2, Batch: 330, Loss: 0.684091
Train - Epoch 2, Batch: 340, Loss: 0.685077
Train - Epoch 2, Batch: 350, Loss: 0.684889
Train - Epoch 2, Batch: 360, Loss: 0.685579
Train - Epoch 2, Batch: 370, Loss: 0.685163
Train - Epoch 2, Batch: 380, Loss: 0.685415
Train - Epoch 2, Batch: 390, Loss: 0.684761
Train - Epoch 2, Batch: 400, Loss: 0.685557
Train - Epoch 2, Batch: 410, Loss: 0.685048
Train - Epoch 2, Batch: 420, Loss: 0.685472
Train - Epoch 2, Batch: 430, Loss: 0.684272
Train - Epoch 2, Batch: 440, Loss: 0.685543
Train - Epoch 2, Batch: 450, Loss: 0.685310
Train - Epoch 2, Batch: 460, Loss: 0.684421
Train - Epoch 2, Batch: 470, Loss: 0.684135
Train - Epoch 2, Batch: 480, Loss: 0.684863
Train - Epoch 2, Batch: 490, Loss: 0.685441
Train - Epoch 2, Batch: 500, Loss: 0.684953
Train - Epoch 2, Batch: 510, Loss: 0.684670
Train - Epoch 2, Batch: 520, Loss: 0.685007
Train - Epoch 2, Batch: 530, Loss: 0.684191
Train - Epoch 2, Batch: 540, Loss: 0.684761
Train - Epoch 2, Batch: 550, Loss: 0.684096/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685156
Train - Epoch 2, Batch: 570, Loss: 0.685064
Train - Epoch 2, Batch: 580, Loss: 0.686322
Train - Epoch 2, Batch: 590, Loss: 0.683104
Train - Epoch 2, Batch: 600, Loss: 0.684832
Train - Epoch 2, Batch: 610, Loss: 0.684284
Train - Epoch 2, Batch: 620, Loss: 0.682945
Train - Epoch 2, Batch: 630, Loss: 0.684028
Train - Epoch 2, Batch: 640, Loss: 0.684193
Train - Epoch 3, Batch: 0, Loss: 0.685282
Train - Epoch 3, Batch: 10, Loss: 0.684345
Train - Epoch 3, Batch: 20, Loss: 0.684874
Train - Epoch 3, Batch: 30, Loss: 0.684534
Train - Epoch 3, Batch: 40, Loss: 0.684974
Train - Epoch 3, Batch: 50, Loss: 0.685993
Train - Epoch 3, Batch: 60, Loss: 0.685442
Train - Epoch 3, Batch: 70, Loss: 0.684737
Train - Epoch 3, Batch: 80, Loss: 0.685223
Train - Epoch 3, Batch: 90, Loss: 0.684449
Train - Epoch 3, Batch: 100, Loss: 0.685400
Train - Epoch 3, Batch: 110, Loss: 0.682789
Train - Epoch 3, Batch: 120, Loss: 0.684195
Train - Epoch 3, Batch: 130, Loss: 0.686051
Train - Epoch 3, Batch: 140, Loss: 0.685735
Train - Epoch 3, Batch: 150, Loss: 0.684937
Train - Epoch 3, Batch: 160, Loss: 0.684955
Train - Epoch 3, Batch: 170, Loss: 0.684954
Train - Epoch 3, Batch: 180, Loss: 0.684803
Train - Epoch 3, Batch: 190, Loss: 0.684819
Train - Epoch 3, Batch: 200, Loss: 0.684717
Train - Epoch 3, Batch: 210, Loss: 0.684078
Train - Epoch 3, Batch: 220, Loss: 0.685085
Train - Epoch 3, Batch: 230, Loss: 0.685235
Train - Epoch 3, Batch: 240, Loss: 0.684789
Train - Epoch 3, Batch: 250, Loss: 0.684166
Train - Epoch 3, Batch: 260, Loss: 0.684935
Train - Epoch 3, Batch: 270, Loss: 0.684649
Train - Epoch 3, Batch: 280, Loss: 0.684777
Train - Epoch 3, Batch: 290, Loss: 0.684527
Train - Epoch 3, Batch: 300, Loss: 0.684834
Train - Epoch 3, Batch: 310, Loss: 0.684586
Train - Epoch 3, Batch: 320, Loss: 0.684161
Train - Epoch 3, Batch: 330, Loss: 0.684448
Train - Epoch 3, Batch: 340, Loss: 0.683351
Train - Epoch 3, Batch: 350, Loss: 0.684488
Train - Epoch 3, Batch: 360, Loss: 0.683637
Train - Epoch 3, Batch: 370, Loss: 0.684902
Train - Epoch 3, Batch: 380, Loss: 0.685155
Train - Epoch 3, Batch: 390, Loss: 0.684493
Train - Epoch 3, Batch: 400, Loss: 0.683147
Train - Epoch 3, Batch: 410, Loss: 0.683621
Train - Epoch 3, Batch: 420, Loss: 0.682728
Train - Epoch 3, Batch: 430, Loss: 0.684307
Train - Epoch 3, Batch: 440, Loss: 0.684256
Train - Epoch 3, Batch: 450, Loss: 0.684547
Train - Epoch 3, Batch: 460, Loss: 0.684722
Train - Epoch 3, Batch: 470, Loss: 0.683998
Train - Epoch 3, Batch: 480, Loss: 0.685338
Train - Epoch 3, Batch: 490, Loss: 0.684176
Train - Epoch 3, Batch: 500, Loss: 0.683293
Train - Epoch 3, Batch: 510, Loss: 0.684337
Train - Epoch 3, Batch: 520, Loss: 0.683328
Train - Epoch 3, Batch: 530, Loss: 0.684036
Train - Epoch 3, Batch: 540, Loss: 0.684075
Train - Epoch 3, Batch: 550, Loss: 0.684232
Train - Epoch 3, Batch: 560, Loss: 0.684113
Train - Epoch 3, Batch: 570, Loss: 0.685163
Train - Epoch 3, Batch: 580, Loss: 0.683613
Train - Epoch 3, Batch: 590, Loss: 0.684777
Train - Epoch 3, Batch: 600, Loss: 0.685000
Train - Epoch 3, Batch: 610, Loss: 0.684364
Train - Epoch 3, Batch: 620, Loss: 0.681822
Train - Epoch 3, Batch: 630, Loss: 0.683878
Train - Epoch 3, Batch: 640, Loss: 0.683693
training_time:: 7.495673179626465
training time full:: 7.495713233947754
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.551698
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 2100
training time is 5.2356767654418945
overhead:: 0
overhead2:: 0
time_baseline:: 5.238874673843384
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551672
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.13404440879821777
overhead3:: 0.2136096954345703
overhead4:: 0.8317983150482178
overhead5:: 0
time_provenance:: 2.766152858734131
curr_diff: 0 tensor(4.8952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8952e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.1370e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1370e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551672
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.19413304328918457
overhead3:: 0.28456830978393555
overhead4:: 1.0064878463745117
overhead5:: 0
time_provenance:: 3.053499221801758
curr_diff: 0 tensor(4.8617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.1565e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1565e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551672
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.20466184616088867
overhead3:: 0.2962794303894043
overhead4:: 1.1247243881225586
overhead5:: 0
time_provenance:: 3.0980310440063477
curr_diff: 0 tensor(4.7492e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7492e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.2225e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2225e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2101607322692871
overhead3:: 0.28166627883911133
overhead4:: 1.131441354751587
overhead5:: 0
time_provenance:: 3.147141695022583
curr_diff: 0 tensor(4.7647e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7647e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.2131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.20090723037719727
overhead3:: 0.29750657081604004
overhead4:: 1.2241618633270264
overhead5:: 0
time_provenance:: 3.2737514972686768
curr_diff: 0 tensor(2.6292e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6292e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22957825660705566
overhead3:: 0.30568957328796387
overhead4:: 1.2030558586120605
overhead5:: 0
time_provenance:: 3.2702441215515137
curr_diff: 0 tensor(2.6148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.24103975296020508
overhead3:: 0.3377983570098877
overhead4:: 1.3916230201721191
overhead5:: 0
time_provenance:: 3.497056722640991
curr_diff: 0 tensor(2.5494e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5494e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.35619664192199707
overhead3:: 0.5345761775970459
overhead4:: 1.863290548324585
overhead5:: 0
time_provenance:: 4.762592554092407
curr_diff: 0 tensor(2.5387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5387e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.271040678024292
overhead3:: 0.3789517879486084
overhead4:: 1.7286126613616943
overhead5:: 0
time_provenance:: 3.9355649948120117
curr_diff: 0 tensor(1.3807e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3807e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.37798595428466797
overhead3:: 0.5211930274963379
overhead4:: 2.0933141708374023
overhead5:: 0
time_provenance:: 5.073384761810303
curr_diff: 0 tensor(1.9189e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9189e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3329639434814453
overhead3:: 0.48550963401794434
overhead4:: 1.9159421920776367
overhead5:: 0
time_provenance:: 4.220337629318237
curr_diff: 0 tensor(1.3848e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3848e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551672
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.33121681213378906
overhead3:: 0.483412504196167
overhead4:: 1.8875486850738525
overhead5:: 0
time_provenance:: 4.211260080337524
curr_diff: 0 tensor(1.3317e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3317e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551676
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.41456079483032227
overhead3:: 0.5936555862426758
overhead4:: 2.446104049682617
overhead5:: 0
time_provenance:: 5.031168222427368
curr_diff: 0 tensor(7.6515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6515e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.47168421745300293
overhead3:: 0.6727478504180908
overhead4:: 2.6695022583007812
overhead5:: 0
time_provenance:: 5.735471963882446
curr_diff: 0 tensor(7.5987e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5987e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.48407578468322754
overhead3:: 0.7117486000061035
overhead4:: 2.7510907649993896
overhead5:: 0
time_provenance:: 5.802277088165283
curr_diff: 0 tensor(7.3400e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3400e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.41722631454467773
overhead3:: 0.5869855880737305
overhead4:: 2.551115036010742
overhead5:: 0
time_provenance:: 4.968188047409058
curr_diff: 0 tensor(7.2118e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2118e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551674
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.8073384761810303
overhead3:: 1.188173770904541
overhead4:: 3.767662763595581
overhead5:: 0
time_provenance:: 6.368591547012329
curr_diff: 0 tensor(2.4026e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4026e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551672
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.694517
Train - Epoch 0, Batch: 10, Loss: 0.693243
Train - Epoch 0, Batch: 20, Loss: 0.693132
Train - Epoch 0, Batch: 30, Loss: 0.693084
Train - Epoch 0, Batch: 40, Loss: 0.692899
Train - Epoch 0, Batch: 50, Loss: 0.691284
Train - Epoch 0, Batch: 60, Loss: 0.691186
Train - Epoch 0, Batch: 70, Loss: 0.690704
Train - Epoch 0, Batch: 80, Loss: 0.690924
Train - Epoch 0, Batch: 90, Loss: 0.690902
Train - Epoch 0, Batch: 100, Loss: 0.690030
Train - Epoch 0, Batch: 110, Loss: 0.690313
Train - Epoch 0, Batch: 120, Loss: 0.691554
Train - Epoch 0, Batch: 130, Loss: 0.691099
Train - Epoch 0, Batch: 140, Loss: 0.689599
Train - Epoch 0, Batch: 150, Loss: 0.690609
Train - Epoch 0, Batch: 160, Loss: 0.688902
Train - Epoch 0, Batch: 170, Loss: 0.689499
Train - Epoch 0, Batch: 180, Loss: 0.689281
Train - Epoch 0, Batch: 190, Loss: 0.688979
Train - Epoch 0, Batch: 200, Loss: 0.689693
Train - Epoch 0, Batch: 210, Loss: 0.688319
Train - Epoch 0, Batch: 220, Loss: 0.688534
Train - Epoch 0, Batch: 230, Loss: 0.689957
Train - Epoch 0, Batch: 240, Loss: 0.688333
Train - Epoch 0, Batch: 250, Loss: 0.689423
Train - Epoch 0, Batch: 260, Loss: 0.687917
Train - Epoch 0, Batch: 270, Loss: 0.689857
Train - Epoch 0, Batch: 280, Loss: 0.688673
Train - Epoch 0, Batch: 290, Loss: 0.688365
Train - Epoch 0, Batch: 300, Loss: 0.688154
Train - Epoch 0, Batch: 310, Loss: 0.688436
Train - Epoch 0, Batch: 320, Loss: 0.688253
Train - Epoch 0, Batch: 330, Loss: 0.688722
Train - Epoch 0, Batch: 340, Loss: 0.687795
Train - Epoch 0, Batch: 350, Loss: 0.687182
Train - Epoch 0, Batch: 360, Loss: 0.687863
Train - Epoch 0, Batch: 370, Loss: 0.688731
Train - Epoch 0, Batch: 380, Loss: 0.688339
Train - Epoch 0, Batch: 390, Loss: 0.687869
Train - Epoch 0, Batch: 400, Loss: 0.688843
Train - Epoch 0, Batch: 410, Loss: 0.687349
Train - Epoch 0, Batch: 420, Loss: 0.687971
Train - Epoch 0, Batch: 430, Loss: 0.688775
Train - Epoch 0, Batch: 440, Loss: 0.688002
Train - Epoch 0, Batch: 450, Loss: 0.688057
Train - Epoch 0, Batch: 460, Loss: 0.688603
Train - Epoch 0, Batch: 470, Loss: 0.687457
Train - Epoch 0, Batch: 480, Loss: 0.687917
Train - Epoch 0, Batch: 490, Loss: 0.688144
Train - Epoch 0, Batch: 500, Loss: 0.687344
Train - Epoch 0, Batch: 510, Loss: 0.686196
Train - Epoch 0, Batch: 520, Loss: 0.686682
Train - Epoch 0, Batch: 530, Loss: 0.688013
Train - Epoch 0, Batch: 540, Loss: 0.688469
Train - Epoch 0, Batch: 550, Loss: 0.686790
Train - Epoch 0, Batch: 560, Loss: 0.687310
Train - Epoch 0, Batch: 570, Loss: 0.687789
Train - Epoch 0, Batch: 580, Loss: 0.686414
Train - Epoch 0, Batch: 590, Loss: 0.686035
Train - Epoch 0, Batch: 600, Loss: 0.687631
Train - Epoch 0, Batch: 610, Loss: 0.687055
Train - Epoch 0, Batch: 620, Loss: 0.686779
Train - Epoch 0, Batch: 630, Loss: 0.687041
Train - Epoch 0, Batch: 640, Loss: 0.687773
Train - Epoch 1, Batch: 0, Loss: 0.687138
Train - Epoch 1, Batch: 10, Loss: 0.686564
Train - Epoch 1, Batch: 20, Loss: 0.686809
Train - Epoch 1, Batch: 30, Loss: 0.686958
Train - Epoch 1, Batch: 40, Loss: 0.687295
Train - Epoch 1, Batch: 50, Loss: 0.687012
Train - Epoch 1, Batch: 60, Loss: 0.686321
Train - Epoch 1, Batch: 70, Loss: 0.686957
Train - Epoch 1, Batch: 80, Loss: 0.686111
Train - Epoch 1, Batch: 90, Loss: 0.687622
Train - Epoch 1, Batch: 100, Loss: 0.687219
Train - Epoch 1, Batch: 110, Loss: 0.686999
Train - Epoch 1, Batch: 120, Loss: 0.685657
Train - Epoch 1, Batch: 130, Loss: 0.685513
Train - Epoch 1, Batch: 140, Loss: 0.687930
Train - Epoch 1, Batch: 150, Loss: 0.686361
Train - Epoch 1, Batch: 160, Loss: 0.687491
Train - Epoch 1, Batch: 170, Loss: 0.686172
Train - Epoch 1, Batch: 180, Loss: 0.687204
Train - Epoch 1, Batch: 190, Loss: 0.685885
Train - Epoch 1, Batch: 200, Loss: 0.687050
Train - Epoch 1, Batch: 210, Loss: 0.685612
Train - Epoch 1, Batch: 220, Loss: 0.685811
Train - Epoch 1, Batch: 230, Loss: 0.685816
Train - Epoch 1, Batch: 240, Loss: 0.686511
Train - Epoch 1, Batch: 250, Loss: 0.685981
Train - Epoch 1, Batch: 260, Loss: 0.687664
Train - Epoch 1, Batch: 270, Loss: 0.684863
Train - Epoch 1, Batch: 280, Loss: 0.686203
Train - Epoch 1, Batch: 290, Loss: 0.685559
Train - Epoch 1, Batch: 300, Loss: 0.687163
Train - Epoch 1, Batch: 310, Loss: 0.685819
Train - Epoch 1, Batch: 320, Loss: 0.686836
Train - Epoch 1, Batch: 330, Loss: 0.687162
Train - Epoch 1, Batch: 340, Loss: 0.685993
Train - Epoch 1, Batch: 350, Loss: 0.685372
Train - Epoch 1, Batch: 360, Loss: 0.685337
Train - Epoch 1, Batch: 370, Loss: 0.686291
Train - Epoch 1, Batch: 380, Loss: 0.685771
Train - Epoch 1, Batch: 390, Loss: 0.686625
Train - Epoch 1, Batch: 400, Loss: 0.686153
Train - Epoch 1, Batch: 410, Loss: 0.685405
Train - Epoch 1, Batch: 420, Loss: 0.685716
Train - Epoch 1, Batch: 430, Loss: 0.685138
Train - Epoch 1, Batch: 440, Loss: 0.686421
Train - Epoch 1, Batch: 450, Loss: 0.685909
Train - Epoch 1, Batch: 460, Loss: 0.684419
Train - Epoch 1, Batch: 470, Loss: 0.686334
Train - Epoch 1, Batch: 480, Loss: 0.685792
Train - Epoch 1, Batch: 490, Loss: 0.686262
Train - Epoch 1, Batch: 500, Loss: 0.687690
Train - Epoch 1, Batch: 510, Loss: 0.685425
Train - Epoch 1, Batch: 520, Loss: 0.684919
Train - Epoch 1, Batch: 530, Loss: 0.684825
Train - Epoch 1, Batch: 540, Loss: 0.685785
Train - Epoch 1, Batch: 550, Loss: 0.684262
Train - Epoch 1, Batch: 560, Loss: 0.685165
Train - Epoch 1, Batch: 570, Loss: 0.685102
Train - Epoch 1, Batch: 580, Loss: 0.685445
Train - Epoch 1, Batch: 590, Loss: 0.685531
Train - Epoch 1, Batch: 600, Loss: 0.685423
Train - Epoch 1, Batch: 610, Loss: 0.685273
Train - Epoch 1, Batch: 620, Loss: 0.685250
Train - Epoch 1, Batch: 630, Loss: 0.684312
Train - Epoch 1, Batch: 640, Loss: 0.685908
Train - Epoch 2, Batch: 0, Loss: 0.687012
Train - Epoch 2, Batch: 10, Loss: 0.686802
Train - Epoch 2, Batch: 20, Loss: 0.686449
Train - Epoch 2, Batch: 30, Loss: 0.684992
Train - Epoch 2, Batch: 40, Loss: 0.685408
Train - Epoch 2, Batch: 50, Loss: 0.685022
Train - Epoch 2, Batch: 60, Loss: 0.685333
Train - Epoch 2, Batch: 70, Loss: 0.684671
Train - Epoch 2, Batch: 80, Loss: 0.683693
Train - Epoch 2, Batch: 90, Loss: 0.684695
Train - Epoch 2, Batch: 100, Loss: 0.684971
Train - Epoch 2, Batch: 110, Loss: 0.685200
Train - Epoch 2, Batch: 120, Loss: 0.684047
Train - Epoch 2, Batch: 130, Loss: 0.685233
Train - Epoch 2, Batch: 140, Loss: 0.686016
Train - Epoch 2, Batch: 150, Loss: 0.685668
Train - Epoch 2, Batch: 160, Loss: 0.685303
Train - Epoch 2, Batch: 170, Loss: 0.685751
Train - Epoch 2, Batch: 180, Loss: 0.684834
Train - Epoch 2, Batch: 190, Loss: 0.684939
Train - Epoch 2, Batch: 200, Loss: 0.684436
Train - Epoch 2, Batch: 210, Loss: 0.685038
Train - Epoch 2, Batch: 220, Loss: 0.685325
Train - Epoch 2, Batch: 230, Loss: 0.684711
Train - Epoch 2, Batch: 240, Loss: 0.683533
Train - Epoch 2, Batch: 250, Loss: 0.684285
Train - Epoch 2, Batch: 260, Loss: 0.685780
Train - Epoch 2, Batch: 270, Loss: 0.685088
Train - Epoch 2, Batch: 280, Loss: 0.684975
Train - Epoch 2, Batch: 290, Loss: 0.684950
Train - Epoch 2, Batch: 300, Loss: 0.684121
Train - Epoch 2, Batch: 310, Loss: 0.684301
Train - Epoch 2, Batch: 320, Loss: 0.684836
Train - Epoch 2, Batch: 330, Loss: 0.684773
Train - Epoch 2, Batch: 340, Loss: 0.685302
Train - Epoch 2, Batch: 350, Loss: 0.685580
Train - Epoch 2, Batch: 360, Loss: 0.684197
Train - Epoch 2, Batch: 370, Loss: 0.684194
Train - Epoch 2, Batch: 380, Loss: 0.683570
Train - Epoch 2, Batch: 390, Loss: 0.683767
Train - Epoch 2, Batch: 400, Loss: 0.684934
Train - Epoch 2, Batch: 410, Loss: 0.684679
Train - Epoch 2, Batch: 420, Loss: 0.685117
Train - Epoch 2, Batch: 430, Loss: 0.684608
Train - Epoch 2, Batch: 440, Loss: 0.686267
Train - Epoch 2, Batch: 450, Loss: 0.685769
Train - Epoch 2, Batch: 460, Loss: 0.683749
Train - Epoch 2, Batch: 470, Loss: 0.685390
Train - Epoch 2, Batch: 480, Loss: 0.685366
Train - Epoch 2, Batch: 490, Loss: 0.684344
Train - Epoch 2, Batch: 500, Loss: 0.684292
Train - Epoch 2, Batch: 510, Loss: 0.683661
Train - Epoch 2, Batch: 520, Loss: 0.683816
Train - Epoch 2, Batch: 530, Loss: 0.684119
Train - Epoch 2, Batch: 540, Loss: 0.683276
Train - Epoch 2, Batch: 550, Loss: 0.684149/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684887
Train - Epoch 2, Batch: 570, Loss: 0.685647
Train - Epoch 2, Batch: 580, Loss: 0.684550
Train - Epoch 2, Batch: 590, Loss: 0.683594
Train - Epoch 2, Batch: 600, Loss: 0.683444
Train - Epoch 2, Batch: 610, Loss: 0.685056
Train - Epoch 2, Batch: 620, Loss: 0.684424
Train - Epoch 2, Batch: 630, Loss: 0.684545
Train - Epoch 2, Batch: 640, Loss: 0.684666
Train - Epoch 3, Batch: 0, Loss: 0.684580
Train - Epoch 3, Batch: 10, Loss: 0.684090
Train - Epoch 3, Batch: 20, Loss: 0.683952
Train - Epoch 3, Batch: 30, Loss: 0.685493
Train - Epoch 3, Batch: 40, Loss: 0.684287
Train - Epoch 3, Batch: 50, Loss: 0.684777
Train - Epoch 3, Batch: 60, Loss: 0.684077
Train - Epoch 3, Batch: 70, Loss: 0.683733
Train - Epoch 3, Batch: 80, Loss: 0.684215
Train - Epoch 3, Batch: 90, Loss: 0.684534
Train - Epoch 3, Batch: 100, Loss: 0.684808
Train - Epoch 3, Batch: 110, Loss: 0.684925
Train - Epoch 3, Batch: 120, Loss: 0.683297
Train - Epoch 3, Batch: 130, Loss: 0.684735
Train - Epoch 3, Batch: 140, Loss: 0.683843
Train - Epoch 3, Batch: 150, Loss: 0.684236
Train - Epoch 3, Batch: 160, Loss: 0.684675
Train - Epoch 3, Batch: 170, Loss: 0.684944
Train - Epoch 3, Batch: 180, Loss: 0.683598
Train - Epoch 3, Batch: 190, Loss: 0.685483
Train - Epoch 3, Batch: 200, Loss: 0.684993
Train - Epoch 3, Batch: 210, Loss: 0.684428
Train - Epoch 3, Batch: 220, Loss: 0.684488
Train - Epoch 3, Batch: 230, Loss: 0.683562
Train - Epoch 3, Batch: 240, Loss: 0.684702
Train - Epoch 3, Batch: 250, Loss: 0.684440
Train - Epoch 3, Batch: 260, Loss: 0.684512
Train - Epoch 3, Batch: 270, Loss: 0.684254
Train - Epoch 3, Batch: 280, Loss: 0.685674
Train - Epoch 3, Batch: 290, Loss: 0.683263
Train - Epoch 3, Batch: 300, Loss: 0.685482
Train - Epoch 3, Batch: 310, Loss: 0.684657
Train - Epoch 3, Batch: 320, Loss: 0.684467
Train - Epoch 3, Batch: 330, Loss: 0.684018
Train - Epoch 3, Batch: 340, Loss: 0.683968
Train - Epoch 3, Batch: 350, Loss: 0.684085
Train - Epoch 3, Batch: 360, Loss: 0.683765
Train - Epoch 3, Batch: 370, Loss: 0.685064
Train - Epoch 3, Batch: 380, Loss: 0.684140
Train - Epoch 3, Batch: 390, Loss: 0.684109
Train - Epoch 3, Batch: 400, Loss: 0.684164
Train - Epoch 3, Batch: 410, Loss: 0.682445
Train - Epoch 3, Batch: 420, Loss: 0.685347
Train - Epoch 3, Batch: 430, Loss: 0.684085
Train - Epoch 3, Batch: 440, Loss: 0.683197
Train - Epoch 3, Batch: 450, Loss: 0.683294
Train - Epoch 3, Batch: 460, Loss: 0.682860
Train - Epoch 3, Batch: 470, Loss: 0.681854
Train - Epoch 3, Batch: 480, Loss: 0.683788
Train - Epoch 3, Batch: 490, Loss: 0.684383
Train - Epoch 3, Batch: 500, Loss: 0.683479
Train - Epoch 3, Batch: 510, Loss: 0.683825
Train - Epoch 3, Batch: 520, Loss: 0.683774
Train - Epoch 3, Batch: 530, Loss: 0.684718
Train - Epoch 3, Batch: 540, Loss: 0.684022
Train - Epoch 3, Batch: 550, Loss: 0.684072
Train - Epoch 3, Batch: 560, Loss: 0.684692
Train - Epoch 3, Batch: 570, Loss: 0.683794
Train - Epoch 3, Batch: 580, Loss: 0.684558
Train - Epoch 3, Batch: 590, Loss: 0.682831
Train - Epoch 3, Batch: 600, Loss: 0.683936
Train - Epoch 3, Batch: 610, Loss: 0.684582
Train - Epoch 3, Batch: 620, Loss: 0.684548
Train - Epoch 3, Batch: 630, Loss: 0.684908
Train - Epoch 3, Batch: 640, Loss: 0.683903
training_time:: 7.602262020111084
training time full:: 7.602301597595215
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553956
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 2100
training time is 5.579819917678833
overhead:: 0
overhead2:: 0
time_baseline:: 5.582708358764648
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553958
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.14847874641418457
overhead3:: 0.20972776412963867
overhead4:: 0.8181507587432861
overhead5:: 0
time_provenance:: 2.7834713459014893
curr_diff: 0 tensor(9.5060e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5060e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2805e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2805e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553940
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.1823868751525879
overhead3:: 0.2501227855682373
overhead4:: 0.9254953861236572
overhead5:: 0
time_provenance:: 2.9084508419036865
curr_diff: 0 tensor(9.5031e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5031e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2783e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2783e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553940
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.19623088836669922
overhead3:: 0.27324533462524414
overhead4:: 1.0716428756713867
overhead5:: 0
time_provenance:: 3.016726016998291
curr_diff: 0 tensor(9.4850e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4850e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2708e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2708e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553940
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22178411483764648
overhead3:: 0.32864975929260254
overhead4:: 1.1732065677642822
overhead5:: 0
time_provenance:: 3.151196241378784
curr_diff: 0 tensor(9.4589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(4.2642e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2642e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553940
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2593569755554199
overhead3:: 0.3630101680755615
overhead4:: 1.4417939186096191
overhead5:: 0
time_provenance:: 4.338717937469482
curr_diff: 0 tensor(3.5573e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5573e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.1736e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1736e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2861802577972412
overhead3:: 0.423018217086792
overhead4:: 1.3397901058197021
overhead5:: 0
time_provenance:: 4.123160123825073
curr_diff: 0 tensor(3.5193e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5193e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2022e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2022e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.27544450759887695
overhead3:: 0.39946532249450684
overhead4:: 1.5808978080749512
overhead5:: 0
time_provenance:: 4.329040288925171
curr_diff: 0 tensor(3.4828e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4828e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2296e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.34151124954223633
overhead3:: 0.5128815174102783
overhead4:: 1.6874947547912598
overhead5:: 0
time_provenance:: 4.310301303863525
curr_diff: 0 tensor(3.4246e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4246e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.2736e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2736e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2872793674468994
overhead3:: 0.4445304870605469
overhead4:: 1.7372117042541504
overhead5:: 0
time_provenance:: 4.0550267696380615
curr_diff: 0 tensor(2.4016e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4016e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3200550079345703
overhead3:: 0.4724266529083252
overhead4:: 1.8720104694366455
overhead5:: 0
time_provenance:: 4.176432371139526
curr_diff: 0 tensor(2.7382e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7382e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.3142e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3142e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553948
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3282740116119385
overhead3:: 0.4623401165008545
overhead4:: 1.8200335502624512
overhead5:: 0
time_provenance:: 4.067036867141724
curr_diff: 0 tensor(2.0132e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0132e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.0009e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0009e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553960
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.36611151695251465
overhead3:: 0.5282583236694336
overhead4:: 2.0435357093811035
overhead5:: 0
time_provenance:: 4.381970167160034
curr_diff: 0 tensor(2.3144e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3144e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.6233e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6233e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.37685108184814453
overhead3:: 0.5324258804321289
overhead4:: 2.3434064388275146
overhead5:: 0
time_provenance:: 4.814827919006348
curr_diff: 0 tensor(9.4745e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4745e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.7688e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7688e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553946
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.42819762229919434
overhead3:: 0.6096227169036865
overhead4:: 2.4501006603240967
overhead5:: 0
time_provenance:: 5.009812116622925
curr_diff: 0 tensor(9.4396e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4396e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.7713e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7713e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553946
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4177372455596924
overhead3:: 0.6352574825286865
overhead4:: 2.576627254486084
overhead5:: 0
time_provenance:: 5.081864356994629
curr_diff: 0 tensor(9.0360e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0360e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.8017e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8017e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553946
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.44678497314453125
overhead3:: 0.6834239959716797
overhead4:: 2.64579439163208
overhead5:: 0
time_provenance:: 5.184846639633179
curr_diff: 0 tensor(8.8450e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8450e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.8170e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8170e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553946
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.7288331985473633
overhead3:: 1.0006840229034424
overhead4:: 3.5568971633911133
overhead5:: 0
time_provenance:: 5.874273300170898
curr_diff: 0 tensor(2.4364e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4364e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553958
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.717830
Train - Epoch 0, Batch: 10, Loss: 0.693293
Train - Epoch 0, Batch: 20, Loss: 0.691484
Train - Epoch 0, Batch: 30, Loss: 0.691242
Train - Epoch 0, Batch: 40, Loss: 0.691672
Train - Epoch 0, Batch: 50, Loss: 0.690338
Train - Epoch 0, Batch: 60, Loss: 0.690600
Train - Epoch 0, Batch: 70, Loss: 0.690315
Train - Epoch 0, Batch: 80, Loss: 0.690675
Train - Epoch 0, Batch: 90, Loss: 0.691174
Train - Epoch 0, Batch: 100, Loss: 0.690112
Train - Epoch 0, Batch: 110, Loss: 0.689228
Train - Epoch 0, Batch: 120, Loss: 0.689801
Train - Epoch 0, Batch: 130, Loss: 0.690003
Train - Epoch 0, Batch: 140, Loss: 0.689534
Train - Epoch 0, Batch: 150, Loss: 0.690074
Train - Epoch 0, Batch: 160, Loss: 0.689038
Train - Epoch 0, Batch: 170, Loss: 0.688610
Train - Epoch 0, Batch: 180, Loss: 0.689451
Train - Epoch 0, Batch: 190, Loss: 0.688537
Train - Epoch 0, Batch: 200, Loss: 0.688507
Train - Epoch 0, Batch: 210, Loss: 0.688546
Train - Epoch 0, Batch: 220, Loss: 0.688304
Train - Epoch 0, Batch: 230, Loss: 0.688765
Train - Epoch 0, Batch: 240, Loss: 0.688242
Train - Epoch 0, Batch: 250, Loss: 0.687662
Train - Epoch 0, Batch: 260, Loss: 0.689038
Train - Epoch 0, Batch: 270, Loss: 0.687764
Train - Epoch 0, Batch: 280, Loss: 0.689043
Train - Epoch 0, Batch: 290, Loss: 0.687000
Train - Epoch 0, Batch: 300, Loss: 0.688511
Train - Epoch 0, Batch: 310, Loss: 0.688104
Train - Epoch 0, Batch: 320, Loss: 0.688610
Train - Epoch 0, Batch: 330, Loss: 0.686930
Train - Epoch 0, Batch: 340, Loss: 0.687705
Train - Epoch 0, Batch: 350, Loss: 0.687952
Train - Epoch 0, Batch: 360, Loss: 0.688291
Train - Epoch 0, Batch: 370, Loss: 0.687303
Train - Epoch 0, Batch: 380, Loss: 0.688174
Train - Epoch 0, Batch: 390, Loss: 0.688182
Train - Epoch 0, Batch: 400, Loss: 0.687119
Train - Epoch 0, Batch: 410, Loss: 0.687106
Train - Epoch 0, Batch: 420, Loss: 0.687468
Train - Epoch 0, Batch: 430, Loss: 0.687103
Train - Epoch 0, Batch: 440, Loss: 0.687568
Train - Epoch 0, Batch: 450, Loss: 0.687632
Train - Epoch 0, Batch: 460, Loss: 0.686471
Train - Epoch 0, Batch: 470, Loss: 0.687727
Train - Epoch 0, Batch: 480, Loss: 0.686948
Train - Epoch 0, Batch: 490, Loss: 0.687395
Train - Epoch 0, Batch: 500, Loss: 0.687030
Train - Epoch 0, Batch: 510, Loss: 0.687399
Train - Epoch 0, Batch: 520, Loss: 0.687475
Train - Epoch 0, Batch: 530, Loss: 0.687883
Train - Epoch 0, Batch: 540, Loss: 0.687983
Train - Epoch 0, Batch: 550, Loss: 0.687166
Train - Epoch 0, Batch: 560, Loss: 0.687593
Train - Epoch 0, Batch: 570, Loss: 0.687051
Train - Epoch 0, Batch: 580, Loss: 0.686354
Train - Epoch 0, Batch: 590, Loss: 0.688206
Train - Epoch 0, Batch: 600, Loss: 0.687161
Train - Epoch 0, Batch: 610, Loss: 0.686725
Train - Epoch 0, Batch: 620, Loss: 0.687687
Train - Epoch 0, Batch: 630, Loss: 0.686680
Train - Epoch 0, Batch: 640, Loss: 0.687247
Train - Epoch 1, Batch: 0, Loss: 0.686094
Train - Epoch 1, Batch: 10, Loss: 0.686955
Train - Epoch 1, Batch: 20, Loss: 0.685708
Train - Epoch 1, Batch: 30, Loss: 0.687265
Train - Epoch 1, Batch: 40, Loss: 0.686231
Train - Epoch 1, Batch: 50, Loss: 0.686153
Train - Epoch 1, Batch: 60, Loss: 0.686552
Train - Epoch 1, Batch: 70, Loss: 0.686109
Train - Epoch 1, Batch: 80, Loss: 0.686583
Train - Epoch 1, Batch: 90, Loss: 0.686937
Train - Epoch 1, Batch: 100, Loss: 0.686366
Train - Epoch 1, Batch: 110, Loss: 0.685586
Train - Epoch 1, Batch: 120, Loss: 0.685855
Train - Epoch 1, Batch: 130, Loss: 0.686249
Train - Epoch 1, Batch: 140, Loss: 0.686875
Train - Epoch 1, Batch: 150, Loss: 0.686530
Train - Epoch 1, Batch: 160, Loss: 0.685907
Train - Epoch 1, Batch: 170, Loss: 0.686576
Train - Epoch 1, Batch: 180, Loss: 0.687904
Train - Epoch 1, Batch: 190, Loss: 0.686539
Train - Epoch 1, Batch: 200, Loss: 0.686129
Train - Epoch 1, Batch: 210, Loss: 0.686649
Train - Epoch 1, Batch: 220, Loss: 0.685401
Train - Epoch 1, Batch: 230, Loss: 0.686143
Train - Epoch 1, Batch: 240, Loss: 0.686205
Train - Epoch 1, Batch: 250, Loss: 0.686875
Train - Epoch 1, Batch: 260, Loss: 0.685583
Train - Epoch 1, Batch: 270, Loss: 0.685599
Train - Epoch 1, Batch: 280, Loss: 0.686090
Train - Epoch 1, Batch: 290, Loss: 0.685990
Train - Epoch 1, Batch: 300, Loss: 0.685763
Train - Epoch 1, Batch: 310, Loss: 0.686511
Train - Epoch 1, Batch: 320, Loss: 0.684940
Train - Epoch 1, Batch: 330, Loss: 0.686639
Train - Epoch 1, Batch: 340, Loss: 0.685833
Train - Epoch 1, Batch: 350, Loss: 0.686045
Train - Epoch 1, Batch: 360, Loss: 0.686240
Train - Epoch 1, Batch: 370, Loss: 0.686042
Train - Epoch 1, Batch: 380, Loss: 0.686587
Train - Epoch 1, Batch: 390, Loss: 0.685843
Train - Epoch 1, Batch: 400, Loss: 0.685037
Train - Epoch 1, Batch: 410, Loss: 0.685869
Train - Epoch 1, Batch: 420, Loss: 0.685801
Train - Epoch 1, Batch: 430, Loss: 0.685070
Train - Epoch 1, Batch: 440, Loss: 0.685561
Train - Epoch 1, Batch: 450, Loss: 0.686448
Train - Epoch 1, Batch: 460, Loss: 0.684980
Train - Epoch 1, Batch: 470, Loss: 0.686637
Train - Epoch 1, Batch: 480, Loss: 0.686172
Train - Epoch 1, Batch: 490, Loss: 0.684499
Train - Epoch 1, Batch: 500, Loss: 0.685815
Train - Epoch 1, Batch: 510, Loss: 0.685806
Train - Epoch 1, Batch: 520, Loss: 0.686419
Train - Epoch 1, Batch: 530, Loss: 0.685320
Train - Epoch 1, Batch: 540, Loss: 0.685595
Train - Epoch 1, Batch: 550, Loss: 0.685614
Train - Epoch 1, Batch: 560, Loss: 0.685527
Train - Epoch 1, Batch: 570, Loss: 0.686491
Train - Epoch 1, Batch: 580, Loss: 0.686751
Train - Epoch 1, Batch: 590, Loss: 0.685415
Train - Epoch 1, Batch: 600, Loss: 0.685466
Train - Epoch 1, Batch: 610, Loss: 0.686275
Train - Epoch 1, Batch: 620, Loss: 0.685335
Train - Epoch 1, Batch: 630, Loss: 0.684366
Train - Epoch 1, Batch: 640, Loss: 0.685233
Train - Epoch 2, Batch: 0, Loss: 0.686318
Train - Epoch 2, Batch: 10, Loss: 0.683804
Train - Epoch 2, Batch: 20, Loss: 0.685780
Train - Epoch 2, Batch: 30, Loss: 0.684850
Train - Epoch 2, Batch: 40, Loss: 0.686353
Train - Epoch 2, Batch: 50, Loss: 0.684397
Train - Epoch 2, Batch: 60, Loss: 0.684940
Train - Epoch 2, Batch: 70, Loss: 0.685425
Train - Epoch 2, Batch: 80, Loss: 0.685485
Train - Epoch 2, Batch: 90, Loss: 0.685483
Train - Epoch 2, Batch: 100, Loss: 0.685534
Train - Epoch 2, Batch: 110, Loss: 0.685013
Train - Epoch 2, Batch: 120, Loss: 0.684847
Train - Epoch 2, Batch: 130, Loss: 0.684331
Train - Epoch 2, Batch: 140, Loss: 0.685609
Train - Epoch 2, Batch: 150, Loss: 0.685288
Train - Epoch 2, Batch: 160, Loss: 0.684631
Train - Epoch 2, Batch: 170, Loss: 0.684829
Train - Epoch 2, Batch: 180, Loss: 0.684678
Train - Epoch 2, Batch: 190, Loss: 0.686125
Train - Epoch 2, Batch: 200, Loss: 0.685286
Train - Epoch 2, Batch: 210, Loss: 0.685458
Train - Epoch 2, Batch: 220, Loss: 0.684368
Train - Epoch 2, Batch: 230, Loss: 0.685169
Train - Epoch 2, Batch: 240, Loss: 0.683546
Train - Epoch 2, Batch: 250, Loss: 0.684953
Train - Epoch 2, Batch: 260, Loss: 0.684623
Train - Epoch 2, Batch: 270, Loss: 0.685867
Train - Epoch 2, Batch: 280, Loss: 0.683973
Train - Epoch 2, Batch: 290, Loss: 0.684905
Train - Epoch 2, Batch: 300, Loss: 0.684442
Train - Epoch 2, Batch: 310, Loss: 0.686564
Train - Epoch 2, Batch: 320, Loss: 0.683848
Train - Epoch 2, Batch: 330, Loss: 0.686360
Train - Epoch 2, Batch: 340, Loss: 0.685212
Train - Epoch 2, Batch: 350, Loss: 0.685054
Train - Epoch 2, Batch: 360, Loss: 0.685530
Train - Epoch 2, Batch: 370, Loss: 0.684730
Train - Epoch 2, Batch: 380, Loss: 0.685375
Train - Epoch 2, Batch: 390, Loss: 0.684463
Train - Epoch 2, Batch: 400, Loss: 0.685144
Train - Epoch 2, Batch: 410, Loss: 0.684421
Train - Epoch 2, Batch: 420, Loss: 0.684205
Train - Epoch 2, Batch: 430, Loss: 0.684544
Train - Epoch 2, Batch: 440, Loss: 0.684827
Train - Epoch 2, Batch: 450, Loss: 0.684956
Train - Epoch 2, Batch: 460, Loss: 0.684237
Train - Epoch 2, Batch: 470, Loss: 0.683495
Train - Epoch 2, Batch: 480, Loss: 0.684107
Train - Epoch 2, Batch: 490, Loss: 0.684623
Train - Epoch 2, Batch: 500, Loss: 0.683244
Train - Epoch 2, Batch: 510, Loss: 0.683857
Train - Epoch 2, Batch: 520, Loss: 0.684151
Train - Epoch 2, Batch: 530, Loss: 0.684412
Train - Epoch 2, Batch: 540, Loss: 0.685392
Train - Epoch 2, Batch: 550, Loss: 0.684425/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684932
Train - Epoch 2, Batch: 570, Loss: 0.684695
Train - Epoch 2, Batch: 580, Loss: 0.684253
Train - Epoch 2, Batch: 590, Loss: 0.685351
Train - Epoch 2, Batch: 600, Loss: 0.685272
Train - Epoch 2, Batch: 610, Loss: 0.684511
Train - Epoch 2, Batch: 620, Loss: 0.684400
Train - Epoch 2, Batch: 630, Loss: 0.684226
Train - Epoch 2, Batch: 640, Loss: 0.684152
Train - Epoch 3, Batch: 0, Loss: 0.683791
Train - Epoch 3, Batch: 10, Loss: 0.683659
Train - Epoch 3, Batch: 20, Loss: 0.685141
Train - Epoch 3, Batch: 30, Loss: 0.685276
Train - Epoch 3, Batch: 40, Loss: 0.684417
Train - Epoch 3, Batch: 50, Loss: 0.683685
Train - Epoch 3, Batch: 60, Loss: 0.685359
Train - Epoch 3, Batch: 70, Loss: 0.686150
Train - Epoch 3, Batch: 80, Loss: 0.683968
Train - Epoch 3, Batch: 90, Loss: 0.686165
Train - Epoch 3, Batch: 100, Loss: 0.684321
Train - Epoch 3, Batch: 110, Loss: 0.684973
Train - Epoch 3, Batch: 120, Loss: 0.684212
Train - Epoch 3, Batch: 130, Loss: 0.685409
Train - Epoch 3, Batch: 140, Loss: 0.684751
Train - Epoch 3, Batch: 150, Loss: 0.684518
Train - Epoch 3, Batch: 160, Loss: 0.685871
Train - Epoch 3, Batch: 170, Loss: 0.684913
Train - Epoch 3, Batch: 180, Loss: 0.684238
Train - Epoch 3, Batch: 190, Loss: 0.683157
Train - Epoch 3, Batch: 200, Loss: 0.685137
Train - Epoch 3, Batch: 210, Loss: 0.683254
Train - Epoch 3, Batch: 220, Loss: 0.685729
Train - Epoch 3, Batch: 230, Loss: 0.683753
Train - Epoch 3, Batch: 240, Loss: 0.684837
Train - Epoch 3, Batch: 250, Loss: 0.685049
Train - Epoch 3, Batch: 260, Loss: 0.684112
Train - Epoch 3, Batch: 270, Loss: 0.684312
Train - Epoch 3, Batch: 280, Loss: 0.684810
Train - Epoch 3, Batch: 290, Loss: 0.684541
Train - Epoch 3, Batch: 300, Loss: 0.683339
Train - Epoch 3, Batch: 310, Loss: 0.685023
Train - Epoch 3, Batch: 320, Loss: 0.684005
Train - Epoch 3, Batch: 330, Loss: 0.684065
Train - Epoch 3, Batch: 340, Loss: 0.683394
Train - Epoch 3, Batch: 350, Loss: 0.683961
Train - Epoch 3, Batch: 360, Loss: 0.684185
Train - Epoch 3, Batch: 370, Loss: 0.684354
Train - Epoch 3, Batch: 380, Loss: 0.684260
Train - Epoch 3, Batch: 390, Loss: 0.684707
Train - Epoch 3, Batch: 400, Loss: 0.683399
Train - Epoch 3, Batch: 410, Loss: 0.684333
Train - Epoch 3, Batch: 420, Loss: 0.684779
Train - Epoch 3, Batch: 430, Loss: 0.684104
Train - Epoch 3, Batch: 440, Loss: 0.683931
Train - Epoch 3, Batch: 450, Loss: 0.684391
Train - Epoch 3, Batch: 460, Loss: 0.683488
Train - Epoch 3, Batch: 470, Loss: 0.684057
Train - Epoch 3, Batch: 480, Loss: 0.684713
Train - Epoch 3, Batch: 490, Loss: 0.685538
Train - Epoch 3, Batch: 500, Loss: 0.684566
Train - Epoch 3, Batch: 510, Loss: 0.683693
Train - Epoch 3, Batch: 520, Loss: 0.684048
Train - Epoch 3, Batch: 530, Loss: 0.684480
Train - Epoch 3, Batch: 540, Loss: 0.683030
Train - Epoch 3, Batch: 550, Loss: 0.684294
Train - Epoch 3, Batch: 560, Loss: 0.684832
Train - Epoch 3, Batch: 570, Loss: 0.683455
Train - Epoch 3, Batch: 580, Loss: 0.684608
Train - Epoch 3, Batch: 590, Loss: 0.685429
Train - Epoch 3, Batch: 600, Loss: 0.684596
Train - Epoch 3, Batch: 610, Loss: 0.684234
Train - Epoch 3, Batch: 620, Loss: 0.684314
Train - Epoch 3, Batch: 630, Loss: 0.684678
Train - Epoch 3, Batch: 640, Loss: 0.683417
training_time:: 7.568786859512329
training time full:: 7.568825721740723
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554436
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 2100
training time is 4.990868091583252
overhead:: 0
overhead2:: 0
time_baseline:: 4.994708776473999
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554440
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.14458727836608887
overhead3:: 0.21455144882202148
overhead4:: 0.8147928714752197
overhead5:: 0
time_provenance:: 2.759836196899414
curr_diff: 0 tensor(6.2368e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2368e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9225e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9225e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554444
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.1687183380126953
overhead3:: 0.24836444854736328
overhead4:: 0.921600341796875
overhead5:: 0
time_provenance:: 2.9051764011383057
curr_diff: 0 tensor(6.2232e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2232e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9325e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9325e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554444
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.20914292335510254
overhead3:: 0.3076975345611572
overhead4:: 1.0948917865753174
overhead5:: 0
time_provenance:: 3.218236207962036
curr_diff: 0 tensor(6.2064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554444
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.24506211280822754
overhead3:: 0.3799777030944824
overhead4:: 1.2533323764801025
overhead5:: 0
time_provenance:: 3.351937770843506
curr_diff: 0 tensor(6.1860e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1860e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(5.9598e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9598e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554444
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22943782806396484
overhead3:: 0.3331766128540039
overhead4:: 1.3639397621154785
overhead5:: 0
time_provenance:: 4.030852556228638
curr_diff: 0 tensor(4.2375e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2375e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.0494e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0494e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554432
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.23337340354919434
overhead3:: 0.35888028144836426
overhead4:: 1.3897509574890137
overhead5:: 0
time_provenance:: 3.525181293487549
curr_diff: 0 tensor(4.1223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1361e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1361e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554432
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.23625946044921875
overhead3:: 0.3437232971191406
overhead4:: 1.352947473526001
overhead5:: 0
time_provenance:: 3.3835015296936035
curr_diff: 0 tensor(4.1051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1492e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1492e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554432
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2936897277832031
overhead3:: 0.4281439781188965
overhead4:: 1.456024408340454
overhead5:: 0
time_provenance:: 3.6003942489624023
curr_diff: 0 tensor(4.0691e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0691e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(7.1771e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1771e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554432
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.28824329376220703
overhead3:: 0.41163110733032227
overhead4:: 1.7905442714691162
overhead5:: 0
time_provenance:: 4.069411277770996
curr_diff: 0 tensor(1.9401e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9401e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7761e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7761e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554442
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3356196880340576
overhead3:: 0.5069601535797119
overhead4:: 1.9391660690307617
overhead5:: 0
time_provenance:: 4.334298849105835
curr_diff: 0 tensor(2.0033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0033e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.7694e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7694e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554434
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3438735008239746
overhead3:: 0.49182581901550293
overhead4:: 1.9892361164093018
overhead5:: 0
time_provenance:: 4.308637857437134
curr_diff: 0 tensor(2.4344e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4344e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.5233e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5233e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554436
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.34385061264038086
overhead3:: 0.5334751605987549
overhead4:: 2.0256588459014893
overhead5:: 0
time_provenance:: 4.338653802871704
curr_diff: 0 tensor(1.8387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8387e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.8502e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8502e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554442
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4107704162597656
overhead3:: 0.5917556285858154
overhead4:: 2.4278745651245117
overhead5:: 0
time_provenance:: 4.988636493682861
curr_diff: 0 tensor(8.7824e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7824e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.5578e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5578e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554438
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4053478240966797
overhead3:: 0.5775880813598633
overhead4:: 2.30838680267334
overhead5:: 0
time_provenance:: 4.778984069824219
curr_diff: 0 tensor(8.6372e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6372e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.5683e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5683e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554438
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.531120777130127
overhead3:: 0.7903783321380615
overhead4:: 2.6706392765045166
overhead5:: 0
time_provenance:: 5.768662452697754
curr_diff: 0 tensor(8.4182e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4182e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.5845e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5845e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554438
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4257962703704834
overhead3:: 0.623812198638916
overhead4:: 2.5941340923309326
overhead5:: 0
time_provenance:: 5.051819086074829
curr_diff: 0 tensor(8.4269e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4269e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.5835e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5835e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554438
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.7489087581634521
overhead3:: 1.0862891674041748
overhead4:: 3.601979970932007
overhead5:: 0
time_provenance:: 6.025039434432983
curr_diff: 0 tensor(2.4462e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4462e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554440
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.698097
Train - Epoch 0, Batch: 10, Loss: 0.696632
Train - Epoch 0, Batch: 20, Loss: 0.694059
Train - Epoch 0, Batch: 30, Loss: 0.695784
Train - Epoch 0, Batch: 40, Loss: 0.692942
Train - Epoch 0, Batch: 50, Loss: 0.692860
Train - Epoch 0, Batch: 60, Loss: 0.693424
Train - Epoch 0, Batch: 70, Loss: 0.690863
Train - Epoch 0, Batch: 80, Loss: 0.691237
Train - Epoch 0, Batch: 90, Loss: 0.690836
Train - Epoch 0, Batch: 100, Loss: 0.691264
Train - Epoch 0, Batch: 110, Loss: 0.691514
Train - Epoch 0, Batch: 120, Loss: 0.692439
Train - Epoch 0, Batch: 130, Loss: 0.691155
Train - Epoch 0, Batch: 140, Loss: 0.690230
Train - Epoch 0, Batch: 150, Loss: 0.690929
Train - Epoch 0, Batch: 160, Loss: 0.690362
Train - Epoch 0, Batch: 170, Loss: 0.689080
Train - Epoch 0, Batch: 180, Loss: 0.689409
Train - Epoch 0, Batch: 190, Loss: 0.689948
Train - Epoch 0, Batch: 200, Loss: 0.690007
Train - Epoch 0, Batch: 210, Loss: 0.689060
Train - Epoch 0, Batch: 220, Loss: 0.689186
Train - Epoch 0, Batch: 230, Loss: 0.687976
Train - Epoch 0, Batch: 240, Loss: 0.688215
Train - Epoch 0, Batch: 250, Loss: 0.688700
Train - Epoch 0, Batch: 260, Loss: 0.688721
Train - Epoch 0, Batch: 270, Loss: 0.687310
Train - Epoch 0, Batch: 280, Loss: 0.687301
Train - Epoch 0, Batch: 290, Loss: 0.688566
Train - Epoch 0, Batch: 300, Loss: 0.687884
Train - Epoch 0, Batch: 310, Loss: 0.686991
Train - Epoch 0, Batch: 320, Loss: 0.687440
Train - Epoch 0, Batch: 330, Loss: 0.687760
Train - Epoch 0, Batch: 340, Loss: 0.687970
Train - Epoch 0, Batch: 350, Loss: 0.688101
Train - Epoch 0, Batch: 360, Loss: 0.687184
Train - Epoch 0, Batch: 370, Loss: 0.686146
Train - Epoch 0, Batch: 380, Loss: 0.687267
Train - Epoch 0, Batch: 390, Loss: 0.688504
Train - Epoch 0, Batch: 400, Loss: 0.687368
Train - Epoch 0, Batch: 410, Loss: 0.686693
Train - Epoch 0, Batch: 420, Loss: 0.689806
Train - Epoch 0, Batch: 430, Loss: 0.688327
Train - Epoch 0, Batch: 440, Loss: 0.687080
Train - Epoch 0, Batch: 450, Loss: 0.686655
Train - Epoch 0, Batch: 460, Loss: 0.687705
Train - Epoch 0, Batch: 470, Loss: 0.687427
Train - Epoch 0, Batch: 480, Loss: 0.687493
Train - Epoch 0, Batch: 490, Loss: 0.687290
Train - Epoch 0, Batch: 500, Loss: 0.686762
Train - Epoch 0, Batch: 510, Loss: 0.686813
Train - Epoch 0, Batch: 520, Loss: 0.686614
Train - Epoch 0, Batch: 530, Loss: 0.687499
Train - Epoch 0, Batch: 540, Loss: 0.687342
Train - Epoch 0, Batch: 550, Loss: 0.686269
Train - Epoch 0, Batch: 560, Loss: 0.687413
Train - Epoch 0, Batch: 570, Loss: 0.685789
Train - Epoch 0, Batch: 580, Loss: 0.686955
Train - Epoch 0, Batch: 590, Loss: 0.687361
Train - Epoch 0, Batch: 600, Loss: 0.687058
Train - Epoch 0, Batch: 610, Loss: 0.686913
Train - Epoch 0, Batch: 620, Loss: 0.685903
Train - Epoch 0, Batch: 630, Loss: 0.686741
Train - Epoch 0, Batch: 640, Loss: 0.686586
Train - Epoch 1, Batch: 0, Loss: 0.686120
Train - Epoch 1, Batch: 10, Loss: 0.686880
Train - Epoch 1, Batch: 20, Loss: 0.687040
Train - Epoch 1, Batch: 30, Loss: 0.687176
Train - Epoch 1, Batch: 40, Loss: 0.686582
Train - Epoch 1, Batch: 50, Loss: 0.686471
Train - Epoch 1, Batch: 60, Loss: 0.685906
Train - Epoch 1, Batch: 70, Loss: 0.686428
Train - Epoch 1, Batch: 80, Loss: 0.685902
Train - Epoch 1, Batch: 90, Loss: 0.687585
Train - Epoch 1, Batch: 100, Loss: 0.686633
Train - Epoch 1, Batch: 110, Loss: 0.686547
Train - Epoch 1, Batch: 120, Loss: 0.686202
Train - Epoch 1, Batch: 130, Loss: 0.685700
Train - Epoch 1, Batch: 140, Loss: 0.686427
Train - Epoch 1, Batch: 150, Loss: 0.686952
Train - Epoch 1, Batch: 160, Loss: 0.685890
Train - Epoch 1, Batch: 170, Loss: 0.685410
Train - Epoch 1, Batch: 180, Loss: 0.686459
Train - Epoch 1, Batch: 190, Loss: 0.686638
Train - Epoch 1, Batch: 200, Loss: 0.686379
Train - Epoch 1, Batch: 210, Loss: 0.686471
Train - Epoch 1, Batch: 220, Loss: 0.685441
Train - Epoch 1, Batch: 230, Loss: 0.686017
Train - Epoch 1, Batch: 240, Loss: 0.685725
Train - Epoch 1, Batch: 250, Loss: 0.686398
Train - Epoch 1, Batch: 260, Loss: 0.685750
Train - Epoch 1, Batch: 270, Loss: 0.686023
Train - Epoch 1, Batch: 280, Loss: 0.685912
Train - Epoch 1, Batch: 290, Loss: 0.685707
Train - Epoch 1, Batch: 300, Loss: 0.686256
Train - Epoch 1, Batch: 310, Loss: 0.686038
Train - Epoch 1, Batch: 320, Loss: 0.686077
Train - Epoch 1, Batch: 330, Loss: 0.685452
Train - Epoch 1, Batch: 340, Loss: 0.685166
Train - Epoch 1, Batch: 350, Loss: 0.685250
Train - Epoch 1, Batch: 360, Loss: 0.685438
Train - Epoch 1, Batch: 370, Loss: 0.685513
Train - Epoch 1, Batch: 380, Loss: 0.685869
Train - Epoch 1, Batch: 390, Loss: 0.684886
Train - Epoch 1, Batch: 400, Loss: 0.686591
Train - Epoch 1, Batch: 410, Loss: 0.685208
Train - Epoch 1, Batch: 420, Loss: 0.686633
Train - Epoch 1, Batch: 430, Loss: 0.685939
Train - Epoch 1, Batch: 440, Loss: 0.685092
Train - Epoch 1, Batch: 450, Loss: 0.687076
Train - Epoch 1, Batch: 460, Loss: 0.687073
Train - Epoch 1, Batch: 470, Loss: 0.685225
Train - Epoch 1, Batch: 480, Loss: 0.685200
Train - Epoch 1, Batch: 490, Loss: 0.684822
Train - Epoch 1, Batch: 500, Loss: 0.685099
Train - Epoch 1, Batch: 510, Loss: 0.686148
Train - Epoch 1, Batch: 520, Loss: 0.685621
Train - Epoch 1, Batch: 530, Loss: 0.685400
Train - Epoch 1, Batch: 540, Loss: 0.686426
Train - Epoch 1, Batch: 550, Loss: 0.683973
Train - Epoch 1, Batch: 560, Loss: 0.685574
Train - Epoch 1, Batch: 570, Loss: 0.683497
Train - Epoch 1, Batch: 580, Loss: 0.686414
Train - Epoch 1, Batch: 590, Loss: 0.685709
Train - Epoch 1, Batch: 600, Loss: 0.686052
Train - Epoch 1, Batch: 610, Loss: 0.685319
Train - Epoch 1, Batch: 620, Loss: 0.686125
Train - Epoch 1, Batch: 630, Loss: 0.686177
Train - Epoch 1, Batch: 640, Loss: 0.684810
Train - Epoch 2, Batch: 0, Loss: 0.685707
Train - Epoch 2, Batch: 10, Loss: 0.686482
Train - Epoch 2, Batch: 20, Loss: 0.686086
Train - Epoch 2, Batch: 30, Loss: 0.684512
Train - Epoch 2, Batch: 40, Loss: 0.685677
Train - Epoch 2, Batch: 50, Loss: 0.684117
Train - Epoch 2, Batch: 60, Loss: 0.684700
Train - Epoch 2, Batch: 70, Loss: 0.684268
Train - Epoch 2, Batch: 80, Loss: 0.684958
Train - Epoch 2, Batch: 90, Loss: 0.685888
Train - Epoch 2, Batch: 100, Loss: 0.685391
Train - Epoch 2, Batch: 110, Loss: 0.685183
Train - Epoch 2, Batch: 120, Loss: 0.683878
Train - Epoch 2, Batch: 130, Loss: 0.685225
Train - Epoch 2, Batch: 140, Loss: 0.686216
Train - Epoch 2, Batch: 150, Loss: 0.685569
Train - Epoch 2, Batch: 160, Loss: 0.685143
Train - Epoch 2, Batch: 170, Loss: 0.685839
Train - Epoch 2, Batch: 180, Loss: 0.683814
Train - Epoch 2, Batch: 190, Loss: 0.685396
Train - Epoch 2, Batch: 200, Loss: 0.684821
Train - Epoch 2, Batch: 210, Loss: 0.686127
Train - Epoch 2, Batch: 220, Loss: 0.685447
Train - Epoch 2, Batch: 230, Loss: 0.684229
Train - Epoch 2, Batch: 240, Loss: 0.684573
Train - Epoch 2, Batch: 250, Loss: 0.684984
Train - Epoch 2, Batch: 260, Loss: 0.684624
Train - Epoch 2, Batch: 270, Loss: 0.683815
Train - Epoch 2, Batch: 280, Loss: 0.684860
Train - Epoch 2, Batch: 290, Loss: 0.684525
Train - Epoch 2, Batch: 300, Loss: 0.685001
Train - Epoch 2, Batch: 310, Loss: 0.685374
Train - Epoch 2, Batch: 320, Loss: 0.683974
Train - Epoch 2, Batch: 330, Loss: 0.684209
Train - Epoch 2, Batch: 340, Loss: 0.685008
Train - Epoch 2, Batch: 350, Loss: 0.685163
Train - Epoch 2, Batch: 360, Loss: 0.685038
Train - Epoch 2, Batch: 370, Loss: 0.684324
Train - Epoch 2, Batch: 380, Loss: 0.683552
Train - Epoch 2, Batch: 390, Loss: 0.684400
Train - Epoch 2, Batch: 400, Loss: 0.685812
Train - Epoch 2, Batch: 410, Loss: 0.684498
Train - Epoch 2, Batch: 420, Loss: 0.684909
Train - Epoch 2, Batch: 430, Loss: 0.684333
Train - Epoch 2, Batch: 440, Loss: 0.684002
Train - Epoch 2, Batch: 450, Loss: 0.684320
Train - Epoch 2, Batch: 460, Loss: 0.683500
Train - Epoch 2, Batch: 470, Loss: 0.685048
Train - Epoch 2, Batch: 480, Loss: 0.684132
Train - Epoch 2, Batch: 490, Loss: 0.686167
Train - Epoch 2, Batch: 500, Loss: 0.684630
Train - Epoch 2, Batch: 510, Loss: 0.685390
Train - Epoch 2, Batch: 520, Loss: 0.683482
Train - Epoch 2, Batch: 530, Loss: 0.684721
Train - Epoch 2, Batch: 540, Loss: 0.684161
Train - Epoch 2, Batch: 550, Loss: 0.684311/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684475
Train - Epoch 2, Batch: 570, Loss: 0.684930
Train - Epoch 2, Batch: 580, Loss: 0.684486
Train - Epoch 2, Batch: 590, Loss: 0.684092
Train - Epoch 2, Batch: 600, Loss: 0.684164
Train - Epoch 2, Batch: 610, Loss: 0.684960
Train - Epoch 2, Batch: 620, Loss: 0.684556
Train - Epoch 2, Batch: 630, Loss: 0.683827
Train - Epoch 2, Batch: 640, Loss: 0.684971
Train - Epoch 3, Batch: 0, Loss: 0.683561
Train - Epoch 3, Batch: 10, Loss: 0.685064
Train - Epoch 3, Batch: 20, Loss: 0.685714
Train - Epoch 3, Batch: 30, Loss: 0.683368
Train - Epoch 3, Batch: 40, Loss: 0.685214
Train - Epoch 3, Batch: 50, Loss: 0.683929
Train - Epoch 3, Batch: 60, Loss: 0.683709
Train - Epoch 3, Batch: 70, Loss: 0.683936
Train - Epoch 3, Batch: 80, Loss: 0.683584
Train - Epoch 3, Batch: 90, Loss: 0.684974
Train - Epoch 3, Batch: 100, Loss: 0.683357
Train - Epoch 3, Batch: 110, Loss: 0.684696
Train - Epoch 3, Batch: 120, Loss: 0.684006
Train - Epoch 3, Batch: 130, Loss: 0.685322
Train - Epoch 3, Batch: 140, Loss: 0.685410
Train - Epoch 3, Batch: 150, Loss: 0.683997
Train - Epoch 3, Batch: 160, Loss: 0.684389
Train - Epoch 3, Batch: 170, Loss: 0.683080
Train - Epoch 3, Batch: 180, Loss: 0.683042
Train - Epoch 3, Batch: 190, Loss: 0.685143
Train - Epoch 3, Batch: 200, Loss: 0.684728
Train - Epoch 3, Batch: 210, Loss: 0.683935
Train - Epoch 3, Batch: 220, Loss: 0.683933
Train - Epoch 3, Batch: 230, Loss: 0.683966
Train - Epoch 3, Batch: 240, Loss: 0.683976
Train - Epoch 3, Batch: 250, Loss: 0.683877
Train - Epoch 3, Batch: 260, Loss: 0.684029
Train - Epoch 3, Batch: 270, Loss: 0.685292
Train - Epoch 3, Batch: 280, Loss: 0.684362
Train - Epoch 3, Batch: 290, Loss: 0.685311
Train - Epoch 3, Batch: 300, Loss: 0.684099
Train - Epoch 3, Batch: 310, Loss: 0.684407
Train - Epoch 3, Batch: 320, Loss: 0.684449
Train - Epoch 3, Batch: 330, Loss: 0.683247
Train - Epoch 3, Batch: 340, Loss: 0.684144
Train - Epoch 3, Batch: 350, Loss: 0.685092
Train - Epoch 3, Batch: 360, Loss: 0.683799
Train - Epoch 3, Batch: 370, Loss: 0.684681
Train - Epoch 3, Batch: 380, Loss: 0.683521
Train - Epoch 3, Batch: 390, Loss: 0.684356
Train - Epoch 3, Batch: 400, Loss: 0.683670
Train - Epoch 3, Batch: 410, Loss: 0.684171
Train - Epoch 3, Batch: 420, Loss: 0.684312
Train - Epoch 3, Batch: 430, Loss: 0.684188
Train - Epoch 3, Batch: 440, Loss: 0.683734
Train - Epoch 3, Batch: 450, Loss: 0.683343
Train - Epoch 3, Batch: 460, Loss: 0.683863
Train - Epoch 3, Batch: 470, Loss: 0.684569
Train - Epoch 3, Batch: 480, Loss: 0.684150
Train - Epoch 3, Batch: 490, Loss: 0.684555
Train - Epoch 3, Batch: 500, Loss: 0.683373
Train - Epoch 3, Batch: 510, Loss: 0.683386
Train - Epoch 3, Batch: 520, Loss: 0.683143
Train - Epoch 3, Batch: 530, Loss: 0.684151
Train - Epoch 3, Batch: 540, Loss: 0.683295
Train - Epoch 3, Batch: 550, Loss: 0.684251
Train - Epoch 3, Batch: 560, Loss: 0.684660
Train - Epoch 3, Batch: 570, Loss: 0.683471
Train - Epoch 3, Batch: 580, Loss: 0.684382
Train - Epoch 3, Batch: 590, Loss: 0.682891
Train - Epoch 3, Batch: 600, Loss: 0.684075
Train - Epoch 3, Batch: 610, Loss: 0.683072
Train - Epoch 3, Batch: 620, Loss: 0.683690
Train - Epoch 3, Batch: 630, Loss: 0.685175
Train - Epoch 3, Batch: 640, Loss: 0.684186
training_time:: 7.802633047103882
training time full:: 7.802671909332275
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553126
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 2100
training time is 5.52921199798584
overhead:: 0
overhead2:: 0
time_baseline:: 5.532288551330566
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553106
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.15168046951293945
overhead3:: 0.22931647300720215
overhead4:: 0.8641691207885742
overhead5:: 0
time_provenance:: 2.8470308780670166
curr_diff: 0 tensor(7.7170e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7170e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.1029e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1029e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553102
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.17699909210205078
overhead3:: 0.27896976470947266
overhead4:: 0.9896481037139893
overhead5:: 0
time_provenance:: 2.9722273349761963
curr_diff: 0 tensor(7.7147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.1048e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1048e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553102
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22363567352294922
overhead3:: 0.31032633781433105
overhead4:: 1.0761151313781738
overhead5:: 0
time_provenance:: 3.21952486038208
curr_diff: 0 tensor(7.6544e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6544e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.1521e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1521e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553100
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22811079025268555
overhead3:: 0.3303706645965576
overhead4:: 1.2172887325286865
overhead5:: 0
time_provenance:: 3.2563648223876953
curr_diff: 0 tensor(7.5977e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5977e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(6.1967e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1967e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553100
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.22175288200378418
overhead3:: 0.3502309322357178
overhead4:: 1.3442952632904053
overhead5:: 0
time_provenance:: 3.4987707138061523
curr_diff: 0 tensor(4.1053e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1053e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9430e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9430e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2747666835784912
overhead3:: 0.3762807846069336
overhead4:: 1.54248046875
overhead5:: 0
time_provenance:: 4.370933771133423
curr_diff: 0 tensor(4.1073e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1073e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9409e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9409e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2595968246459961
overhead3:: 0.3757822513580322
overhead4:: 1.428335189819336
overhead5:: 0
time_provenance:: 3.573763370513916
curr_diff: 0 tensor(4.0249e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0249e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9927e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9927e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2555418014526367
overhead3:: 0.3693561553955078
overhead4:: 1.5378341674804688
overhead5:: 0
time_provenance:: 3.6048154830932617
curr_diff: 0 tensor(4.0005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(9.0080e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0080e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.28902101516723633
overhead3:: 0.41692256927490234
overhead4:: 1.7496356964111328
overhead5:: 0
time_provenance:: 4.094210863113403
curr_diff: 0 tensor(1.6194e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6194e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553106
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.32942843437194824
overhead3:: 0.5491528511047363
overhead4:: 1.9555492401123047
overhead5:: 0
time_provenance:: 4.362133264541626
curr_diff: 0 tensor(2.1900e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1900e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3413882255554199
overhead3:: 0.4810059070587158
overhead4:: 2.005840301513672
overhead5:: 0
time_provenance:: 4.363187551498413
curr_diff: 0 tensor(2.2278e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2278e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553104
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4106276035308838
overhead3:: 0.599172830581665
overhead4:: 2.341188907623291
overhead5:: 0
time_provenance:: 5.356506824493408
curr_diff: 0 tensor(1.5173e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5173e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553106
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.49958372116088867
overhead3:: 0.7183666229248047
overhead4:: 2.9496333599090576
overhead5:: 0
time_provenance:: 6.344927072525024
curr_diff: 0 tensor(1.1955e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1955e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4031205177307129
overhead3:: 0.5989594459533691
overhead4:: 2.5009448528289795
overhead5:: 0
time_provenance:: 5.01474928855896
curr_diff: 0 tensor(1.1915e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1915e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.42992424964904785
overhead3:: 0.6013953685760498
overhead4:: 2.385305881500244
overhead5:: 0
time_provenance:: 4.858560085296631
curr_diff: 0 tensor(1.1638e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1638e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4188253879547119
overhead3:: 0.5985209941864014
overhead4:: 2.512519359588623
overhead5:: 0
time_provenance:: 4.946733474731445
curr_diff: 0 tensor(1.1350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553108
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.7842364311218262
overhead3:: 1.1197292804718018
overhead4:: 3.657174825668335
overhead5:: 0
time_provenance:: 6.165239095687866
curr_diff: 0 tensor(2.1181e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1181e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553106
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.701300
Train - Epoch 0, Batch: 10, Loss: 0.696907
Train - Epoch 0, Batch: 20, Loss: 0.695689
Train - Epoch 0, Batch: 30, Loss: 0.695788
Train - Epoch 0, Batch: 40, Loss: 0.695614
Train - Epoch 0, Batch: 50, Loss: 0.694228
Train - Epoch 0, Batch: 60, Loss: 0.694083
Train - Epoch 0, Batch: 70, Loss: 0.693494
Train - Epoch 0, Batch: 80, Loss: 0.693062
Train - Epoch 0, Batch: 90, Loss: 0.693126
Train - Epoch 0, Batch: 100, Loss: 0.692467
Train - Epoch 0, Batch: 110, Loss: 0.691997
Train - Epoch 0, Batch: 120, Loss: 0.691058
Train - Epoch 0, Batch: 130, Loss: 0.692721
Train - Epoch 0, Batch: 140, Loss: 0.691946
Train - Epoch 0, Batch: 150, Loss: 0.692932
Train - Epoch 0, Batch: 160, Loss: 0.691586
Train - Epoch 0, Batch: 170, Loss: 0.691091
Train - Epoch 0, Batch: 180, Loss: 0.691556
Train - Epoch 0, Batch: 190, Loss: 0.690762
Train - Epoch 0, Batch: 200, Loss: 0.690682
Train - Epoch 0, Batch: 210, Loss: 0.690914
Train - Epoch 0, Batch: 220, Loss: 0.690173
Train - Epoch 0, Batch: 230, Loss: 0.690425
Train - Epoch 0, Batch: 240, Loss: 0.691710
Train - Epoch 0, Batch: 250, Loss: 0.690981
Train - Epoch 0, Batch: 260, Loss: 0.690893
Train - Epoch 0, Batch: 270, Loss: 0.689858
Train - Epoch 0, Batch: 280, Loss: 0.689761
Train - Epoch 0, Batch: 290, Loss: 0.690590
Train - Epoch 0, Batch: 300, Loss: 0.689976
Train - Epoch 0, Batch: 310, Loss: 0.689057
Train - Epoch 0, Batch: 320, Loss: 0.689486
Train - Epoch 0, Batch: 330, Loss: 0.689579
Train - Epoch 0, Batch: 340, Loss: 0.689792
Train - Epoch 0, Batch: 350, Loss: 0.689813
Train - Epoch 0, Batch: 360, Loss: 0.689273
Train - Epoch 0, Batch: 370, Loss: 0.689831
Train - Epoch 0, Batch: 380, Loss: 0.688841
Train - Epoch 0, Batch: 390, Loss: 0.689142
Train - Epoch 0, Batch: 400, Loss: 0.689231
Train - Epoch 0, Batch: 410, Loss: 0.688615
Train - Epoch 0, Batch: 420, Loss: 0.688559
Train - Epoch 0, Batch: 430, Loss: 0.688949
Train - Epoch 0, Batch: 440, Loss: 0.689625
Train - Epoch 0, Batch: 450, Loss: 0.688802
Train - Epoch 0, Batch: 460, Loss: 0.688758
Train - Epoch 0, Batch: 470, Loss: 0.688113
Train - Epoch 0, Batch: 480, Loss: 0.688762
Train - Epoch 0, Batch: 490, Loss: 0.687883
Train - Epoch 0, Batch: 500, Loss: 0.689241
Train - Epoch 0, Batch: 510, Loss: 0.687685
Train - Epoch 0, Batch: 520, Loss: 0.688408
Train - Epoch 0, Batch: 530, Loss: 0.688557
Train - Epoch 0, Batch: 540, Loss: 0.687996
Train - Epoch 0, Batch: 550, Loss: 0.688440
Train - Epoch 0, Batch: 560, Loss: 0.686938
Train - Epoch 0, Batch: 570, Loss: 0.687991
Train - Epoch 0, Batch: 580, Loss: 0.688361
Train - Epoch 0, Batch: 590, Loss: 0.688016
Train - Epoch 0, Batch: 600, Loss: 0.687806
Train - Epoch 0, Batch: 610, Loss: 0.688498
Train - Epoch 0, Batch: 620, Loss: 0.686828
Train - Epoch 0, Batch: 630, Loss: 0.687653
Train - Epoch 0, Batch: 640, Loss: 0.687427
Train - Epoch 1, Batch: 0, Loss: 0.687583
Train - Epoch 1, Batch: 10, Loss: 0.687606
Train - Epoch 1, Batch: 20, Loss: 0.687159
Train - Epoch 1, Batch: 30, Loss: 0.687952
Train - Epoch 1, Batch: 40, Loss: 0.687366
Train - Epoch 1, Batch: 50, Loss: 0.686768
Train - Epoch 1, Batch: 60, Loss: 0.688021
Train - Epoch 1, Batch: 70, Loss: 0.687016
Train - Epoch 1, Batch: 80, Loss: 0.687132
Train - Epoch 1, Batch: 90, Loss: 0.686697
Train - Epoch 1, Batch: 100, Loss: 0.688373
Train - Epoch 1, Batch: 110, Loss: 0.686762
Train - Epoch 1, Batch: 120, Loss: 0.688026
Train - Epoch 1, Batch: 130, Loss: 0.687713
Train - Epoch 1, Batch: 140, Loss: 0.686987
Train - Epoch 1, Batch: 150, Loss: 0.686773
Train - Epoch 1, Batch: 160, Loss: 0.686820
Train - Epoch 1, Batch: 170, Loss: 0.686273
Train - Epoch 1, Batch: 180, Loss: 0.687194
Train - Epoch 1, Batch: 190, Loss: 0.686565
Train - Epoch 1, Batch: 200, Loss: 0.686295
Train - Epoch 1, Batch: 210, Loss: 0.685502
Train - Epoch 1, Batch: 220, Loss: 0.685660
Train - Epoch 1, Batch: 230, Loss: 0.687119
Train - Epoch 1, Batch: 240, Loss: 0.686371
Train - Epoch 1, Batch: 250, Loss: 0.686976
Train - Epoch 1, Batch: 260, Loss: 0.686045
Train - Epoch 1, Batch: 270, Loss: 0.687393
Train - Epoch 1, Batch: 280, Loss: 0.687153
Train - Epoch 1, Batch: 290, Loss: 0.686386
Train - Epoch 1, Batch: 300, Loss: 0.686299
Train - Epoch 1, Batch: 310, Loss: 0.686077
Train - Epoch 1, Batch: 320, Loss: 0.686459
Train - Epoch 1, Batch: 330, Loss: 0.686958
Train - Epoch 1, Batch: 340, Loss: 0.687234
Train - Epoch 1, Batch: 350, Loss: 0.686965
Train - Epoch 1, Batch: 360, Loss: 0.686512
Train - Epoch 1, Batch: 370, Loss: 0.686123
Train - Epoch 1, Batch: 380, Loss: 0.686707
Train - Epoch 1, Batch: 390, Loss: 0.686184
Train - Epoch 1, Batch: 400, Loss: 0.687195
Train - Epoch 1, Batch: 410, Loss: 0.686051
Train - Epoch 1, Batch: 420, Loss: 0.685903
Train - Epoch 1, Batch: 430, Loss: 0.687183
Train - Epoch 1, Batch: 440, Loss: 0.685413
Train - Epoch 1, Batch: 450, Loss: 0.686065
Train - Epoch 1, Batch: 460, Loss: 0.686744
Train - Epoch 1, Batch: 470, Loss: 0.686731
Train - Epoch 1, Batch: 480, Loss: 0.687384
Train - Epoch 1, Batch: 490, Loss: 0.686448
Train - Epoch 1, Batch: 500, Loss: 0.685517
Train - Epoch 1, Batch: 510, Loss: 0.686948
Train - Epoch 1, Batch: 520, Loss: 0.685726
Train - Epoch 1, Batch: 530, Loss: 0.684938
Train - Epoch 1, Batch: 540, Loss: 0.686159
Train - Epoch 1, Batch: 550, Loss: 0.685084
Train - Epoch 1, Batch: 560, Loss: 0.685041
Train - Epoch 1, Batch: 570, Loss: 0.685297
Train - Epoch 1, Batch: 580, Loss: 0.685736
Train - Epoch 1, Batch: 590, Loss: 0.685554
Train - Epoch 1, Batch: 600, Loss: 0.685562
Train - Epoch 1, Batch: 610, Loss: 0.684947
Train - Epoch 1, Batch: 620, Loss: 0.686121
Train - Epoch 1, Batch: 630, Loss: 0.687402
Train - Epoch 1, Batch: 640, Loss: 0.685093
Train - Epoch 2, Batch: 0, Loss: 0.685790
Train - Epoch 2, Batch: 10, Loss: 0.686230
Train - Epoch 2, Batch: 20, Loss: 0.685610
Train - Epoch 2, Batch: 30, Loss: 0.685409
Train - Epoch 2, Batch: 40, Loss: 0.685182
Train - Epoch 2, Batch: 50, Loss: 0.685894
Train - Epoch 2, Batch: 60, Loss: 0.684768
Train - Epoch 2, Batch: 70, Loss: 0.685032
Train - Epoch 2, Batch: 80, Loss: 0.686152
Train - Epoch 2, Batch: 90, Loss: 0.686206
Train - Epoch 2, Batch: 100, Loss: 0.685482
Train - Epoch 2, Batch: 110, Loss: 0.685188
Train - Epoch 2, Batch: 120, Loss: 0.685322
Train - Epoch 2, Batch: 130, Loss: 0.684801
Train - Epoch 2, Batch: 140, Loss: 0.685069
Train - Epoch 2, Batch: 150, Loss: 0.686524
Train - Epoch 2, Batch: 160, Loss: 0.685468
Train - Epoch 2, Batch: 170, Loss: 0.685650
Train - Epoch 2, Batch: 180, Loss: 0.685647
Train - Epoch 2, Batch: 190, Loss: 0.685381
Train - Epoch 2, Batch: 200, Loss: 0.685688
Train - Epoch 2, Batch: 210, Loss: 0.685872
Train - Epoch 2, Batch: 220, Loss: 0.685184
Train - Epoch 2, Batch: 230, Loss: 0.685452
Train - Epoch 2, Batch: 240, Loss: 0.685373
Train - Epoch 2, Batch: 250, Loss: 0.684189
Train - Epoch 2, Batch: 260, Loss: 0.684202
Train - Epoch 2, Batch: 270, Loss: 0.684826
Train - Epoch 2, Batch: 280, Loss: 0.684835
Train - Epoch 2, Batch: 290, Loss: 0.685728
Train - Epoch 2, Batch: 300, Loss: 0.684068
Train - Epoch 2, Batch: 310, Loss: 0.684543
Train - Epoch 2, Batch: 320, Loss: 0.684276
Train - Epoch 2, Batch: 330, Loss: 0.684815
Train - Epoch 2, Batch: 340, Loss: 0.685206
Train - Epoch 2, Batch: 350, Loss: 0.685792
Train - Epoch 2, Batch: 360, Loss: 0.685058
Train - Epoch 2, Batch: 370, Loss: 0.685130
Train - Epoch 2, Batch: 380, Loss: 0.685089
Train - Epoch 2, Batch: 390, Loss: 0.685400
Train - Epoch 2, Batch: 400, Loss: 0.684692
Train - Epoch 2, Batch: 410, Loss: 0.685193
Train - Epoch 2, Batch: 420, Loss: 0.684566
Train - Epoch 2, Batch: 430, Loss: 0.683970
Train - Epoch 2, Batch: 440, Loss: 0.685753
Train - Epoch 2, Batch: 450, Loss: 0.684470
Train - Epoch 2, Batch: 460, Loss: 0.684526
Train - Epoch 2, Batch: 470, Loss: 0.684501
Train - Epoch 2, Batch: 480, Loss: 0.683905
Train - Epoch 2, Batch: 490, Loss: 0.685420
Train - Epoch 2, Batch: 500, Loss: 0.685791
Train - Epoch 2, Batch: 510, Loss: 0.685424
Train - Epoch 2, Batch: 520, Loss: 0.685544
Train - Epoch 2, Batch: 530, Loss: 0.685133
Train - Epoch 2, Batch: 540, Loss: 0.684995
Train - Epoch 2, Batch: 550, Loss: 0.685468/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.686077
Train - Epoch 2, Batch: 570, Loss: 0.684344
Train - Epoch 2, Batch: 580, Loss: 0.684828
Train - Epoch 2, Batch: 590, Loss: 0.683789
Train - Epoch 2, Batch: 600, Loss: 0.684830
Train - Epoch 2, Batch: 610, Loss: 0.683979
Train - Epoch 2, Batch: 620, Loss: 0.684533
Train - Epoch 2, Batch: 630, Loss: 0.684245
Train - Epoch 2, Batch: 640, Loss: 0.684969
Train - Epoch 3, Batch: 0, Loss: 0.684637
Train - Epoch 3, Batch: 10, Loss: 0.685432
Train - Epoch 3, Batch: 20, Loss: 0.683401
Train - Epoch 3, Batch: 30, Loss: 0.684815
Train - Epoch 3, Batch: 40, Loss: 0.684870
Train - Epoch 3, Batch: 50, Loss: 0.684842
Train - Epoch 3, Batch: 60, Loss: 0.683489
Train - Epoch 3, Batch: 70, Loss: 0.684922
Train - Epoch 3, Batch: 80, Loss: 0.684624
Train - Epoch 3, Batch: 90, Loss: 0.683771
Train - Epoch 3, Batch: 100, Loss: 0.684187
Train - Epoch 3, Batch: 110, Loss: 0.684478
Train - Epoch 3, Batch: 120, Loss: 0.684611
Train - Epoch 3, Batch: 130, Loss: 0.683867
Train - Epoch 3, Batch: 140, Loss: 0.683542
Train - Epoch 3, Batch: 150, Loss: 0.685224
Train - Epoch 3, Batch: 160, Loss: 0.685537
Train - Epoch 3, Batch: 170, Loss: 0.684181
Train - Epoch 3, Batch: 180, Loss: 0.684710
Train - Epoch 3, Batch: 190, Loss: 0.685330
Train - Epoch 3, Batch: 200, Loss: 0.684052
Train - Epoch 3, Batch: 210, Loss: 0.684416
Train - Epoch 3, Batch: 220, Loss: 0.684237
Train - Epoch 3, Batch: 230, Loss: 0.684391
Train - Epoch 3, Batch: 240, Loss: 0.684695
Train - Epoch 3, Batch: 250, Loss: 0.684342
Train - Epoch 3, Batch: 260, Loss: 0.685501
Train - Epoch 3, Batch: 270, Loss: 0.686233
Train - Epoch 3, Batch: 280, Loss: 0.684065
Train - Epoch 3, Batch: 290, Loss: 0.684644
Train - Epoch 3, Batch: 300, Loss: 0.684461
Train - Epoch 3, Batch: 310, Loss: 0.683907
Train - Epoch 3, Batch: 320, Loss: 0.682610
Train - Epoch 3, Batch: 330, Loss: 0.683897
Train - Epoch 3, Batch: 340, Loss: 0.683993
Train - Epoch 3, Batch: 350, Loss: 0.684506
Train - Epoch 3, Batch: 360, Loss: 0.684853
Train - Epoch 3, Batch: 370, Loss: 0.683093
Train - Epoch 3, Batch: 380, Loss: 0.684179
Train - Epoch 3, Batch: 390, Loss: 0.684543
Train - Epoch 3, Batch: 400, Loss: 0.685036
Train - Epoch 3, Batch: 410, Loss: 0.685585
Train - Epoch 3, Batch: 420, Loss: 0.685404
Train - Epoch 3, Batch: 430, Loss: 0.683734
Train - Epoch 3, Batch: 440, Loss: 0.685833
Train - Epoch 3, Batch: 450, Loss: 0.684264
Train - Epoch 3, Batch: 460, Loss: 0.683854
Train - Epoch 3, Batch: 470, Loss: 0.683530
Train - Epoch 3, Batch: 480, Loss: 0.683977
Train - Epoch 3, Batch: 490, Loss: 0.684628
Train - Epoch 3, Batch: 500, Loss: 0.683142
Train - Epoch 3, Batch: 510, Loss: 0.684077
Train - Epoch 3, Batch: 520, Loss: 0.683124
Train - Epoch 3, Batch: 530, Loss: 0.682433
Train - Epoch 3, Batch: 540, Loss: 0.682850
Train - Epoch 3, Batch: 550, Loss: 0.683918
Train - Epoch 3, Batch: 560, Loss: 0.683390
Train - Epoch 3, Batch: 570, Loss: 0.684149
Train - Epoch 3, Batch: 580, Loss: 0.684449
Train - Epoch 3, Batch: 590, Loss: 0.684042
Train - Epoch 3, Batch: 600, Loss: 0.683297
Train - Epoch 3, Batch: 610, Loss: 0.684656
Train - Epoch 3, Batch: 620, Loss: 0.683142
Train - Epoch 3, Batch: 630, Loss: 0.683771
Train - Epoch 3, Batch: 640, Loss: 0.683276
training_time:: 7.644213438034058
training time full:: 7.644254207611084
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.551716
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 2100
training time is 4.506685018539429
overhead:: 0
overhead2:: 0
time_baseline:: 4.51107120513916
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551750
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.17592644691467285
overhead3:: 0.27660632133483887
overhead4:: 0.9187057018280029
overhead5:: 0
time_provenance:: 3.003659248352051
curr_diff: 0 tensor(4.4556e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4556e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551756
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.1700592041015625
overhead3:: 0.2549920082092285
overhead4:: 0.9144282341003418
overhead5:: 0
time_provenance:: 2.8961379528045654
curr_diff: 0 tensor(4.4193e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4193e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551756
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.20254778861999512
overhead3:: 0.268627405166626
overhead4:: 1.0128848552703857
overhead5:: 0
time_provenance:: 3.0483005046844482
curr_diff: 0 tensor(4.3317e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3317e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551756
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2198166847229004
overhead3:: 0.28142404556274414
overhead4:: 1.100144386291504
overhead5:: 0
time_provenance:: 3.1071391105651855
curr_diff: 0 tensor(4.3300e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3300e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551756
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.20732641220092773
overhead3:: 0.30066680908203125
overhead4:: 1.2230935096740723
overhead5:: 0
time_provenance:: 3.3203775882720947
curr_diff: 0 tensor(3.4689e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4689e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551748
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.21537399291992188
overhead3:: 0.3219492435455322
overhead4:: 1.3192002773284912
overhead5:: 0
time_provenance:: 3.3951926231384277
curr_diff: 0 tensor(3.4402e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4402e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551748
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.25293803215026855
overhead3:: 0.368375301361084
overhead4:: 1.4446287155151367
overhead5:: 0
time_provenance:: 3.5668022632598877
curr_diff: 0 tensor(3.3947e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3947e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551748
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3543827533721924
overhead3:: 0.5803329944610596
overhead4:: 1.6931123733520508
overhead5:: 0
time_provenance:: 4.239625453948975
curr_diff: 0 tensor(3.3479e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3479e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551748
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.2862272262573242
overhead3:: 0.41744112968444824
overhead4:: 1.761336088180542
overhead5:: 0
time_provenance:: 4.051128387451172
curr_diff: 0 tensor(1.8779e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8779e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.3643066883087158
overhead3:: 0.5451040267944336
overhead4:: 2.037168025970459
overhead5:: 0
time_provenance:: 4.733104467391968
curr_diff: 0 tensor(1.6404e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6404e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551748
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.341510534286499
overhead3:: 0.4991888999938965
overhead4:: 1.9519836902618408
overhead5:: 0
time_provenance:: 4.282122850418091
curr_diff: 0 tensor(1.6017e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6017e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551752
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.34197235107421875
overhead3:: 0.48917531967163086
overhead4:: 2.0862371921539307
overhead5:: 0
time_provenance:: 4.354291677474976
curr_diff: 0 tensor(1.7820e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7820e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.40438294410705566
overhead3:: 0.5610785484313965
overhead4:: 2.443127393722534
overhead5:: 0
time_provenance:: 4.9913671016693115
curr_diff: 0 tensor(1.0104e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0104e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4163970947265625
overhead3:: 0.6070168018341064
overhead4:: 2.5569727420806885
overhead5:: 0
time_provenance:: 5.1427321434021
curr_diff: 0 tensor(1.0005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.4274272918701172
overhead3:: 0.5969479084014893
overhead4:: 2.574664354324341
overhead5:: 0
time_provenance:: 5.048045873641968
curr_diff: 0 tensor(9.9514e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9514e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.44469451904296875
overhead3:: 0.6157534122467041
overhead4:: 2.619795322418213
overhead5:: 0
time_provenance:: 5.097159385681152
curr_diff: 0 tensor(9.8102e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8102e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551746
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 2100
max_epoch:: 4
overhead:: 0
overhead2:: 0.6740188598632812
overhead3:: 0.9485650062561035
overhead4:: 3.4196059703826904
overhead5:: 0
time_provenance:: 5.604196548461914
curr_diff: 0 tensor(3.1803e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1803e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551750
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  higgs 0
tensor([6578176, 4554755, 1622019,  ..., 6586363,  319485,  393215])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.703255
Train - Epoch 0, Batch: 10, Loss: 0.692115
Train - Epoch 0, Batch: 20, Loss: 0.692283
Train - Epoch 0, Batch: 30, Loss: 0.690868
Train - Epoch 0, Batch: 40, Loss: 0.691474
Train - Epoch 0, Batch: 50, Loss: 0.691322
Train - Epoch 0, Batch: 60, Loss: 0.689902
Train - Epoch 0, Batch: 70, Loss: 0.690590
Train - Epoch 0, Batch: 80, Loss: 0.689999
Train - Epoch 0, Batch: 90, Loss: 0.689640
Train - Epoch 0, Batch: 100, Loss: 0.689791
Train - Epoch 0, Batch: 110, Loss: 0.689765
Train - Epoch 0, Batch: 120, Loss: 0.689857
Train - Epoch 0, Batch: 130, Loss: 0.690320
Train - Epoch 0, Batch: 140, Loss: 0.688316
Train - Epoch 0, Batch: 150, Loss: 0.688617
Train - Epoch 0, Batch: 160, Loss: 0.689690
Train - Epoch 0, Batch: 170, Loss: 0.688578
Train - Epoch 0, Batch: 180, Loss: 0.688846
Train - Epoch 0, Batch: 190, Loss: 0.689689
Train - Epoch 0, Batch: 200, Loss: 0.688317
Train - Epoch 0, Batch: 210, Loss: 0.688019
Train - Epoch 0, Batch: 220, Loss: 0.687481
Train - Epoch 0, Batch: 230, Loss: 0.688247
Train - Epoch 0, Batch: 240, Loss: 0.687006
Train - Epoch 0, Batch: 250, Loss: 0.687232
Train - Epoch 0, Batch: 260, Loss: 0.687610
Train - Epoch 0, Batch: 270, Loss: 0.687953
Train - Epoch 0, Batch: 280, Loss: 0.687959
Train - Epoch 0, Batch: 290, Loss: 0.688792
Train - Epoch 0, Batch: 300, Loss: 0.688264
Train - Epoch 0, Batch: 310, Loss: 0.688638
Train - Epoch 0, Batch: 320, Loss: 0.688237
Train - Epoch 0, Batch: 330, Loss: 0.687878
Train - Epoch 0, Batch: 340, Loss: 0.687133
Train - Epoch 0, Batch: 350, Loss: 0.688288
Train - Epoch 0, Batch: 360, Loss: 0.686811
Train - Epoch 0, Batch: 370, Loss: 0.688772
Train - Epoch 0, Batch: 380, Loss: 0.687296
Train - Epoch 0, Batch: 390, Loss: 0.686743
Train - Epoch 0, Batch: 400, Loss: 0.686603
Train - Epoch 0, Batch: 410, Loss: 0.688158
Train - Epoch 0, Batch: 420, Loss: 0.686271
Train - Epoch 0, Batch: 430, Loss: 0.686467
Train - Epoch 0, Batch: 440, Loss: 0.687970
Train - Epoch 0, Batch: 450, Loss: 0.687202
Train - Epoch 0, Batch: 460, Loss: 0.687421
Train - Epoch 0, Batch: 470, Loss: 0.686781
Train - Epoch 0, Batch: 480, Loss: 0.685699
Train - Epoch 0, Batch: 490, Loss: 0.686583
Train - Epoch 0, Batch: 500, Loss: 0.686172
Train - Epoch 0, Batch: 510, Loss: 0.687239
Train - Epoch 0, Batch: 520, Loss: 0.687630
Train - Epoch 0, Batch: 530, Loss: 0.686133
Train - Epoch 0, Batch: 540, Loss: 0.685329
Train - Epoch 0, Batch: 550, Loss: 0.686367
Train - Epoch 0, Batch: 560, Loss: 0.686028
Train - Epoch 0, Batch: 570, Loss: 0.687556
Train - Epoch 0, Batch: 580, Loss: 0.686612
Train - Epoch 0, Batch: 590, Loss: 0.686840
Train - Epoch 0, Batch: 600, Loss: 0.686649
Train - Epoch 0, Batch: 610, Loss: 0.686721
Train - Epoch 0, Batch: 620, Loss: 0.687668
Train - Epoch 0, Batch: 630, Loss: 0.687945
Train - Epoch 0, Batch: 640, Loss: 0.686597
Train - Epoch 1, Batch: 0, Loss: 0.685487
Train - Epoch 1, Batch: 10, Loss: 0.686045
Train - Epoch 1, Batch: 20, Loss: 0.686070
Train - Epoch 1, Batch: 30, Loss: 0.687184
Train - Epoch 1, Batch: 40, Loss: 0.685650
Train - Epoch 1, Batch: 50, Loss: 0.686956
Train - Epoch 1, Batch: 60, Loss: 0.686913
Train - Epoch 1, Batch: 70, Loss: 0.687847
Train - Epoch 1, Batch: 80, Loss: 0.686193
Train - Epoch 1, Batch: 90, Loss: 0.686125
Train - Epoch 1, Batch: 100, Loss: 0.685987
Train - Epoch 1, Batch: 110, Loss: 0.686362
Train - Epoch 1, Batch: 120, Loss: 0.686168
Train - Epoch 1, Batch: 130, Loss: 0.686598
Train - Epoch 1, Batch: 140, Loss: 0.686395
Train - Epoch 1, Batch: 150, Loss: 0.686054
Train - Epoch 1, Batch: 160, Loss: 0.685704
Train - Epoch 1, Batch: 170, Loss: 0.686048
Train - Epoch 1, Batch: 180, Loss: 0.685378
Train - Epoch 1, Batch: 190, Loss: 0.686515
Train - Epoch 1, Batch: 200, Loss: 0.685973
Train - Epoch 1, Batch: 210, Loss: 0.686454
Train - Epoch 1, Batch: 220, Loss: 0.685250
Train - Epoch 1, Batch: 230, Loss: 0.685986
Train - Epoch 1, Batch: 240, Loss: 0.685146
Train - Epoch 1, Batch: 250, Loss: 0.685457
Train - Epoch 1, Batch: 260, Loss: 0.686873
Train - Epoch 1, Batch: 270, Loss: 0.684783
Train - Epoch 1, Batch: 280, Loss: 0.684307
Train - Epoch 1, Batch: 290, Loss: 0.685955
Train - Epoch 1, Batch: 300, Loss: 0.685193
Train - Epoch 1, Batch: 310, Loss: 0.685795
Train - Epoch 1, Batch: 320, Loss: 0.685472
Train - Epoch 1, Batch: 330, Loss: 0.684725
Train - Epoch 1, Batch: 340, Loss: 0.685897
Train - Epoch 1, Batch: 350, Loss: 0.685093
Train - Epoch 1, Batch: 360, Loss: 0.684972
Train - Epoch 1, Batch: 370, Loss: 0.685586
Train - Epoch 1, Batch: 380, Loss: 0.686568
Train - Epoch 1, Batch: 390, Loss: 0.686312
Train - Epoch 1, Batch: 400, Loss: 0.685054
Train - Epoch 1, Batch: 410, Loss: 0.685766
Train - Epoch 1, Batch: 420, Loss: 0.686929
Train - Epoch 1, Batch: 430, Loss: 0.684419
Train - Epoch 1, Batch: 440, Loss: 0.684689
Train - Epoch 1, Batch: 450, Loss: 0.686654
Train - Epoch 1, Batch: 460, Loss: 0.685068
Train - Epoch 1, Batch: 470, Loss: 0.684740
Train - Epoch 1, Batch: 480, Loss: 0.685007
Train - Epoch 1, Batch: 490, Loss: 0.685366
Train - Epoch 1, Batch: 500, Loss: 0.685814
Train - Epoch 1, Batch: 510, Loss: 0.684426
Train - Epoch 1, Batch: 520, Loss: 0.685820
Train - Epoch 1, Batch: 530, Loss: 0.685857
Train - Epoch 1, Batch: 540, Loss: 0.684899
Train - Epoch 1, Batch: 550, Loss: 0.684981
Train - Epoch 1, Batch: 560, Loss: 0.686105
Train - Epoch 1, Batch: 570, Loss: 0.685034
Train - Epoch 1, Batch: 580, Loss: 0.685580
Train - Epoch 1, Batch: 590, Loss: 0.685461
Train - Epoch 1, Batch: 600, Loss: 0.685597
Train - Epoch 1, Batch: 610, Loss: 0.684465
Train - Epoch 1, Batch: 620, Loss: 0.684561
Train - Epoch 1, Batch: 630, Loss: 0.685201
Train - Epoch 1, Batch: 640, Loss: 0.685846
Train - Epoch 2, Batch: 0, Loss: 0.685396
Train - Epoch 2, Batch: 10, Loss: 0.684858
Train - Epoch 2, Batch: 20, Loss: 0.685507
Train - Epoch 2, Batch: 30, Loss: 0.683830
Train - Epoch 2, Batch: 40, Loss: 0.685075
Train - Epoch 2, Batch: 50, Loss: 0.685365
Train - Epoch 2, Batch: 60, Loss: 0.685861
Train - Epoch 2, Batch: 70, Loss: 0.685033
Train - Epoch 2, Batch: 80, Loss: 0.685066
Train - Epoch 2, Batch: 90, Loss: 0.685328
Train - Epoch 2, Batch: 100, Loss: 0.683643
Train - Epoch 2, Batch: 110, Loss: 0.684919
Train - Epoch 2, Batch: 120, Loss: 0.684358
Train - Epoch 2, Batch: 130, Loss: 0.684473
Train - Epoch 2, Batch: 140, Loss: 0.685034
Train - Epoch 2, Batch: 150, Loss: 0.684984
Train - Epoch 2, Batch: 160, Loss: 0.685365
Train - Epoch 2, Batch: 170, Loss: 0.685158
Train - Epoch 2, Batch: 180, Loss: 0.684767
Train - Epoch 2, Batch: 190, Loss: 0.684859
Train - Epoch 2, Batch: 200, Loss: 0.684862
Train - Epoch 2, Batch: 210, Loss: 0.687098
Train - Epoch 2, Batch: 220, Loss: 0.684823
Train - Epoch 2, Batch: 230, Loss: 0.685489
Train - Epoch 2, Batch: 240, Loss: 0.685245
Train - Epoch 2, Batch: 250, Loss: 0.684852
Train - Epoch 2, Batch: 260, Loss: 0.683891
Train - Epoch 2, Batch: 270, Loss: 0.684631
Train - Epoch 2, Batch: 280, Loss: 0.683885
Train - Epoch 2, Batch: 290, Loss: 0.685627
Train - Epoch 2, Batch: 300, Loss: 0.684332
Train - Epoch 2, Batch: 310, Loss: 0.684004
Train - Epoch 2, Batch: 320, Loss: 0.685152
Train - Epoch 2, Batch: 330, Loss: 0.683614
Train - Epoch 2, Batch: 340, Loss: 0.682764
Train - Epoch 2, Batch: 350, Loss: 0.684602
Train - Epoch 2, Batch: 360, Loss: 0.684981
Train - Epoch 2, Batch: 370, Loss: 0.684550
Train - Epoch 2, Batch: 380, Loss: 0.685183
Train - Epoch 2, Batch: 390, Loss: 0.685225
Train - Epoch 2, Batch: 400, Loss: 0.683167
Train - Epoch 2, Batch: 410, Loss: 0.684459
Train - Epoch 2, Batch: 420, Loss: 0.685892
Train - Epoch 2, Batch: 430, Loss: 0.684585
Train - Epoch 2, Batch: 440, Loss: 0.683501
Train - Epoch 2, Batch: 450, Loss: 0.683903
Train - Epoch 2, Batch: 460, Loss: 0.683592
Train - Epoch 2, Batch: 470, Loss: 0.684014
Train - Epoch 2, Batch: 480, Loss: 0.684581
Train - Epoch 2, Batch: 490, Loss: 0.684500
Train - Epoch 2, Batch: 500, Loss: 0.684702
Train - Epoch 2, Batch: 510, Loss: 0.684009
Train - Epoch 2, Batch: 520, Loss: 0.685245
Train - Epoch 2, Batch: 530, Loss: 0.684811
Train - Epoch 2, Batch: 540, Loss: 0.684669
Train - Epoch 2, Batch: 550, Loss: 0.683583/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684944
Train - Epoch 2, Batch: 570, Loss: 0.684251
Train - Epoch 2, Batch: 580, Loss: 0.683452
Train - Epoch 2, Batch: 590, Loss: 0.684274
Train - Epoch 2, Batch: 600, Loss: 0.686297
Train - Epoch 2, Batch: 610, Loss: 0.685535
Train - Epoch 2, Batch: 620, Loss: 0.683670
Train - Epoch 2, Batch: 630, Loss: 0.683739
Train - Epoch 2, Batch: 640, Loss: 0.684903
Train - Epoch 3, Batch: 0, Loss: 0.684140
Train - Epoch 3, Batch: 10, Loss: 0.684602
Train - Epoch 3, Batch: 20, Loss: 0.682975
Train - Epoch 3, Batch: 30, Loss: 0.685300
Train - Epoch 3, Batch: 40, Loss: 0.683688
Train - Epoch 3, Batch: 50, Loss: 0.684440
Train - Epoch 3, Batch: 60, Loss: 0.683341
Train - Epoch 3, Batch: 70, Loss: 0.684521
Train - Epoch 3, Batch: 80, Loss: 0.684473
Train - Epoch 3, Batch: 90, Loss: 0.684266
Train - Epoch 3, Batch: 100, Loss: 0.683919
Train - Epoch 3, Batch: 110, Loss: 0.683891
Train - Epoch 3, Batch: 120, Loss: 0.685290
Train - Epoch 3, Batch: 130, Loss: 0.684914
Train - Epoch 3, Batch: 140, Loss: 0.684583
Train - Epoch 3, Batch: 150, Loss: 0.683823
Train - Epoch 3, Batch: 160, Loss: 0.682903
Train - Epoch 3, Batch: 170, Loss: 0.682686
Train - Epoch 3, Batch: 180, Loss: 0.684916
Train - Epoch 3, Batch: 190, Loss: 0.683101
Train - Epoch 3, Batch: 200, Loss: 0.685638
Train - Epoch 3, Batch: 210, Loss: 0.683336
Train - Epoch 3, Batch: 220, Loss: 0.684097
Train - Epoch 3, Batch: 230, Loss: 0.683675
Train - Epoch 3, Batch: 240, Loss: 0.683805
Train - Epoch 3, Batch: 250, Loss: 0.684653
Train - Epoch 3, Batch: 260, Loss: 0.684453
Train - Epoch 3, Batch: 270, Loss: 0.684557
Train - Epoch 3, Batch: 280, Loss: 0.683854
Train - Epoch 3, Batch: 290, Loss: 0.683380
Train - Epoch 3, Batch: 300, Loss: 0.682785
Train - Epoch 3, Batch: 310, Loss: 0.684491
Train - Epoch 3, Batch: 320, Loss: 0.684038
Train - Epoch 3, Batch: 330, Loss: 0.683757
Train - Epoch 3, Batch: 340, Loss: 0.683679
Train - Epoch 3, Batch: 350, Loss: 0.684532
Train - Epoch 3, Batch: 360, Loss: 0.685292
Train - Epoch 3, Batch: 370, Loss: 0.683656
Train - Epoch 3, Batch: 380, Loss: 0.682925
Train - Epoch 3, Batch: 390, Loss: 0.684454
Train - Epoch 3, Batch: 400, Loss: 0.683889
Train - Epoch 3, Batch: 410, Loss: 0.683442
Train - Epoch 3, Batch: 420, Loss: 0.682799
Train - Epoch 3, Batch: 430, Loss: 0.684061
Train - Epoch 3, Batch: 440, Loss: 0.683012
Train - Epoch 3, Batch: 450, Loss: 0.683463
Train - Epoch 3, Batch: 460, Loss: 0.684024
Train - Epoch 3, Batch: 470, Loss: 0.684378
Train - Epoch 3, Batch: 480, Loss: 0.685083
Train - Epoch 3, Batch: 490, Loss: 0.683023
Train - Epoch 3, Batch: 500, Loss: 0.684644
Train - Epoch 3, Batch: 510, Loss: 0.683725
Train - Epoch 3, Batch: 520, Loss: 0.684694
Train - Epoch 3, Batch: 530, Loss: 0.683204
Train - Epoch 3, Batch: 540, Loss: 0.684075
Train - Epoch 3, Batch: 550, Loss: 0.684167
Train - Epoch 3, Batch: 560, Loss: 0.684514
Train - Epoch 3, Batch: 570, Loss: 0.684282
Train - Epoch 3, Batch: 580, Loss: 0.683415
Train - Epoch 3, Batch: 590, Loss: 0.684682
Train - Epoch 3, Batch: 600, Loss: 0.684187
Train - Epoch 3, Batch: 610, Loss: 0.683584
Train - Epoch 3, Batch: 620, Loss: 0.682391
Train - Epoch 3, Batch: 630, Loss: 0.683290
Train - Epoch 3, Batch: 640, Loss: 0.683742
training_time:: 7.689921855926514
training time full:: 7.689964056015015
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554016
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 5250
training time is 5.438891649246216
overhead:: 0
overhead2:: 0
time_baseline:: 5.441948175430298
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553974
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.14108538627624512
overhead3:: 0.19412636756896973
overhead4:: 0.833115816116333
overhead5:: 0
time_provenance:: 2.775114059448242
curr_diff: 0 tensor(6.2151e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2151e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553978
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1908876895904541
overhead3:: 0.2636234760284424
overhead4:: 0.9213507175445557
overhead5:: 0
time_provenance:: 3.004730224609375
curr_diff: 0 tensor(6.1110e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1110e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553980
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2265462875366211
overhead3:: 0.3022332191467285
overhead4:: 1.0597224235534668
overhead5:: 0
time_provenance:: 3.1261932849884033
curr_diff: 0 tensor(6.1152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553980
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2131803035736084
overhead3:: 0.290637731552124
overhead4:: 1.1655147075653076
overhead5:: 0
time_provenance:: 3.164175271987915
curr_diff: 0 tensor(6.0809e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0809e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553980
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2511141300201416
overhead3:: 0.34836769104003906
overhead4:: 1.3855538368225098
overhead5:: 0
time_provenance:: 4.185812473297119
curr_diff: 0 tensor(2.9221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.23666095733642578
overhead3:: 0.306654691696167
overhead4:: 1.3162689208984375
overhead5:: 0
time_provenance:: 3.539963722229004
curr_diff: 0 tensor(2.7800e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7800e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.31559062004089355
overhead3:: 0.43963193893432617
overhead4:: 1.602919340133667
overhead5:: 0
time_provenance:: 4.3645031452178955
curr_diff: 0 tensor(2.7723e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7723e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.30457520484924316
overhead3:: 0.42192506790161133
overhead4:: 1.7138960361480713
overhead5:: 0
time_provenance:: 4.448414325714111
curr_diff: 0 tensor(2.7059e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7059e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2846677303314209
overhead3:: 0.4005300998687744
overhead4:: 1.7565670013427734
overhead5:: 0
time_provenance:: 4.027036905288696
curr_diff: 0 tensor(2.0183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3032379150390625
overhead3:: 0.4440798759460449
overhead4:: 1.8227229118347168
overhead5:: 0
time_provenance:: 4.100418329238892
curr_diff: 0 tensor(1.6687e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6687e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.39010143280029297
overhead3:: 0.514662504196167
overhead4:: 2.1559674739837646
overhead5:: 0
time_provenance:: 5.173404932022095
curr_diff: 0 tensor(1.7765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7765e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3525722026824951
overhead3:: 0.4841277599334717
overhead4:: 2.0573675632476807
overhead5:: 0
time_provenance:: 4.341515302658081
curr_diff: 0 tensor(1.9612e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9612e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3929777145385742
overhead3:: 0.5585455894470215
overhead4:: 2.4496865272521973
overhead5:: 0
time_provenance:: 4.977699041366577
curr_diff: 0 tensor(1.0985e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0985e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.409104585647583
overhead3:: 0.5651636123657227
overhead4:: 2.4785048961639404
overhead5:: 0
time_provenance:: 4.963075399398804
curr_diff: 0 tensor(1.0747e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0747e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.43824267387390137
overhead3:: 0.6207449436187744
overhead4:: 2.4402427673339844
overhead5:: 0
time_provenance:: 4.944307088851929
curr_diff: 0 tensor(1.0623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4258146286010742
overhead3:: 0.5858519077301025
overhead4:: 2.5541107654571533
overhead5:: 0
time_provenance:: 4.982914686203003
curr_diff: 0 tensor(1.0561e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0561e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553976
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.799008846282959
overhead3:: 1.0994231700897217
overhead4:: 3.642103433609009
overhead5:: 0
time_provenance:: 6.142768383026123
curr_diff: 0 tensor(2.7691e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7691e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553974
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.712753
Train - Epoch 0, Batch: 10, Loss: 0.693537
Train - Epoch 0, Batch: 20, Loss: 0.692831
Train - Epoch 0, Batch: 30, Loss: 0.691619
Train - Epoch 0, Batch: 40, Loss: 0.691124
Train - Epoch 0, Batch: 50, Loss: 0.690305
Train - Epoch 0, Batch: 60, Loss: 0.690057
Train - Epoch 0, Batch: 70, Loss: 0.689901
Train - Epoch 0, Batch: 80, Loss: 0.690500
Train - Epoch 0, Batch: 90, Loss: 0.689035
Train - Epoch 0, Batch: 100, Loss: 0.688070
Train - Epoch 0, Batch: 110, Loss: 0.689105
Train - Epoch 0, Batch: 120, Loss: 0.688418
Train - Epoch 0, Batch: 130, Loss: 0.686744
Train - Epoch 0, Batch: 140, Loss: 0.689158
Train - Epoch 0, Batch: 150, Loss: 0.688813
Train - Epoch 0, Batch: 160, Loss: 0.688708
Train - Epoch 0, Batch: 170, Loss: 0.688568
Train - Epoch 0, Batch: 180, Loss: 0.687972
Train - Epoch 0, Batch: 190, Loss: 0.688742
Train - Epoch 0, Batch: 200, Loss: 0.688418
Train - Epoch 0, Batch: 210, Loss: 0.687121
Train - Epoch 0, Batch: 220, Loss: 0.686527
Train - Epoch 0, Batch: 230, Loss: 0.688152
Train - Epoch 0, Batch: 240, Loss: 0.687106
Train - Epoch 0, Batch: 250, Loss: 0.687970
Train - Epoch 0, Batch: 260, Loss: 0.687136
Train - Epoch 0, Batch: 270, Loss: 0.687106
Train - Epoch 0, Batch: 280, Loss: 0.687275
Train - Epoch 0, Batch: 290, Loss: 0.686942
Train - Epoch 0, Batch: 300, Loss: 0.688105
Train - Epoch 0, Batch: 310, Loss: 0.687443
Train - Epoch 0, Batch: 320, Loss: 0.687689
Train - Epoch 0, Batch: 330, Loss: 0.687349
Train - Epoch 0, Batch: 340, Loss: 0.688035
Train - Epoch 0, Batch: 350, Loss: 0.687654
Train - Epoch 0, Batch: 360, Loss: 0.688157
Train - Epoch 0, Batch: 370, Loss: 0.687837
Train - Epoch 0, Batch: 380, Loss: 0.688237
Train - Epoch 0, Batch: 390, Loss: 0.688630
Train - Epoch 0, Batch: 400, Loss: 0.687904
Train - Epoch 0, Batch: 410, Loss: 0.687666
Train - Epoch 0, Batch: 420, Loss: 0.686500
Train - Epoch 0, Batch: 430, Loss: 0.686170
Train - Epoch 0, Batch: 440, Loss: 0.686644
Train - Epoch 0, Batch: 450, Loss: 0.686837
Train - Epoch 0, Batch: 460, Loss: 0.687499
Train - Epoch 0, Batch: 470, Loss: 0.685584
Train - Epoch 0, Batch: 480, Loss: 0.687131
Train - Epoch 0, Batch: 490, Loss: 0.686337
Train - Epoch 0, Batch: 500, Loss: 0.686224
Train - Epoch 0, Batch: 510, Loss: 0.686837
Train - Epoch 0, Batch: 520, Loss: 0.687314
Train - Epoch 0, Batch: 530, Loss: 0.687376
Train - Epoch 0, Batch: 540, Loss: 0.687739
Train - Epoch 0, Batch: 550, Loss: 0.686134
Train - Epoch 0, Batch: 560, Loss: 0.686424
Train - Epoch 0, Batch: 570, Loss: 0.686539
Train - Epoch 0, Batch: 580, Loss: 0.685858
Train - Epoch 0, Batch: 590, Loss: 0.686843
Train - Epoch 0, Batch: 600, Loss: 0.686717
Train - Epoch 0, Batch: 610, Loss: 0.686867
Train - Epoch 0, Batch: 620, Loss: 0.687257
Train - Epoch 0, Batch: 630, Loss: 0.687289
Train - Epoch 0, Batch: 640, Loss: 0.687203
Train - Epoch 1, Batch: 0, Loss: 0.687700
Train - Epoch 1, Batch: 10, Loss: 0.686282
Train - Epoch 1, Batch: 20, Loss: 0.686058
Train - Epoch 1, Batch: 30, Loss: 0.686847
Train - Epoch 1, Batch: 40, Loss: 0.686320
Train - Epoch 1, Batch: 50, Loss: 0.685810
Train - Epoch 1, Batch: 60, Loss: 0.686049
Train - Epoch 1, Batch: 70, Loss: 0.687782
Train - Epoch 1, Batch: 80, Loss: 0.687718
Train - Epoch 1, Batch: 90, Loss: 0.686062
Train - Epoch 1, Batch: 100, Loss: 0.684559
Train - Epoch 1, Batch: 110, Loss: 0.685414
Train - Epoch 1, Batch: 120, Loss: 0.687300
Train - Epoch 1, Batch: 130, Loss: 0.686725
Train - Epoch 1, Batch: 140, Loss: 0.686239
Train - Epoch 1, Batch: 150, Loss: 0.685929
Train - Epoch 1, Batch: 160, Loss: 0.685689
Train - Epoch 1, Batch: 170, Loss: 0.685006
Train - Epoch 1, Batch: 180, Loss: 0.686732
Train - Epoch 1, Batch: 190, Loss: 0.685680
Train - Epoch 1, Batch: 200, Loss: 0.686269
Train - Epoch 1, Batch: 210, Loss: 0.685963
Train - Epoch 1, Batch: 220, Loss: 0.685450
Train - Epoch 1, Batch: 230, Loss: 0.686452
Train - Epoch 1, Batch: 240, Loss: 0.686535
Train - Epoch 1, Batch: 250, Loss: 0.685060
Train - Epoch 1, Batch: 260, Loss: 0.686031
Train - Epoch 1, Batch: 270, Loss: 0.684650
Train - Epoch 1, Batch: 280, Loss: 0.684935
Train - Epoch 1, Batch: 290, Loss: 0.685828
Train - Epoch 1, Batch: 300, Loss: 0.685947
Train - Epoch 1, Batch: 310, Loss: 0.687294
Train - Epoch 1, Batch: 320, Loss: 0.686118
Train - Epoch 1, Batch: 330, Loss: 0.685400
Train - Epoch 1, Batch: 340, Loss: 0.685123
Train - Epoch 1, Batch: 350, Loss: 0.684381
Train - Epoch 1, Batch: 360, Loss: 0.686023
Train - Epoch 1, Batch: 370, Loss: 0.686001
Train - Epoch 1, Batch: 380, Loss: 0.685586
Train - Epoch 1, Batch: 390, Loss: 0.685990
Train - Epoch 1, Batch: 400, Loss: 0.684598
Train - Epoch 1, Batch: 410, Loss: 0.685714
Train - Epoch 1, Batch: 420, Loss: 0.684486
Train - Epoch 1, Batch: 430, Loss: 0.685412
Train - Epoch 1, Batch: 440, Loss: 0.685861
Train - Epoch 1, Batch: 450, Loss: 0.685093
Train - Epoch 1, Batch: 460, Loss: 0.686350
Train - Epoch 1, Batch: 470, Loss: 0.685598
Train - Epoch 1, Batch: 480, Loss: 0.685837
Train - Epoch 1, Batch: 490, Loss: 0.684987
Train - Epoch 1, Batch: 500, Loss: 0.684873
Train - Epoch 1, Batch: 510, Loss: 0.686190
Train - Epoch 1, Batch: 520, Loss: 0.684732
Train - Epoch 1, Batch: 530, Loss: 0.685982
Train - Epoch 1, Batch: 540, Loss: 0.686310
Train - Epoch 1, Batch: 550, Loss: 0.684429
Train - Epoch 1, Batch: 560, Loss: 0.686596
Train - Epoch 1, Batch: 570, Loss: 0.684469
Train - Epoch 1, Batch: 580, Loss: 0.686197
Train - Epoch 1, Batch: 590, Loss: 0.684017
Train - Epoch 1, Batch: 600, Loss: 0.685107
Train - Epoch 1, Batch: 610, Loss: 0.684800
Train - Epoch 1, Batch: 620, Loss: 0.684654
Train - Epoch 1, Batch: 630, Loss: 0.684852
Train - Epoch 1, Batch: 640, Loss: 0.684415
Train - Epoch 2, Batch: 0, Loss: 0.685070
Train - Epoch 2, Batch: 10, Loss: 0.684926
Train - Epoch 2, Batch: 20, Loss: 0.685089
Train - Epoch 2, Batch: 30, Loss: 0.685924
Train - Epoch 2, Batch: 40, Loss: 0.685758
Train - Epoch 2, Batch: 50, Loss: 0.686167
Train - Epoch 2, Batch: 60, Loss: 0.684569
Train - Epoch 2, Batch: 70, Loss: 0.684106
Train - Epoch 2, Batch: 80, Loss: 0.685432
Train - Epoch 2, Batch: 90, Loss: 0.684290
Train - Epoch 2, Batch: 100, Loss: 0.683291
Train - Epoch 2, Batch: 110, Loss: 0.684266
Train - Epoch 2, Batch: 120, Loss: 0.685323
Train - Epoch 2, Batch: 130, Loss: 0.684969
Train - Epoch 2, Batch: 140, Loss: 0.686275
Train - Epoch 2, Batch: 150, Loss: 0.685812
Train - Epoch 2, Batch: 160, Loss: 0.684680
Train - Epoch 2, Batch: 170, Loss: 0.685031
Train - Epoch 2, Batch: 180, Loss: 0.685157
Train - Epoch 2, Batch: 190, Loss: 0.684949
Train - Epoch 2, Batch: 200, Loss: 0.684856
Train - Epoch 2, Batch: 210, Loss: 0.684711
Train - Epoch 2, Batch: 220, Loss: 0.684306
Train - Epoch 2, Batch: 230, Loss: 0.685304
Train - Epoch 2, Batch: 240, Loss: 0.685033
Train - Epoch 2, Batch: 250, Loss: 0.685203
Train - Epoch 2, Batch: 260, Loss: 0.684290
Train - Epoch 2, Batch: 270, Loss: 0.684167
Train - Epoch 2, Batch: 280, Loss: 0.683933
Train - Epoch 2, Batch: 290, Loss: 0.684558
Train - Epoch 2, Batch: 300, Loss: 0.685179
Train - Epoch 2, Batch: 310, Loss: 0.685292
Train - Epoch 2, Batch: 320, Loss: 0.685172
Train - Epoch 2, Batch: 330, Loss: 0.684666
Train - Epoch 2, Batch: 340, Loss: 0.683966
Train - Epoch 2, Batch: 350, Loss: 0.685342
Train - Epoch 2, Batch: 360, Loss: 0.685393
Train - Epoch 2, Batch: 370, Loss: 0.684534
Train - Epoch 2, Batch: 380, Loss: 0.683628
Train - Epoch 2, Batch: 390, Loss: 0.684760
Train - Epoch 2, Batch: 400, Loss: 0.684193
Train - Epoch 2, Batch: 410, Loss: 0.684531
Train - Epoch 2, Batch: 420, Loss: 0.684558
Train - Epoch 2, Batch: 430, Loss: 0.684325
Train - Epoch 2, Batch: 440, Loss: 0.683496
Train - Epoch 2, Batch: 450, Loss: 0.685396
Train - Epoch 2, Batch: 460, Loss: 0.684183
Train - Epoch 2, Batch: 470, Loss: 0.684952
Train - Epoch 2, Batch: 480, Loss: 0.684937
Train - Epoch 2, Batch: 490, Loss: 0.684285
Train - Epoch 2, Batch: 500, Loss: 0.684810
Train - Epoch 2, Batch: 510, Loss: 0.683310
Train - Epoch 2, Batch: 520, Loss: 0.684186
Train - Epoch 2, Batch: 530, Loss: 0.683778
Train - Epoch 2, Batch: 540, Loss: 0.683256
Train - Epoch 2, Batch: 550, Loss: 0.684228/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684971
Train - Epoch 2, Batch: 570, Loss: 0.684054
Train - Epoch 2, Batch: 580, Loss: 0.684516
Train - Epoch 2, Batch: 590, Loss: 0.684862
Train - Epoch 2, Batch: 600, Loss: 0.685557
Train - Epoch 2, Batch: 610, Loss: 0.683817
Train - Epoch 2, Batch: 620, Loss: 0.684321
Train - Epoch 2, Batch: 630, Loss: 0.683834
Train - Epoch 2, Batch: 640, Loss: 0.683751
Train - Epoch 3, Batch: 0, Loss: 0.683263
Train - Epoch 3, Batch: 10, Loss: 0.684453
Train - Epoch 3, Batch: 20, Loss: 0.684795
Train - Epoch 3, Batch: 30, Loss: 0.684807
Train - Epoch 3, Batch: 40, Loss: 0.684030
Train - Epoch 3, Batch: 50, Loss: 0.684787
Train - Epoch 3, Batch: 60, Loss: 0.683529
Train - Epoch 3, Batch: 70, Loss: 0.683868
Train - Epoch 3, Batch: 80, Loss: 0.684492
Train - Epoch 3, Batch: 90, Loss: 0.683209
Train - Epoch 3, Batch: 100, Loss: 0.684331
Train - Epoch 3, Batch: 110, Loss: 0.684505
Train - Epoch 3, Batch: 120, Loss: 0.684464
Train - Epoch 3, Batch: 130, Loss: 0.684899
Train - Epoch 3, Batch: 140, Loss: 0.684192
Train - Epoch 3, Batch: 150, Loss: 0.684023
Train - Epoch 3, Batch: 160, Loss: 0.683926
Train - Epoch 3, Batch: 170, Loss: 0.683239
Train - Epoch 3, Batch: 180, Loss: 0.683826
Train - Epoch 3, Batch: 190, Loss: 0.685070
Train - Epoch 3, Batch: 200, Loss: 0.683430
Train - Epoch 3, Batch: 210, Loss: 0.684072
Train - Epoch 3, Batch: 220, Loss: 0.684446
Train - Epoch 3, Batch: 230, Loss: 0.683421
Train - Epoch 3, Batch: 240, Loss: 0.683397
Train - Epoch 3, Batch: 250, Loss: 0.683513
Train - Epoch 3, Batch: 260, Loss: 0.682822
Train - Epoch 3, Batch: 270, Loss: 0.682993
Train - Epoch 3, Batch: 280, Loss: 0.683881
Train - Epoch 3, Batch: 290, Loss: 0.684339
Train - Epoch 3, Batch: 300, Loss: 0.683022
Train - Epoch 3, Batch: 310, Loss: 0.683742
Train - Epoch 3, Batch: 320, Loss: 0.683976
Train - Epoch 3, Batch: 330, Loss: 0.682694
Train - Epoch 3, Batch: 340, Loss: 0.684223
Train - Epoch 3, Batch: 350, Loss: 0.683107
Train - Epoch 3, Batch: 360, Loss: 0.685080
Train - Epoch 3, Batch: 370, Loss: 0.683862
Train - Epoch 3, Batch: 380, Loss: 0.684399
Train - Epoch 3, Batch: 390, Loss: 0.685022
Train - Epoch 3, Batch: 400, Loss: 0.684298
Train - Epoch 3, Batch: 410, Loss: 0.684209
Train - Epoch 3, Batch: 420, Loss: 0.683973
Train - Epoch 3, Batch: 430, Loss: 0.684020
Train - Epoch 3, Batch: 440, Loss: 0.683799
Train - Epoch 3, Batch: 450, Loss: 0.683641
Train - Epoch 3, Batch: 460, Loss: 0.683498
Train - Epoch 3, Batch: 470, Loss: 0.684018
Train - Epoch 3, Batch: 480, Loss: 0.683299
Train - Epoch 3, Batch: 490, Loss: 0.683865
Train - Epoch 3, Batch: 500, Loss: 0.683897
Train - Epoch 3, Batch: 510, Loss: 0.682884
Train - Epoch 3, Batch: 520, Loss: 0.685046
Train - Epoch 3, Batch: 530, Loss: 0.682975
Train - Epoch 3, Batch: 540, Loss: 0.683827
Train - Epoch 3, Batch: 550, Loss: 0.683158
Train - Epoch 3, Batch: 560, Loss: 0.682997
Train - Epoch 3, Batch: 570, Loss: 0.683545
Train - Epoch 3, Batch: 580, Loss: 0.682705
Train - Epoch 3, Batch: 590, Loss: 0.682605
Train - Epoch 3, Batch: 600, Loss: 0.683565
Train - Epoch 3, Batch: 610, Loss: 0.682788
Train - Epoch 3, Batch: 620, Loss: 0.684280
Train - Epoch 3, Batch: 630, Loss: 0.683645
Train - Epoch 3, Batch: 640, Loss: 0.684437
training_time:: 7.807888507843018
training time full:: 7.807929992675781
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553850
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 5250
training time is 5.7463696002960205
overhead:: 0
overhead2:: 0
time_baseline:: 5.749531507492065
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553864
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1414172649383545
overhead3:: 0.19784927368164062
overhead4:: 0.8278508186340332
overhead5:: 0
time_provenance:: 2.7872204780578613
curr_diff: 0 tensor(6.2559e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2559e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1727745532989502
overhead3:: 0.22966217994689941
overhead4:: 0.8690195083618164
overhead5:: 0
time_provenance:: 2.875810384750366
curr_diff: 0 tensor(6.1755e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1755e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1994304656982422
overhead3:: 0.2769134044647217
overhead4:: 1.0786960124969482
overhead5:: 0
time_provenance:: 3.094658851623535
curr_diff: 0 tensor(6.0059e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0059e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.23397588729858398
overhead3:: 0.32166028022766113
overhead4:: 1.1844472885131836
overhead5:: 0
time_provenance:: 3.2128355503082275
curr_diff: 0 tensor(5.8363e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8363e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2163689136505127
overhead3:: 0.29511356353759766
overhead4:: 1.2313640117645264
overhead5:: 0
time_provenance:: 3.344529628753662
curr_diff: 0 tensor(3.7059e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7059e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553866
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.24656200408935547
overhead3:: 0.3392336368560791
overhead4:: 1.367011308670044
overhead5:: 0
time_provenance:: 3.562716484069824
curr_diff: 0 tensor(3.6663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6663e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553866
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2654545307159424
overhead3:: 0.3909447193145752
overhead4:: 1.39668869972229
overhead5:: 0
time_provenance:: 3.62469744682312
curr_diff: 0 tensor(3.5674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553866
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.32381772994995117
overhead3:: 0.45366740226745605
overhead4:: 1.7355244159698486
overhead5:: 0
time_provenance:: 4.575881481170654
curr_diff: 0 tensor(3.5709e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5709e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553866
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.34329867362976074
overhead3:: 0.47516608238220215
overhead4:: 1.9504327774047852
overhead5:: 0
time_provenance:: 4.827965974807739
curr_diff: 0 tensor(1.8194e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8194e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553866
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3313307762145996
overhead3:: 0.45065975189208984
overhead4:: 1.9096946716308594
overhead5:: 0
time_provenance:: 4.4164979457855225
curr_diff: 0 tensor(2.0415e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0415e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.33165788650512695
overhead3:: 0.4493985176086426
overhead4:: 1.888655662536621
overhead5:: 0
time_provenance:: 4.1777684688568115
curr_diff: 0 tensor(1.4784e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4784e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.34850502014160156
overhead3:: 0.4751853942871094
overhead4:: 1.989356517791748
overhead5:: 0
time_provenance:: 4.256071090698242
curr_diff: 0 tensor(1.7159e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7159e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553864
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4344196319580078
overhead3:: 0.5989539623260498
overhead4:: 2.4137041568756104
overhead5:: 0
time_provenance:: 5.053546190261841
curr_diff: 0 tensor(1.0032e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0032e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.40563535690307617
overhead3:: 0.5786557197570801
overhead4:: 2.453517198562622
overhead5:: 0
time_provenance:: 4.945685863494873
curr_diff: 0 tensor(9.9277e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9277e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.5315115451812744
overhead3:: 0.6915647983551025
overhead4:: 2.993560314178467
overhead5:: 0
time_provenance:: 6.32933497428894
curr_diff: 0 tensor(9.7155e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7155e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.43009471893310547
overhead3:: 0.599057674407959
overhead4:: 2.5133395195007324
overhead5:: 0
time_provenance:: 4.955450534820557
curr_diff: 0 tensor(9.3976e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3976e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553868
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.7039070129394531
overhead3:: 0.94887375831604
overhead4:: 3.4202182292938232
overhead5:: 0
time_provenance:: 5.642234563827515
curr_diff: 0 tensor(2.2909e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2909e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553864
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.700635
Train - Epoch 0, Batch: 10, Loss: 0.697280
Train - Epoch 0, Batch: 20, Loss: 0.695859
Train - Epoch 0, Batch: 30, Loss: 0.695557
Train - Epoch 0, Batch: 40, Loss: 0.693771
Train - Epoch 0, Batch: 50, Loss: 0.693152
Train - Epoch 0, Batch: 60, Loss: 0.692919
Train - Epoch 0, Batch: 70, Loss: 0.692159
Train - Epoch 0, Batch: 80, Loss: 0.691603
Train - Epoch 0, Batch: 90, Loss: 0.691971
Train - Epoch 0, Batch: 100, Loss: 0.691273
Train - Epoch 0, Batch: 110, Loss: 0.691108
Train - Epoch 0, Batch: 120, Loss: 0.692300
Train - Epoch 0, Batch: 130, Loss: 0.690346
Train - Epoch 0, Batch: 140, Loss: 0.691298
Train - Epoch 0, Batch: 150, Loss: 0.690223
Train - Epoch 0, Batch: 160, Loss: 0.689270
Train - Epoch 0, Batch: 170, Loss: 0.690685
Train - Epoch 0, Batch: 180, Loss: 0.690373
Train - Epoch 0, Batch: 190, Loss: 0.690249
Train - Epoch 0, Batch: 200, Loss: 0.690669
Train - Epoch 0, Batch: 210, Loss: 0.689195
Train - Epoch 0, Batch: 220, Loss: 0.689989
Train - Epoch 0, Batch: 230, Loss: 0.689265
Train - Epoch 0, Batch: 240, Loss: 0.688668
Train - Epoch 0, Batch: 250, Loss: 0.688904
Train - Epoch 0, Batch: 260, Loss: 0.688600
Train - Epoch 0, Batch: 270, Loss: 0.688747
Train - Epoch 0, Batch: 280, Loss: 0.688658
Train - Epoch 0, Batch: 290, Loss: 0.689566
Train - Epoch 0, Batch: 300, Loss: 0.689266
Train - Epoch 0, Batch: 310, Loss: 0.687889
Train - Epoch 0, Batch: 320, Loss: 0.688433
Train - Epoch 0, Batch: 330, Loss: 0.687933
Train - Epoch 0, Batch: 340, Loss: 0.687785
Train - Epoch 0, Batch: 350, Loss: 0.687643
Train - Epoch 0, Batch: 360, Loss: 0.688739
Train - Epoch 0, Batch: 370, Loss: 0.688416
Train - Epoch 0, Batch: 380, Loss: 0.688005
Train - Epoch 0, Batch: 390, Loss: 0.688259
Train - Epoch 0, Batch: 400, Loss: 0.688785
Train - Epoch 0, Batch: 410, Loss: 0.688487
Train - Epoch 0, Batch: 420, Loss: 0.688344
Train - Epoch 0, Batch: 430, Loss: 0.688561
Train - Epoch 0, Batch: 440, Loss: 0.688088
Train - Epoch 0, Batch: 450, Loss: 0.686906
Train - Epoch 0, Batch: 460, Loss: 0.686695
Train - Epoch 0, Batch: 470, Loss: 0.688659
Train - Epoch 0, Batch: 480, Loss: 0.688388
Train - Epoch 0, Batch: 490, Loss: 0.687995
Train - Epoch 0, Batch: 500, Loss: 0.688139
Train - Epoch 0, Batch: 510, Loss: 0.687333
Train - Epoch 0, Batch: 520, Loss: 0.687262
Train - Epoch 0, Batch: 530, Loss: 0.686644
Train - Epoch 0, Batch: 540, Loss: 0.687980
Train - Epoch 0, Batch: 550, Loss: 0.688227
Train - Epoch 0, Batch: 560, Loss: 0.687837
Train - Epoch 0, Batch: 570, Loss: 0.687924
Train - Epoch 0, Batch: 580, Loss: 0.687134
Train - Epoch 0, Batch: 590, Loss: 0.687924
Train - Epoch 0, Batch: 600, Loss: 0.687002
Train - Epoch 0, Batch: 610, Loss: 0.687851
Train - Epoch 0, Batch: 620, Loss: 0.687404
Train - Epoch 0, Batch: 630, Loss: 0.686982
Train - Epoch 0, Batch: 640, Loss: 0.688819
Train - Epoch 1, Batch: 0, Loss: 0.687531
Train - Epoch 1, Batch: 10, Loss: 0.687942
Train - Epoch 1, Batch: 20, Loss: 0.687596
Train - Epoch 1, Batch: 30, Loss: 0.687855
Train - Epoch 1, Batch: 40, Loss: 0.686425
Train - Epoch 1, Batch: 50, Loss: 0.686699
Train - Epoch 1, Batch: 60, Loss: 0.686304
Train - Epoch 1, Batch: 70, Loss: 0.687786
Train - Epoch 1, Batch: 80, Loss: 0.686073
Train - Epoch 1, Batch: 90, Loss: 0.686181
Train - Epoch 1, Batch: 100, Loss: 0.686517
Train - Epoch 1, Batch: 110, Loss: 0.687816
Train - Epoch 1, Batch: 120, Loss: 0.688345
Train - Epoch 1, Batch: 130, Loss: 0.687268
Train - Epoch 1, Batch: 140, Loss: 0.686197
Train - Epoch 1, Batch: 150, Loss: 0.686887
Train - Epoch 1, Batch: 160, Loss: 0.685992
Train - Epoch 1, Batch: 170, Loss: 0.687099
Train - Epoch 1, Batch: 180, Loss: 0.687880
Train - Epoch 1, Batch: 190, Loss: 0.687068
Train - Epoch 1, Batch: 200, Loss: 0.686793
Train - Epoch 1, Batch: 210, Loss: 0.686543
Train - Epoch 1, Batch: 220, Loss: 0.686486
Train - Epoch 1, Batch: 230, Loss: 0.686232
Train - Epoch 1, Batch: 240, Loss: 0.686952
Train - Epoch 1, Batch: 250, Loss: 0.687320
Train - Epoch 1, Batch: 260, Loss: 0.686990
Train - Epoch 1, Batch: 270, Loss: 0.687140
Train - Epoch 1, Batch: 280, Loss: 0.685966
Train - Epoch 1, Batch: 290, Loss: 0.686356
Train - Epoch 1, Batch: 300, Loss: 0.686228
Train - Epoch 1, Batch: 310, Loss: 0.686959
Train - Epoch 1, Batch: 320, Loss: 0.686414
Train - Epoch 1, Batch: 330, Loss: 0.685917
Train - Epoch 1, Batch: 340, Loss: 0.686637
Train - Epoch 1, Batch: 350, Loss: 0.685985
Train - Epoch 1, Batch: 360, Loss: 0.686384
Train - Epoch 1, Batch: 370, Loss: 0.686850
Train - Epoch 1, Batch: 380, Loss: 0.686507
Train - Epoch 1, Batch: 390, Loss: 0.686626
Train - Epoch 1, Batch: 400, Loss: 0.685826
Train - Epoch 1, Batch: 410, Loss: 0.684617
Train - Epoch 1, Batch: 420, Loss: 0.686037
Train - Epoch 1, Batch: 430, Loss: 0.685561
Train - Epoch 1, Batch: 440, Loss: 0.685347
Train - Epoch 1, Batch: 450, Loss: 0.685649
Train - Epoch 1, Batch: 460, Loss: 0.686234
Train - Epoch 1, Batch: 470, Loss: 0.686689
Train - Epoch 1, Batch: 480, Loss: 0.685951
Train - Epoch 1, Batch: 490, Loss: 0.685483
Train - Epoch 1, Batch: 500, Loss: 0.685991
Train - Epoch 1, Batch: 510, Loss: 0.687611
Train - Epoch 1, Batch: 520, Loss: 0.685692
Train - Epoch 1, Batch: 530, Loss: 0.686908
Train - Epoch 1, Batch: 540, Loss: 0.686975
Train - Epoch 1, Batch: 550, Loss: 0.686207
Train - Epoch 1, Batch: 560, Loss: 0.686280
Train - Epoch 1, Batch: 570, Loss: 0.686263
Train - Epoch 1, Batch: 580, Loss: 0.686633
Train - Epoch 1, Batch: 590, Loss: 0.685759
Train - Epoch 1, Batch: 600, Loss: 0.684488
Train - Epoch 1, Batch: 610, Loss: 0.686240
Train - Epoch 1, Batch: 620, Loss: 0.686052
Train - Epoch 1, Batch: 630, Loss: 0.686441
Train - Epoch 1, Batch: 640, Loss: 0.685673
Train - Epoch 2, Batch: 0, Loss: 0.686016
Train - Epoch 2, Batch: 10, Loss: 0.685079
Train - Epoch 2, Batch: 20, Loss: 0.684631
Train - Epoch 2, Batch: 30, Loss: 0.685834
Train - Epoch 2, Batch: 40, Loss: 0.686116
Train - Epoch 2, Batch: 50, Loss: 0.685317
Train - Epoch 2, Batch: 60, Loss: 0.684107
Train - Epoch 2, Batch: 70, Loss: 0.685984
Train - Epoch 2, Batch: 80, Loss: 0.684305
Train - Epoch 2, Batch: 90, Loss: 0.685580
Train - Epoch 2, Batch: 100, Loss: 0.686132
Train - Epoch 2, Batch: 110, Loss: 0.685060
Train - Epoch 2, Batch: 120, Loss: 0.685794
Train - Epoch 2, Batch: 130, Loss: 0.685561
Train - Epoch 2, Batch: 140, Loss: 0.685320
Train - Epoch 2, Batch: 150, Loss: 0.685781
Train - Epoch 2, Batch: 160, Loss: 0.684850
Train - Epoch 2, Batch: 170, Loss: 0.685073
Train - Epoch 2, Batch: 180, Loss: 0.685636
Train - Epoch 2, Batch: 190, Loss: 0.684862
Train - Epoch 2, Batch: 200, Loss: 0.684443
Train - Epoch 2, Batch: 210, Loss: 0.684488
Train - Epoch 2, Batch: 220, Loss: 0.685066
Train - Epoch 2, Batch: 230, Loss: 0.685336
Train - Epoch 2, Batch: 240, Loss: 0.684872
Train - Epoch 2, Batch: 250, Loss: 0.685385
Train - Epoch 2, Batch: 260, Loss: 0.685842
Train - Epoch 2, Batch: 270, Loss: 0.685627
Train - Epoch 2, Batch: 280, Loss: 0.685985
Train - Epoch 2, Batch: 290, Loss: 0.685956
Train - Epoch 2, Batch: 300, Loss: 0.685635
Train - Epoch 2, Batch: 310, Loss: 0.685335
Train - Epoch 2, Batch: 320, Loss: 0.685802
Train - Epoch 2, Batch: 330, Loss: 0.685295
Train - Epoch 2, Batch: 340, Loss: 0.685591
Train - Epoch 2, Batch: 350, Loss: 0.684986
Train - Epoch 2, Batch: 360, Loss: 0.685243
Train - Epoch 2, Batch: 370, Loss: 0.685574
Train - Epoch 2, Batch: 380, Loss: 0.685310
Train - Epoch 2, Batch: 390, Loss: 0.686517
Train - Epoch 2, Batch: 400, Loss: 0.684126
Train - Epoch 2, Batch: 410, Loss: 0.683550
Train - Epoch 2, Batch: 420, Loss: 0.683974
Train - Epoch 2, Batch: 430, Loss: 0.685127
Train - Epoch 2, Batch: 440, Loss: 0.685370
Train - Epoch 2, Batch: 450, Loss: 0.685081
Train - Epoch 2, Batch: 460, Loss: 0.685223
Train - Epoch 2, Batch: 470, Loss: 0.685334
Train - Epoch 2, Batch: 480, Loss: 0.686067
Train - Epoch 2, Batch: 490, Loss: 0.684938
Train - Epoch 2, Batch: 500, Loss: 0.683866
Train - Epoch 2, Batch: 510, Loss: 0.685294
Train - Epoch 2, Batch: 520, Loss: 0.684069
Train - Epoch 2, Batch: 530, Loss: 0.685588
Train - Epoch 2, Batch: 540, Loss: 0.685385
Train - Epoch 2, Batch: 550, Loss: 0.684883/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684352
Train - Epoch 2, Batch: 570, Loss: 0.685014
Train - Epoch 2, Batch: 580, Loss: 0.682798
Train - Epoch 2, Batch: 590, Loss: 0.684181
Train - Epoch 2, Batch: 600, Loss: 0.684853
Train - Epoch 2, Batch: 610, Loss: 0.683906
Train - Epoch 2, Batch: 620, Loss: 0.684734
Train - Epoch 2, Batch: 630, Loss: 0.685211
Train - Epoch 2, Batch: 640, Loss: 0.685874
Train - Epoch 3, Batch: 0, Loss: 0.684318
Train - Epoch 3, Batch: 10, Loss: 0.685152
Train - Epoch 3, Batch: 20, Loss: 0.684741
Train - Epoch 3, Batch: 30, Loss: 0.685126
Train - Epoch 3, Batch: 40, Loss: 0.684245
Train - Epoch 3, Batch: 50, Loss: 0.683527
Train - Epoch 3, Batch: 60, Loss: 0.684990
Train - Epoch 3, Batch: 70, Loss: 0.684533
Train - Epoch 3, Batch: 80, Loss: 0.685538
Train - Epoch 3, Batch: 90, Loss: 0.685199
Train - Epoch 3, Batch: 100, Loss: 0.685384
Train - Epoch 3, Batch: 110, Loss: 0.684561
Train - Epoch 3, Batch: 120, Loss: 0.684243
Train - Epoch 3, Batch: 130, Loss: 0.685340
Train - Epoch 3, Batch: 140, Loss: 0.684432
Train - Epoch 3, Batch: 150, Loss: 0.684347
Train - Epoch 3, Batch: 160, Loss: 0.684078
Train - Epoch 3, Batch: 170, Loss: 0.685145
Train - Epoch 3, Batch: 180, Loss: 0.684848
Train - Epoch 3, Batch: 190, Loss: 0.684997
Train - Epoch 3, Batch: 200, Loss: 0.685177
Train - Epoch 3, Batch: 210, Loss: 0.684222
Train - Epoch 3, Batch: 220, Loss: 0.685269
Train - Epoch 3, Batch: 230, Loss: 0.684014
Train - Epoch 3, Batch: 240, Loss: 0.683785
Train - Epoch 3, Batch: 250, Loss: 0.685269
Train - Epoch 3, Batch: 260, Loss: 0.684056
Train - Epoch 3, Batch: 270, Loss: 0.684091
Train - Epoch 3, Batch: 280, Loss: 0.684129
Train - Epoch 3, Batch: 290, Loss: 0.684381
Train - Epoch 3, Batch: 300, Loss: 0.684718
Train - Epoch 3, Batch: 310, Loss: 0.683504
Train - Epoch 3, Batch: 320, Loss: 0.684645
Train - Epoch 3, Batch: 330, Loss: 0.684730
Train - Epoch 3, Batch: 340, Loss: 0.684631
Train - Epoch 3, Batch: 350, Loss: 0.684916
Train - Epoch 3, Batch: 360, Loss: 0.683661
Train - Epoch 3, Batch: 370, Loss: 0.685075
Train - Epoch 3, Batch: 380, Loss: 0.684643
Train - Epoch 3, Batch: 390, Loss: 0.684792
Train - Epoch 3, Batch: 400, Loss: 0.684403
Train - Epoch 3, Batch: 410, Loss: 0.683720
Train - Epoch 3, Batch: 420, Loss: 0.684596
Train - Epoch 3, Batch: 430, Loss: 0.684129
Train - Epoch 3, Batch: 440, Loss: 0.685027
Train - Epoch 3, Batch: 450, Loss: 0.683790
Train - Epoch 3, Batch: 460, Loss: 0.684513
Train - Epoch 3, Batch: 470, Loss: 0.683989
Train - Epoch 3, Batch: 480, Loss: 0.683932
Train - Epoch 3, Batch: 490, Loss: 0.683454
Train - Epoch 3, Batch: 500, Loss: 0.685078
Train - Epoch 3, Batch: 510, Loss: 0.683745
Train - Epoch 3, Batch: 520, Loss: 0.684682
Train - Epoch 3, Batch: 530, Loss: 0.683404
Train - Epoch 3, Batch: 540, Loss: 0.683900
Train - Epoch 3, Batch: 550, Loss: 0.683710
Train - Epoch 3, Batch: 560, Loss: 0.685499
Train - Epoch 3, Batch: 570, Loss: 0.684523
Train - Epoch 3, Batch: 580, Loss: 0.683445
Train - Epoch 3, Batch: 590, Loss: 0.685612
Train - Epoch 3, Batch: 600, Loss: 0.684509
Train - Epoch 3, Batch: 610, Loss: 0.684393
Train - Epoch 3, Batch: 620, Loss: 0.684002
Train - Epoch 3, Batch: 630, Loss: 0.684284
Train - Epoch 3, Batch: 640, Loss: 0.683504
training_time:: 7.668745040893555
training time full:: 7.6687843799591064
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553074
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 5250
training time is 5.4497997760772705
overhead:: 0
overhead2:: 0
time_baseline:: 5.45327091217041
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553070
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.15717029571533203
overhead3:: 0.2191469669342041
overhead4:: 0.8448338508605957
overhead5:: 0
time_provenance:: 2.874650001525879
curr_diff: 0 tensor(7.9703e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9703e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553078
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1832568645477295
overhead3:: 0.25075626373291016
overhead4:: 0.9165177345275879
overhead5:: 0
time_provenance:: 2.9364242553710938
curr_diff: 0 tensor(7.8968e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8968e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553078
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2174701690673828
overhead3:: 0.3105168342590332
overhead4:: 1.1078503131866455
overhead5:: 0
time_provenance:: 3.2311668395996094
curr_diff: 0 tensor(7.7070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553078
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.23195362091064453
overhead3:: 0.29390835762023926
overhead4:: 1.1743052005767822
overhead5:: 0
time_provenance:: 3.2265045642852783
curr_diff: 0 tensor(7.3327e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3327e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553078
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.20221638679504395
overhead3:: 0.288224458694458
overhead4:: 1.1802501678466797
overhead5:: 0
time_provenance:: 3.282691240310669
curr_diff: 0 tensor(3.1449e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1449e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553074
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.24899768829345703
overhead3:: 0.3398246765136719
overhead4:: 1.3068606853485107
overhead5:: 0
time_provenance:: 3.509108543395996
curr_diff: 0 tensor(3.1152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553074
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.30843114852905273
overhead3:: 0.43427133560180664
overhead4:: 1.6866083145141602
overhead5:: 0
time_provenance:: 4.546288967132568
curr_diff: 0 tensor(3.0216e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0216e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553074
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3360600471496582
overhead3:: 0.4818718433380127
overhead4:: 1.6125354766845703
overhead5:: 0
time_provenance:: 4.1574554443359375
curr_diff: 0 tensor(2.8536e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8536e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553074
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.30765485763549805
overhead3:: 0.4256322383880615
overhead4:: 1.7712020874023438
overhead5:: 0
time_provenance:: 4.251087665557861
curr_diff: 0 tensor(1.4093e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4093e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553070
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3227686882019043
overhead3:: 0.45064711570739746
overhead4:: 1.927729606628418
overhead5:: 0
time_provenance:: 4.293518304824829
curr_diff: 0 tensor(2.1314e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1314e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553076
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3298640251159668
overhead3:: 0.47466182708740234
overhead4:: 1.9456734657287598
overhead5:: 0
time_provenance:: 4.250690460205078
curr_diff: 0 tensor(1.2782e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2782e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3406345844268799
overhead3:: 0.462235689163208
overhead4:: 1.9512724876403809
overhead5:: 0
time_provenance:: 4.212853908538818
curr_diff: 0 tensor(1.2505e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2505e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553070
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4008471965789795
overhead3:: 0.5634584426879883
overhead4:: 2.432644844055176
overhead5:: 0
time_provenance:: 4.948326587677002
curr_diff: 0 tensor(9.2012e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2012e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.39622926712036133
overhead3:: 0.5540516376495361
overhead4:: 2.3802413940429688
overhead5:: 0
time_provenance:: 4.821629762649536
curr_diff: 0 tensor(9.0697e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0697e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4455680847167969
overhead3:: 0.619987964630127
overhead4:: 2.5006210803985596
overhead5:: 0
time_provenance:: 5.0452210903167725
curr_diff: 0 tensor(8.8073e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8073e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.45898866653442383
overhead3:: 0.6080806255340576
overhead4:: 2.5594656467437744
overhead5:: 0
time_provenance:: 5.0761096477508545
curr_diff: 0 tensor(8.7215e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7215e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553066
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.8623712062835693
overhead3:: 1.23740816116333
overhead4:: 3.826117515563965
overhead5:: 0
time_provenance:: 6.550720691680908
curr_diff: 0 tensor(2.3696e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3696e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553070
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702825
Train - Epoch 0, Batch: 10, Loss: 0.692513
Train - Epoch 0, Batch: 20, Loss: 0.691957
Train - Epoch 0, Batch: 30, Loss: 0.692370
Train - Epoch 0, Batch: 40, Loss: 0.692303
Train - Epoch 0, Batch: 50, Loss: 0.691839
Train - Epoch 0, Batch: 60, Loss: 0.692554
Train - Epoch 0, Batch: 70, Loss: 0.691704
Train - Epoch 0, Batch: 80, Loss: 0.692257
Train - Epoch 0, Batch: 90, Loss: 0.690064
Train - Epoch 0, Batch: 100, Loss: 0.691502
Train - Epoch 0, Batch: 110, Loss: 0.691141
Train - Epoch 0, Batch: 120, Loss: 0.692527
Train - Epoch 0, Batch: 130, Loss: 0.689978
Train - Epoch 0, Batch: 140, Loss: 0.690221
Train - Epoch 0, Batch: 150, Loss: 0.690150
Train - Epoch 0, Batch: 160, Loss: 0.690752
Train - Epoch 0, Batch: 170, Loss: 0.689896
Train - Epoch 0, Batch: 180, Loss: 0.691039
Train - Epoch 0, Batch: 190, Loss: 0.690483
Train - Epoch 0, Batch: 200, Loss: 0.690137
Train - Epoch 0, Batch: 210, Loss: 0.690042
Train - Epoch 0, Batch: 220, Loss: 0.690891
Train - Epoch 0, Batch: 230, Loss: 0.689573
Train - Epoch 0, Batch: 240, Loss: 0.688676
Train - Epoch 0, Batch: 250, Loss: 0.689547
Train - Epoch 0, Batch: 260, Loss: 0.689266
Train - Epoch 0, Batch: 270, Loss: 0.689546
Train - Epoch 0, Batch: 280, Loss: 0.688656
Train - Epoch 0, Batch: 290, Loss: 0.689876
Train - Epoch 0, Batch: 300, Loss: 0.688577
Train - Epoch 0, Batch: 310, Loss: 0.689693
Train - Epoch 0, Batch: 320, Loss: 0.688777
Train - Epoch 0, Batch: 330, Loss: 0.689263
Train - Epoch 0, Batch: 340, Loss: 0.690072
Train - Epoch 0, Batch: 350, Loss: 0.687361
Train - Epoch 0, Batch: 360, Loss: 0.688332
Train - Epoch 0, Batch: 370, Loss: 0.688531
Train - Epoch 0, Batch: 380, Loss: 0.688991
Train - Epoch 0, Batch: 390, Loss: 0.687501
Train - Epoch 0, Batch: 400, Loss: 0.689519
Train - Epoch 0, Batch: 410, Loss: 0.688413
Train - Epoch 0, Batch: 420, Loss: 0.688910
Train - Epoch 0, Batch: 430, Loss: 0.688325
Train - Epoch 0, Batch: 440, Loss: 0.687877
Train - Epoch 0, Batch: 450, Loss: 0.687788
Train - Epoch 0, Batch: 460, Loss: 0.687589
Train - Epoch 0, Batch: 470, Loss: 0.687831
Train - Epoch 0, Batch: 480, Loss: 0.687943
Train - Epoch 0, Batch: 490, Loss: 0.688159
Train - Epoch 0, Batch: 500, Loss: 0.687648
Train - Epoch 0, Batch: 510, Loss: 0.688658
Train - Epoch 0, Batch: 520, Loss: 0.688763
Train - Epoch 0, Batch: 530, Loss: 0.687742
Train - Epoch 0, Batch: 540, Loss: 0.686371
Train - Epoch 0, Batch: 550, Loss: 0.687557
Train - Epoch 0, Batch: 560, Loss: 0.688402
Train - Epoch 0, Batch: 570, Loss: 0.686573
Train - Epoch 0, Batch: 580, Loss: 0.686904
Train - Epoch 0, Batch: 590, Loss: 0.687822
Train - Epoch 0, Batch: 600, Loss: 0.686791
Train - Epoch 0, Batch: 610, Loss: 0.688107
Train - Epoch 0, Batch: 620, Loss: 0.686037
Train - Epoch 0, Batch: 630, Loss: 0.686527
Train - Epoch 0, Batch: 640, Loss: 0.686987
Train - Epoch 1, Batch: 0, Loss: 0.686826
Train - Epoch 1, Batch: 10, Loss: 0.687866
Train - Epoch 1, Batch: 20, Loss: 0.686212
Train - Epoch 1, Batch: 30, Loss: 0.686772
Train - Epoch 1, Batch: 40, Loss: 0.687564
Train - Epoch 1, Batch: 50, Loss: 0.686700
Train - Epoch 1, Batch: 60, Loss: 0.687217
Train - Epoch 1, Batch: 70, Loss: 0.686420
Train - Epoch 1, Batch: 80, Loss: 0.686805
Train - Epoch 1, Batch: 90, Loss: 0.687967
Train - Epoch 1, Batch: 100, Loss: 0.687711
Train - Epoch 1, Batch: 110, Loss: 0.687367
Train - Epoch 1, Batch: 120, Loss: 0.687674
Train - Epoch 1, Batch: 130, Loss: 0.686896
Train - Epoch 1, Batch: 140, Loss: 0.687360
Train - Epoch 1, Batch: 150, Loss: 0.686170
Train - Epoch 1, Batch: 160, Loss: 0.686924
Train - Epoch 1, Batch: 170, Loss: 0.686592
Train - Epoch 1, Batch: 180, Loss: 0.687286
Train - Epoch 1, Batch: 190, Loss: 0.686830
Train - Epoch 1, Batch: 200, Loss: 0.686012
Train - Epoch 1, Batch: 210, Loss: 0.686422
Train - Epoch 1, Batch: 220, Loss: 0.686206
Train - Epoch 1, Batch: 230, Loss: 0.686727
Train - Epoch 1, Batch: 240, Loss: 0.686600
Train - Epoch 1, Batch: 250, Loss: 0.687087
Train - Epoch 1, Batch: 260, Loss: 0.686291
Train - Epoch 1, Batch: 270, Loss: 0.686967
Train - Epoch 1, Batch: 280, Loss: 0.686936
Train - Epoch 1, Batch: 290, Loss: 0.686802
Train - Epoch 1, Batch: 300, Loss: 0.688192
Train - Epoch 1, Batch: 310, Loss: 0.685816
Train - Epoch 1, Batch: 320, Loss: 0.686542
Train - Epoch 1, Batch: 330, Loss: 0.686735
Train - Epoch 1, Batch: 340, Loss: 0.685916
Train - Epoch 1, Batch: 350, Loss: 0.686634
Train - Epoch 1, Batch: 360, Loss: 0.686765
Train - Epoch 1, Batch: 370, Loss: 0.685471
Train - Epoch 1, Batch: 380, Loss: 0.685586
Train - Epoch 1, Batch: 390, Loss: 0.686219
Train - Epoch 1, Batch: 400, Loss: 0.685329
Train - Epoch 1, Batch: 410, Loss: 0.686172
Train - Epoch 1, Batch: 420, Loss: 0.685672
Train - Epoch 1, Batch: 430, Loss: 0.685563
Train - Epoch 1, Batch: 440, Loss: 0.685744
Train - Epoch 1, Batch: 450, Loss: 0.685200
Train - Epoch 1, Batch: 460, Loss: 0.686439
Train - Epoch 1, Batch: 470, Loss: 0.685716
Train - Epoch 1, Batch: 480, Loss: 0.687119
Train - Epoch 1, Batch: 490, Loss: 0.686207
Train - Epoch 1, Batch: 500, Loss: 0.685891
Train - Epoch 1, Batch: 510, Loss: 0.686309
Train - Epoch 1, Batch: 520, Loss: 0.686848
Train - Epoch 1, Batch: 530, Loss: 0.686253
Train - Epoch 1, Batch: 540, Loss: 0.685363
Train - Epoch 1, Batch: 550, Loss: 0.686424
Train - Epoch 1, Batch: 560, Loss: 0.686024
Train - Epoch 1, Batch: 570, Loss: 0.685030
Train - Epoch 1, Batch: 580, Loss: 0.685664
Train - Epoch 1, Batch: 590, Loss: 0.686395
Train - Epoch 1, Batch: 600, Loss: 0.685943
Train - Epoch 1, Batch: 610, Loss: 0.685378
Train - Epoch 1, Batch: 620, Loss: 0.684904
Train - Epoch 1, Batch: 630, Loss: 0.686347
Train - Epoch 1, Batch: 640, Loss: 0.685989
Train - Epoch 2, Batch: 0, Loss: 0.685259
Train - Epoch 2, Batch: 10, Loss: 0.686365
Train - Epoch 2, Batch: 20, Loss: 0.685349
Train - Epoch 2, Batch: 30, Loss: 0.686517
Train - Epoch 2, Batch: 40, Loss: 0.686613
Train - Epoch 2, Batch: 50, Loss: 0.686294
Train - Epoch 2, Batch: 60, Loss: 0.685532
Train - Epoch 2, Batch: 70, Loss: 0.684289
Train - Epoch 2, Batch: 80, Loss: 0.685449
Train - Epoch 2, Batch: 90, Loss: 0.685785
Train - Epoch 2, Batch: 100, Loss: 0.686470
Train - Epoch 2, Batch: 110, Loss: 0.685731
Train - Epoch 2, Batch: 120, Loss: 0.685377
Train - Epoch 2, Batch: 130, Loss: 0.684510
Train - Epoch 2, Batch: 140, Loss: 0.686725
Train - Epoch 2, Batch: 150, Loss: 0.685797
Train - Epoch 2, Batch: 160, Loss: 0.685377
Train - Epoch 2, Batch: 170, Loss: 0.684899
Train - Epoch 2, Batch: 180, Loss: 0.684823
Train - Epoch 2, Batch: 190, Loss: 0.685403
Train - Epoch 2, Batch: 200, Loss: 0.686076
Train - Epoch 2, Batch: 210, Loss: 0.684718
Train - Epoch 2, Batch: 220, Loss: 0.685950
Train - Epoch 2, Batch: 230, Loss: 0.684346
Train - Epoch 2, Batch: 240, Loss: 0.687053
Train - Epoch 2, Batch: 250, Loss: 0.685050
Train - Epoch 2, Batch: 260, Loss: 0.686115
Train - Epoch 2, Batch: 270, Loss: 0.684587
Train - Epoch 2, Batch: 280, Loss: 0.685248
Train - Epoch 2, Batch: 290, Loss: 0.685884
Train - Epoch 2, Batch: 300, Loss: 0.685238
Train - Epoch 2, Batch: 310, Loss: 0.686340
Train - Epoch 2, Batch: 320, Loss: 0.684406
Train - Epoch 2, Batch: 330, Loss: 0.684507
Train - Epoch 2, Batch: 340, Loss: 0.684553
Train - Epoch 2, Batch: 350, Loss: 0.685061
Train - Epoch 2, Batch: 360, Loss: 0.685068
Train - Epoch 2, Batch: 370, Loss: 0.685460
Train - Epoch 2, Batch: 380, Loss: 0.684572
Train - Epoch 2, Batch: 390, Loss: 0.684712
Train - Epoch 2, Batch: 400, Loss: 0.686813
Train - Epoch 2, Batch: 410, Loss: 0.684003
Train - Epoch 2, Batch: 420, Loss: 0.685020
Train - Epoch 2, Batch: 430, Loss: 0.685005
Train - Epoch 2, Batch: 440, Loss: 0.684985
Train - Epoch 2, Batch: 450, Loss: 0.686049
Train - Epoch 2, Batch: 460, Loss: 0.685865
Train - Epoch 2, Batch: 470, Loss: 0.685945
Train - Epoch 2, Batch: 480, Loss: 0.685235
Train - Epoch 2, Batch: 490, Loss: 0.685930
Train - Epoch 2, Batch: 500, Loss: 0.685280
Train - Epoch 2, Batch: 510, Loss: 0.684025
Train - Epoch 2, Batch: 520, Loss: 0.685064
Train - Epoch 2, Batch: 530, Loss: 0.684353
Train - Epoch 2, Batch: 540, Loss: 0.684900
Train - Epoch 2, Batch: 550, Loss: 0.684377/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684486
Train - Epoch 2, Batch: 570, Loss: 0.685026
Train - Epoch 2, Batch: 580, Loss: 0.684334
Train - Epoch 2, Batch: 590, Loss: 0.684394
Train - Epoch 2, Batch: 600, Loss: 0.685479
Train - Epoch 2, Batch: 610, Loss: 0.684954
Train - Epoch 2, Batch: 620, Loss: 0.685228
Train - Epoch 2, Batch: 630, Loss: 0.683982
Train - Epoch 2, Batch: 640, Loss: 0.683823
Train - Epoch 3, Batch: 0, Loss: 0.685871
Train - Epoch 3, Batch: 10, Loss: 0.684021
Train - Epoch 3, Batch: 20, Loss: 0.684927
Train - Epoch 3, Batch: 30, Loss: 0.685081
Train - Epoch 3, Batch: 40, Loss: 0.684910
Train - Epoch 3, Batch: 50, Loss: 0.684072
Train - Epoch 3, Batch: 60, Loss: 0.683911
Train - Epoch 3, Batch: 70, Loss: 0.684555
Train - Epoch 3, Batch: 80, Loss: 0.685076
Train - Epoch 3, Batch: 90, Loss: 0.684241
Train - Epoch 3, Batch: 100, Loss: 0.685377
Train - Epoch 3, Batch: 110, Loss: 0.683177
Train - Epoch 3, Batch: 120, Loss: 0.684395
Train - Epoch 3, Batch: 130, Loss: 0.684372
Train - Epoch 3, Batch: 140, Loss: 0.684164
Train - Epoch 3, Batch: 150, Loss: 0.683851
Train - Epoch 3, Batch: 160, Loss: 0.684275
Train - Epoch 3, Batch: 170, Loss: 0.684632
Train - Epoch 3, Batch: 180, Loss: 0.685089
Train - Epoch 3, Batch: 190, Loss: 0.684818
Train - Epoch 3, Batch: 200, Loss: 0.684155
Train - Epoch 3, Batch: 210, Loss: 0.682010
Train - Epoch 3, Batch: 220, Loss: 0.684024
Train - Epoch 3, Batch: 230, Loss: 0.685183
Train - Epoch 3, Batch: 240, Loss: 0.683912
Train - Epoch 3, Batch: 250, Loss: 0.684014
Train - Epoch 3, Batch: 260, Loss: 0.683948
Train - Epoch 3, Batch: 270, Loss: 0.684313
Train - Epoch 3, Batch: 280, Loss: 0.683879
Train - Epoch 3, Batch: 290, Loss: 0.683857
Train - Epoch 3, Batch: 300, Loss: 0.683581
Train - Epoch 3, Batch: 310, Loss: 0.685556
Train - Epoch 3, Batch: 320, Loss: 0.683789
Train - Epoch 3, Batch: 330, Loss: 0.684258
Train - Epoch 3, Batch: 340, Loss: 0.683950
Train - Epoch 3, Batch: 350, Loss: 0.683763
Train - Epoch 3, Batch: 360, Loss: 0.683533
Train - Epoch 3, Batch: 370, Loss: 0.683597
Train - Epoch 3, Batch: 380, Loss: 0.684442
Train - Epoch 3, Batch: 390, Loss: 0.684952
Train - Epoch 3, Batch: 400, Loss: 0.683571
Train - Epoch 3, Batch: 410, Loss: 0.684558
Train - Epoch 3, Batch: 420, Loss: 0.684239
Train - Epoch 3, Batch: 430, Loss: 0.683894
Train - Epoch 3, Batch: 440, Loss: 0.684587
Train - Epoch 3, Batch: 450, Loss: 0.684537
Train - Epoch 3, Batch: 460, Loss: 0.685681
Train - Epoch 3, Batch: 470, Loss: 0.682677
Train - Epoch 3, Batch: 480, Loss: 0.684172
Train - Epoch 3, Batch: 490, Loss: 0.684995
Train - Epoch 3, Batch: 500, Loss: 0.684717
Train - Epoch 3, Batch: 510, Loss: 0.684755
Train - Epoch 3, Batch: 520, Loss: 0.684678
Train - Epoch 3, Batch: 530, Loss: 0.683869
Train - Epoch 3, Batch: 540, Loss: 0.684841
Train - Epoch 3, Batch: 550, Loss: 0.682568
Train - Epoch 3, Batch: 560, Loss: 0.683149
Train - Epoch 3, Batch: 570, Loss: 0.683587
Train - Epoch 3, Batch: 580, Loss: 0.684472
Train - Epoch 3, Batch: 590, Loss: 0.684029
Train - Epoch 3, Batch: 600, Loss: 0.684866
Train - Epoch 3, Batch: 610, Loss: 0.682563
Train - Epoch 3, Batch: 620, Loss: 0.683972
Train - Epoch 3, Batch: 630, Loss: 0.684211
Train - Epoch 3, Batch: 640, Loss: 0.684055
training_time:: 7.813594579696655
training time full:: 7.813632249832153
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553596
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 5250
training time is 4.692345142364502
overhead:: 0
overhead2:: 0
time_baseline:: 4.69532585144043
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553576
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.14975237846374512
overhead3:: 0.19970250129699707
overhead4:: 0.8079760074615479
overhead5:: 0
time_provenance:: 2.7566516399383545
curr_diff: 0 tensor(7.2642e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2642e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553578
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.17914414405822754
overhead3:: 0.24046897888183594
overhead4:: 0.9054832458496094
overhead5:: 0
time_provenance:: 2.880431890487671
curr_diff: 0 tensor(7.2645e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2645e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553578
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.19213485717773438
overhead3:: 0.26190733909606934
overhead4:: 1.0715100765228271
overhead5:: 0
time_provenance:: 3.058814525604248
curr_diff: 0 tensor(6.8457e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8457e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553578
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.22959303855895996
overhead3:: 0.32668256759643555
overhead4:: 1.220526933670044
overhead5:: 0
time_provenance:: 3.2979729175567627
curr_diff: 0 tensor(6.8485e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8485e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553578
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2153453826904297
overhead3:: 0.3030571937561035
overhead4:: 1.2240512371063232
overhead5:: 0
time_provenance:: 3.3342394828796387
curr_diff: 0 tensor(3.5770e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5770e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553588
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.23409652709960938
overhead3:: 0.3253762722015381
overhead4:: 1.328467607498169
overhead5:: 0
time_provenance:: 3.5304880142211914
curr_diff: 0 tensor(3.5250e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5250e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553588
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2811105251312256
overhead3:: 0.40612196922302246
overhead4:: 1.4874167442321777
overhead5:: 0
time_provenance:: 3.73354172706604
curr_diff: 0 tensor(3.5100e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5100e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553588
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2816774845123291
overhead3:: 0.3842480182647705
overhead4:: 1.417738676071167
overhead5:: 0
time_provenance:: 3.511613607406616
curr_diff: 0 tensor(3.4712e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4712e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553588
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2911567687988281
overhead3:: 0.4232821464538574
overhead4:: 1.7639267444610596
overhead5:: 0
time_provenance:: 4.073532342910767
curr_diff: 0 tensor(2.3803e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3803e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553594
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3508436679840088
overhead3:: 0.547947883605957
overhead4:: 1.9539828300476074
overhead5:: 0
time_provenance:: 4.4147560596466064
curr_diff: 0 tensor(2.8165e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8165e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553578
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.32225513458251953
overhead3:: 0.45668697357177734
overhead4:: 1.9278802871704102
overhead5:: 0
time_provenance:: 4.185004472732544
curr_diff: 0 tensor(1.9994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9994e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553592
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3701601028442383
overhead3:: 0.5431673526763916
overhead4:: 2.09962797164917
overhead5:: 0
time_provenance:: 4.48223614692688
curr_diff: 0 tensor(2.1704e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1704e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553594
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.5101919174194336
overhead3:: 0.7512528896331787
overhead4:: 2.3785407543182373
overhead5:: 0
time_provenance:: 5.324256658554077
curr_diff: 0 tensor(1.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553584
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4291563034057617
overhead3:: 0.6063518524169922
overhead4:: 2.4495110511779785
overhead5:: 0
time_provenance:: 5.08122444152832
curr_diff: 0 tensor(1.0653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0653e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553584
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.44219374656677246
overhead3:: 0.6204323768615723
overhead4:: 2.6460933685302734
overhead5:: 0
time_provenance:: 5.270169496536255
curr_diff: 0 tensor(1.0481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0481e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553584
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4373629093170166
overhead3:: 0.5758042335510254
overhead4:: 2.555091142654419
overhead5:: 0
time_provenance:: 5.047486305236816
curr_diff: 0 tensor(1.0271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0271e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553584
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.7596611976623535
overhead3:: 1.0762701034545898
overhead4:: 3.6068968772888184
overhead5:: 0
time_provenance:: 6.040301084518433
curr_diff: 0 tensor(2.2820e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2820e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553576
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.726091
Train - Epoch 0, Batch: 10, Loss: 0.693273
Train - Epoch 0, Batch: 20, Loss: 0.691833
Train - Epoch 0, Batch: 30, Loss: 0.690594
Train - Epoch 0, Batch: 40, Loss: 0.691282
Train - Epoch 0, Batch: 50, Loss: 0.690549
Train - Epoch 0, Batch: 60, Loss: 0.690158
Train - Epoch 0, Batch: 70, Loss: 0.689304
Train - Epoch 0, Batch: 80, Loss: 0.689360
Train - Epoch 0, Batch: 90, Loss: 0.688797
Train - Epoch 0, Batch: 100, Loss: 0.689187
Train - Epoch 0, Batch: 110, Loss: 0.689648
Train - Epoch 0, Batch: 120, Loss: 0.688778
Train - Epoch 0, Batch: 130, Loss: 0.689164
Train - Epoch 0, Batch: 140, Loss: 0.689518
Train - Epoch 0, Batch: 150, Loss: 0.690434
Train - Epoch 0, Batch: 160, Loss: 0.689264
Train - Epoch 0, Batch: 170, Loss: 0.689041
Train - Epoch 0, Batch: 180, Loss: 0.688431
Train - Epoch 0, Batch: 190, Loss: 0.688890
Train - Epoch 0, Batch: 200, Loss: 0.688448
Train - Epoch 0, Batch: 210, Loss: 0.687168
Train - Epoch 0, Batch: 220, Loss: 0.687956
Train - Epoch 0, Batch: 230, Loss: 0.689014
Train - Epoch 0, Batch: 240, Loss: 0.687776
Train - Epoch 0, Batch: 250, Loss: 0.688171
Train - Epoch 0, Batch: 260, Loss: 0.688515
Train - Epoch 0, Batch: 270, Loss: 0.688095
Train - Epoch 0, Batch: 280, Loss: 0.688062
Train - Epoch 0, Batch: 290, Loss: 0.687981
Train - Epoch 0, Batch: 300, Loss: 0.689430
Train - Epoch 0, Batch: 310, Loss: 0.686560
Train - Epoch 0, Batch: 320, Loss: 0.688746
Train - Epoch 0, Batch: 330, Loss: 0.688136
Train - Epoch 0, Batch: 340, Loss: 0.687526
Train - Epoch 0, Batch: 350, Loss: 0.688023
Train - Epoch 0, Batch: 360, Loss: 0.687515
Train - Epoch 0, Batch: 370, Loss: 0.687421
Train - Epoch 0, Batch: 380, Loss: 0.688198
Train - Epoch 0, Batch: 390, Loss: 0.687609
Train - Epoch 0, Batch: 400, Loss: 0.687865
Train - Epoch 0, Batch: 410, Loss: 0.687126
Train - Epoch 0, Batch: 420, Loss: 0.688002
Train - Epoch 0, Batch: 430, Loss: 0.688016
Train - Epoch 0, Batch: 440, Loss: 0.687590
Train - Epoch 0, Batch: 450, Loss: 0.687574
Train - Epoch 0, Batch: 460, Loss: 0.688141
Train - Epoch 0, Batch: 470, Loss: 0.687979
Train - Epoch 0, Batch: 480, Loss: 0.687961
Train - Epoch 0, Batch: 490, Loss: 0.687856
Train - Epoch 0, Batch: 500, Loss: 0.688037
Train - Epoch 0, Batch: 510, Loss: 0.688130
Train - Epoch 0, Batch: 520, Loss: 0.688181
Train - Epoch 0, Batch: 530, Loss: 0.686522
Train - Epoch 0, Batch: 540, Loss: 0.687615
Train - Epoch 0, Batch: 550, Loss: 0.687228
Train - Epoch 0, Batch: 560, Loss: 0.687763
Train - Epoch 0, Batch: 570, Loss: 0.687578
Train - Epoch 0, Batch: 580, Loss: 0.687959
Train - Epoch 0, Batch: 590, Loss: 0.687002
Train - Epoch 0, Batch: 600, Loss: 0.687556
Train - Epoch 0, Batch: 610, Loss: 0.687270
Train - Epoch 0, Batch: 620, Loss: 0.686803
Train - Epoch 0, Batch: 630, Loss: 0.688167
Train - Epoch 0, Batch: 640, Loss: 0.687285
Train - Epoch 1, Batch: 0, Loss: 0.685798
Train - Epoch 1, Batch: 10, Loss: 0.687309
Train - Epoch 1, Batch: 20, Loss: 0.686690
Train - Epoch 1, Batch: 30, Loss: 0.687169
Train - Epoch 1, Batch: 40, Loss: 0.687170
Train - Epoch 1, Batch: 50, Loss: 0.686583
Train - Epoch 1, Batch: 60, Loss: 0.686753
Train - Epoch 1, Batch: 70, Loss: 0.685853
Train - Epoch 1, Batch: 80, Loss: 0.687049
Train - Epoch 1, Batch: 90, Loss: 0.686570
Train - Epoch 1, Batch: 100, Loss: 0.685838
Train - Epoch 1, Batch: 110, Loss: 0.686935
Train - Epoch 1, Batch: 120, Loss: 0.685704
Train - Epoch 1, Batch: 130, Loss: 0.687008
Train - Epoch 1, Batch: 140, Loss: 0.686409
Train - Epoch 1, Batch: 150, Loss: 0.686013
Train - Epoch 1, Batch: 160, Loss: 0.686528
Train - Epoch 1, Batch: 170, Loss: 0.688210
Train - Epoch 1, Batch: 180, Loss: 0.686971
Train - Epoch 1, Batch: 190, Loss: 0.687149
Train - Epoch 1, Batch: 200, Loss: 0.686333
Train - Epoch 1, Batch: 210, Loss: 0.686862
Train - Epoch 1, Batch: 220, Loss: 0.686167
Train - Epoch 1, Batch: 230, Loss: 0.686010
Train - Epoch 1, Batch: 240, Loss: 0.686276
Train - Epoch 1, Batch: 250, Loss: 0.686482
Train - Epoch 1, Batch: 260, Loss: 0.686116
Train - Epoch 1, Batch: 270, Loss: 0.685497
Train - Epoch 1, Batch: 280, Loss: 0.686726
Train - Epoch 1, Batch: 290, Loss: 0.686482
Train - Epoch 1, Batch: 300, Loss: 0.687306
Train - Epoch 1, Batch: 310, Loss: 0.687541
Train - Epoch 1, Batch: 320, Loss: 0.686543
Train - Epoch 1, Batch: 330, Loss: 0.686803
Train - Epoch 1, Batch: 340, Loss: 0.684924
Train - Epoch 1, Batch: 350, Loss: 0.686971
Train - Epoch 1, Batch: 360, Loss: 0.686934
Train - Epoch 1, Batch: 370, Loss: 0.686334
Train - Epoch 1, Batch: 380, Loss: 0.685775
Train - Epoch 1, Batch: 390, Loss: 0.684829
Train - Epoch 1, Batch: 400, Loss: 0.685318
Train - Epoch 1, Batch: 410, Loss: 0.685949
Train - Epoch 1, Batch: 420, Loss: 0.685508
Train - Epoch 1, Batch: 430, Loss: 0.685419
Train - Epoch 1, Batch: 440, Loss: 0.685114
Train - Epoch 1, Batch: 450, Loss: 0.686328
Train - Epoch 1, Batch: 460, Loss: 0.684698
Train - Epoch 1, Batch: 470, Loss: 0.685109
Train - Epoch 1, Batch: 480, Loss: 0.686160
Train - Epoch 1, Batch: 490, Loss: 0.685790
Train - Epoch 1, Batch: 500, Loss: 0.685838
Train - Epoch 1, Batch: 510, Loss: 0.684791
Train - Epoch 1, Batch: 520, Loss: 0.686752
Train - Epoch 1, Batch: 530, Loss: 0.685739
Train - Epoch 1, Batch: 540, Loss: 0.685671
Train - Epoch 1, Batch: 550, Loss: 0.685111
Train - Epoch 1, Batch: 560, Loss: 0.685420
Train - Epoch 1, Batch: 570, Loss: 0.686016
Train - Epoch 1, Batch: 580, Loss: 0.685408
Train - Epoch 1, Batch: 590, Loss: 0.685385
Train - Epoch 1, Batch: 600, Loss: 0.685551
Train - Epoch 1, Batch: 610, Loss: 0.687037
Train - Epoch 1, Batch: 620, Loss: 0.685374
Train - Epoch 1, Batch: 630, Loss: 0.685006
Train - Epoch 1, Batch: 640, Loss: 0.686012
Train - Epoch 2, Batch: 0, Loss: 0.685533
Train - Epoch 2, Batch: 10, Loss: 0.684797
Train - Epoch 2, Batch: 20, Loss: 0.686062
Train - Epoch 2, Batch: 30, Loss: 0.685116
Train - Epoch 2, Batch: 40, Loss: 0.685416
Train - Epoch 2, Batch: 50, Loss: 0.686726
Train - Epoch 2, Batch: 60, Loss: 0.685416
Train - Epoch 2, Batch: 70, Loss: 0.685577
Train - Epoch 2, Batch: 80, Loss: 0.685716
Train - Epoch 2, Batch: 90, Loss: 0.684869
Train - Epoch 2, Batch: 100, Loss: 0.685473
Train - Epoch 2, Batch: 110, Loss: 0.685040
Train - Epoch 2, Batch: 120, Loss: 0.685524
Train - Epoch 2, Batch: 130, Loss: 0.684883
Train - Epoch 2, Batch: 140, Loss: 0.686509
Train - Epoch 2, Batch: 150, Loss: 0.685541
Train - Epoch 2, Batch: 160, Loss: 0.684344
Train - Epoch 2, Batch: 170, Loss: 0.685897
Train - Epoch 2, Batch: 180, Loss: 0.685948
Train - Epoch 2, Batch: 190, Loss: 0.685837
Train - Epoch 2, Batch: 200, Loss: 0.685328
Train - Epoch 2, Batch: 210, Loss: 0.684748
Train - Epoch 2, Batch: 220, Loss: 0.685096
Train - Epoch 2, Batch: 230, Loss: 0.684975
Train - Epoch 2, Batch: 240, Loss: 0.686530
Train - Epoch 2, Batch: 250, Loss: 0.684760
Train - Epoch 2, Batch: 260, Loss: 0.684327
Train - Epoch 2, Batch: 270, Loss: 0.685035
Train - Epoch 2, Batch: 280, Loss: 0.683793
Train - Epoch 2, Batch: 290, Loss: 0.685427
Train - Epoch 2, Batch: 300, Loss: 0.685363
Train - Epoch 2, Batch: 310, Loss: 0.684152
Train - Epoch 2, Batch: 320, Loss: 0.684927
Train - Epoch 2, Batch: 330, Loss: 0.685560
Train - Epoch 2, Batch: 340, Loss: 0.685069
Train - Epoch 2, Batch: 350, Loss: 0.685426
Train - Epoch 2, Batch: 360, Loss: 0.685328
Train - Epoch 2, Batch: 370, Loss: 0.685879
Train - Epoch 2, Batch: 380, Loss: 0.684718
Train - Epoch 2, Batch: 390, Loss: 0.683879
Train - Epoch 2, Batch: 400, Loss: 0.684910
Train - Epoch 2, Batch: 410, Loss: 0.685603
Train - Epoch 2, Batch: 420, Loss: 0.685448
Train - Epoch 2, Batch: 430, Loss: 0.684697
Train - Epoch 2, Batch: 440, Loss: 0.685073
Train - Epoch 2, Batch: 450, Loss: 0.685337
Train - Epoch 2, Batch: 460, Loss: 0.684176
Train - Epoch 2, Batch: 470, Loss: 0.685641
Train - Epoch 2, Batch: 480, Loss: 0.684370
Train - Epoch 2, Batch: 490, Loss: 0.684650
Train - Epoch 2, Batch: 500, Loss: 0.685179
Train - Epoch 2, Batch: 510, Loss: 0.684222
Train - Epoch 2, Batch: 520, Loss: 0.685541
Train - Epoch 2, Batch: 530, Loss: 0.684939
Train - Epoch 2, Batch: 540, Loss: 0.684154
Train - Epoch 2, Batch: 550, Loss: 0.685010/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683067
Train - Epoch 2, Batch: 570, Loss: 0.684027
Train - Epoch 2, Batch: 580, Loss: 0.684674
Train - Epoch 2, Batch: 590, Loss: 0.684102
Train - Epoch 2, Batch: 600, Loss: 0.685677
Train - Epoch 2, Batch: 610, Loss: 0.685435
Train - Epoch 2, Batch: 620, Loss: 0.683982
Train - Epoch 2, Batch: 630, Loss: 0.684933
Train - Epoch 2, Batch: 640, Loss: 0.685180
Train - Epoch 3, Batch: 0, Loss: 0.685161
Train - Epoch 3, Batch: 10, Loss: 0.686003
Train - Epoch 3, Batch: 20, Loss: 0.685319
Train - Epoch 3, Batch: 30, Loss: 0.684279
Train - Epoch 3, Batch: 40, Loss: 0.685566
Train - Epoch 3, Batch: 50, Loss: 0.684880
Train - Epoch 3, Batch: 60, Loss: 0.684098
Train - Epoch 3, Batch: 70, Loss: 0.684491
Train - Epoch 3, Batch: 80, Loss: 0.683579
Train - Epoch 3, Batch: 90, Loss: 0.685101
Train - Epoch 3, Batch: 100, Loss: 0.686212
Train - Epoch 3, Batch: 110, Loss: 0.685685
Train - Epoch 3, Batch: 120, Loss: 0.683404
Train - Epoch 3, Batch: 130, Loss: 0.684766
Train - Epoch 3, Batch: 140, Loss: 0.685207
Train - Epoch 3, Batch: 150, Loss: 0.684410
Train - Epoch 3, Batch: 160, Loss: 0.684987
Train - Epoch 3, Batch: 170, Loss: 0.684560
Train - Epoch 3, Batch: 180, Loss: 0.684611
Train - Epoch 3, Batch: 190, Loss: 0.684098
Train - Epoch 3, Batch: 200, Loss: 0.684432
Train - Epoch 3, Batch: 210, Loss: 0.685807
Train - Epoch 3, Batch: 220, Loss: 0.684280
Train - Epoch 3, Batch: 230, Loss: 0.683887
Train - Epoch 3, Batch: 240, Loss: 0.684446
Train - Epoch 3, Batch: 250, Loss: 0.684185
Train - Epoch 3, Batch: 260, Loss: 0.684501
Train - Epoch 3, Batch: 270, Loss: 0.684659
Train - Epoch 3, Batch: 280, Loss: 0.684808
Train - Epoch 3, Batch: 290, Loss: 0.684517
Train - Epoch 3, Batch: 300, Loss: 0.683449
Train - Epoch 3, Batch: 310, Loss: 0.683929
Train - Epoch 3, Batch: 320, Loss: 0.683137
Train - Epoch 3, Batch: 330, Loss: 0.684513
Train - Epoch 3, Batch: 340, Loss: 0.684669
Train - Epoch 3, Batch: 350, Loss: 0.683218
Train - Epoch 3, Batch: 360, Loss: 0.684196
Train - Epoch 3, Batch: 370, Loss: 0.683864
Train - Epoch 3, Batch: 380, Loss: 0.684363
Train - Epoch 3, Batch: 390, Loss: 0.684747
Train - Epoch 3, Batch: 400, Loss: 0.684396
Train - Epoch 3, Batch: 410, Loss: 0.683065
Train - Epoch 3, Batch: 420, Loss: 0.683933
Train - Epoch 3, Batch: 430, Loss: 0.684271
Train - Epoch 3, Batch: 440, Loss: 0.684057
Train - Epoch 3, Batch: 450, Loss: 0.683917
Train - Epoch 3, Batch: 460, Loss: 0.684225
Train - Epoch 3, Batch: 470, Loss: 0.683724
Train - Epoch 3, Batch: 480, Loss: 0.682942
Train - Epoch 3, Batch: 490, Loss: 0.683975
Train - Epoch 3, Batch: 500, Loss: 0.684172
Train - Epoch 3, Batch: 510, Loss: 0.683719
Train - Epoch 3, Batch: 520, Loss: 0.684245
Train - Epoch 3, Batch: 530, Loss: 0.683665
Train - Epoch 3, Batch: 540, Loss: 0.683447
Train - Epoch 3, Batch: 550, Loss: 0.683675
Train - Epoch 3, Batch: 560, Loss: 0.683285
Train - Epoch 3, Batch: 570, Loss: 0.684528
Train - Epoch 3, Batch: 580, Loss: 0.684304
Train - Epoch 3, Batch: 590, Loss: 0.684912
Train - Epoch 3, Batch: 600, Loss: 0.684686
Train - Epoch 3, Batch: 610, Loss: 0.684948
Train - Epoch 3, Batch: 620, Loss: 0.684570
Train - Epoch 3, Batch: 630, Loss: 0.684088
Train - Epoch 3, Batch: 640, Loss: 0.684429
training_time:: 7.695512056350708
training time full:: 7.69555139541626
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554612
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 5250
training time is 4.834949016571045
overhead:: 0
overhead2:: 0
time_baseline:: 4.8378541469573975
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554616
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.1523275375366211
overhead3:: 0.2098679542541504
overhead4:: 0.8282637596130371
overhead5:: 0
time_provenance:: 2.8609964847564697
curr_diff: 0 tensor(5.0462e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0462e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554606
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.16462230682373047
overhead3:: 0.2165393829345703
overhead4:: 0.8310284614562988
overhead5:: 0
time_provenance:: 2.7507143020629883
curr_diff: 0 tensor(4.9325e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9325e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554604
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2660355567932129
overhead3:: 0.38732290267944336
overhead4:: 1.1582527160644531
overhead5:: 0
time_provenance:: 3.452991485595703
curr_diff: 0 tensor(4.8098e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8098e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554606
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.26957130432128906
overhead3:: 0.40136146545410156
overhead4:: 1.3022093772888184
overhead5:: 0
time_provenance:: 3.5096092224121094
curr_diff: 0 tensor(4.6535e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6535e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554606
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.23370718955993652
overhead3:: 0.3436014652252197
overhead4:: 1.321523666381836
overhead5:: 0
time_provenance:: 3.5127744674682617
curr_diff: 0 tensor(3.0731e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0731e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554602
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.25197863578796387
overhead3:: 0.35401320457458496
overhead4:: 1.3630397319793701
overhead5:: 0
time_provenance:: 3.5559074878692627
curr_diff: 0 tensor(2.9914e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9914e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554602
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3060274124145508
overhead3:: 0.4422743320465088
overhead4:: 1.5933716297149658
overhead5:: 0
time_provenance:: 4.064316272735596
curr_diff: 0 tensor(2.9457e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9457e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554604
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3182857036590576
overhead3:: 0.43595314025878906
overhead4:: 1.6223890781402588
overhead5:: 0
time_provenance:: 4.2992470264434814
curr_diff: 0 tensor(2.7719e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7719e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554606
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.2871372699737549
overhead3:: 0.3970482349395752
overhead4:: 1.6442251205444336
overhead5:: 0
time_provenance:: 3.9144434928894043
curr_diff: 0 tensor(1.7240e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7240e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554616
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3384554386138916
overhead3:: 0.45843052864074707
overhead4:: 1.8275854587554932
overhead5:: 0
time_provenance:: 4.413523197174072
curr_diff: 0 tensor(1.6806e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6806e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554616
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3300628662109375
overhead3:: 0.4632270336151123
overhead4:: 1.8790113925933838
overhead5:: 0
time_provenance:: 4.196285724639893
curr_diff: 0 tensor(1.8698e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8698e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554618
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.36434221267700195
overhead3:: 0.5345559120178223
overhead4:: 2.1597917079925537
overhead5:: 0
time_provenance:: 4.52510142326355
curr_diff: 0 tensor(1.5878e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5878e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554616
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.38695549964904785
overhead3:: 0.5307340621948242
overhead4:: 2.451960802078247
overhead5:: 0
time_provenance:: 4.927728176116943
curr_diff: 0 tensor(9.7312e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7312e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554618
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.3960075378417969
overhead3:: 0.5612773895263672
overhead4:: 2.455113410949707
overhead5:: 0
time_provenance:: 4.915584087371826
curr_diff: 0 tensor(9.5432e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5432e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554618
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.46672749519348145
overhead3:: 0.6343884468078613
overhead4:: 2.714595317840576
overhead5:: 0
time_provenance:: 5.5513646602630615
curr_diff: 0 tensor(9.3876e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3876e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554618
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.4475440979003906
overhead3:: 0.6300671100616455
overhead4:: 2.700270414352417
overhead5:: 0
time_provenance:: 5.2639453411102295
curr_diff: 0 tensor(8.8878e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8878e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554618
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 5250
max_epoch:: 4
overhead:: 0
overhead2:: 0.6742479801177979
overhead3:: 0.8935434818267822
overhead4:: 3.328071117401123
overhead5:: 0
time_provenance:: 5.462256908416748
curr_diff: 0 tensor(2.3628e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3628e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554616
deletion rate:: 0.001
python3 generate_rand_ids 0.001  higgs 0
tensor([5521409, 7864321, 1622019,  ..., 6586363, 7241726,  393215])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702833
Train - Epoch 0, Batch: 10, Loss: 0.696039
Train - Epoch 0, Batch: 20, Loss: 0.693645
Train - Epoch 0, Batch: 30, Loss: 0.694432
Train - Epoch 0, Batch: 40, Loss: 0.692842
Train - Epoch 0, Batch: 50, Loss: 0.692338
Train - Epoch 0, Batch: 60, Loss: 0.691445
Train - Epoch 0, Batch: 70, Loss: 0.691156
Train - Epoch 0, Batch: 80, Loss: 0.691034
Train - Epoch 0, Batch: 90, Loss: 0.691346
Train - Epoch 0, Batch: 100, Loss: 0.690946
Train - Epoch 0, Batch: 110, Loss: 0.689759
Train - Epoch 0, Batch: 120, Loss: 0.691125
Train - Epoch 0, Batch: 130, Loss: 0.688905
Train - Epoch 0, Batch: 140, Loss: 0.689056
Train - Epoch 0, Batch: 150, Loss: 0.689894
Train - Epoch 0, Batch: 160, Loss: 0.689487
Train - Epoch 0, Batch: 170, Loss: 0.689241
Train - Epoch 0, Batch: 180, Loss: 0.689957
Train - Epoch 0, Batch: 190, Loss: 0.689757
Train - Epoch 0, Batch: 200, Loss: 0.688173
Train - Epoch 0, Batch: 210, Loss: 0.688535
Train - Epoch 0, Batch: 220, Loss: 0.689417
Train - Epoch 0, Batch: 230, Loss: 0.688878
Train - Epoch 0, Batch: 240, Loss: 0.688004
Train - Epoch 0, Batch: 250, Loss: 0.688560
Train - Epoch 0, Batch: 260, Loss: 0.688630
Train - Epoch 0, Batch: 270, Loss: 0.688463
Train - Epoch 0, Batch: 280, Loss: 0.690182
Train - Epoch 0, Batch: 290, Loss: 0.686894
Train - Epoch 0, Batch: 300, Loss: 0.688714
Train - Epoch 0, Batch: 310, Loss: 0.688388
Train - Epoch 0, Batch: 320, Loss: 0.688449
Train - Epoch 0, Batch: 330, Loss: 0.687795
Train - Epoch 0, Batch: 340, Loss: 0.687751
Train - Epoch 0, Batch: 350, Loss: 0.688127
Train - Epoch 0, Batch: 360, Loss: 0.688267
Train - Epoch 0, Batch: 370, Loss: 0.689429
Train - Epoch 0, Batch: 380, Loss: 0.688425
Train - Epoch 0, Batch: 390, Loss: 0.687024
Train - Epoch 0, Batch: 400, Loss: 0.686688
Train - Epoch 0, Batch: 410, Loss: 0.688016
Train - Epoch 0, Batch: 420, Loss: 0.687932
Train - Epoch 0, Batch: 430, Loss: 0.687715
Train - Epoch 0, Batch: 440, Loss: 0.686516
Train - Epoch 0, Batch: 450, Loss: 0.687552
Train - Epoch 0, Batch: 460, Loss: 0.689282
Train - Epoch 0, Batch: 470, Loss: 0.686940
Train - Epoch 0, Batch: 480, Loss: 0.688355
Train - Epoch 0, Batch: 490, Loss: 0.687029
Train - Epoch 0, Batch: 500, Loss: 0.687307
Train - Epoch 0, Batch: 510, Loss: 0.687343
Train - Epoch 0, Batch: 520, Loss: 0.686241
Train - Epoch 0, Batch: 530, Loss: 0.686403
Train - Epoch 0, Batch: 540, Loss: 0.688058
Train - Epoch 0, Batch: 550, Loss: 0.685890
Train - Epoch 0, Batch: 560, Loss: 0.687050
Train - Epoch 0, Batch: 570, Loss: 0.685498
Train - Epoch 0, Batch: 580, Loss: 0.687340
Train - Epoch 0, Batch: 590, Loss: 0.687095
Train - Epoch 0, Batch: 600, Loss: 0.687563
Train - Epoch 0, Batch: 610, Loss: 0.687112
Train - Epoch 0, Batch: 620, Loss: 0.686623
Train - Epoch 0, Batch: 630, Loss: 0.686368
Train - Epoch 0, Batch: 640, Loss: 0.687866
Train - Epoch 1, Batch: 0, Loss: 0.686612
Train - Epoch 1, Batch: 10, Loss: 0.686678
Train - Epoch 1, Batch: 20, Loss: 0.687992
Train - Epoch 1, Batch: 30, Loss: 0.686385
Train - Epoch 1, Batch: 40, Loss: 0.687483
Train - Epoch 1, Batch: 50, Loss: 0.685972
Train - Epoch 1, Batch: 60, Loss: 0.686714
Train - Epoch 1, Batch: 70, Loss: 0.686782
Train - Epoch 1, Batch: 80, Loss: 0.686462
Train - Epoch 1, Batch: 90, Loss: 0.687346
Train - Epoch 1, Batch: 100, Loss: 0.687201
Train - Epoch 1, Batch: 110, Loss: 0.687623
Train - Epoch 1, Batch: 120, Loss: 0.686979
Train - Epoch 1, Batch: 130, Loss: 0.686115
Train - Epoch 1, Batch: 140, Loss: 0.686467
Train - Epoch 1, Batch: 150, Loss: 0.686734
Train - Epoch 1, Batch: 160, Loss: 0.687441
Train - Epoch 1, Batch: 170, Loss: 0.687024
Train - Epoch 1, Batch: 180, Loss: 0.687686
Train - Epoch 1, Batch: 190, Loss: 0.687499
Train - Epoch 1, Batch: 200, Loss: 0.686158
Train - Epoch 1, Batch: 210, Loss: 0.686199
Train - Epoch 1, Batch: 220, Loss: 0.685834
Train - Epoch 1, Batch: 230, Loss: 0.686717
Train - Epoch 1, Batch: 240, Loss: 0.686952
Train - Epoch 1, Batch: 250, Loss: 0.685702
Train - Epoch 1, Batch: 260, Loss: 0.686696
Train - Epoch 1, Batch: 270, Loss: 0.685231
Train - Epoch 1, Batch: 280, Loss: 0.685040
Train - Epoch 1, Batch: 290, Loss: 0.686909
Train - Epoch 1, Batch: 300, Loss: 0.685388
Train - Epoch 1, Batch: 310, Loss: 0.686041
Train - Epoch 1, Batch: 320, Loss: 0.685560
Train - Epoch 1, Batch: 330, Loss: 0.686261
Train - Epoch 1, Batch: 340, Loss: 0.685495
Train - Epoch 1, Batch: 350, Loss: 0.686809
Train - Epoch 1, Batch: 360, Loss: 0.685460
Train - Epoch 1, Batch: 370, Loss: 0.685991
Train - Epoch 1, Batch: 380, Loss: 0.686749
Train - Epoch 1, Batch: 390, Loss: 0.687673
Train - Epoch 1, Batch: 400, Loss: 0.685845
Train - Epoch 1, Batch: 410, Loss: 0.685442
Train - Epoch 1, Batch: 420, Loss: 0.685633
Train - Epoch 1, Batch: 430, Loss: 0.686037
Train - Epoch 1, Batch: 440, Loss: 0.685460
Train - Epoch 1, Batch: 450, Loss: 0.684636
Train - Epoch 1, Batch: 460, Loss: 0.685953
Train - Epoch 1, Batch: 470, Loss: 0.686302
Train - Epoch 1, Batch: 480, Loss: 0.686113
Train - Epoch 1, Batch: 490, Loss: 0.686646
Train - Epoch 1, Batch: 500, Loss: 0.684144
Train - Epoch 1, Batch: 510, Loss: 0.686613
Train - Epoch 1, Batch: 520, Loss: 0.685686
Train - Epoch 1, Batch: 530, Loss: 0.683483
Train - Epoch 1, Batch: 540, Loss: 0.685954
Train - Epoch 1, Batch: 550, Loss: 0.684657
Train - Epoch 1, Batch: 560, Loss: 0.685847
Train - Epoch 1, Batch: 570, Loss: 0.686694
Train - Epoch 1, Batch: 580, Loss: 0.684408
Train - Epoch 1, Batch: 590, Loss: 0.687067
Train - Epoch 1, Batch: 600, Loss: 0.685313
Train - Epoch 1, Batch: 610, Loss: 0.685397
Train - Epoch 1, Batch: 620, Loss: 0.685649
Train - Epoch 1, Batch: 630, Loss: 0.687422
Train - Epoch 1, Batch: 640, Loss: 0.685659
Train - Epoch 2, Batch: 0, Loss: 0.684284
Train - Epoch 2, Batch: 10, Loss: 0.686029
Train - Epoch 2, Batch: 20, Loss: 0.685404
Train - Epoch 2, Batch: 30, Loss: 0.686280
Train - Epoch 2, Batch: 40, Loss: 0.684940
Train - Epoch 2, Batch: 50, Loss: 0.686500
Train - Epoch 2, Batch: 60, Loss: 0.684469
Train - Epoch 2, Batch: 70, Loss: 0.684045
Train - Epoch 2, Batch: 80, Loss: 0.685769
Train - Epoch 2, Batch: 90, Loss: 0.683606
Train - Epoch 2, Batch: 100, Loss: 0.685451
Train - Epoch 2, Batch: 110, Loss: 0.686051
Train - Epoch 2, Batch: 120, Loss: 0.685784
Train - Epoch 2, Batch: 130, Loss: 0.684379
Train - Epoch 2, Batch: 140, Loss: 0.685017
Train - Epoch 2, Batch: 150, Loss: 0.685641
Train - Epoch 2, Batch: 160, Loss: 0.684788
Train - Epoch 2, Batch: 170, Loss: 0.685729
Train - Epoch 2, Batch: 180, Loss: 0.684635
Train - Epoch 2, Batch: 190, Loss: 0.685456
Train - Epoch 2, Batch: 200, Loss: 0.685887
Train - Epoch 2, Batch: 210, Loss: 0.685037
Train - Epoch 2, Batch: 220, Loss: 0.686074
Train - Epoch 2, Batch: 230, Loss: 0.684099
Train - Epoch 2, Batch: 240, Loss: 0.685380
Train - Epoch 2, Batch: 250, Loss: 0.685611
Train - Epoch 2, Batch: 260, Loss: 0.683709
Train - Epoch 2, Batch: 270, Loss: 0.685764
Train - Epoch 2, Batch: 280, Loss: 0.684599
Train - Epoch 2, Batch: 290, Loss: 0.685775
Train - Epoch 2, Batch: 300, Loss: 0.685109
Train - Epoch 2, Batch: 310, Loss: 0.684800
Train - Epoch 2, Batch: 320, Loss: 0.685220
Train - Epoch 2, Batch: 330, Loss: 0.685331
Train - Epoch 2, Batch: 340, Loss: 0.684823
Train - Epoch 2, Batch: 350, Loss: 0.685767
Train - Epoch 2, Batch: 360, Loss: 0.685741
Train - Epoch 2, Batch: 370, Loss: 0.684260
Train - Epoch 2, Batch: 380, Loss: 0.684853
Train - Epoch 2, Batch: 390, Loss: 0.684352
Train - Epoch 2, Batch: 400, Loss: 0.685544
Train - Epoch 2, Batch: 410, Loss: 0.684788
Train - Epoch 2, Batch: 420, Loss: 0.684043
Train - Epoch 2, Batch: 430, Loss: 0.685737
Train - Epoch 2, Batch: 440, Loss: 0.684699
Train - Epoch 2, Batch: 450, Loss: 0.684353
Train - Epoch 2, Batch: 460, Loss: 0.684770
Train - Epoch 2, Batch: 470, Loss: 0.684870
Train - Epoch 2, Batch: 480, Loss: 0.684885
Train - Epoch 2, Batch: 490, Loss: 0.684484
Train - Epoch 2, Batch: 500, Loss: 0.686094
Train - Epoch 2, Batch: 510, Loss: 0.686177
Train - Epoch 2, Batch: 520, Loss: 0.684486
Train - Epoch 2, Batch: 530, Loss: 0.685707
Train - Epoch 2, Batch: 540, Loss: 0.686243
Train - Epoch 2, Batch: 550, Loss: 0.685274/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.686019
Train - Epoch 2, Batch: 570, Loss: 0.684707
Train - Epoch 2, Batch: 580, Loss: 0.684834
Train - Epoch 2, Batch: 590, Loss: 0.683461
Train - Epoch 2, Batch: 600, Loss: 0.685586
Train - Epoch 2, Batch: 610, Loss: 0.683516
Train - Epoch 2, Batch: 620, Loss: 0.684902
Train - Epoch 2, Batch: 630, Loss: 0.684036
Train - Epoch 2, Batch: 640, Loss: 0.685347
Train - Epoch 3, Batch: 0, Loss: 0.684432
Train - Epoch 3, Batch: 10, Loss: 0.684914
Train - Epoch 3, Batch: 20, Loss: 0.684353
Train - Epoch 3, Batch: 30, Loss: 0.683906
Train - Epoch 3, Batch: 40, Loss: 0.684233
Train - Epoch 3, Batch: 50, Loss: 0.685006
Train - Epoch 3, Batch: 60, Loss: 0.684538
Train - Epoch 3, Batch: 70, Loss: 0.685099
Train - Epoch 3, Batch: 80, Loss: 0.683140
Train - Epoch 3, Batch: 90, Loss: 0.685494
Train - Epoch 3, Batch: 100, Loss: 0.684057
Train - Epoch 3, Batch: 110, Loss: 0.683823
Train - Epoch 3, Batch: 120, Loss: 0.683579
Train - Epoch 3, Batch: 130, Loss: 0.683470
Train - Epoch 3, Batch: 140, Loss: 0.684144
Train - Epoch 3, Batch: 150, Loss: 0.683729
Train - Epoch 3, Batch: 160, Loss: 0.683109
Train - Epoch 3, Batch: 170, Loss: 0.682841
Train - Epoch 3, Batch: 180, Loss: 0.684175
Train - Epoch 3, Batch: 190, Loss: 0.685287
Train - Epoch 3, Batch: 200, Loss: 0.683502
Train - Epoch 3, Batch: 210, Loss: 0.684399
Train - Epoch 3, Batch: 220, Loss: 0.684008
Train - Epoch 3, Batch: 230, Loss: 0.685058
Train - Epoch 3, Batch: 240, Loss: 0.684699
Train - Epoch 3, Batch: 250, Loss: 0.684281
Train - Epoch 3, Batch: 260, Loss: 0.684183
Train - Epoch 3, Batch: 270, Loss: 0.684616
Train - Epoch 3, Batch: 280, Loss: 0.684197
Train - Epoch 3, Batch: 290, Loss: 0.683907
Train - Epoch 3, Batch: 300, Loss: 0.684213
Train - Epoch 3, Batch: 310, Loss: 0.684225
Train - Epoch 3, Batch: 320, Loss: 0.684037
Train - Epoch 3, Batch: 330, Loss: 0.684229
Train - Epoch 3, Batch: 340, Loss: 0.684964
Train - Epoch 3, Batch: 350, Loss: 0.683818
Train - Epoch 3, Batch: 360, Loss: 0.683607
Train - Epoch 3, Batch: 370, Loss: 0.684322
Train - Epoch 3, Batch: 380, Loss: 0.683384
Train - Epoch 3, Batch: 390, Loss: 0.684150
Train - Epoch 3, Batch: 400, Loss: 0.684106
Train - Epoch 3, Batch: 410, Loss: 0.683735
Train - Epoch 3, Batch: 420, Loss: 0.684129
Train - Epoch 3, Batch: 430, Loss: 0.684699
Train - Epoch 3, Batch: 440, Loss: 0.683141
Train - Epoch 3, Batch: 450, Loss: 0.684650
Train - Epoch 3, Batch: 460, Loss: 0.683926
Train - Epoch 3, Batch: 470, Loss: 0.684028
Train - Epoch 3, Batch: 480, Loss: 0.684487
Train - Epoch 3, Batch: 490, Loss: 0.683898
Train - Epoch 3, Batch: 500, Loss: 0.684351
Train - Epoch 3, Batch: 510, Loss: 0.682947
Train - Epoch 3, Batch: 520, Loss: 0.684495
Train - Epoch 3, Batch: 530, Loss: 0.684226
Train - Epoch 3, Batch: 540, Loss: 0.683848
Train - Epoch 3, Batch: 550, Loss: 0.684583
Train - Epoch 3, Batch: 560, Loss: 0.683299
Train - Epoch 3, Batch: 570, Loss: 0.683556
Train - Epoch 3, Batch: 580, Loss: 0.684630
Train - Epoch 3, Batch: 590, Loss: 0.684420
Train - Epoch 3, Batch: 600, Loss: 0.684454
Train - Epoch 3, Batch: 610, Loss: 0.684852
Train - Epoch 3, Batch: 620, Loss: 0.685180
Train - Epoch 3, Batch: 630, Loss: 0.682377
Train - Epoch 3, Batch: 640, Loss: 0.683128
training_time:: 7.844864845275879
training time full:: 7.84490442276001
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.553982
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 10500
training time is 5.682676792144775
overhead:: 0
overhead2:: 0
time_baseline:: 5.6861793994903564
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553974
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.17254853248596191
overhead3:: 0.24171185493469238
overhead4:: 0.8585927486419678
overhead5:: 0
time_provenance:: 2.9736599922180176
curr_diff: 0 tensor(8.8602e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8602e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553960
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.19487380981445312
overhead3:: 0.28125762939453125
overhead4:: 0.9718520641326904
overhead5:: 0
time_provenance:: 3.112699508666992
curr_diff: 0 tensor(8.7395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7395e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553962
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.1977550983428955
overhead3:: 0.262561559677124
overhead4:: 1.0612800121307373
overhead5:: 0
time_provenance:: 3.108222723007202
curr_diff: 0 tensor(8.5850e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5850e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553962
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2696995735168457
overhead3:: 0.41342592239379883
overhead4:: 1.2533936500549316
overhead5:: 0
time_provenance:: 3.5089566707611084
curr_diff: 0 tensor(8.6093e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6093e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553962
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.20853924751281738
overhead3:: 0.29017019271850586
overhead4:: 1.1801459789276123
overhead5:: 0
time_provenance:: 3.3314878940582275
curr_diff: 0 tensor(5.1383e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1383e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553964
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2291865348815918
overhead3:: 0.32407474517822266
overhead4:: 1.320286750793457
overhead5:: 0
time_provenance:: 3.4331583976745605
curr_diff: 0 tensor(5.0970e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0970e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553964
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.25177431106567383
overhead3:: 0.33000612258911133
overhead4:: 1.442720651626587
overhead5:: 0
time_provenance:: 3.5452911853790283
curr_diff: 0 tensor(5.0340e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0340e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553966
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.32821226119995117
overhead3:: 0.44580674171447754
overhead4:: 1.6498639583587646
overhead5:: 0
time_provenance:: 4.38329815864563
curr_diff: 0 tensor(4.9741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553966
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3612332344055176
overhead3:: 0.48606395721435547
overhead4:: 2.022535800933838
overhead5:: 0
time_provenance:: 5.124145030975342
curr_diff: 0 tensor(3.3515e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3515e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553970
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.30495405197143555
overhead3:: 0.421250581741333
overhead4:: 1.828199863433838
overhead5:: 0
time_provenance:: 4.097314357757568
curr_diff: 0 tensor(3.7818e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7818e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553970
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3862595558166504
overhead3:: 0.5215818881988525
overhead4:: 2.0926389694213867
overhead5:: 0
time_provenance:: 5.0889060497283936
curr_diff: 0 tensor(2.9820e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9820e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.36652302742004395
overhead3:: 0.4828958511352539
overhead4:: 2.029611825942993
overhead5:: 0
time_provenance:: 4.346745729446411
curr_diff: 0 tensor(3.2594e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2594e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553970
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4023125171661377
overhead3:: 0.5400147438049316
overhead4:: 2.263371467590332
overhead5:: 0
time_provenance:: 4.768887996673584
curr_diff: 0 tensor(2.0133e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0133e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.41417670249938965
overhead3:: 0.5832312107086182
overhead4:: 2.3187308311462402
overhead5:: 0
time_provenance:: 4.835588455200195
curr_diff: 0 tensor(2.0069e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0069e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.42330145835876465
overhead3:: 0.5769057273864746
overhead4:: 2.531052827835083
overhead5:: 0
time_provenance:: 5.016221761703491
curr_diff: 0 tensor(1.9817e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9817e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.44698333740234375
overhead3:: 0.6438198089599609
overhead4:: 2.5856072902679443
overhead5:: 0
time_provenance:: 5.120930433273315
curr_diff: 0 tensor(1.9652e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9652e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553972
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.9073696136474609
overhead3:: 1.3852167129516602
overhead4:: 4.069528341293335
overhead5:: 0
time_provenance:: 6.98354959487915
curr_diff: 0 tensor(2.1959e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1959e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553974
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.694536
Train - Epoch 0, Batch: 10, Loss: 0.692434
Train - Epoch 0, Batch: 20, Loss: 0.691294
Train - Epoch 0, Batch: 30, Loss: 0.691224
Train - Epoch 0, Batch: 40, Loss: 0.690212
Train - Epoch 0, Batch: 50, Loss: 0.690049
Train - Epoch 0, Batch: 60, Loss: 0.690212
Train - Epoch 0, Batch: 70, Loss: 0.689332
Train - Epoch 0, Batch: 80, Loss: 0.689599
Train - Epoch 0, Batch: 90, Loss: 0.688209
Train - Epoch 0, Batch: 100, Loss: 0.689193
Train - Epoch 0, Batch: 110, Loss: 0.689185
Train - Epoch 0, Batch: 120, Loss: 0.689042
Train - Epoch 0, Batch: 130, Loss: 0.688293
Train - Epoch 0, Batch: 140, Loss: 0.689694
Train - Epoch 0, Batch: 150, Loss: 0.688112
Train - Epoch 0, Batch: 160, Loss: 0.687817
Train - Epoch 0, Batch: 170, Loss: 0.688375
Train - Epoch 0, Batch: 180, Loss: 0.687511
Train - Epoch 0, Batch: 190, Loss: 0.688651
Train - Epoch 0, Batch: 200, Loss: 0.688581
Train - Epoch 0, Batch: 210, Loss: 0.686659
Train - Epoch 0, Batch: 220, Loss: 0.688318
Train - Epoch 0, Batch: 230, Loss: 0.688255
Train - Epoch 0, Batch: 240, Loss: 0.688342
Train - Epoch 0, Batch: 250, Loss: 0.688128
Train - Epoch 0, Batch: 260, Loss: 0.685480
Train - Epoch 0, Batch: 270, Loss: 0.687528
Train - Epoch 0, Batch: 280, Loss: 0.687250
Train - Epoch 0, Batch: 290, Loss: 0.687385
Train - Epoch 0, Batch: 300, Loss: 0.687421
Train - Epoch 0, Batch: 310, Loss: 0.687878
Train - Epoch 0, Batch: 320, Loss: 0.687081
Train - Epoch 0, Batch: 330, Loss: 0.686639
Train - Epoch 0, Batch: 340, Loss: 0.687645
Train - Epoch 0, Batch: 350, Loss: 0.687343
Train - Epoch 0, Batch: 360, Loss: 0.685747
Train - Epoch 0, Batch: 370, Loss: 0.687254
Train - Epoch 0, Batch: 380, Loss: 0.687253
Train - Epoch 0, Batch: 390, Loss: 0.686698
Train - Epoch 0, Batch: 400, Loss: 0.686430
Train - Epoch 0, Batch: 410, Loss: 0.685279
Train - Epoch 0, Batch: 420, Loss: 0.686816
Train - Epoch 0, Batch: 430, Loss: 0.686488
Train - Epoch 0, Batch: 440, Loss: 0.686730
Train - Epoch 0, Batch: 450, Loss: 0.687436
Train - Epoch 0, Batch: 460, Loss: 0.686620
Train - Epoch 0, Batch: 470, Loss: 0.686205
Train - Epoch 0, Batch: 480, Loss: 0.685101
Train - Epoch 0, Batch: 490, Loss: 0.686819
Train - Epoch 0, Batch: 500, Loss: 0.686569
Train - Epoch 0, Batch: 510, Loss: 0.685841
Train - Epoch 0, Batch: 520, Loss: 0.686644
Train - Epoch 0, Batch: 530, Loss: 0.686714
Train - Epoch 0, Batch: 540, Loss: 0.685973
Train - Epoch 0, Batch: 550, Loss: 0.686231
Train - Epoch 0, Batch: 560, Loss: 0.685463
Train - Epoch 0, Batch: 570, Loss: 0.686396
Train - Epoch 0, Batch: 580, Loss: 0.685289
Train - Epoch 0, Batch: 590, Loss: 0.686084
Train - Epoch 0, Batch: 600, Loss: 0.685088
Train - Epoch 0, Batch: 610, Loss: 0.685550
Train - Epoch 0, Batch: 620, Loss: 0.686464
Train - Epoch 0, Batch: 630, Loss: 0.686346
Train - Epoch 0, Batch: 640, Loss: 0.684346
Train - Epoch 1, Batch: 0, Loss: 0.684201
Train - Epoch 1, Batch: 10, Loss: 0.685743
Train - Epoch 1, Batch: 20, Loss: 0.685628
Train - Epoch 1, Batch: 30, Loss: 0.685940
Train - Epoch 1, Batch: 40, Loss: 0.686820
Train - Epoch 1, Batch: 50, Loss: 0.686477
Train - Epoch 1, Batch: 60, Loss: 0.685023
Train - Epoch 1, Batch: 70, Loss: 0.686158
Train - Epoch 1, Batch: 80, Loss: 0.684924
Train - Epoch 1, Batch: 90, Loss: 0.684872
Train - Epoch 1, Batch: 100, Loss: 0.685772
Train - Epoch 1, Batch: 110, Loss: 0.685531
Train - Epoch 1, Batch: 120, Loss: 0.685445
Train - Epoch 1, Batch: 130, Loss: 0.685681
Train - Epoch 1, Batch: 140, Loss: 0.684953
Train - Epoch 1, Batch: 150, Loss: 0.686392
Train - Epoch 1, Batch: 160, Loss: 0.685998
Train - Epoch 1, Batch: 170, Loss: 0.684089
Train - Epoch 1, Batch: 180, Loss: 0.685964
Train - Epoch 1, Batch: 190, Loss: 0.685638
Train - Epoch 1, Batch: 200, Loss: 0.687040
Train - Epoch 1, Batch: 210, Loss: 0.685038
Train - Epoch 1, Batch: 220, Loss: 0.685423
Train - Epoch 1, Batch: 230, Loss: 0.685322
Train - Epoch 1, Batch: 240, Loss: 0.684157
Train - Epoch 1, Batch: 250, Loss: 0.684766
Train - Epoch 1, Batch: 260, Loss: 0.684811
Train - Epoch 1, Batch: 270, Loss: 0.685050
Train - Epoch 1, Batch: 280, Loss: 0.685447
Train - Epoch 1, Batch: 290, Loss: 0.685833
Train - Epoch 1, Batch: 300, Loss: 0.686273
Train - Epoch 1, Batch: 310, Loss: 0.685555
Train - Epoch 1, Batch: 320, Loss: 0.686417
Train - Epoch 1, Batch: 330, Loss: 0.685073
Train - Epoch 1, Batch: 340, Loss: 0.686214
Train - Epoch 1, Batch: 350, Loss: 0.685793
Train - Epoch 1, Batch: 360, Loss: 0.685355
Train - Epoch 1, Batch: 370, Loss: 0.685263
Train - Epoch 1, Batch: 380, Loss: 0.684176
Train - Epoch 1, Batch: 390, Loss: 0.686295
Train - Epoch 1, Batch: 400, Loss: 0.685228
Train - Epoch 1, Batch: 410, Loss: 0.685132
Train - Epoch 1, Batch: 420, Loss: 0.685418
Train - Epoch 1, Batch: 430, Loss: 0.684329
Train - Epoch 1, Batch: 440, Loss: 0.685785
Train - Epoch 1, Batch: 450, Loss: 0.686158
Train - Epoch 1, Batch: 460, Loss: 0.684557
Train - Epoch 1, Batch: 470, Loss: 0.684605
Train - Epoch 1, Batch: 480, Loss: 0.685427
Train - Epoch 1, Batch: 490, Loss: 0.684949
Train - Epoch 1, Batch: 500, Loss: 0.684620
Train - Epoch 1, Batch: 510, Loss: 0.683950
Train - Epoch 1, Batch: 520, Loss: 0.684399
Train - Epoch 1, Batch: 530, Loss: 0.685914
Train - Epoch 1, Batch: 540, Loss: 0.684918
Train - Epoch 1, Batch: 550, Loss: 0.683933
Train - Epoch 1, Batch: 560, Loss: 0.685196
Train - Epoch 1, Batch: 570, Loss: 0.685470
Train - Epoch 1, Batch: 580, Loss: 0.684679
Train - Epoch 1, Batch: 590, Loss: 0.685121
Train - Epoch 1, Batch: 600, Loss: 0.684714
Train - Epoch 1, Batch: 610, Loss: 0.684422
Train - Epoch 1, Batch: 620, Loss: 0.684216
Train - Epoch 1, Batch: 630, Loss: 0.686257
Train - Epoch 1, Batch: 640, Loss: 0.684403
Train - Epoch 2, Batch: 0, Loss: 0.683432
Train - Epoch 2, Batch: 10, Loss: 0.686933
Train - Epoch 2, Batch: 20, Loss: 0.685213
Train - Epoch 2, Batch: 30, Loss: 0.684784
Train - Epoch 2, Batch: 40, Loss: 0.684685
Train - Epoch 2, Batch: 50, Loss: 0.685314
Train - Epoch 2, Batch: 60, Loss: 0.684675
Train - Epoch 2, Batch: 70, Loss: 0.685214
Train - Epoch 2, Batch: 80, Loss: 0.683627
Train - Epoch 2, Batch: 90, Loss: 0.685402
Train - Epoch 2, Batch: 100, Loss: 0.683886
Train - Epoch 2, Batch: 110, Loss: 0.683237
Train - Epoch 2, Batch: 120, Loss: 0.684156
Train - Epoch 2, Batch: 130, Loss: 0.684830
Train - Epoch 2, Batch: 140, Loss: 0.683312
Train - Epoch 2, Batch: 150, Loss: 0.684914
Train - Epoch 2, Batch: 160, Loss: 0.684220
Train - Epoch 2, Batch: 170, Loss: 0.684908
Train - Epoch 2, Batch: 180, Loss: 0.685635
Train - Epoch 2, Batch: 190, Loss: 0.683884
Train - Epoch 2, Batch: 200, Loss: 0.683950
Train - Epoch 2, Batch: 210, Loss: 0.684751
Train - Epoch 2, Batch: 220, Loss: 0.685119
Train - Epoch 2, Batch: 230, Loss: 0.684652
Train - Epoch 2, Batch: 240, Loss: 0.683724
Train - Epoch 2, Batch: 250, Loss: 0.685294
Train - Epoch 2, Batch: 260, Loss: 0.683591
Train - Epoch 2, Batch: 270, Loss: 0.685964
Train - Epoch 2, Batch: 280, Loss: 0.684856
Train - Epoch 2, Batch: 290, Loss: 0.684973
Train - Epoch 2, Batch: 300, Loss: 0.685013
Train - Epoch 2, Batch: 310, Loss: 0.684124
Train - Epoch 2, Batch: 320, Loss: 0.684951
Train - Epoch 2, Batch: 330, Loss: 0.684135
Train - Epoch 2, Batch: 340, Loss: 0.684251
Train - Epoch 2, Batch: 350, Loss: 0.684753
Train - Epoch 2, Batch: 360, Loss: 0.683849
Train - Epoch 2, Batch: 370, Loss: 0.686614
Train - Epoch 2, Batch: 380, Loss: 0.684822
Train - Epoch 2, Batch: 390, Loss: 0.684367
Train - Epoch 2, Batch: 400, Loss: 0.683763
Train - Epoch 2, Batch: 410, Loss: 0.684808
Train - Epoch 2, Batch: 420, Loss: 0.685961
Train - Epoch 2, Batch: 430, Loss: 0.685387
Train - Epoch 2, Batch: 440, Loss: 0.685051
Train - Epoch 2, Batch: 450, Loss: 0.683235
Train - Epoch 2, Batch: 460, Loss: 0.683931
Train - Epoch 2, Batch: 470, Loss: 0.684726
Train - Epoch 2, Batch: 480, Loss: 0.684445
Train - Epoch 2, Batch: 490, Loss: 0.683936
Train - Epoch 2, Batch: 500, Loss: 0.684234
Train - Epoch 2, Batch: 510, Loss: 0.684282
Train - Epoch 2, Batch: 520, Loss: 0.685090
Train - Epoch 2, Batch: 530, Loss: 0.685182
Train - Epoch 2, Batch: 540, Loss: 0.684224
Train - Epoch 2, Batch: 550, Loss: 0.684841/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684738
Train - Epoch 2, Batch: 570, Loss: 0.683108
Train - Epoch 2, Batch: 580, Loss: 0.684583
Train - Epoch 2, Batch: 590, Loss: 0.684301
Train - Epoch 2, Batch: 600, Loss: 0.684520
Train - Epoch 2, Batch: 610, Loss: 0.683398
Train - Epoch 2, Batch: 620, Loss: 0.683390
Train - Epoch 2, Batch: 630, Loss: 0.684090
Train - Epoch 2, Batch: 640, Loss: 0.683755
Train - Epoch 3, Batch: 0, Loss: 0.683246
Train - Epoch 3, Batch: 10, Loss: 0.684822
Train - Epoch 3, Batch: 20, Loss: 0.684150
Train - Epoch 3, Batch: 30, Loss: 0.682911
Train - Epoch 3, Batch: 40, Loss: 0.684583
Train - Epoch 3, Batch: 50, Loss: 0.683860
Train - Epoch 3, Batch: 60, Loss: 0.684705
Train - Epoch 3, Batch: 70, Loss: 0.683638
Train - Epoch 3, Batch: 80, Loss: 0.683817
Train - Epoch 3, Batch: 90, Loss: 0.684117
Train - Epoch 3, Batch: 100, Loss: 0.684079
Train - Epoch 3, Batch: 110, Loss: 0.683949
Train - Epoch 3, Batch: 120, Loss: 0.683254
Train - Epoch 3, Batch: 130, Loss: 0.683638
Train - Epoch 3, Batch: 140, Loss: 0.684098
Train - Epoch 3, Batch: 150, Loss: 0.683606
Train - Epoch 3, Batch: 160, Loss: 0.683470
Train - Epoch 3, Batch: 170, Loss: 0.683814
Train - Epoch 3, Batch: 180, Loss: 0.683535
Train - Epoch 3, Batch: 190, Loss: 0.685142
Train - Epoch 3, Batch: 200, Loss: 0.684835
Train - Epoch 3, Batch: 210, Loss: 0.684806
Train - Epoch 3, Batch: 220, Loss: 0.683147
Train - Epoch 3, Batch: 230, Loss: 0.683877
Train - Epoch 3, Batch: 240, Loss: 0.683739
Train - Epoch 3, Batch: 250, Loss: 0.683175
Train - Epoch 3, Batch: 260, Loss: 0.683154
Train - Epoch 3, Batch: 270, Loss: 0.684144
Train - Epoch 3, Batch: 280, Loss: 0.684032
Train - Epoch 3, Batch: 290, Loss: 0.684527
Train - Epoch 3, Batch: 300, Loss: 0.684214
Train - Epoch 3, Batch: 310, Loss: 0.684109
Train - Epoch 3, Batch: 320, Loss: 0.684606
Train - Epoch 3, Batch: 330, Loss: 0.683691
Train - Epoch 3, Batch: 340, Loss: 0.683788
Train - Epoch 3, Batch: 350, Loss: 0.683608
Train - Epoch 3, Batch: 360, Loss: 0.682860
Train - Epoch 3, Batch: 370, Loss: 0.683901
Train - Epoch 3, Batch: 380, Loss: 0.684784
Train - Epoch 3, Batch: 390, Loss: 0.683239
Train - Epoch 3, Batch: 400, Loss: 0.683368
Train - Epoch 3, Batch: 410, Loss: 0.684299
Train - Epoch 3, Batch: 420, Loss: 0.683352
Train - Epoch 3, Batch: 430, Loss: 0.684143
Train - Epoch 3, Batch: 440, Loss: 0.683345
Train - Epoch 3, Batch: 450, Loss: 0.683103
Train - Epoch 3, Batch: 460, Loss: 0.682584
Train - Epoch 3, Batch: 470, Loss: 0.684883
Train - Epoch 3, Batch: 480, Loss: 0.683264
Train - Epoch 3, Batch: 490, Loss: 0.684797
Train - Epoch 3, Batch: 500, Loss: 0.683886
Train - Epoch 3, Batch: 510, Loss: 0.683818
Train - Epoch 3, Batch: 520, Loss: 0.683977
Train - Epoch 3, Batch: 530, Loss: 0.683141
Train - Epoch 3, Batch: 540, Loss: 0.684617
Train - Epoch 3, Batch: 550, Loss: 0.682840
Train - Epoch 3, Batch: 560, Loss: 0.683910
Train - Epoch 3, Batch: 570, Loss: 0.682756
Train - Epoch 3, Batch: 580, Loss: 0.683430
Train - Epoch 3, Batch: 590, Loss: 0.683574
Train - Epoch 3, Batch: 600, Loss: 0.683779
Train - Epoch 3, Batch: 610, Loss: 0.682686
Train - Epoch 3, Batch: 620, Loss: 0.684308
Train - Epoch 3, Batch: 630, Loss: 0.683633
Train - Epoch 3, Batch: 640, Loss: 0.684157
training_time:: 7.720094203948975
training time full:: 7.72013783454895
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.555856
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 10500
training time is 5.4803266525268555
overhead:: 0
overhead2:: 0
time_baseline:: 5.484049081802368
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555864
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.18030738830566406
overhead3:: 0.28719305992126465
overhead4:: 0.909752607345581
overhead5:: 0
time_provenance:: 3.023479461669922
curr_diff: 0 tensor(8.4103e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4103e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555880
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.18772411346435547
overhead3:: 0.26076769828796387
overhead4:: 0.9408981800079346
overhead5:: 0
time_provenance:: 2.995811700820923
curr_diff: 0 tensor(8.3680e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3680e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555880
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.23830723762512207
overhead3:: 0.3581366539001465
overhead4:: 1.1595206260681152
overhead5:: 0
time_provenance:: 3.3661298751831055
curr_diff: 0 tensor(7.9411e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9411e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555880
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2591972351074219
overhead3:: 0.3803260326385498
overhead4:: 1.2528090476989746
overhead5:: 0
time_provenance:: 3.3397774696350098
curr_diff: 0 tensor(7.8037e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8037e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555880
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2875216007232666
overhead3:: 0.41477084159851074
overhead4:: 1.4679391384124756
overhead5:: 0
time_provenance:: 4.461401462554932
curr_diff: 0 tensor(4.2270e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2270e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555876
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2633225917816162
overhead3:: 0.36699843406677246
overhead4:: 1.406656265258789
overhead5:: 0
time_provenance:: 3.9630126953125
curr_diff: 0 tensor(4.1065e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1065e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555876
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.25464868545532227
overhead3:: 0.3550872802734375
overhead4:: 1.350996971130371
overhead5:: 0
time_provenance:: 3.499783992767334
curr_diff: 0 tensor(3.9216e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9216e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555876
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2604336738586426
overhead3:: 0.3677206039428711
overhead4:: 1.4571094512939453
overhead5:: 0
time_provenance:: 3.5458574295043945
curr_diff: 0 tensor(3.8218e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8218e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555876
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.37802886962890625
overhead3:: 0.5209250450134277
overhead4:: 2.1519484519958496
overhead5:: 0
time_provenance:: 5.335566759109497
curr_diff: 0 tensor(2.7756e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7756e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555880
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3860633373260498
overhead3:: 0.5263490676879883
overhead4:: 2.1454946994781494
overhead5:: 0
time_provenance:: 5.2376649379730225
curr_diff: 0 tensor(2.8673e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8673e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555864
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.33976125717163086
overhead3:: 0.44914722442626953
overhead4:: 1.8893959522247314
overhead5:: 0
time_provenance:: 4.187869548797607
curr_diff: 0 tensor(3.3909e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3909e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555866
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4017796516418457
overhead3:: 0.6105527877807617
overhead4:: 2.164893388748169
overhead5:: 0
time_provenance:: 4.66093373298645
curr_diff: 0 tensor(2.5591e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5591e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555878
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.5149848461151123
overhead3:: 0.7717311382293701
overhead4:: 2.35386061668396
overhead5:: 0
time_provenance:: 5.3293280601501465
curr_diff: 0 tensor(1.8070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555860
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4327125549316406
overhead3:: 0.5786592960357666
overhead4:: 2.4724152088165283
overhead5:: 0
time_provenance:: 5.061020374298096
curr_diff: 0 tensor(1.7801e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7801e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555860
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.5144937038421631
overhead3:: 0.6935632228851318
overhead4:: 2.9457144737243652
overhead5:: 0
time_provenance:: 6.242102861404419
curr_diff: 0 tensor(1.7249e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7249e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555860
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.5040020942687988
overhead3:: 0.6799721717834473
overhead4:: 2.8398008346557617
overhead5:: 0
time_provenance:: 5.947265625
curr_diff: 0 tensor(1.6790e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6790e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555860
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.782850980758667
overhead3:: 1.1113855838775635
overhead4:: 3.6730923652648926
overhead5:: 0
time_provenance:: 6.172577857971191
curr_diff: 0 tensor(2.8799e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8799e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555864
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.701308
Train - Epoch 0, Batch: 10, Loss: 0.695634
Train - Epoch 0, Batch: 20, Loss: 0.694028
Train - Epoch 0, Batch: 30, Loss: 0.693849
Train - Epoch 0, Batch: 40, Loss: 0.694496
Train - Epoch 0, Batch: 50, Loss: 0.692815
Train - Epoch 0, Batch: 60, Loss: 0.692728
Train - Epoch 0, Batch: 70, Loss: 0.692605
Train - Epoch 0, Batch: 80, Loss: 0.691310
Train - Epoch 0, Batch: 90, Loss: 0.692004
Train - Epoch 0, Batch: 100, Loss: 0.691980
Train - Epoch 0, Batch: 110, Loss: 0.692147
Train - Epoch 0, Batch: 120, Loss: 0.690828
Train - Epoch 0, Batch: 130, Loss: 0.690978
Train - Epoch 0, Batch: 140, Loss: 0.690730
Train - Epoch 0, Batch: 150, Loss: 0.689835
Train - Epoch 0, Batch: 160, Loss: 0.690365
Train - Epoch 0, Batch: 170, Loss: 0.689101
Train - Epoch 0, Batch: 180, Loss: 0.690384
Train - Epoch 0, Batch: 190, Loss: 0.689702
Train - Epoch 0, Batch: 200, Loss: 0.689338
Train - Epoch 0, Batch: 210, Loss: 0.689889
Train - Epoch 0, Batch: 220, Loss: 0.689530
Train - Epoch 0, Batch: 230, Loss: 0.690508
Train - Epoch 0, Batch: 240, Loss: 0.690651
Train - Epoch 0, Batch: 250, Loss: 0.689880
Train - Epoch 0, Batch: 260, Loss: 0.690011
Train - Epoch 0, Batch: 270, Loss: 0.689067
Train - Epoch 0, Batch: 280, Loss: 0.690028
Train - Epoch 0, Batch: 290, Loss: 0.690166
Train - Epoch 0, Batch: 300, Loss: 0.687367
Train - Epoch 0, Batch: 310, Loss: 0.688149
Train - Epoch 0, Batch: 320, Loss: 0.689368
Train - Epoch 0, Batch: 330, Loss: 0.688884
Train - Epoch 0, Batch: 340, Loss: 0.687079
Train - Epoch 0, Batch: 350, Loss: 0.687631
Train - Epoch 0, Batch: 360, Loss: 0.689072
Train - Epoch 0, Batch: 370, Loss: 0.688801
Train - Epoch 0, Batch: 380, Loss: 0.688071
Train - Epoch 0, Batch: 390, Loss: 0.688390
Train - Epoch 0, Batch: 400, Loss: 0.688551
Train - Epoch 0, Batch: 410, Loss: 0.688806
Train - Epoch 0, Batch: 420, Loss: 0.689004
Train - Epoch 0, Batch: 430, Loss: 0.687369
Train - Epoch 0, Batch: 440, Loss: 0.687865
Train - Epoch 0, Batch: 450, Loss: 0.688353
Train - Epoch 0, Batch: 460, Loss: 0.688590
Train - Epoch 0, Batch: 470, Loss: 0.687702
Train - Epoch 0, Batch: 480, Loss: 0.686967
Train - Epoch 0, Batch: 490, Loss: 0.686823
Train - Epoch 0, Batch: 500, Loss: 0.687697
Train - Epoch 0, Batch: 510, Loss: 0.687368
Train - Epoch 0, Batch: 520, Loss: 0.687773
Train - Epoch 0, Batch: 530, Loss: 0.688558
Train - Epoch 0, Batch: 540, Loss: 0.687039
Train - Epoch 0, Batch: 550, Loss: 0.687724
Train - Epoch 0, Batch: 560, Loss: 0.687483
Train - Epoch 0, Batch: 570, Loss: 0.687315
Train - Epoch 0, Batch: 580, Loss: 0.686548
Train - Epoch 0, Batch: 590, Loss: 0.687009
Train - Epoch 0, Batch: 600, Loss: 0.686598
Train - Epoch 0, Batch: 610, Loss: 0.687259
Train - Epoch 0, Batch: 620, Loss: 0.687255
Train - Epoch 0, Batch: 630, Loss: 0.687367
Train - Epoch 0, Batch: 640, Loss: 0.687336
Train - Epoch 1, Batch: 0, Loss: 0.686567
Train - Epoch 1, Batch: 10, Loss: 0.687830
Train - Epoch 1, Batch: 20, Loss: 0.686094
Train - Epoch 1, Batch: 30, Loss: 0.687065
Train - Epoch 1, Batch: 40, Loss: 0.686852
Train - Epoch 1, Batch: 50, Loss: 0.685567
Train - Epoch 1, Batch: 60, Loss: 0.686668
Train - Epoch 1, Batch: 70, Loss: 0.686146
Train - Epoch 1, Batch: 80, Loss: 0.687459
Train - Epoch 1, Batch: 90, Loss: 0.687109
Train - Epoch 1, Batch: 100, Loss: 0.687763
Train - Epoch 1, Batch: 110, Loss: 0.686621
Train - Epoch 1, Batch: 120, Loss: 0.687639
Train - Epoch 1, Batch: 130, Loss: 0.685232
Train - Epoch 1, Batch: 140, Loss: 0.686680
Train - Epoch 1, Batch: 150, Loss: 0.686075
Train - Epoch 1, Batch: 160, Loss: 0.686807
Train - Epoch 1, Batch: 170, Loss: 0.685824
Train - Epoch 1, Batch: 180, Loss: 0.686364
Train - Epoch 1, Batch: 190, Loss: 0.686774
Train - Epoch 1, Batch: 200, Loss: 0.687001
Train - Epoch 1, Batch: 210, Loss: 0.686416
Train - Epoch 1, Batch: 220, Loss: 0.686141
Train - Epoch 1, Batch: 230, Loss: 0.686626
Train - Epoch 1, Batch: 240, Loss: 0.686741
Train - Epoch 1, Batch: 250, Loss: 0.686906
Train - Epoch 1, Batch: 260, Loss: 0.687095
Train - Epoch 1, Batch: 270, Loss: 0.685829
Train - Epoch 1, Batch: 280, Loss: 0.686350
Train - Epoch 1, Batch: 290, Loss: 0.685764
Train - Epoch 1, Batch: 300, Loss: 0.686092
Train - Epoch 1, Batch: 310, Loss: 0.686986
Train - Epoch 1, Batch: 320, Loss: 0.685165
Train - Epoch 1, Batch: 330, Loss: 0.686717
Train - Epoch 1, Batch: 340, Loss: 0.686406
Train - Epoch 1, Batch: 350, Loss: 0.686402
Train - Epoch 1, Batch: 360, Loss: 0.687350
Train - Epoch 1, Batch: 370, Loss: 0.687199
Train - Epoch 1, Batch: 380, Loss: 0.685549
Train - Epoch 1, Batch: 390, Loss: 0.686235
Train - Epoch 1, Batch: 400, Loss: 0.686608
Train - Epoch 1, Batch: 410, Loss: 0.685272
Train - Epoch 1, Batch: 420, Loss: 0.686154
Train - Epoch 1, Batch: 430, Loss: 0.685881
Train - Epoch 1, Batch: 440, Loss: 0.685074
Train - Epoch 1, Batch: 450, Loss: 0.685488
Train - Epoch 1, Batch: 460, Loss: 0.686828
Train - Epoch 1, Batch: 470, Loss: 0.686238
Train - Epoch 1, Batch: 480, Loss: 0.684783
Train - Epoch 1, Batch: 490, Loss: 0.687133
Train - Epoch 1, Batch: 500, Loss: 0.687142
Train - Epoch 1, Batch: 510, Loss: 0.686260
Train - Epoch 1, Batch: 520, Loss: 0.686394
Train - Epoch 1, Batch: 530, Loss: 0.685450
Train - Epoch 1, Batch: 540, Loss: 0.685220
Train - Epoch 1, Batch: 550, Loss: 0.685946
Train - Epoch 1, Batch: 560, Loss: 0.687305
Train - Epoch 1, Batch: 570, Loss: 0.685590
Train - Epoch 1, Batch: 580, Loss: 0.685892
Train - Epoch 1, Batch: 590, Loss: 0.686859
Train - Epoch 1, Batch: 600, Loss: 0.685840
Train - Epoch 1, Batch: 610, Loss: 0.684990
Train - Epoch 1, Batch: 620, Loss: 0.686054
Train - Epoch 1, Batch: 630, Loss: 0.686473
Train - Epoch 1, Batch: 640, Loss: 0.684936
Train - Epoch 2, Batch: 0, Loss: 0.685288
Train - Epoch 2, Batch: 10, Loss: 0.686491
Train - Epoch 2, Batch: 20, Loss: 0.685836
Train - Epoch 2, Batch: 30, Loss: 0.685194
Train - Epoch 2, Batch: 40, Loss: 0.685441
Train - Epoch 2, Batch: 50, Loss: 0.683939
Train - Epoch 2, Batch: 60, Loss: 0.686022
Train - Epoch 2, Batch: 70, Loss: 0.686849
Train - Epoch 2, Batch: 80, Loss: 0.686550
Train - Epoch 2, Batch: 90, Loss: 0.685490
Train - Epoch 2, Batch: 100, Loss: 0.686757
Train - Epoch 2, Batch: 110, Loss: 0.685238
Train - Epoch 2, Batch: 120, Loss: 0.685086
Train - Epoch 2, Batch: 130, Loss: 0.683371
Train - Epoch 2, Batch: 140, Loss: 0.684779
Train - Epoch 2, Batch: 150, Loss: 0.685334
Train - Epoch 2, Batch: 160, Loss: 0.684276
Train - Epoch 2, Batch: 170, Loss: 0.685932
Train - Epoch 2, Batch: 180, Loss: 0.685314
Train - Epoch 2, Batch: 190, Loss: 0.685660
Train - Epoch 2, Batch: 200, Loss: 0.684924
Train - Epoch 2, Batch: 210, Loss: 0.684742
Train - Epoch 2, Batch: 220, Loss: 0.685276
Train - Epoch 2, Batch: 230, Loss: 0.684362
Train - Epoch 2, Batch: 240, Loss: 0.685743
Train - Epoch 2, Batch: 250, Loss: 0.684532
Train - Epoch 2, Batch: 260, Loss: 0.686043
Train - Epoch 2, Batch: 270, Loss: 0.686441
Train - Epoch 2, Batch: 280, Loss: 0.686538
Train - Epoch 2, Batch: 290, Loss: 0.685226
Train - Epoch 2, Batch: 300, Loss: 0.684818
Train - Epoch 2, Batch: 310, Loss: 0.684710
Train - Epoch 2, Batch: 320, Loss: 0.685048
Train - Epoch 2, Batch: 330, Loss: 0.685116
Train - Epoch 2, Batch: 340, Loss: 0.685659
Train - Epoch 2, Batch: 350, Loss: 0.684386
Train - Epoch 2, Batch: 360, Loss: 0.685628
Train - Epoch 2, Batch: 370, Loss: 0.685593
Train - Epoch 2, Batch: 380, Loss: 0.685655
Train - Epoch 2, Batch: 390, Loss: 0.685492
Train - Epoch 2, Batch: 400, Loss: 0.684378
Train - Epoch 2, Batch: 410, Loss: 0.684968
Train - Epoch 2, Batch: 420, Loss: 0.684268
Train - Epoch 2, Batch: 430, Loss: 0.685212
Train - Epoch 2, Batch: 440, Loss: 0.685675
Train - Epoch 2, Batch: 450, Loss: 0.685240
Train - Epoch 2, Batch: 460, Loss: 0.685535
Train - Epoch 2, Batch: 470, Loss: 0.685276
Train - Epoch 2, Batch: 480, Loss: 0.684825
Train - Epoch 2, Batch: 490, Loss: 0.685229
Train - Epoch 2, Batch: 500, Loss: 0.684069
Train - Epoch 2, Batch: 510, Loss: 0.684365
Train - Epoch 2, Batch: 520, Loss: 0.684716
Train - Epoch 2, Batch: 530, Loss: 0.684635
Train - Epoch 2, Batch: 540, Loss: 0.685781
Train - Epoch 2, Batch: 550, Loss: 0.683946/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685984
Train - Epoch 2, Batch: 570, Loss: 0.683909
Train - Epoch 2, Batch: 580, Loss: 0.684276
Train - Epoch 2, Batch: 590, Loss: 0.685654
Train - Epoch 2, Batch: 600, Loss: 0.686074
Train - Epoch 2, Batch: 610, Loss: 0.684493
Train - Epoch 2, Batch: 620, Loss: 0.684971
Train - Epoch 2, Batch: 630, Loss: 0.685451
Train - Epoch 2, Batch: 640, Loss: 0.683366
Train - Epoch 3, Batch: 0, Loss: 0.685527
Train - Epoch 3, Batch: 10, Loss: 0.684659
Train - Epoch 3, Batch: 20, Loss: 0.684575
Train - Epoch 3, Batch: 30, Loss: 0.683583
Train - Epoch 3, Batch: 40, Loss: 0.684684
Train - Epoch 3, Batch: 50, Loss: 0.684952
Train - Epoch 3, Batch: 60, Loss: 0.684278
Train - Epoch 3, Batch: 70, Loss: 0.684704
Train - Epoch 3, Batch: 80, Loss: 0.685241
Train - Epoch 3, Batch: 90, Loss: 0.684238
Train - Epoch 3, Batch: 100, Loss: 0.684263
Train - Epoch 3, Batch: 110, Loss: 0.685409
Train - Epoch 3, Batch: 120, Loss: 0.684461
Train - Epoch 3, Batch: 130, Loss: 0.684697
Train - Epoch 3, Batch: 140, Loss: 0.683396
Train - Epoch 3, Batch: 150, Loss: 0.685617
Train - Epoch 3, Batch: 160, Loss: 0.684228
Train - Epoch 3, Batch: 170, Loss: 0.683903
Train - Epoch 3, Batch: 180, Loss: 0.682899
Train - Epoch 3, Batch: 190, Loss: 0.684896
Train - Epoch 3, Batch: 200, Loss: 0.684760
Train - Epoch 3, Batch: 210, Loss: 0.684064
Train - Epoch 3, Batch: 220, Loss: 0.684545
Train - Epoch 3, Batch: 230, Loss: 0.684179
Train - Epoch 3, Batch: 240, Loss: 0.684790
Train - Epoch 3, Batch: 250, Loss: 0.684944
Train - Epoch 3, Batch: 260, Loss: 0.684998
Train - Epoch 3, Batch: 270, Loss: 0.684687
Train - Epoch 3, Batch: 280, Loss: 0.683007
Train - Epoch 3, Batch: 290, Loss: 0.685404
Train - Epoch 3, Batch: 300, Loss: 0.684829
Train - Epoch 3, Batch: 310, Loss: 0.684970
Train - Epoch 3, Batch: 320, Loss: 0.684954
Train - Epoch 3, Batch: 330, Loss: 0.684180
Train - Epoch 3, Batch: 340, Loss: 0.683667
Train - Epoch 3, Batch: 350, Loss: 0.683654
Train - Epoch 3, Batch: 360, Loss: 0.684451
Train - Epoch 3, Batch: 370, Loss: 0.684089
Train - Epoch 3, Batch: 380, Loss: 0.683888
Train - Epoch 3, Batch: 390, Loss: 0.684968
Train - Epoch 3, Batch: 400, Loss: 0.683579
Train - Epoch 3, Batch: 410, Loss: 0.684531
Train - Epoch 3, Batch: 420, Loss: 0.684212
Train - Epoch 3, Batch: 430, Loss: 0.683491
Train - Epoch 3, Batch: 440, Loss: 0.684156
Train - Epoch 3, Batch: 450, Loss: 0.685389
Train - Epoch 3, Batch: 460, Loss: 0.684322
Train - Epoch 3, Batch: 470, Loss: 0.684502
Train - Epoch 3, Batch: 480, Loss: 0.684967
Train - Epoch 3, Batch: 490, Loss: 0.684118
Train - Epoch 3, Batch: 500, Loss: 0.684389
Train - Epoch 3, Batch: 510, Loss: 0.683712
Train - Epoch 3, Batch: 520, Loss: 0.683982
Train - Epoch 3, Batch: 530, Loss: 0.683502
Train - Epoch 3, Batch: 540, Loss: 0.684944
Train - Epoch 3, Batch: 550, Loss: 0.683054
Train - Epoch 3, Batch: 560, Loss: 0.684655
Train - Epoch 3, Batch: 570, Loss: 0.683943
Train - Epoch 3, Batch: 580, Loss: 0.684390
Train - Epoch 3, Batch: 590, Loss: 0.684780
Train - Epoch 3, Batch: 600, Loss: 0.683573
Train - Epoch 3, Batch: 610, Loss: 0.684248
Train - Epoch 3, Batch: 620, Loss: 0.682951
Train - Epoch 3, Batch: 630, Loss: 0.683294
Train - Epoch 3, Batch: 640, Loss: 0.684807
training_time:: 7.815662145614624
training time full:: 7.815701961517334
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554682
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 10500
training time is 5.506855249404907
overhead:: 0
overhead2:: 0
time_baseline:: 5.509829044342041
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554716
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.14754438400268555
overhead3:: 0.2116239070892334
overhead4:: 0.7863054275512695
overhead5:: 0
time_provenance:: 2.781571388244629
curr_diff: 0 tensor(9.8336e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8336e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554706
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.18728113174438477
overhead3:: 0.23353862762451172
overhead4:: 0.8690567016601562
overhead5:: 0
time_provenance:: 2.894590139389038
curr_diff: 0 tensor(9.7330e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7330e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554708
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.23090672492980957
overhead3:: 0.3594961166381836
overhead4:: 1.1679465770721436
overhead5:: 0
time_provenance:: 3.274705648422241
curr_diff: 0 tensor(9.6636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554706
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.23912739753723145
overhead3:: 0.34979915618896484
overhead4:: 1.2205216884613037
overhead5:: 0
time_provenance:: 3.317873001098633
curr_diff: 0 tensor(9.4316e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4316e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554702
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.23305606842041016
overhead3:: 0.35518813133239746
overhead4:: 1.301178216934204
overhead5:: 0
time_provenance:: 3.501727342605591
curr_diff: 0 tensor(5.0977e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0977e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554730
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2406470775604248
overhead3:: 0.3199934959411621
overhead4:: 1.324098825454712
overhead5:: 0
time_provenance:: 3.498854398727417
curr_diff: 0 tensor(4.9698e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9698e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554730
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2638661861419678
overhead3:: 0.36836957931518555
overhead4:: 1.4895365238189697
overhead5:: 0
time_provenance:: 3.6484906673431396
curr_diff: 0 tensor(4.9161e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9161e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554730
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.29870152473449707
overhead3:: 0.4247739315032959
overhead4:: 1.4597842693328857
overhead5:: 0
time_provenance:: 3.733670234680176
curr_diff: 0 tensor(4.7894e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7894e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554728
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2989475727081299
overhead3:: 0.40562891960144043
overhead4:: 1.7391905784606934
overhead5:: 0
time_provenance:: 4.059375047683716
curr_diff: 0 tensor(3.2134e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2134e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554714
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3435688018798828
overhead3:: 0.5331974029541016
overhead4:: 1.9362168312072754
overhead5:: 0
time_provenance:: 4.368058681488037
curr_diff: 0 tensor(2.4094e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4094e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554714
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.333981990814209
overhead3:: 0.44333505630493164
overhead4:: 1.8038661479949951
overhead5:: 0
time_provenance:: 4.097796678543091
curr_diff: 0 tensor(3.1180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1180e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554714
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.36457085609436035
overhead3:: 0.5135645866394043
overhead4:: 2.031904935836792
overhead5:: 0
time_provenance:: 4.372299432754517
curr_diff: 0 tensor(3.0818e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0818e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554714
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.5222868919372559
overhead3:: 0.7157566547393799
overhead4:: 2.8828928470611572
overhead5:: 0
time_provenance:: 6.32666802406311
curr_diff: 0 tensor(1.6774e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6774e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554712
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.45865917205810547
overhead3:: 0.6109108924865723
overhead4:: 2.442866802215576
overhead5:: 0
time_provenance:: 5.060243368148804
curr_diff: 0 tensor(1.6384e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6384e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554712
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4779171943664551
overhead3:: 0.661186933517456
overhead4:: 2.6955740451812744
overhead5:: 0
time_provenance:: 5.624769687652588
curr_diff: 0 tensor(1.6149e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6149e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554712
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.44048452377319336
overhead3:: 0.6221280097961426
overhead4:: 2.7214620113372803
overhead5:: 0
time_provenance:: 5.343865394592285
curr_diff: 0 tensor(1.5807e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5807e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554712
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.6822693347930908
overhead3:: 0.9145960807800293
overhead4:: 3.357447624206543
overhead5:: 0
time_provenance:: 5.526857376098633
curr_diff: 0 tensor(2.7121e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7121e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554716
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.734516
Train - Epoch 0, Batch: 10, Loss: 0.694899
Train - Epoch 0, Batch: 20, Loss: 0.694269
Train - Epoch 0, Batch: 30, Loss: 0.694265
Train - Epoch 0, Batch: 40, Loss: 0.694107
Train - Epoch 0, Batch: 50, Loss: 0.693448
Train - Epoch 0, Batch: 60, Loss: 0.691271
Train - Epoch 0, Batch: 70, Loss: 0.691790
Train - Epoch 0, Batch: 80, Loss: 0.691386
Train - Epoch 0, Batch: 90, Loss: 0.691681
Train - Epoch 0, Batch: 100, Loss: 0.691276
Train - Epoch 0, Batch: 110, Loss: 0.691076
Train - Epoch 0, Batch: 120, Loss: 0.691044
Train - Epoch 0, Batch: 130, Loss: 0.690568
Train - Epoch 0, Batch: 140, Loss: 0.690126
Train - Epoch 0, Batch: 150, Loss: 0.691154
Train - Epoch 0, Batch: 160, Loss: 0.690628
Train - Epoch 0, Batch: 170, Loss: 0.690228
Train - Epoch 0, Batch: 180, Loss: 0.690348
Train - Epoch 0, Batch: 190, Loss: 0.690340
Train - Epoch 0, Batch: 200, Loss: 0.689756
Train - Epoch 0, Batch: 210, Loss: 0.690229
Train - Epoch 0, Batch: 220, Loss: 0.689122
Train - Epoch 0, Batch: 230, Loss: 0.689103
Train - Epoch 0, Batch: 240, Loss: 0.688578
Train - Epoch 0, Batch: 250, Loss: 0.688656
Train - Epoch 0, Batch: 260, Loss: 0.689351
Train - Epoch 0, Batch: 270, Loss: 0.688681
Train - Epoch 0, Batch: 280, Loss: 0.688860
Train - Epoch 0, Batch: 290, Loss: 0.690508
Train - Epoch 0, Batch: 300, Loss: 0.689853
Train - Epoch 0, Batch: 310, Loss: 0.688351
Train - Epoch 0, Batch: 320, Loss: 0.690270
Train - Epoch 0, Batch: 330, Loss: 0.688197
Train - Epoch 0, Batch: 340, Loss: 0.688132
Train - Epoch 0, Batch: 350, Loss: 0.689246
Train - Epoch 0, Batch: 360, Loss: 0.688995
Train - Epoch 0, Batch: 370, Loss: 0.687068
Train - Epoch 0, Batch: 380, Loss: 0.687594
Train - Epoch 0, Batch: 390, Loss: 0.687812
Train - Epoch 0, Batch: 400, Loss: 0.688978
Train - Epoch 0, Batch: 410, Loss: 0.688702
Train - Epoch 0, Batch: 420, Loss: 0.687384
Train - Epoch 0, Batch: 430, Loss: 0.689302
Train - Epoch 0, Batch: 440, Loss: 0.688493
Train - Epoch 0, Batch: 450, Loss: 0.688680
Train - Epoch 0, Batch: 460, Loss: 0.687291
Train - Epoch 0, Batch: 470, Loss: 0.687619
Train - Epoch 0, Batch: 480, Loss: 0.687733
Train - Epoch 0, Batch: 490, Loss: 0.688393
Train - Epoch 0, Batch: 500, Loss: 0.687560
Train - Epoch 0, Batch: 510, Loss: 0.688344
Train - Epoch 0, Batch: 520, Loss: 0.687642
Train - Epoch 0, Batch: 530, Loss: 0.688933
Train - Epoch 0, Batch: 540, Loss: 0.686798
Train - Epoch 0, Batch: 550, Loss: 0.686927
Train - Epoch 0, Batch: 560, Loss: 0.687707
Train - Epoch 0, Batch: 570, Loss: 0.687110
Train - Epoch 0, Batch: 580, Loss: 0.687342
Train - Epoch 0, Batch: 590, Loss: 0.687065
Train - Epoch 0, Batch: 600, Loss: 0.687248
Train - Epoch 0, Batch: 610, Loss: 0.688287
Train - Epoch 0, Batch: 620, Loss: 0.688515
Train - Epoch 0, Batch: 630, Loss: 0.688015
Train - Epoch 0, Batch: 640, Loss: 0.686995
Train - Epoch 1, Batch: 0, Loss: 0.686172
Train - Epoch 1, Batch: 10, Loss: 0.687645
Train - Epoch 1, Batch: 20, Loss: 0.688218
Train - Epoch 1, Batch: 30, Loss: 0.685906
Train - Epoch 1, Batch: 40, Loss: 0.687222
Train - Epoch 1, Batch: 50, Loss: 0.688980
Train - Epoch 1, Batch: 60, Loss: 0.688305
Train - Epoch 1, Batch: 70, Loss: 0.686771
Train - Epoch 1, Batch: 80, Loss: 0.686339
Train - Epoch 1, Batch: 90, Loss: 0.687038
Train - Epoch 1, Batch: 100, Loss: 0.686377
Train - Epoch 1, Batch: 110, Loss: 0.686591
Train - Epoch 1, Batch: 120, Loss: 0.687395
Train - Epoch 1, Batch: 130, Loss: 0.687002
Train - Epoch 1, Batch: 140, Loss: 0.686763
Train - Epoch 1, Batch: 150, Loss: 0.687166
Train - Epoch 1, Batch: 160, Loss: 0.686182
Train - Epoch 1, Batch: 170, Loss: 0.687058
Train - Epoch 1, Batch: 180, Loss: 0.686824
Train - Epoch 1, Batch: 190, Loss: 0.687056
Train - Epoch 1, Batch: 200, Loss: 0.685595
Train - Epoch 1, Batch: 210, Loss: 0.687718
Train - Epoch 1, Batch: 220, Loss: 0.686388
Train - Epoch 1, Batch: 230, Loss: 0.687143
Train - Epoch 1, Batch: 240, Loss: 0.686723
Train - Epoch 1, Batch: 250, Loss: 0.686937
Train - Epoch 1, Batch: 260, Loss: 0.685866
Train - Epoch 1, Batch: 270, Loss: 0.687207
Train - Epoch 1, Batch: 280, Loss: 0.686259
Train - Epoch 1, Batch: 290, Loss: 0.686102
Train - Epoch 1, Batch: 300, Loss: 0.686535
Train - Epoch 1, Batch: 310, Loss: 0.687291
Train - Epoch 1, Batch: 320, Loss: 0.685998
Train - Epoch 1, Batch: 330, Loss: 0.685734
Train - Epoch 1, Batch: 340, Loss: 0.686575
Train - Epoch 1, Batch: 350, Loss: 0.687170
Train - Epoch 1, Batch: 360, Loss: 0.686439
Train - Epoch 1, Batch: 370, Loss: 0.686995
Train - Epoch 1, Batch: 380, Loss: 0.686438
Train - Epoch 1, Batch: 390, Loss: 0.684686
Train - Epoch 1, Batch: 400, Loss: 0.685931
Train - Epoch 1, Batch: 410, Loss: 0.685977
Train - Epoch 1, Batch: 420, Loss: 0.685070
Train - Epoch 1, Batch: 430, Loss: 0.685105
Train - Epoch 1, Batch: 440, Loss: 0.686303
Train - Epoch 1, Batch: 450, Loss: 0.685650
Train - Epoch 1, Batch: 460, Loss: 0.685518
Train - Epoch 1, Batch: 470, Loss: 0.686620
Train - Epoch 1, Batch: 480, Loss: 0.686079
Train - Epoch 1, Batch: 490, Loss: 0.686275
Train - Epoch 1, Batch: 500, Loss: 0.686456
Train - Epoch 1, Batch: 510, Loss: 0.686376
Train - Epoch 1, Batch: 520, Loss: 0.686970
Train - Epoch 1, Batch: 530, Loss: 0.686437
Train - Epoch 1, Batch: 540, Loss: 0.685105
Train - Epoch 1, Batch: 550, Loss: 0.686284
Train - Epoch 1, Batch: 560, Loss: 0.685197
Train - Epoch 1, Batch: 570, Loss: 0.686300
Train - Epoch 1, Batch: 580, Loss: 0.686851
Train - Epoch 1, Batch: 590, Loss: 0.685306
Train - Epoch 1, Batch: 600, Loss: 0.686728
Train - Epoch 1, Batch: 610, Loss: 0.686985
Train - Epoch 1, Batch: 620, Loss: 0.685244
Train - Epoch 1, Batch: 630, Loss: 0.685501
Train - Epoch 1, Batch: 640, Loss: 0.687200
Train - Epoch 2, Batch: 0, Loss: 0.685365
Train - Epoch 2, Batch: 10, Loss: 0.684845
Train - Epoch 2, Batch: 20, Loss: 0.684660
Train - Epoch 2, Batch: 30, Loss: 0.685043
Train - Epoch 2, Batch: 40, Loss: 0.685619
Train - Epoch 2, Batch: 50, Loss: 0.685866
Train - Epoch 2, Batch: 60, Loss: 0.685848
Train - Epoch 2, Batch: 70, Loss: 0.685806
Train - Epoch 2, Batch: 80, Loss: 0.685790
Train - Epoch 2, Batch: 90, Loss: 0.685561
Train - Epoch 2, Batch: 100, Loss: 0.685411
Train - Epoch 2, Batch: 110, Loss: 0.685518
Train - Epoch 2, Batch: 120, Loss: 0.686125
Train - Epoch 2, Batch: 130, Loss: 0.685322
Train - Epoch 2, Batch: 140, Loss: 0.685460
Train - Epoch 2, Batch: 150, Loss: 0.685787
Train - Epoch 2, Batch: 160, Loss: 0.684719
Train - Epoch 2, Batch: 170, Loss: 0.684594
Train - Epoch 2, Batch: 180, Loss: 0.685821
Train - Epoch 2, Batch: 190, Loss: 0.685389
Train - Epoch 2, Batch: 200, Loss: 0.685145
Train - Epoch 2, Batch: 210, Loss: 0.685688
Train - Epoch 2, Batch: 220, Loss: 0.685079
Train - Epoch 2, Batch: 230, Loss: 0.684482
Train - Epoch 2, Batch: 240, Loss: 0.685849
Train - Epoch 2, Batch: 250, Loss: 0.685279
Train - Epoch 2, Batch: 260, Loss: 0.685607
Train - Epoch 2, Batch: 270, Loss: 0.684811
Train - Epoch 2, Batch: 280, Loss: 0.685202
Train - Epoch 2, Batch: 290, Loss: 0.685444
Train - Epoch 2, Batch: 300, Loss: 0.684527
Train - Epoch 2, Batch: 310, Loss: 0.685290
Train - Epoch 2, Batch: 320, Loss: 0.685095
Train - Epoch 2, Batch: 330, Loss: 0.684261
Train - Epoch 2, Batch: 340, Loss: 0.686521
Train - Epoch 2, Batch: 350, Loss: 0.685390
Train - Epoch 2, Batch: 360, Loss: 0.684858
Train - Epoch 2, Batch: 370, Loss: 0.684648
Train - Epoch 2, Batch: 380, Loss: 0.683574
Train - Epoch 2, Batch: 390, Loss: 0.685074
Train - Epoch 2, Batch: 400, Loss: 0.684347
Train - Epoch 2, Batch: 410, Loss: 0.684160
Train - Epoch 2, Batch: 420, Loss: 0.686006
Train - Epoch 2, Batch: 430, Loss: 0.684635
Train - Epoch 2, Batch: 440, Loss: 0.684512
Train - Epoch 2, Batch: 450, Loss: 0.685303
Train - Epoch 2, Batch: 460, Loss: 0.685539
Train - Epoch 2, Batch: 470, Loss: 0.685771
Train - Epoch 2, Batch: 480, Loss: 0.683819
Train - Epoch 2, Batch: 490, Loss: 0.685499
Train - Epoch 2, Batch: 500, Loss: 0.684249
Train - Epoch 2, Batch: 510, Loss: 0.683804
Train - Epoch 2, Batch: 520, Loss: 0.685215
Train - Epoch 2, Batch: 530, Loss: 0.685928
Train - Epoch 2, Batch: 540, Loss: 0.685860
Train - Epoch 2, Batch: 550, Loss: 0.684729/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685454
Train - Epoch 2, Batch: 570, Loss: 0.684659
Train - Epoch 2, Batch: 580, Loss: 0.683953
Train - Epoch 2, Batch: 590, Loss: 0.684796
Train - Epoch 2, Batch: 600, Loss: 0.685734
Train - Epoch 2, Batch: 610, Loss: 0.684741
Train - Epoch 2, Batch: 620, Loss: 0.684581
Train - Epoch 2, Batch: 630, Loss: 0.685377
Train - Epoch 2, Batch: 640, Loss: 0.685473
Train - Epoch 3, Batch: 0, Loss: 0.686168
Train - Epoch 3, Batch: 10, Loss: 0.685529
Train - Epoch 3, Batch: 20, Loss: 0.685235
Train - Epoch 3, Batch: 30, Loss: 0.684168
Train - Epoch 3, Batch: 40, Loss: 0.685465
Train - Epoch 3, Batch: 50, Loss: 0.684916
Train - Epoch 3, Batch: 60, Loss: 0.684481
Train - Epoch 3, Batch: 70, Loss: 0.683565
Train - Epoch 3, Batch: 80, Loss: 0.684731
Train - Epoch 3, Batch: 90, Loss: 0.684863
Train - Epoch 3, Batch: 100, Loss: 0.684753
Train - Epoch 3, Batch: 110, Loss: 0.684265
Train - Epoch 3, Batch: 120, Loss: 0.684015
Train - Epoch 3, Batch: 130, Loss: 0.683899
Train - Epoch 3, Batch: 140, Loss: 0.685393
Train - Epoch 3, Batch: 150, Loss: 0.683086
Train - Epoch 3, Batch: 160, Loss: 0.683906
Train - Epoch 3, Batch: 170, Loss: 0.683974
Train - Epoch 3, Batch: 180, Loss: 0.684961
Train - Epoch 3, Batch: 190, Loss: 0.684776
Train - Epoch 3, Batch: 200, Loss: 0.684364
Train - Epoch 3, Batch: 210, Loss: 0.685576
Train - Epoch 3, Batch: 220, Loss: 0.683614
Train - Epoch 3, Batch: 230, Loss: 0.685881
Train - Epoch 3, Batch: 240, Loss: 0.685458
Train - Epoch 3, Batch: 250, Loss: 0.683715
Train - Epoch 3, Batch: 260, Loss: 0.683262
Train - Epoch 3, Batch: 270, Loss: 0.685311
Train - Epoch 3, Batch: 280, Loss: 0.684841
Train - Epoch 3, Batch: 290, Loss: 0.683911
Train - Epoch 3, Batch: 300, Loss: 0.684124
Train - Epoch 3, Batch: 310, Loss: 0.683702
Train - Epoch 3, Batch: 320, Loss: 0.683013
Train - Epoch 3, Batch: 330, Loss: 0.685483
Train - Epoch 3, Batch: 340, Loss: 0.684140
Train - Epoch 3, Batch: 350, Loss: 0.683692
Train - Epoch 3, Batch: 360, Loss: 0.683814
Train - Epoch 3, Batch: 370, Loss: 0.684697
Train - Epoch 3, Batch: 380, Loss: 0.684780
Train - Epoch 3, Batch: 390, Loss: 0.683622
Train - Epoch 3, Batch: 400, Loss: 0.684254
Train - Epoch 3, Batch: 410, Loss: 0.685089
Train - Epoch 3, Batch: 420, Loss: 0.684041
Train - Epoch 3, Batch: 430, Loss: 0.682921
Train - Epoch 3, Batch: 440, Loss: 0.684422
Train - Epoch 3, Batch: 450, Loss: 0.684320
Train - Epoch 3, Batch: 460, Loss: 0.685654
Train - Epoch 3, Batch: 470, Loss: 0.683337
Train - Epoch 3, Batch: 480, Loss: 0.684084
Train - Epoch 3, Batch: 490, Loss: 0.684128
Train - Epoch 3, Batch: 500, Loss: 0.685439
Train - Epoch 3, Batch: 510, Loss: 0.685045
Train - Epoch 3, Batch: 520, Loss: 0.684269
Train - Epoch 3, Batch: 530, Loss: 0.684303
Train - Epoch 3, Batch: 540, Loss: 0.683857
Train - Epoch 3, Batch: 550, Loss: 0.684322
Train - Epoch 3, Batch: 560, Loss: 0.684746
Train - Epoch 3, Batch: 570, Loss: 0.685175
Train - Epoch 3, Batch: 580, Loss: 0.684335
Train - Epoch 3, Batch: 590, Loss: 0.684884
Train - Epoch 3, Batch: 600, Loss: 0.684914
Train - Epoch 3, Batch: 610, Loss: 0.684399
Train - Epoch 3, Batch: 620, Loss: 0.683679
Train - Epoch 3, Batch: 630, Loss: 0.684897
Train - Epoch 3, Batch: 640, Loss: 0.684860
training_time:: 7.966737747192383
training time full:: 7.966780185699463
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554560
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 10500
training time is 4.6888744831085205
overhead:: 0
overhead2:: 0
time_baseline:: 4.692033529281616
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.17577314376831055
overhead3:: 0.24858403205871582
overhead4:: 0.8711264133453369
overhead5:: 0
time_provenance:: 2.971804141998291
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554604
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.18563413619995117
overhead3:: 0.26049351692199707
overhead4:: 0.9818108081817627
overhead5:: 0
time_provenance:: 3.0556349754333496
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554604
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.21304082870483398
overhead3:: 0.29532408714294434
overhead4:: 1.0638618469238281
overhead5:: 0
time_provenance:: 3.239422559738159
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554600
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.24182701110839844
overhead3:: 0.3427886962890625
overhead4:: 1.1554474830627441
overhead5:: 0
time_provenance:: 3.2182540893554688
curr_diff: 0 tensor(9.8952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8952e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554600
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.20449542999267578
overhead3:: 0.3103318214416504
overhead4:: 1.2776732444763184
overhead5:: 0
time_provenance:: 3.3930232524871826
curr_diff: 0 tensor(6.6624e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6624e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554620
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2545187473297119
overhead3:: 0.3561103343963623
overhead4:: 1.3283731937408447
overhead5:: 0
time_provenance:: 3.5794966220855713
curr_diff: 0 tensor(6.6147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554620
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2580294609069824
overhead3:: 0.3544602394104004
overhead4:: 1.454932451248169
overhead5:: 0
time_provenance:: 3.5899174213409424
curr_diff: 0 tensor(6.4374e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4374e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554620
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.29575276374816895
overhead3:: 0.42093443870544434
overhead4:: 1.5627543926239014
overhead5:: 0
time_provenance:: 3.744576930999756
curr_diff: 0 tensor(6.2589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554620
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3026468753814697
overhead3:: 0.4545116424560547
overhead4:: 1.8033506870269775
overhead5:: 0
time_provenance:: 4.16218113899231
curr_diff: 0 tensor(4.6147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3232855796813965
overhead3:: 0.44617462158203125
overhead4:: 1.8244478702545166
overhead5:: 0
time_provenance:: 4.192524194717407
curr_diff: 0 tensor(3.5243e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5243e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554614
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3219459056854248
overhead3:: 0.4387247562408447
overhead4:: 1.8916230201721191
overhead5:: 0
time_provenance:: 4.178009986877441
curr_diff: 0 tensor(2.9065e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9065e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554610
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3720743656158447
overhead3:: 0.5548076629638672
overhead4:: 2.0777456760406494
overhead5:: 0
time_provenance:: 4.476677656173706
curr_diff: 0 tensor(4.3788e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3788e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4213979244232178
overhead3:: 0.6233153343200684
overhead4:: 2.4964077472686768
overhead5:: 0
time_provenance:: 5.112183570861816
curr_diff: 0 tensor(2.0627e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0627e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4003715515136719
overhead3:: 0.5669751167297363
overhead4:: 2.452636957168579
overhead5:: 0
time_provenance:: 4.931900262832642
curr_diff: 0 tensor(2.0173e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0173e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.43589162826538086
overhead3:: 0.5891282558441162
overhead4:: 2.4440605640411377
overhead5:: 0
time_provenance:: 5.040262222290039
curr_diff: 0 tensor(1.9827e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9827e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.418637752532959
overhead3:: 0.5663542747497559
overhead4:: 2.5999319553375244
overhead5:: 0
time_provenance:: 5.015082359313965
curr_diff: 0 tensor(1.9488e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9488e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.8425874710083008
overhead3:: 1.1766464710235596
overhead4:: 3.7501094341278076
overhead5:: 0
time_provenance:: 6.400103807449341
curr_diff: 0 tensor(2.3485e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3485e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554612
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.693570
Train - Epoch 0, Batch: 10, Loss: 0.691557
Train - Epoch 0, Batch: 20, Loss: 0.691245
Train - Epoch 0, Batch: 30, Loss: 0.691777
Train - Epoch 0, Batch: 40, Loss: 0.690405
Train - Epoch 0, Batch: 50, Loss: 0.690375
Train - Epoch 0, Batch: 60, Loss: 0.690188
Train - Epoch 0, Batch: 70, Loss: 0.688887
Train - Epoch 0, Batch: 80, Loss: 0.689122
Train - Epoch 0, Batch: 90, Loss: 0.689242
Train - Epoch 0, Batch: 100, Loss: 0.689634
Train - Epoch 0, Batch: 110, Loss: 0.690220
Train - Epoch 0, Batch: 120, Loss: 0.688549
Train - Epoch 0, Batch: 130, Loss: 0.688253
Train - Epoch 0, Batch: 140, Loss: 0.689021
Train - Epoch 0, Batch: 150, Loss: 0.688384
Train - Epoch 0, Batch: 160, Loss: 0.689005
Train - Epoch 0, Batch: 170, Loss: 0.689095
Train - Epoch 0, Batch: 180, Loss: 0.688718
Train - Epoch 0, Batch: 190, Loss: 0.688409
Train - Epoch 0, Batch: 200, Loss: 0.687845
Train - Epoch 0, Batch: 210, Loss: 0.687456
Train - Epoch 0, Batch: 220, Loss: 0.687284
Train - Epoch 0, Batch: 230, Loss: 0.687662
Train - Epoch 0, Batch: 240, Loss: 0.687959
Train - Epoch 0, Batch: 250, Loss: 0.687642
Train - Epoch 0, Batch: 260, Loss: 0.687252
Train - Epoch 0, Batch: 270, Loss: 0.686978
Train - Epoch 0, Batch: 280, Loss: 0.687340
Train - Epoch 0, Batch: 290, Loss: 0.686733
Train - Epoch 0, Batch: 300, Loss: 0.687553
Train - Epoch 0, Batch: 310, Loss: 0.686682
Train - Epoch 0, Batch: 320, Loss: 0.687205
Train - Epoch 0, Batch: 330, Loss: 0.687442
Train - Epoch 0, Batch: 340, Loss: 0.686699
Train - Epoch 0, Batch: 350, Loss: 0.686758
Train - Epoch 0, Batch: 360, Loss: 0.686884
Train - Epoch 0, Batch: 370, Loss: 0.687472
Train - Epoch 0, Batch: 380, Loss: 0.687023
Train - Epoch 0, Batch: 390, Loss: 0.686764
Train - Epoch 0, Batch: 400, Loss: 0.686812
Train - Epoch 0, Batch: 410, Loss: 0.687088
Train - Epoch 0, Batch: 420, Loss: 0.686571
Train - Epoch 0, Batch: 430, Loss: 0.686620
Train - Epoch 0, Batch: 440, Loss: 0.686809
Train - Epoch 0, Batch: 450, Loss: 0.686897
Train - Epoch 0, Batch: 460, Loss: 0.685498
Train - Epoch 0, Batch: 470, Loss: 0.687985
Train - Epoch 0, Batch: 480, Loss: 0.687191
Train - Epoch 0, Batch: 490, Loss: 0.686139
Train - Epoch 0, Batch: 500, Loss: 0.686216
Train - Epoch 0, Batch: 510, Loss: 0.686273
Train - Epoch 0, Batch: 520, Loss: 0.686171
Train - Epoch 0, Batch: 530, Loss: 0.686139
Train - Epoch 0, Batch: 540, Loss: 0.685922
Train - Epoch 0, Batch: 550, Loss: 0.687251
Train - Epoch 0, Batch: 560, Loss: 0.686277
Train - Epoch 0, Batch: 570, Loss: 0.686271
Train - Epoch 0, Batch: 580, Loss: 0.687127
Train - Epoch 0, Batch: 590, Loss: 0.686675
Train - Epoch 0, Batch: 600, Loss: 0.685585
Train - Epoch 0, Batch: 610, Loss: 0.686262
Train - Epoch 0, Batch: 620, Loss: 0.685451
Train - Epoch 0, Batch: 630, Loss: 0.685557
Train - Epoch 0, Batch: 640, Loss: 0.686471
Train - Epoch 1, Batch: 0, Loss: 0.686935
Train - Epoch 1, Batch: 10, Loss: 0.684735
Train - Epoch 1, Batch: 20, Loss: 0.684805
Train - Epoch 1, Batch: 30, Loss: 0.685739
Train - Epoch 1, Batch: 40, Loss: 0.686281
Train - Epoch 1, Batch: 50, Loss: 0.687214
Train - Epoch 1, Batch: 60, Loss: 0.686809
Train - Epoch 1, Batch: 70, Loss: 0.685312
Train - Epoch 1, Batch: 80, Loss: 0.685439
Train - Epoch 1, Batch: 90, Loss: 0.685386
Train - Epoch 1, Batch: 100, Loss: 0.685655
Train - Epoch 1, Batch: 110, Loss: 0.685989
Train - Epoch 1, Batch: 120, Loss: 0.685664
Train - Epoch 1, Batch: 130, Loss: 0.684980
Train - Epoch 1, Batch: 140, Loss: 0.686650
Train - Epoch 1, Batch: 150, Loss: 0.685123
Train - Epoch 1, Batch: 160, Loss: 0.685621
Train - Epoch 1, Batch: 170, Loss: 0.687241
Train - Epoch 1, Batch: 180, Loss: 0.685830
Train - Epoch 1, Batch: 190, Loss: 0.685058
Train - Epoch 1, Batch: 200, Loss: 0.685087
Train - Epoch 1, Batch: 210, Loss: 0.685370
Train - Epoch 1, Batch: 220, Loss: 0.685987
Train - Epoch 1, Batch: 230, Loss: 0.685967
Train - Epoch 1, Batch: 240, Loss: 0.685873
Train - Epoch 1, Batch: 250, Loss: 0.684408
Train - Epoch 1, Batch: 260, Loss: 0.685261
Train - Epoch 1, Batch: 270, Loss: 0.685016
Train - Epoch 1, Batch: 280, Loss: 0.684824
Train - Epoch 1, Batch: 290, Loss: 0.686734
Train - Epoch 1, Batch: 300, Loss: 0.684934
Train - Epoch 1, Batch: 310, Loss: 0.686960
Train - Epoch 1, Batch: 320, Loss: 0.686023
Train - Epoch 1, Batch: 330, Loss: 0.684305
Train - Epoch 1, Batch: 340, Loss: 0.685800
Train - Epoch 1, Batch: 350, Loss: 0.685206
Train - Epoch 1, Batch: 360, Loss: 0.685501
Train - Epoch 1, Batch: 370, Loss: 0.685152
Train - Epoch 1, Batch: 380, Loss: 0.685686
Train - Epoch 1, Batch: 390, Loss: 0.686210
Train - Epoch 1, Batch: 400, Loss: 0.685466
Train - Epoch 1, Batch: 410, Loss: 0.685268
Train - Epoch 1, Batch: 420, Loss: 0.685447
Train - Epoch 1, Batch: 430, Loss: 0.685518
Train - Epoch 1, Batch: 440, Loss: 0.684961
Train - Epoch 1, Batch: 450, Loss: 0.685590
Train - Epoch 1, Batch: 460, Loss: 0.685430
Train - Epoch 1, Batch: 470, Loss: 0.684591
Train - Epoch 1, Batch: 480, Loss: 0.686544
Train - Epoch 1, Batch: 490, Loss: 0.684194
Train - Epoch 1, Batch: 500, Loss: 0.684972
Train - Epoch 1, Batch: 510, Loss: 0.685611
Train - Epoch 1, Batch: 520, Loss: 0.685298
Train - Epoch 1, Batch: 530, Loss: 0.684298
Train - Epoch 1, Batch: 540, Loss: 0.684365
Train - Epoch 1, Batch: 550, Loss: 0.685219
Train - Epoch 1, Batch: 560, Loss: 0.685308
Train - Epoch 1, Batch: 570, Loss: 0.684507
Train - Epoch 1, Batch: 580, Loss: 0.685331
Train - Epoch 1, Batch: 590, Loss: 0.684676
Train - Epoch 1, Batch: 600, Loss: 0.684265
Train - Epoch 1, Batch: 610, Loss: 0.685449
Train - Epoch 1, Batch: 620, Loss: 0.684846
Train - Epoch 1, Batch: 630, Loss: 0.684650
Train - Epoch 1, Batch: 640, Loss: 0.686319
Train - Epoch 2, Batch: 0, Loss: 0.684059
Train - Epoch 2, Batch: 10, Loss: 0.686357
Train - Epoch 2, Batch: 20, Loss: 0.684261
Train - Epoch 2, Batch: 30, Loss: 0.686168
Train - Epoch 2, Batch: 40, Loss: 0.684811
Train - Epoch 2, Batch: 50, Loss: 0.684267
Train - Epoch 2, Batch: 60, Loss: 0.685030
Train - Epoch 2, Batch: 70, Loss: 0.685197
Train - Epoch 2, Batch: 80, Loss: 0.684763
Train - Epoch 2, Batch: 90, Loss: 0.684639
Train - Epoch 2, Batch: 100, Loss: 0.685966
Train - Epoch 2, Batch: 110, Loss: 0.685031
Train - Epoch 2, Batch: 120, Loss: 0.684701
Train - Epoch 2, Batch: 130, Loss: 0.684311
Train - Epoch 2, Batch: 140, Loss: 0.684927
Train - Epoch 2, Batch: 150, Loss: 0.684531
Train - Epoch 2, Batch: 160, Loss: 0.685632
Train - Epoch 2, Batch: 170, Loss: 0.683873
Train - Epoch 2, Batch: 180, Loss: 0.685672
Train - Epoch 2, Batch: 190, Loss: 0.684134
Train - Epoch 2, Batch: 200, Loss: 0.684431
Train - Epoch 2, Batch: 210, Loss: 0.684265
Train - Epoch 2, Batch: 220, Loss: 0.684229
Train - Epoch 2, Batch: 230, Loss: 0.684968
Train - Epoch 2, Batch: 240, Loss: 0.684330
Train - Epoch 2, Batch: 250, Loss: 0.683589
Train - Epoch 2, Batch: 260, Loss: 0.684241
Train - Epoch 2, Batch: 270, Loss: 0.685446
Train - Epoch 2, Batch: 280, Loss: 0.685610
Train - Epoch 2, Batch: 290, Loss: 0.683840
Train - Epoch 2, Batch: 300, Loss: 0.684886
Train - Epoch 2, Batch: 310, Loss: 0.685012
Train - Epoch 2, Batch: 320, Loss: 0.684220
Train - Epoch 2, Batch: 330, Loss: 0.684523
Train - Epoch 2, Batch: 340, Loss: 0.684156
Train - Epoch 2, Batch: 350, Loss: 0.684785
Train - Epoch 2, Batch: 360, Loss: 0.685531
Train - Epoch 2, Batch: 370, Loss: 0.685280
Train - Epoch 2, Batch: 380, Loss: 0.684080
Train - Epoch 2, Batch: 390, Loss: 0.684648
Train - Epoch 2, Batch: 400, Loss: 0.684835
Train - Epoch 2, Batch: 410, Loss: 0.683174
Train - Epoch 2, Batch: 420, Loss: 0.684718
Train - Epoch 2, Batch: 430, Loss: 0.684096
Train - Epoch 2, Batch: 440, Loss: 0.683944
Train - Epoch 2, Batch: 450, Loss: 0.684529
Train - Epoch 2, Batch: 460, Loss: 0.685764
Train - Epoch 2, Batch: 470, Loss: 0.683178
Train - Epoch 2, Batch: 480, Loss: 0.683464
Train - Epoch 2, Batch: 490, Loss: 0.684686
Train - Epoch 2, Batch: 500, Loss: 0.684270
Train - Epoch 2, Batch: 510, Loss: 0.684206
Train - Epoch 2, Batch: 520, Loss: 0.685011
Train - Epoch 2, Batch: 530, Loss: 0.684490
Train - Epoch 2, Batch: 540, Loss: 0.684435
Train - Epoch 2, Batch: 550, Loss: 0.684376/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683339
Train - Epoch 2, Batch: 570, Loss: 0.683661
Train - Epoch 2, Batch: 580, Loss: 0.684340
Train - Epoch 2, Batch: 590, Loss: 0.685151
Train - Epoch 2, Batch: 600, Loss: 0.683965
Train - Epoch 2, Batch: 610, Loss: 0.684303
Train - Epoch 2, Batch: 620, Loss: 0.683537
Train - Epoch 2, Batch: 630, Loss: 0.683332
Train - Epoch 2, Batch: 640, Loss: 0.684060
Train - Epoch 3, Batch: 0, Loss: 0.683070
Train - Epoch 3, Batch: 10, Loss: 0.683931
Train - Epoch 3, Batch: 20, Loss: 0.685094
Train - Epoch 3, Batch: 30, Loss: 0.683609
Train - Epoch 3, Batch: 40, Loss: 0.683338
Train - Epoch 3, Batch: 50, Loss: 0.684031
Train - Epoch 3, Batch: 60, Loss: 0.683969
Train - Epoch 3, Batch: 70, Loss: 0.682803
Train - Epoch 3, Batch: 80, Loss: 0.684006
Train - Epoch 3, Batch: 90, Loss: 0.684208
Train - Epoch 3, Batch: 100, Loss: 0.684570
Train - Epoch 3, Batch: 110, Loss: 0.683857
Train - Epoch 3, Batch: 120, Loss: 0.683081
Train - Epoch 3, Batch: 130, Loss: 0.683655
Train - Epoch 3, Batch: 140, Loss: 0.684518
Train - Epoch 3, Batch: 150, Loss: 0.683402
Train - Epoch 3, Batch: 160, Loss: 0.683901
Train - Epoch 3, Batch: 170, Loss: 0.683799
Train - Epoch 3, Batch: 180, Loss: 0.683905
Train - Epoch 3, Batch: 190, Loss: 0.684096
Train - Epoch 3, Batch: 200, Loss: 0.683827
Train - Epoch 3, Batch: 210, Loss: 0.683824
Train - Epoch 3, Batch: 220, Loss: 0.684524
Train - Epoch 3, Batch: 230, Loss: 0.682975
Train - Epoch 3, Batch: 240, Loss: 0.683996
Train - Epoch 3, Batch: 250, Loss: 0.683242
Train - Epoch 3, Batch: 260, Loss: 0.685222
Train - Epoch 3, Batch: 270, Loss: 0.683479
Train - Epoch 3, Batch: 280, Loss: 0.683443
Train - Epoch 3, Batch: 290, Loss: 0.684554
Train - Epoch 3, Batch: 300, Loss: 0.683156
Train - Epoch 3, Batch: 310, Loss: 0.682565
Train - Epoch 3, Batch: 320, Loss: 0.682776
Train - Epoch 3, Batch: 330, Loss: 0.683592
Train - Epoch 3, Batch: 340, Loss: 0.682805
Train - Epoch 3, Batch: 350, Loss: 0.684875
Train - Epoch 3, Batch: 360, Loss: 0.683176
Train - Epoch 3, Batch: 370, Loss: 0.685224
Train - Epoch 3, Batch: 380, Loss: 0.684674
Train - Epoch 3, Batch: 390, Loss: 0.683776
Train - Epoch 3, Batch: 400, Loss: 0.683977
Train - Epoch 3, Batch: 410, Loss: 0.684406
Train - Epoch 3, Batch: 420, Loss: 0.684716
Train - Epoch 3, Batch: 430, Loss: 0.683056
Train - Epoch 3, Batch: 440, Loss: 0.683877
Train - Epoch 3, Batch: 450, Loss: 0.683832
Train - Epoch 3, Batch: 460, Loss: 0.683901
Train - Epoch 3, Batch: 470, Loss: 0.684539
Train - Epoch 3, Batch: 480, Loss: 0.684013
Train - Epoch 3, Batch: 490, Loss: 0.685173
Train - Epoch 3, Batch: 500, Loss: 0.683641
Train - Epoch 3, Batch: 510, Loss: 0.684497
Train - Epoch 3, Batch: 520, Loss: 0.684426
Train - Epoch 3, Batch: 530, Loss: 0.684225
Train - Epoch 3, Batch: 540, Loss: 0.683740
Train - Epoch 3, Batch: 550, Loss: 0.683280
Train - Epoch 3, Batch: 560, Loss: 0.683468
Train - Epoch 3, Batch: 570, Loss: 0.683498
Train - Epoch 3, Batch: 580, Loss: 0.683930
Train - Epoch 3, Batch: 590, Loss: 0.683447
Train - Epoch 3, Batch: 600, Loss: 0.683099
Train - Epoch 3, Batch: 610, Loss: 0.684554
Train - Epoch 3, Batch: 620, Loss: 0.683384
Train - Epoch 3, Batch: 630, Loss: 0.681693
Train - Epoch 3, Batch: 640, Loss: 0.683269
training_time:: 7.605715274810791
training time full:: 7.605756998062134
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553810
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 10500
training time is 4.46551251411438
overhead:: 0
overhead2:: 0
time_baseline:: 4.4685869216918945
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.16875410079956055
overhead3:: 0.26309800148010254
overhead4:: 0.8849008083343506
overhead5:: 0
time_provenance:: 2.924143075942993
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553818
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.165543794631958
overhead3:: 0.23983407020568848
overhead4:: 0.9262676239013672
overhead5:: 0
time_provenance:: 2.9424145221710205
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553820
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.21442770957946777
overhead3:: 0.30267858505249023
overhead4:: 1.0744819641113281
overhead5:: 0
time_provenance:: 3.1369400024414062
curr_diff: 0 tensor(9.5844e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5844e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553818
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.202467679977417
overhead3:: 0.2777750492095947
overhead4:: 1.0709865093231201
overhead5:: 0
time_provenance:: 2.9258217811584473
curr_diff: 0 tensor(9.1327e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1327e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553818
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2359609603881836
overhead3:: 0.33229589462280273
overhead4:: 1.3166284561157227
overhead5:: 0
time_provenance:: 3.7099223136901855
curr_diff: 0 tensor(5.4843e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4843e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553830
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.22960472106933594
overhead3:: 0.3200082778930664
overhead4:: 1.3683788776397705
overhead5:: 0
time_provenance:: 3.4748895168304443
curr_diff: 0 tensor(5.3700e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3700e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553826
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2556474208831787
overhead3:: 0.3590226173400879
overhead4:: 1.520991563796997
overhead5:: 0
time_provenance:: 3.649279832839966
curr_diff: 0 tensor(5.2601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553826
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3020212650299072
overhead3:: 0.40379858016967773
overhead4:: 1.642502784729004
overhead5:: 0
time_provenance:: 4.379920721054077
curr_diff: 0 tensor(4.8354e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8354e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553826
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.2945992946624756
overhead3:: 0.40337681770324707
overhead4:: 1.7916042804718018
overhead5:: 0
time_provenance:: 4.132823944091797
curr_diff: 0 tensor(4.6112e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6112e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.31725168228149414
overhead3:: 0.40660715103149414
overhead4:: 1.8033573627471924
overhead5:: 0
time_provenance:: 4.070613622665405
curr_diff: 0 tensor(2.9518e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9518e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.32565999031066895
overhead3:: 0.4471008777618408
overhead4:: 1.8808073997497559
overhead5:: 0
time_provenance:: 4.182775974273682
curr_diff: 0 tensor(3.5366e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5366e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3597700595855713
overhead3:: 0.49597597122192383
overhead4:: 2.0459671020507812
overhead5:: 0
time_provenance:: 4.35801362991333
curr_diff: 0 tensor(4.2252e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2252e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.3758857250213623
overhead3:: 0.5228674411773682
overhead4:: 2.4278481006622314
overhead5:: 0
time_provenance:: 4.88372015953064
curr_diff: 0 tensor(2.2105e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2105e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.42922449111938477
overhead3:: 0.6093308925628662
overhead4:: 2.431952476501465
overhead5:: 0
time_provenance:: 5.010067939758301
curr_diff: 0 tensor(2.1896e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1896e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.4614284038543701
overhead3:: 0.660308837890625
overhead4:: 2.653526782989502
overhead5:: 0
time_provenance:: 5.274473190307617
curr_diff: 0 tensor(2.1146e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1146e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.5651500225067139
overhead3:: 0.7924239635467529
overhead4:: 3.0156452655792236
overhead5:: 0
time_provenance:: 6.3158533573150635
curr_diff: 0 tensor(2.0677e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0677e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 10500
max_epoch:: 4
overhead:: 0
overhead2:: 0.7173385620117188
overhead3:: 0.9682896137237549
overhead4:: 3.45668888092041
overhead5:: 0
time_provenance:: 5.7269275188446045
curr_diff: 0 tensor(2.4865e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4865e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553832
deletion rate:: 0.002
python3 generate_rand_ids 0.002  higgs 0
tensor([7864321, 4554755, 9371652,  ..., 9568245, 7241726,  393215])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.722834
Train - Epoch 0, Batch: 10, Loss: 0.697918
Train - Epoch 0, Batch: 20, Loss: 0.695362
Train - Epoch 0, Batch: 30, Loss: 0.693780
Train - Epoch 0, Batch: 40, Loss: 0.693559
Train - Epoch 0, Batch: 50, Loss: 0.691444
Train - Epoch 0, Batch: 60, Loss: 0.690970
Train - Epoch 0, Batch: 70, Loss: 0.690759
Train - Epoch 0, Batch: 80, Loss: 0.690926
Train - Epoch 0, Batch: 90, Loss: 0.691360
Train - Epoch 0, Batch: 100, Loss: 0.690337
Train - Epoch 0, Batch: 110, Loss: 0.689302
Train - Epoch 0, Batch: 120, Loss: 0.688715
Train - Epoch 0, Batch: 130, Loss: 0.689172
Train - Epoch 0, Batch: 140, Loss: 0.688283
Train - Epoch 0, Batch: 150, Loss: 0.688963
Train - Epoch 0, Batch: 160, Loss: 0.688602
Train - Epoch 0, Batch: 170, Loss: 0.687935
Train - Epoch 0, Batch: 180, Loss: 0.688532
Train - Epoch 0, Batch: 190, Loss: 0.688338
Train - Epoch 0, Batch: 200, Loss: 0.687146
Train - Epoch 0, Batch: 210, Loss: 0.687876
Train - Epoch 0, Batch: 220, Loss: 0.687816
Train - Epoch 0, Batch: 230, Loss: 0.687935
Train - Epoch 0, Batch: 240, Loss: 0.688455
Train - Epoch 0, Batch: 250, Loss: 0.688982
Train - Epoch 0, Batch: 260, Loss: 0.686966
Train - Epoch 0, Batch: 270, Loss: 0.687270
Train - Epoch 0, Batch: 280, Loss: 0.687832
Train - Epoch 0, Batch: 290, Loss: 0.687956
Train - Epoch 0, Batch: 300, Loss: 0.687603
Train - Epoch 0, Batch: 310, Loss: 0.687431
Train - Epoch 0, Batch: 320, Loss: 0.688224
Train - Epoch 0, Batch: 330, Loss: 0.688291
Train - Epoch 0, Batch: 340, Loss: 0.687350
Train - Epoch 0, Batch: 350, Loss: 0.687115
Train - Epoch 0, Batch: 360, Loss: 0.686777
Train - Epoch 0, Batch: 370, Loss: 0.686836
Train - Epoch 0, Batch: 380, Loss: 0.685883
Train - Epoch 0, Batch: 390, Loss: 0.687292
Train - Epoch 0, Batch: 400, Loss: 0.686362
Train - Epoch 0, Batch: 410, Loss: 0.688067
Train - Epoch 0, Batch: 420, Loss: 0.688239
Train - Epoch 0, Batch: 430, Loss: 0.686313
Train - Epoch 0, Batch: 440, Loss: 0.686879
Train - Epoch 0, Batch: 450, Loss: 0.686661
Train - Epoch 0, Batch: 460, Loss: 0.686700
Train - Epoch 0, Batch: 470, Loss: 0.686907
Train - Epoch 0, Batch: 480, Loss: 0.686535
Train - Epoch 0, Batch: 490, Loss: 0.686491
Train - Epoch 0, Batch: 500, Loss: 0.685527
Train - Epoch 0, Batch: 510, Loss: 0.686809
Train - Epoch 0, Batch: 520, Loss: 0.685521
Train - Epoch 0, Batch: 530, Loss: 0.684558
Train - Epoch 0, Batch: 540, Loss: 0.686315
Train - Epoch 0, Batch: 550, Loss: 0.686696
Train - Epoch 0, Batch: 560, Loss: 0.686978
Train - Epoch 0, Batch: 570, Loss: 0.687477
Train - Epoch 0, Batch: 580, Loss: 0.686098
Train - Epoch 0, Batch: 590, Loss: 0.685816
Train - Epoch 0, Batch: 600, Loss: 0.686779
Train - Epoch 0, Batch: 610, Loss: 0.686772
Train - Epoch 0, Batch: 620, Loss: 0.685008
Train - Epoch 0, Batch: 630, Loss: 0.686379
Train - Epoch 0, Batch: 640, Loss: 0.685248
Train - Epoch 1, Batch: 0, Loss: 0.686826
Train - Epoch 1, Batch: 10, Loss: 0.685819
Train - Epoch 1, Batch: 20, Loss: 0.685821
Train - Epoch 1, Batch: 30, Loss: 0.686057
Train - Epoch 1, Batch: 40, Loss: 0.686744
Train - Epoch 1, Batch: 50, Loss: 0.685810
Train - Epoch 1, Batch: 60, Loss: 0.685864
Train - Epoch 1, Batch: 70, Loss: 0.687346
Train - Epoch 1, Batch: 80, Loss: 0.686652
Train - Epoch 1, Batch: 90, Loss: 0.685510
Train - Epoch 1, Batch: 100, Loss: 0.686450
Train - Epoch 1, Batch: 110, Loss: 0.686224
Train - Epoch 1, Batch: 120, Loss: 0.686557
Train - Epoch 1, Batch: 130, Loss: 0.686947
Train - Epoch 1, Batch: 140, Loss: 0.686001
Train - Epoch 1, Batch: 150, Loss: 0.686895
Train - Epoch 1, Batch: 160, Loss: 0.685689
Train - Epoch 1, Batch: 170, Loss: 0.684903
Train - Epoch 1, Batch: 180, Loss: 0.686366
Train - Epoch 1, Batch: 190, Loss: 0.686023
Train - Epoch 1, Batch: 200, Loss: 0.686260
Train - Epoch 1, Batch: 210, Loss: 0.685101
Train - Epoch 1, Batch: 220, Loss: 0.686058
Train - Epoch 1, Batch: 230, Loss: 0.684955
Train - Epoch 1, Batch: 240, Loss: 0.685592
Train - Epoch 1, Batch: 250, Loss: 0.685780
Train - Epoch 1, Batch: 260, Loss: 0.686304
Train - Epoch 1, Batch: 270, Loss: 0.685764
Train - Epoch 1, Batch: 280, Loss: 0.686136
Train - Epoch 1, Batch: 290, Loss: 0.685915
Train - Epoch 1, Batch: 300, Loss: 0.686236
Train - Epoch 1, Batch: 310, Loss: 0.686222
Train - Epoch 1, Batch: 320, Loss: 0.685308
Train - Epoch 1, Batch: 330, Loss: 0.686633
Train - Epoch 1, Batch: 340, Loss: 0.685988
Train - Epoch 1, Batch: 350, Loss: 0.686418
Train - Epoch 1, Batch: 360, Loss: 0.684849
Train - Epoch 1, Batch: 370, Loss: 0.684995
Train - Epoch 1, Batch: 380, Loss: 0.685264
Train - Epoch 1, Batch: 390, Loss: 0.686100
Train - Epoch 1, Batch: 400, Loss: 0.686214
Train - Epoch 1, Batch: 410, Loss: 0.684740
Train - Epoch 1, Batch: 420, Loss: 0.685219
Train - Epoch 1, Batch: 430, Loss: 0.684897
Train - Epoch 1, Batch: 440, Loss: 0.684429
Train - Epoch 1, Batch: 450, Loss: 0.685405
Train - Epoch 1, Batch: 460, Loss: 0.684991
Train - Epoch 1, Batch: 470, Loss: 0.685947
Train - Epoch 1, Batch: 480, Loss: 0.685507
Train - Epoch 1, Batch: 490, Loss: 0.685149
Train - Epoch 1, Batch: 500, Loss: 0.684364
Train - Epoch 1, Batch: 510, Loss: 0.686724
Train - Epoch 1, Batch: 520, Loss: 0.685194
Train - Epoch 1, Batch: 530, Loss: 0.684750
Train - Epoch 1, Batch: 540, Loss: 0.684941
Train - Epoch 1, Batch: 550, Loss: 0.685752
Train - Epoch 1, Batch: 560, Loss: 0.684313
Train - Epoch 1, Batch: 570, Loss: 0.684183
Train - Epoch 1, Batch: 580, Loss: 0.685027
Train - Epoch 1, Batch: 590, Loss: 0.686130
Train - Epoch 1, Batch: 600, Loss: 0.685849
Train - Epoch 1, Batch: 610, Loss: 0.685056
Train - Epoch 1, Batch: 620, Loss: 0.685319
Train - Epoch 1, Batch: 630, Loss: 0.684999
Train - Epoch 1, Batch: 640, Loss: 0.685301
Train - Epoch 2, Batch: 0, Loss: 0.684870
Train - Epoch 2, Batch: 10, Loss: 0.684147
Train - Epoch 2, Batch: 20, Loss: 0.685609
Train - Epoch 2, Batch: 30, Loss: 0.685397
Train - Epoch 2, Batch: 40, Loss: 0.682939
Train - Epoch 2, Batch: 50, Loss: 0.685093
Train - Epoch 2, Batch: 60, Loss: 0.684554
Train - Epoch 2, Batch: 70, Loss: 0.684657
Train - Epoch 2, Batch: 80, Loss: 0.684970
Train - Epoch 2, Batch: 90, Loss: 0.685541
Train - Epoch 2, Batch: 100, Loss: 0.685893
Train - Epoch 2, Batch: 110, Loss: 0.685557
Train - Epoch 2, Batch: 120, Loss: 0.685377
Train - Epoch 2, Batch: 130, Loss: 0.684431
Train - Epoch 2, Batch: 140, Loss: 0.685159
Train - Epoch 2, Batch: 150, Loss: 0.685379
Train - Epoch 2, Batch: 160, Loss: 0.684292
Train - Epoch 2, Batch: 170, Loss: 0.684565
Train - Epoch 2, Batch: 180, Loss: 0.685160
Train - Epoch 2, Batch: 190, Loss: 0.684535
Train - Epoch 2, Batch: 200, Loss: 0.683925
Train - Epoch 2, Batch: 210, Loss: 0.685208
Train - Epoch 2, Batch: 220, Loss: 0.684664
Train - Epoch 2, Batch: 230, Loss: 0.684361
Train - Epoch 2, Batch: 240, Loss: 0.685346
Train - Epoch 2, Batch: 250, Loss: 0.685581
Train - Epoch 2, Batch: 260, Loss: 0.685385
Train - Epoch 2, Batch: 270, Loss: 0.684333
Train - Epoch 2, Batch: 280, Loss: 0.685564
Train - Epoch 2, Batch: 290, Loss: 0.685172
Train - Epoch 2, Batch: 300, Loss: 0.685872
Train - Epoch 2, Batch: 310, Loss: 0.685848
Train - Epoch 2, Batch: 320, Loss: 0.683957
Train - Epoch 2, Batch: 330, Loss: 0.684711
Train - Epoch 2, Batch: 340, Loss: 0.684196
Train - Epoch 2, Batch: 350, Loss: 0.683400
Train - Epoch 2, Batch: 360, Loss: 0.685078
Train - Epoch 2, Batch: 370, Loss: 0.684252
Train - Epoch 2, Batch: 380, Loss: 0.684402
Train - Epoch 2, Batch: 390, Loss: 0.685052
Train - Epoch 2, Batch: 400, Loss: 0.684916
Train - Epoch 2, Batch: 410, Loss: 0.684567
Train - Epoch 2, Batch: 420, Loss: 0.685376
Train - Epoch 2, Batch: 430, Loss: 0.684253
Train - Epoch 2, Batch: 440, Loss: 0.684275
Train - Epoch 2, Batch: 450, Loss: 0.684450
Train - Epoch 2, Batch: 460, Loss: 0.684406
Train - Epoch 2, Batch: 470, Loss: 0.684234
Train - Epoch 2, Batch: 480, Loss: 0.685207
Train - Epoch 2, Batch: 490, Loss: 0.685452
Train - Epoch 2, Batch: 500, Loss: 0.683972
Train - Epoch 2, Batch: 510, Loss: 0.685087
Train - Epoch 2, Batch: 520, Loss: 0.684112
Train - Epoch 2, Batch: 530, Loss: 0.682767
Train - Epoch 2, Batch: 540, Loss: 0.684110
Train - Epoch 2, Batch: 550, Loss: 0.684303/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685128
Train - Epoch 2, Batch: 570, Loss: 0.683470
Train - Epoch 2, Batch: 580, Loss: 0.684336
Train - Epoch 2, Batch: 590, Loss: 0.684683
Train - Epoch 2, Batch: 600, Loss: 0.684567
Train - Epoch 2, Batch: 610, Loss: 0.685054
Train - Epoch 2, Batch: 620, Loss: 0.684709
Train - Epoch 2, Batch: 630, Loss: 0.684122
Train - Epoch 2, Batch: 640, Loss: 0.683973
Train - Epoch 3, Batch: 0, Loss: 0.683979
Train - Epoch 3, Batch: 10, Loss: 0.684399
Train - Epoch 3, Batch: 20, Loss: 0.684971
Train - Epoch 3, Batch: 30, Loss: 0.683382
Train - Epoch 3, Batch: 40, Loss: 0.683374
Train - Epoch 3, Batch: 50, Loss: 0.684817
Train - Epoch 3, Batch: 60, Loss: 0.685572
Train - Epoch 3, Batch: 70, Loss: 0.684660
Train - Epoch 3, Batch: 80, Loss: 0.683719
Train - Epoch 3, Batch: 90, Loss: 0.684076
Train - Epoch 3, Batch: 100, Loss: 0.683806
Train - Epoch 3, Batch: 110, Loss: 0.682498
Train - Epoch 3, Batch: 120, Loss: 0.683734
Train - Epoch 3, Batch: 130, Loss: 0.684149
Train - Epoch 3, Batch: 140, Loss: 0.685304
Train - Epoch 3, Batch: 150, Loss: 0.684222
Train - Epoch 3, Batch: 160, Loss: 0.684489
Train - Epoch 3, Batch: 170, Loss: 0.683738
Train - Epoch 3, Batch: 180, Loss: 0.684724
Train - Epoch 3, Batch: 190, Loss: 0.683436
Train - Epoch 3, Batch: 200, Loss: 0.683821
Train - Epoch 3, Batch: 210, Loss: 0.683944
Train - Epoch 3, Batch: 220, Loss: 0.684729
Train - Epoch 3, Batch: 230, Loss: 0.683848
Train - Epoch 3, Batch: 240, Loss: 0.683761
Train - Epoch 3, Batch: 250, Loss: 0.683518
Train - Epoch 3, Batch: 260, Loss: 0.682781
Train - Epoch 3, Batch: 270, Loss: 0.684293
Train - Epoch 3, Batch: 280, Loss: 0.683654
Train - Epoch 3, Batch: 290, Loss: 0.684161
Train - Epoch 3, Batch: 300, Loss: 0.682807
Train - Epoch 3, Batch: 310, Loss: 0.684326
Train - Epoch 3, Batch: 320, Loss: 0.683989
Train - Epoch 3, Batch: 330, Loss: 0.683441
Train - Epoch 3, Batch: 340, Loss: 0.684103
Train - Epoch 3, Batch: 350, Loss: 0.684016
Train - Epoch 3, Batch: 360, Loss: 0.684089
Train - Epoch 3, Batch: 370, Loss: 0.682650
Train - Epoch 3, Batch: 380, Loss: 0.684537
Train - Epoch 3, Batch: 390, Loss: 0.683539
Train - Epoch 3, Batch: 400, Loss: 0.685506
Train - Epoch 3, Batch: 410, Loss: 0.684461
Train - Epoch 3, Batch: 420, Loss: 0.683490
Train - Epoch 3, Batch: 430, Loss: 0.683493
Train - Epoch 3, Batch: 440, Loss: 0.683068
Train - Epoch 3, Batch: 450, Loss: 0.684032
Train - Epoch 3, Batch: 460, Loss: 0.684348
Train - Epoch 3, Batch: 470, Loss: 0.685387
Train - Epoch 3, Batch: 480, Loss: 0.682932
Train - Epoch 3, Batch: 490, Loss: 0.684204
Train - Epoch 3, Batch: 500, Loss: 0.683657
Train - Epoch 3, Batch: 510, Loss: 0.683857
Train - Epoch 3, Batch: 520, Loss: 0.683635
Train - Epoch 3, Batch: 530, Loss: 0.683118
Train - Epoch 3, Batch: 540, Loss: 0.682620
Train - Epoch 3, Batch: 550, Loss: 0.684652
Train - Epoch 3, Batch: 560, Loss: 0.684283
Train - Epoch 3, Batch: 570, Loss: 0.684658
Train - Epoch 3, Batch: 580, Loss: 0.683352
Train - Epoch 3, Batch: 590, Loss: 0.683277
Train - Epoch 3, Batch: 600, Loss: 0.683261
Train - Epoch 3, Batch: 610, Loss: 0.684493
Train - Epoch 3, Batch: 620, Loss: 0.683389
Train - Epoch 3, Batch: 630, Loss: 0.684095
Train - Epoch 3, Batch: 640, Loss: 0.682887
training_time:: 7.640156984329224
training time full:: 7.640197277069092
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553376
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 21000
training time is 5.050114393234253
overhead:: 0
overhead2:: 0
time_baseline:: 5.053223371505737
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.16453123092651367
overhead3:: 0.2500171661376953
overhead4:: 0.8507413864135742
overhead5:: 0
time_provenance:: 2.8877203464508057
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1779320240020752
overhead3:: 0.22676634788513184
overhead4:: 0.9174132347106934
overhead5:: 0
time_provenance:: 2.9657511711120605
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553388
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2252655029296875
overhead3:: 0.3304438591003418
overhead4:: 1.1428730487823486
overhead5:: 0
time_provenance:: 3.283048629760742
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553388
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2255842685699463
overhead3:: 0.2911257743835449
overhead4:: 1.2320313453674316
overhead5:: 0
time_provenance:: 3.3513832092285156
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553388
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.21931123733520508
overhead3:: 0.2858855724334717
overhead4:: 1.190842866897583
overhead5:: 0
time_provenance:: 3.3604209423065186
curr_diff: 0 tensor(9.1992e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1992e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553394
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2646324634552002
overhead3:: 0.38569068908691406
overhead4:: 1.4003875255584717
overhead5:: 0
time_provenance:: 3.992602586746216
curr_diff: 0 tensor(9.0653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0653e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553394
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2755436897277832
overhead3:: 0.3812563419342041
overhead4:: 1.5294973850250244
overhead5:: 0
time_provenance:: 4.194387912750244
curr_diff: 0 tensor(8.9114e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9114e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553394
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.27414464950561523
overhead3:: 0.3598940372467041
overhead4:: 1.5135107040405273
overhead5:: 0
time_provenance:: 3.637629270553589
curr_diff: 0 tensor(8.5821e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5821e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553394
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2999098300933838
overhead3:: 0.4227864742279053
overhead4:: 1.7906200885772705
overhead5:: 0
time_provenance:: 4.118074655532837
curr_diff: 0 tensor(5.7737e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7737e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553392
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3697984218597412
overhead3:: 0.5568993091583252
overhead4:: 1.9237215518951416
overhead5:: 0
time_provenance:: 4.478647708892822
curr_diff: 0 tensor(5.6027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553396
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.33505940437316895
overhead3:: 0.44226932525634766
overhead4:: 1.9048399925231934
overhead5:: 0
time_provenance:: 4.227459192276001
curr_diff: 0 tensor(3.6826e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6826e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553388
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4031233787536621
overhead3:: 0.609987735748291
overhead4:: 2.134326696395874
overhead5:: 0
time_provenance:: 4.6949782371521
curr_diff: 0 tensor(5.4765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4765e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553390
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4000113010406494
overhead3:: 0.5630123615264893
overhead4:: 2.4787135124206543
overhead5:: 0
time_provenance:: 5.027780294418335
curr_diff: 0 tensor(2.8418e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8418e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4307727813720703
overhead3:: 0.5845551490783691
overhead4:: 2.567294120788574
overhead5:: 0
time_provenance:: 5.1860105991363525
curr_diff: 0 tensor(2.7801e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7801e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4461076259613037
overhead3:: 0.6683340072631836
overhead4:: 2.5954322814941406
overhead5:: 0
time_provenance:: 5.219440937042236
curr_diff: 0 tensor(2.7167e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7167e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.438138484954834
overhead3:: 0.6043102741241455
overhead4:: 2.4722156524658203
overhead5:: 0
time_provenance:: 4.968575954437256
curr_diff: 0 tensor(2.6161e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6161e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.776597261428833
overhead3:: 1.1034202575683594
overhead4:: 3.7291524410247803
overhead5:: 0
time_provenance:: 6.224842309951782
curr_diff: 0 tensor(2.4287e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4287e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553386
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.701918
Train - Epoch 0, Batch: 10, Loss: 0.692115
Train - Epoch 0, Batch: 20, Loss: 0.689961
Train - Epoch 0, Batch: 30, Loss: 0.692234
Train - Epoch 0, Batch: 40, Loss: 0.691187
Train - Epoch 0, Batch: 50, Loss: 0.689926
Train - Epoch 0, Batch: 60, Loss: 0.689761
Train - Epoch 0, Batch: 70, Loss: 0.689109
Train - Epoch 0, Batch: 80, Loss: 0.689866
Train - Epoch 0, Batch: 90, Loss: 0.690262
Train - Epoch 0, Batch: 100, Loss: 0.689551
Train - Epoch 0, Batch: 110, Loss: 0.688683
Train - Epoch 0, Batch: 120, Loss: 0.689545
Train - Epoch 0, Batch: 130, Loss: 0.688037
Train - Epoch 0, Batch: 140, Loss: 0.688705
Train - Epoch 0, Batch: 150, Loss: 0.687184
Train - Epoch 0, Batch: 160, Loss: 0.688622
Train - Epoch 0, Batch: 170, Loss: 0.688739
Train - Epoch 0, Batch: 180, Loss: 0.689424
Train - Epoch 0, Batch: 190, Loss: 0.688210
Train - Epoch 0, Batch: 200, Loss: 0.688527
Train - Epoch 0, Batch: 210, Loss: 0.689011
Train - Epoch 0, Batch: 220, Loss: 0.687650
Train - Epoch 0, Batch: 230, Loss: 0.688211
Train - Epoch 0, Batch: 240, Loss: 0.687592
Train - Epoch 0, Batch: 250, Loss: 0.687216
Train - Epoch 0, Batch: 260, Loss: 0.687796
Train - Epoch 0, Batch: 270, Loss: 0.687922
Train - Epoch 0, Batch: 280, Loss: 0.687717
Train - Epoch 0, Batch: 290, Loss: 0.687196
Train - Epoch 0, Batch: 300, Loss: 0.687200
Train - Epoch 0, Batch: 310, Loss: 0.686504
Train - Epoch 0, Batch: 320, Loss: 0.687285
Train - Epoch 0, Batch: 330, Loss: 0.687041
Train - Epoch 0, Batch: 340, Loss: 0.688633
Train - Epoch 0, Batch: 350, Loss: 0.687408
Train - Epoch 0, Batch: 360, Loss: 0.687024
Train - Epoch 0, Batch: 370, Loss: 0.687151
Train - Epoch 0, Batch: 380, Loss: 0.688117
Train - Epoch 0, Batch: 390, Loss: 0.686228
Train - Epoch 0, Batch: 400, Loss: 0.686733
Train - Epoch 0, Batch: 410, Loss: 0.687083
Train - Epoch 0, Batch: 420, Loss: 0.686334
Train - Epoch 0, Batch: 430, Loss: 0.686266
Train - Epoch 0, Batch: 440, Loss: 0.687143
Train - Epoch 0, Batch: 450, Loss: 0.686966
Train - Epoch 0, Batch: 460, Loss: 0.687325
Train - Epoch 0, Batch: 470, Loss: 0.687771
Train - Epoch 0, Batch: 480, Loss: 0.687359
Train - Epoch 0, Batch: 490, Loss: 0.686552
Train - Epoch 0, Batch: 500, Loss: 0.685608
Train - Epoch 0, Batch: 510, Loss: 0.685643
Train - Epoch 0, Batch: 520, Loss: 0.688516
Train - Epoch 0, Batch: 530, Loss: 0.687235
Train - Epoch 0, Batch: 540, Loss: 0.686027
Train - Epoch 0, Batch: 550, Loss: 0.686268
Train - Epoch 0, Batch: 560, Loss: 0.686060
Train - Epoch 0, Batch: 570, Loss: 0.685811
Train - Epoch 0, Batch: 580, Loss: 0.685950
Train - Epoch 0, Batch: 590, Loss: 0.686698
Train - Epoch 0, Batch: 600, Loss: 0.686896
Train - Epoch 0, Batch: 610, Loss: 0.686554
Train - Epoch 0, Batch: 620, Loss: 0.685800
Train - Epoch 0, Batch: 630, Loss: 0.686480
Train - Epoch 0, Batch: 640, Loss: 0.686086
Train - Epoch 1, Batch: 0, Loss: 0.686088
Train - Epoch 1, Batch: 10, Loss: 0.686132
Train - Epoch 1, Batch: 20, Loss: 0.685338
Train - Epoch 1, Batch: 30, Loss: 0.685692
Train - Epoch 1, Batch: 40, Loss: 0.685114
Train - Epoch 1, Batch: 50, Loss: 0.685379
Train - Epoch 1, Batch: 60, Loss: 0.685791
Train - Epoch 1, Batch: 70, Loss: 0.686509
Train - Epoch 1, Batch: 80, Loss: 0.686032
Train - Epoch 1, Batch: 90, Loss: 0.686793
Train - Epoch 1, Batch: 100, Loss: 0.685970
Train - Epoch 1, Batch: 110, Loss: 0.686077
Train - Epoch 1, Batch: 120, Loss: 0.686828
Train - Epoch 1, Batch: 130, Loss: 0.685713
Train - Epoch 1, Batch: 140, Loss: 0.686024
Train - Epoch 1, Batch: 150, Loss: 0.686327
Train - Epoch 1, Batch: 160, Loss: 0.684710
Train - Epoch 1, Batch: 170, Loss: 0.685707
Train - Epoch 1, Batch: 180, Loss: 0.685488
Train - Epoch 1, Batch: 190, Loss: 0.684921
Train - Epoch 1, Batch: 200, Loss: 0.685253
Train - Epoch 1, Batch: 210, Loss: 0.685318
Train - Epoch 1, Batch: 220, Loss: 0.686197
Train - Epoch 1, Batch: 230, Loss: 0.686120
Train - Epoch 1, Batch: 240, Loss: 0.685390
Train - Epoch 1, Batch: 250, Loss: 0.686135
Train - Epoch 1, Batch: 260, Loss: 0.685567
Train - Epoch 1, Batch: 270, Loss: 0.686176
Train - Epoch 1, Batch: 280, Loss: 0.686213
Train - Epoch 1, Batch: 290, Loss: 0.685541
Train - Epoch 1, Batch: 300, Loss: 0.685460
Train - Epoch 1, Batch: 310, Loss: 0.684195
Train - Epoch 1, Batch: 320, Loss: 0.685260
Train - Epoch 1, Batch: 330, Loss: 0.684575
Train - Epoch 1, Batch: 340, Loss: 0.685468
Train - Epoch 1, Batch: 350, Loss: 0.686521
Train - Epoch 1, Batch: 360, Loss: 0.686056
Train - Epoch 1, Batch: 370, Loss: 0.685831
Train - Epoch 1, Batch: 380, Loss: 0.685593
Train - Epoch 1, Batch: 390, Loss: 0.685607
Train - Epoch 1, Batch: 400, Loss: 0.685972
Train - Epoch 1, Batch: 410, Loss: 0.685228
Train - Epoch 1, Batch: 420, Loss: 0.684555
Train - Epoch 1, Batch: 430, Loss: 0.685457
Train - Epoch 1, Batch: 440, Loss: 0.686078
Train - Epoch 1, Batch: 450, Loss: 0.685119
Train - Epoch 1, Batch: 460, Loss: 0.686254
Train - Epoch 1, Batch: 470, Loss: 0.684458
Train - Epoch 1, Batch: 480, Loss: 0.684996
Train - Epoch 1, Batch: 490, Loss: 0.683267
Train - Epoch 1, Batch: 500, Loss: 0.684453
Train - Epoch 1, Batch: 510, Loss: 0.684653
Train - Epoch 1, Batch: 520, Loss: 0.685542
Train - Epoch 1, Batch: 530, Loss: 0.684665
Train - Epoch 1, Batch: 540, Loss: 0.686346
Train - Epoch 1, Batch: 550, Loss: 0.684806
Train - Epoch 1, Batch: 560, Loss: 0.686285
Train - Epoch 1, Batch: 570, Loss: 0.685496
Train - Epoch 1, Batch: 580, Loss: 0.685723
Train - Epoch 1, Batch: 590, Loss: 0.684755
Train - Epoch 1, Batch: 600, Loss: 0.684299
Train - Epoch 1, Batch: 610, Loss: 0.685048
Train - Epoch 1, Batch: 620, Loss: 0.685179
Train - Epoch 1, Batch: 630, Loss: 0.684412
Train - Epoch 1, Batch: 640, Loss: 0.685149
Train - Epoch 2, Batch: 0, Loss: 0.684723
Train - Epoch 2, Batch: 10, Loss: 0.684790
Train - Epoch 2, Batch: 20, Loss: 0.684716
Train - Epoch 2, Batch: 30, Loss: 0.685100
Train - Epoch 2, Batch: 40, Loss: 0.684955
Train - Epoch 2, Batch: 50, Loss: 0.684946
Train - Epoch 2, Batch: 60, Loss: 0.684788
Train - Epoch 2, Batch: 70, Loss: 0.684768
Train - Epoch 2, Batch: 80, Loss: 0.684644
Train - Epoch 2, Batch: 90, Loss: 0.685875
Train - Epoch 2, Batch: 100, Loss: 0.685238
Train - Epoch 2, Batch: 110, Loss: 0.684955
Train - Epoch 2, Batch: 120, Loss: 0.684287
Train - Epoch 2, Batch: 130, Loss: 0.684925
Train - Epoch 2, Batch: 140, Loss: 0.685009
Train - Epoch 2, Batch: 150, Loss: 0.683805
Train - Epoch 2, Batch: 160, Loss: 0.684741
Train - Epoch 2, Batch: 170, Loss: 0.684044
Train - Epoch 2, Batch: 180, Loss: 0.684933
Train - Epoch 2, Batch: 190, Loss: 0.685689
Train - Epoch 2, Batch: 200, Loss: 0.685183
Train - Epoch 2, Batch: 210, Loss: 0.684787
Train - Epoch 2, Batch: 220, Loss: 0.684048
Train - Epoch 2, Batch: 230, Loss: 0.685920
Train - Epoch 2, Batch: 240, Loss: 0.684106
Train - Epoch 2, Batch: 250, Loss: 0.684521
Train - Epoch 2, Batch: 260, Loss: 0.685784
Train - Epoch 2, Batch: 270, Loss: 0.684648
Train - Epoch 2, Batch: 280, Loss: 0.683977
Train - Epoch 2, Batch: 290, Loss: 0.685425
Train - Epoch 2, Batch: 300, Loss: 0.685107
Train - Epoch 2, Batch: 310, Loss: 0.683197
Train - Epoch 2, Batch: 320, Loss: 0.684905
Train - Epoch 2, Batch: 330, Loss: 0.684175
Train - Epoch 2, Batch: 340, Loss: 0.683761
Train - Epoch 2, Batch: 350, Loss: 0.685296
Train - Epoch 2, Batch: 360, Loss: 0.683858
Train - Epoch 2, Batch: 370, Loss: 0.683586
Train - Epoch 2, Batch: 380, Loss: 0.683077
Train - Epoch 2, Batch: 390, Loss: 0.684333
Train - Epoch 2, Batch: 400, Loss: 0.683648
Train - Epoch 2, Batch: 410, Loss: 0.685267
Train - Epoch 2, Batch: 420, Loss: 0.684729
Train - Epoch 2, Batch: 430, Loss: 0.684717
Train - Epoch 2, Batch: 440, Loss: 0.683753
Train - Epoch 2, Batch: 450, Loss: 0.685320
Train - Epoch 2, Batch: 460, Loss: 0.684565
Train - Epoch 2, Batch: 470, Loss: 0.685498
Train - Epoch 2, Batch: 480, Loss: 0.684338
Train - Epoch 2, Batch: 490, Loss: 0.684118
Train - Epoch 2, Batch: 500, Loss: 0.683900
Train - Epoch 2, Batch: 510, Loss: 0.683634
Train - Epoch 2, Batch: 520, Loss: 0.684079
Train - Epoch 2, Batch: 530, Loss: 0.684814
Train - Epoch 2, Batch: 540, Loss: 0.685258
Train - Epoch 2, Batch: 550, Loss: 0.684783/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.682812
Train - Epoch 2, Batch: 570, Loss: 0.684142
Train - Epoch 2, Batch: 580, Loss: 0.685181
Train - Epoch 2, Batch: 590, Loss: 0.684538
Train - Epoch 2, Batch: 600, Loss: 0.684555
Train - Epoch 2, Batch: 610, Loss: 0.684131
Train - Epoch 2, Batch: 620, Loss: 0.683372
Train - Epoch 2, Batch: 630, Loss: 0.683772
Train - Epoch 2, Batch: 640, Loss: 0.683741
Train - Epoch 3, Batch: 0, Loss: 0.682857
Train - Epoch 3, Batch: 10, Loss: 0.684388
Train - Epoch 3, Batch: 20, Loss: 0.684866
Train - Epoch 3, Batch: 30, Loss: 0.683314
Train - Epoch 3, Batch: 40, Loss: 0.684340
Train - Epoch 3, Batch: 50, Loss: 0.684996
Train - Epoch 3, Batch: 60, Loss: 0.685630
Train - Epoch 3, Batch: 70, Loss: 0.684499
Train - Epoch 3, Batch: 80, Loss: 0.683778
Train - Epoch 3, Batch: 90, Loss: 0.683382
Train - Epoch 3, Batch: 100, Loss: 0.684657
Train - Epoch 3, Batch: 110, Loss: 0.685533
Train - Epoch 3, Batch: 120, Loss: 0.683892
Train - Epoch 3, Batch: 130, Loss: 0.684349
Train - Epoch 3, Batch: 140, Loss: 0.685006
Train - Epoch 3, Batch: 150, Loss: 0.684385
Train - Epoch 3, Batch: 160, Loss: 0.683010
Train - Epoch 3, Batch: 170, Loss: 0.684370
Train - Epoch 3, Batch: 180, Loss: 0.683083
Train - Epoch 3, Batch: 190, Loss: 0.684817
Train - Epoch 3, Batch: 200, Loss: 0.685146
Train - Epoch 3, Batch: 210, Loss: 0.684284
Train - Epoch 3, Batch: 220, Loss: 0.684620
Train - Epoch 3, Batch: 230, Loss: 0.683644
Train - Epoch 3, Batch: 240, Loss: 0.683700
Train - Epoch 3, Batch: 250, Loss: 0.685458
Train - Epoch 3, Batch: 260, Loss: 0.684569
Train - Epoch 3, Batch: 270, Loss: 0.683236
Train - Epoch 3, Batch: 280, Loss: 0.684588
Train - Epoch 3, Batch: 290, Loss: 0.682405
Train - Epoch 3, Batch: 300, Loss: 0.684631
Train - Epoch 3, Batch: 310, Loss: 0.683192
Train - Epoch 3, Batch: 320, Loss: 0.683150
Train - Epoch 3, Batch: 330, Loss: 0.682867
Train - Epoch 3, Batch: 340, Loss: 0.684378
Train - Epoch 3, Batch: 350, Loss: 0.682501
Train - Epoch 3, Batch: 360, Loss: 0.684361
Train - Epoch 3, Batch: 370, Loss: 0.684213
Train - Epoch 3, Batch: 380, Loss: 0.683206
Train - Epoch 3, Batch: 390, Loss: 0.685194
Train - Epoch 3, Batch: 400, Loss: 0.683397
Train - Epoch 3, Batch: 410, Loss: 0.684216
Train - Epoch 3, Batch: 420, Loss: 0.684907
Train - Epoch 3, Batch: 430, Loss: 0.684195
Train - Epoch 3, Batch: 440, Loss: 0.683226
Train - Epoch 3, Batch: 450, Loss: 0.683444
Train - Epoch 3, Batch: 460, Loss: 0.683667
Train - Epoch 3, Batch: 470, Loss: 0.684339
Train - Epoch 3, Batch: 480, Loss: 0.684447
Train - Epoch 3, Batch: 490, Loss: 0.684337
Train - Epoch 3, Batch: 500, Loss: 0.683457
Train - Epoch 3, Batch: 510, Loss: 0.683045
Train - Epoch 3, Batch: 520, Loss: 0.684484
Train - Epoch 3, Batch: 530, Loss: 0.683188
Train - Epoch 3, Batch: 540, Loss: 0.684101
Train - Epoch 3, Batch: 550, Loss: 0.683601
Train - Epoch 3, Batch: 560, Loss: 0.683918
Train - Epoch 3, Batch: 570, Loss: 0.684201
Train - Epoch 3, Batch: 580, Loss: 0.684469
Train - Epoch 3, Batch: 590, Loss: 0.683758
Train - Epoch 3, Batch: 600, Loss: 0.683800
Train - Epoch 3, Batch: 610, Loss: 0.682843
Train - Epoch 3, Batch: 620, Loss: 0.682928
Train - Epoch 3, Batch: 630, Loss: 0.684622
Train - Epoch 3, Batch: 640, Loss: 0.685120
training_time:: 7.79366660118103
training time full:: 7.7937092781066895
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554110
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 21000
training time is 5.201890707015991
overhead:: 0
overhead2:: 0
time_baseline:: 5.2052812576293945
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554144
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.16069555282592773
overhead3:: 0.21967244148254395
overhead4:: 0.8310143947601318
overhead5:: 0
time_provenance:: 2.941689968109131
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554154
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.17173051834106445
overhead3:: 0.22916531562805176
overhead4:: 0.9431760311126709
overhead5:: 0
time_provenance:: 2.9355928897857666
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554154
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24995160102844238
overhead3:: 0.3775656223297119
overhead4:: 1.1463611125946045
overhead5:: 0
time_provenance:: 3.3659000396728516
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554154
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24763178825378418
overhead3:: 0.348787784576416
overhead4:: 1.2210588455200195
overhead5:: 0
time_provenance:: 3.4529576301574707
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554154
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.20506787300109863
overhead3:: 0.2730987071990967
overhead4:: 1.2295215129852295
overhead5:: 0
time_provenance:: 3.3124213218688965
curr_diff: 0 tensor(8.7759e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7759e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554158
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.27867889404296875
overhead3:: 0.40488457679748535
overhead4:: 1.4512860774993896
overhead5:: 0
time_provenance:: 3.7571425437927246
curr_diff: 0 tensor(8.7881e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7881e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554158
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2721235752105713
overhead3:: 0.36825132369995117
overhead4:: 1.450998067855835
overhead5:: 0
time_provenance:: 3.632540464401245
curr_diff: 0 tensor(8.7603e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7603e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554158
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3103187084197998
overhead3:: 0.413100004196167
overhead4:: 1.623091220855713
overhead5:: 0
time_provenance:: 4.105488538742065
curr_diff: 0 tensor(8.6419e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6419e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554156
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.29709625244140625
overhead3:: 0.40802884101867676
overhead4:: 1.7299931049346924
overhead5:: 0
time_provenance:: 4.033076047897339
curr_diff: 0 tensor(5.5403e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5403e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554162
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3562192916870117
overhead3:: 0.5463228225708008
overhead4:: 1.9327919483184814
overhead5:: 0
time_provenance:: 4.436095476150513
curr_diff: 0 tensor(5.0313e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0313e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554146
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.34879398345947266
overhead3:: 0.4780895709991455
overhead4:: 1.851818561553955
overhead5:: 0
time_provenance:: 4.253170728683472
curr_diff: 0 tensor(4.5269e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5269e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554146
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.37558841705322266
overhead3:: 0.49312472343444824
overhead4:: 1.8867249488830566
overhead5:: 0
time_provenance:: 4.410590648651123
curr_diff: 0 tensor(5.5171e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5171e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554162
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3909907341003418
overhead3:: 0.5517544746398926
overhead4:: 2.4784417152404785
overhead5:: 0
time_provenance:: 5.00763463973999
curr_diff: 0 tensor(3.0468e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0468e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554152
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4131612777709961
overhead3:: 0.5750095844268799
overhead4:: 2.4962196350097656
overhead5:: 0
time_provenance:: 5.01877236366272
curr_diff: 0 tensor(3.0378e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0378e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554152
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4635460376739502
overhead3:: 0.6689636707305908
overhead4:: 2.6844656467437744
overhead5:: 0
time_provenance:: 5.335461378097534
curr_diff: 0 tensor(3.0251e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0251e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554152
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.5085837841033936
overhead3:: 0.6812760829925537
overhead4:: 2.8642265796661377
overhead5:: 0
time_provenance:: 5.9746997356414795
curr_diff: 0 tensor(3.0127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0127e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554152
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7547128200531006
overhead3:: 0.9903857707977295
overhead4:: 3.478055000305176
overhead5:: 0
time_provenance:: 5.825968980789185
curr_diff: 0 tensor(2.4226e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4226e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554144
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.719976
Train - Epoch 0, Batch: 10, Loss: 0.694162
Train - Epoch 0, Batch: 20, Loss: 0.693409
Train - Epoch 0, Batch: 30, Loss: 0.692692
Train - Epoch 0, Batch: 40, Loss: 0.693283
Train - Epoch 0, Batch: 50, Loss: 0.692634
Train - Epoch 0, Batch: 60, Loss: 0.692991
Train - Epoch 0, Batch: 70, Loss: 0.691611
Train - Epoch 0, Batch: 80, Loss: 0.691811
Train - Epoch 0, Batch: 90, Loss: 0.691790
Train - Epoch 0, Batch: 100, Loss: 0.691301
Train - Epoch 0, Batch: 110, Loss: 0.690712
Train - Epoch 0, Batch: 120, Loss: 0.691481
Train - Epoch 0, Batch: 130, Loss: 0.691445
Train - Epoch 0, Batch: 140, Loss: 0.691643
Train - Epoch 0, Batch: 150, Loss: 0.689308
Train - Epoch 0, Batch: 160, Loss: 0.689421
Train - Epoch 0, Batch: 170, Loss: 0.690150
Train - Epoch 0, Batch: 180, Loss: 0.689363
Train - Epoch 0, Batch: 190, Loss: 0.689182
Train - Epoch 0, Batch: 200, Loss: 0.689561
Train - Epoch 0, Batch: 210, Loss: 0.691014
Train - Epoch 0, Batch: 220, Loss: 0.689915
Train - Epoch 0, Batch: 230, Loss: 0.689279
Train - Epoch 0, Batch: 240, Loss: 0.688386
Train - Epoch 0, Batch: 250, Loss: 0.689375
Train - Epoch 0, Batch: 260, Loss: 0.688905
Train - Epoch 0, Batch: 270, Loss: 0.689247
Train - Epoch 0, Batch: 280, Loss: 0.687814
Train - Epoch 0, Batch: 290, Loss: 0.689540
Train - Epoch 0, Batch: 300, Loss: 0.688674
Train - Epoch 0, Batch: 310, Loss: 0.688586
Train - Epoch 0, Batch: 320, Loss: 0.688568
Train - Epoch 0, Batch: 330, Loss: 0.688365
Train - Epoch 0, Batch: 340, Loss: 0.687806
Train - Epoch 0, Batch: 350, Loss: 0.688140
Train - Epoch 0, Batch: 360, Loss: 0.689126
Train - Epoch 0, Batch: 370, Loss: 0.688286
Train - Epoch 0, Batch: 380, Loss: 0.687818
Train - Epoch 0, Batch: 390, Loss: 0.687790
Train - Epoch 0, Batch: 400, Loss: 0.688164
Train - Epoch 0, Batch: 410, Loss: 0.689456
Train - Epoch 0, Batch: 420, Loss: 0.689143
Train - Epoch 0, Batch: 430, Loss: 0.687572
Train - Epoch 0, Batch: 440, Loss: 0.688135
Train - Epoch 0, Batch: 450, Loss: 0.688348
Train - Epoch 0, Batch: 460, Loss: 0.687786
Train - Epoch 0, Batch: 470, Loss: 0.687157
Train - Epoch 0, Batch: 480, Loss: 0.688312
Train - Epoch 0, Batch: 490, Loss: 0.688041
Train - Epoch 0, Batch: 500, Loss: 0.686882
Train - Epoch 0, Batch: 510, Loss: 0.688566
Train - Epoch 0, Batch: 520, Loss: 0.687761
Train - Epoch 0, Batch: 530, Loss: 0.686495
Train - Epoch 0, Batch: 540, Loss: 0.687678
Train - Epoch 0, Batch: 550, Loss: 0.687440
Train - Epoch 0, Batch: 560, Loss: 0.687456
Train - Epoch 0, Batch: 570, Loss: 0.687572
Train - Epoch 0, Batch: 580, Loss: 0.687316
Train - Epoch 0, Batch: 590, Loss: 0.687213
Train - Epoch 0, Batch: 600, Loss: 0.688313
Train - Epoch 0, Batch: 610, Loss: 0.687565
Train - Epoch 0, Batch: 620, Loss: 0.687963
Train - Epoch 0, Batch: 630, Loss: 0.687104
Train - Epoch 0, Batch: 640, Loss: 0.686557
Train - Epoch 1, Batch: 0, Loss: 0.686753
Train - Epoch 1, Batch: 10, Loss: 0.686693
Train - Epoch 1, Batch: 20, Loss: 0.687409
Train - Epoch 1, Batch: 30, Loss: 0.687518
Train - Epoch 1, Batch: 40, Loss: 0.687851
Train - Epoch 1, Batch: 50, Loss: 0.687576
Train - Epoch 1, Batch: 60, Loss: 0.687944
Train - Epoch 1, Batch: 70, Loss: 0.686507
Train - Epoch 1, Batch: 80, Loss: 0.687810
Train - Epoch 1, Batch: 90, Loss: 0.687854
Train - Epoch 1, Batch: 100, Loss: 0.686573
Train - Epoch 1, Batch: 110, Loss: 0.687671
Train - Epoch 1, Batch: 120, Loss: 0.687632
Train - Epoch 1, Batch: 130, Loss: 0.687023
Train - Epoch 1, Batch: 140, Loss: 0.685667
Train - Epoch 1, Batch: 150, Loss: 0.686876
Train - Epoch 1, Batch: 160, Loss: 0.685140
Train - Epoch 1, Batch: 170, Loss: 0.687308
Train - Epoch 1, Batch: 180, Loss: 0.686192
Train - Epoch 1, Batch: 190, Loss: 0.686237
Train - Epoch 1, Batch: 200, Loss: 0.686784
Train - Epoch 1, Batch: 210, Loss: 0.687239
Train - Epoch 1, Batch: 220, Loss: 0.685981
Train - Epoch 1, Batch: 230, Loss: 0.687078
Train - Epoch 1, Batch: 240, Loss: 0.685319
Train - Epoch 1, Batch: 250, Loss: 0.685753
Train - Epoch 1, Batch: 260, Loss: 0.686610
Train - Epoch 1, Batch: 270, Loss: 0.686739
Train - Epoch 1, Batch: 280, Loss: 0.686999
Train - Epoch 1, Batch: 290, Loss: 0.686118
Train - Epoch 1, Batch: 300, Loss: 0.686230
Train - Epoch 1, Batch: 310, Loss: 0.687204
Train - Epoch 1, Batch: 320, Loss: 0.686895
Train - Epoch 1, Batch: 330, Loss: 0.684781
Train - Epoch 1, Batch: 340, Loss: 0.685866
Train - Epoch 1, Batch: 350, Loss: 0.685964
Train - Epoch 1, Batch: 360, Loss: 0.686186
Train - Epoch 1, Batch: 370, Loss: 0.686735
Train - Epoch 1, Batch: 380, Loss: 0.686183
Train - Epoch 1, Batch: 390, Loss: 0.685931
Train - Epoch 1, Batch: 400, Loss: 0.686728
Train - Epoch 1, Batch: 410, Loss: 0.685521
Train - Epoch 1, Batch: 420, Loss: 0.687025
Train - Epoch 1, Batch: 430, Loss: 0.686192
Train - Epoch 1, Batch: 440, Loss: 0.686209
Train - Epoch 1, Batch: 450, Loss: 0.685827
Train - Epoch 1, Batch: 460, Loss: 0.685535
Train - Epoch 1, Batch: 470, Loss: 0.686191
Train - Epoch 1, Batch: 480, Loss: 0.687365
Train - Epoch 1, Batch: 490, Loss: 0.685782
Train - Epoch 1, Batch: 500, Loss: 0.684356
Train - Epoch 1, Batch: 510, Loss: 0.685902
Train - Epoch 1, Batch: 520, Loss: 0.685663
Train - Epoch 1, Batch: 530, Loss: 0.685569
Train - Epoch 1, Batch: 540, Loss: 0.684416
Train - Epoch 1, Batch: 550, Loss: 0.686014
Train - Epoch 1, Batch: 560, Loss: 0.685697
Train - Epoch 1, Batch: 570, Loss: 0.686212
Train - Epoch 1, Batch: 580, Loss: 0.687068
Train - Epoch 1, Batch: 590, Loss: 0.684773
Train - Epoch 1, Batch: 600, Loss: 0.685841
Train - Epoch 1, Batch: 610, Loss: 0.685438
Train - Epoch 1, Batch: 620, Loss: 0.685614
Train - Epoch 1, Batch: 630, Loss: 0.686908
Train - Epoch 1, Batch: 640, Loss: 0.685161
Train - Epoch 2, Batch: 0, Loss: 0.685428
Train - Epoch 2, Batch: 10, Loss: 0.685177
Train - Epoch 2, Batch: 20, Loss: 0.686652
Train - Epoch 2, Batch: 30, Loss: 0.686360
Train - Epoch 2, Batch: 40, Loss: 0.684944
Train - Epoch 2, Batch: 50, Loss: 0.685990
Train - Epoch 2, Batch: 60, Loss: 0.685735
Train - Epoch 2, Batch: 70, Loss: 0.686009
Train - Epoch 2, Batch: 80, Loss: 0.686454
Train - Epoch 2, Batch: 90, Loss: 0.684513
Train - Epoch 2, Batch: 100, Loss: 0.685736
Train - Epoch 2, Batch: 110, Loss: 0.684748
Train - Epoch 2, Batch: 120, Loss: 0.686386
Train - Epoch 2, Batch: 130, Loss: 0.685136
Train - Epoch 2, Batch: 140, Loss: 0.686716
Train - Epoch 2, Batch: 150, Loss: 0.685902
Train - Epoch 2, Batch: 160, Loss: 0.685469
Train - Epoch 2, Batch: 170, Loss: 0.684574
Train - Epoch 2, Batch: 180, Loss: 0.685916
Train - Epoch 2, Batch: 190, Loss: 0.685147
Train - Epoch 2, Batch: 200, Loss: 0.685569
Train - Epoch 2, Batch: 210, Loss: 0.685680
Train - Epoch 2, Batch: 220, Loss: 0.685460
Train - Epoch 2, Batch: 230, Loss: 0.686200
Train - Epoch 2, Batch: 240, Loss: 0.685612
Train - Epoch 2, Batch: 250, Loss: 0.685846
Train - Epoch 2, Batch: 260, Loss: 0.684879
Train - Epoch 2, Batch: 270, Loss: 0.685093
Train - Epoch 2, Batch: 280, Loss: 0.684117
Train - Epoch 2, Batch: 290, Loss: 0.685766
Train - Epoch 2, Batch: 300, Loss: 0.685516
Train - Epoch 2, Batch: 310, Loss: 0.684809
Train - Epoch 2, Batch: 320, Loss: 0.685176
Train - Epoch 2, Batch: 330, Loss: 0.684780
Train - Epoch 2, Batch: 340, Loss: 0.685922
Train - Epoch 2, Batch: 350, Loss: 0.684764
Train - Epoch 2, Batch: 360, Loss: 0.685565
Train - Epoch 2, Batch: 370, Loss: 0.683795
Train - Epoch 2, Batch: 380, Loss: 0.684257
Train - Epoch 2, Batch: 390, Loss: 0.685269
Train - Epoch 2, Batch: 400, Loss: 0.684654
Train - Epoch 2, Batch: 410, Loss: 0.684566
Train - Epoch 2, Batch: 420, Loss: 0.685172
Train - Epoch 2, Batch: 430, Loss: 0.684775
Train - Epoch 2, Batch: 440, Loss: 0.684734
Train - Epoch 2, Batch: 450, Loss: 0.684234
Train - Epoch 2, Batch: 460, Loss: 0.684627
Train - Epoch 2, Batch: 470, Loss: 0.685924
Train - Epoch 2, Batch: 480, Loss: 0.684925
Train - Epoch 2, Batch: 490, Loss: 0.683914
Train - Epoch 2, Batch: 500, Loss: 0.684984
Train - Epoch 2, Batch: 510, Loss: 0.684807
Train - Epoch 2, Batch: 520, Loss: 0.684511
Train - Epoch 2, Batch: 530, Loss: 0.685757
Train - Epoch 2, Batch: 540, Loss: 0.685480
Train - Epoch 2, Batch: 550, Loss: 0.685639/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684523
Train - Epoch 2, Batch: 570, Loss: 0.685028
Train - Epoch 2, Batch: 580, Loss: 0.685090
Train - Epoch 2, Batch: 590, Loss: 0.685612
Train - Epoch 2, Batch: 600, Loss: 0.685307
Train - Epoch 2, Batch: 610, Loss: 0.684362
Train - Epoch 2, Batch: 620, Loss: 0.682824
Train - Epoch 2, Batch: 630, Loss: 0.684737
Train - Epoch 2, Batch: 640, Loss: 0.685254
Train - Epoch 3, Batch: 0, Loss: 0.685448
Train - Epoch 3, Batch: 10, Loss: 0.683965
Train - Epoch 3, Batch: 20, Loss: 0.684212
Train - Epoch 3, Batch: 30, Loss: 0.685840
Train - Epoch 3, Batch: 40, Loss: 0.685166
Train - Epoch 3, Batch: 50, Loss: 0.685209
Train - Epoch 3, Batch: 60, Loss: 0.685002
Train - Epoch 3, Batch: 70, Loss: 0.684894
Train - Epoch 3, Batch: 80, Loss: 0.684638
Train - Epoch 3, Batch: 90, Loss: 0.684759
Train - Epoch 3, Batch: 100, Loss: 0.683642
Train - Epoch 3, Batch: 110, Loss: 0.684194
Train - Epoch 3, Batch: 120, Loss: 0.683071
Train - Epoch 3, Batch: 130, Loss: 0.685490
Train - Epoch 3, Batch: 140, Loss: 0.684616
Train - Epoch 3, Batch: 150, Loss: 0.684311
Train - Epoch 3, Batch: 160, Loss: 0.684861
Train - Epoch 3, Batch: 170, Loss: 0.683973
Train - Epoch 3, Batch: 180, Loss: 0.685327
Train - Epoch 3, Batch: 190, Loss: 0.684012
Train - Epoch 3, Batch: 200, Loss: 0.683849
Train - Epoch 3, Batch: 210, Loss: 0.684733
Train - Epoch 3, Batch: 220, Loss: 0.682289
Train - Epoch 3, Batch: 230, Loss: 0.684415
Train - Epoch 3, Batch: 240, Loss: 0.683876
Train - Epoch 3, Batch: 250, Loss: 0.684581
Train - Epoch 3, Batch: 260, Loss: 0.684497
Train - Epoch 3, Batch: 270, Loss: 0.683958
Train - Epoch 3, Batch: 280, Loss: 0.683686
Train - Epoch 3, Batch: 290, Loss: 0.684480
Train - Epoch 3, Batch: 300, Loss: 0.684386
Train - Epoch 3, Batch: 310, Loss: 0.684209
Train - Epoch 3, Batch: 320, Loss: 0.684403
Train - Epoch 3, Batch: 330, Loss: 0.685490
Train - Epoch 3, Batch: 340, Loss: 0.683523
Train - Epoch 3, Batch: 350, Loss: 0.685820
Train - Epoch 3, Batch: 360, Loss: 0.683659
Train - Epoch 3, Batch: 370, Loss: 0.683872
Train - Epoch 3, Batch: 380, Loss: 0.683244
Train - Epoch 3, Batch: 390, Loss: 0.684031
Train - Epoch 3, Batch: 400, Loss: 0.683470
Train - Epoch 3, Batch: 410, Loss: 0.684506
Train - Epoch 3, Batch: 420, Loss: 0.683056
Train - Epoch 3, Batch: 430, Loss: 0.683887
Train - Epoch 3, Batch: 440, Loss: 0.684785
Train - Epoch 3, Batch: 450, Loss: 0.684099
Train - Epoch 3, Batch: 460, Loss: 0.684298
Train - Epoch 3, Batch: 470, Loss: 0.684057
Train - Epoch 3, Batch: 480, Loss: 0.683928
Train - Epoch 3, Batch: 490, Loss: 0.684524
Train - Epoch 3, Batch: 500, Loss: 0.682898
Train - Epoch 3, Batch: 510, Loss: 0.683079
Train - Epoch 3, Batch: 520, Loss: 0.685209
Train - Epoch 3, Batch: 530, Loss: 0.685325
Train - Epoch 3, Batch: 540, Loss: 0.684417
Train - Epoch 3, Batch: 550, Loss: 0.684889
Train - Epoch 3, Batch: 560, Loss: 0.684176
Train - Epoch 3, Batch: 570, Loss: 0.685365
Train - Epoch 3, Batch: 580, Loss: 0.682786
Train - Epoch 3, Batch: 590, Loss: 0.684758
Train - Epoch 3, Batch: 600, Loss: 0.684437
Train - Epoch 3, Batch: 610, Loss: 0.684595
Train - Epoch 3, Batch: 620, Loss: 0.683881
Train - Epoch 3, Batch: 630, Loss: 0.682514
Train - Epoch 3, Batch: 640, Loss: 0.684465
training_time:: 7.399353742599487
training time full:: 7.399392366409302
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554284
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 21000
training time is 4.63221001625061
overhead:: 0
overhead2:: 0
time_baseline:: 4.635078191757202
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554252
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1564316749572754
overhead3:: 0.22309517860412598
overhead4:: 0.8626446723937988
overhead5:: 0
time_provenance:: 2.987070322036743
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554256
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1866011619567871
overhead3:: 0.2449960708618164
overhead4:: 0.9643270969390869
overhead5:: 0
time_provenance:: 3.0481536388397217
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554256
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22766971588134766
overhead3:: 0.34116578102111816
overhead4:: 1.1191599369049072
overhead5:: 0
time_provenance:: 3.2990431785583496
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554256
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24399805068969727
overhead3:: 0.3369898796081543
overhead4:: 1.2680106163024902
overhead5:: 0
time_provenance:: 3.393012046813965
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.23914051055908203
overhead3:: 0.32526540756225586
overhead4:: 1.3172173500061035
overhead5:: 0
time_provenance:: 3.919059991836548
curr_diff: 0 tensor(6.7409e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7409e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2643260955810547
overhead3:: 0.39279961585998535
overhead4:: 1.437352180480957
overhead5:: 0
time_provenance:: 3.6694722175598145
curr_diff: 0 tensor(6.6142e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6142e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.26216673851013184
overhead3:: 0.3724324703216553
overhead4:: 1.451355218887329
overhead5:: 0
time_provenance:: 3.609994411468506
curr_diff: 0 tensor(6.5543e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5543e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.276625394821167
overhead3:: 0.377779483795166
overhead4:: 1.5233721733093262
overhead5:: 0
time_provenance:: 3.667978048324585
curr_diff: 0 tensor(6.4276e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4276e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.33374738693237305
overhead3:: 0.45569944381713867
overhead4:: 1.7570343017578125
overhead5:: 0
time_provenance:: 4.464419841766357
curr_diff: 0 tensor(4.6224e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6224e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3940422534942627
overhead3:: 0.5427711009979248
overhead4:: 2.232079029083252
overhead5:: 0
time_provenance:: 5.436353921890259
curr_diff: 0 tensor(3.6254e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6254e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554250
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.32184314727783203
overhead3:: 0.4402284622192383
overhead4:: 1.9204959869384766
overhead5:: 0
time_provenance:: 4.219595193862915
curr_diff: 0 tensor(3.7448e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7448e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554258
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3685433864593506
overhead3:: 0.5209906101226807
overhead4:: 2.0816564559936523
overhead5:: 0
time_provenance:: 4.448692560195923
curr_diff: 0 tensor(4.4336e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4336e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.383145809173584
overhead3:: 0.5270192623138428
overhead4:: 2.3068959712982178
overhead5:: 0
time_provenance:: 4.793152332305908
curr_diff: 0 tensor(1.8124e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8124e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4110066890716553
overhead3:: 0.5846946239471436
overhead4:: 2.5207319259643555
overhead5:: 0
time_provenance:: 5.041711091995239
curr_diff: 0 tensor(1.7730e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7730e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.408893346786499
overhead3:: 0.5988843441009521
overhead4:: 2.468653440475464
overhead5:: 0
time_provenance:: 4.967954874038696
curr_diff: 0 tensor(1.7273e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7273e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.45498085021972656
overhead3:: 0.6219203472137451
overhead4:: 2.565584659576416
overhead5:: 0
time_provenance:: 5.100136756896973
curr_diff: 0 tensor(1.6566e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6566e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554246
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7864360809326172
overhead3:: 1.1035926342010498
overhead4:: 3.6896274089813232
overhead5:: 0
time_provenance:: 6.200585842132568
curr_diff: 0 tensor(2.4306e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4306e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554252
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.702773
Train - Epoch 0, Batch: 10, Loss: 0.695841
Train - Epoch 0, Batch: 20, Loss: 0.695588
Train - Epoch 0, Batch: 30, Loss: 0.693319
Train - Epoch 0, Batch: 40, Loss: 0.692751
Train - Epoch 0, Batch: 50, Loss: 0.693096
Train - Epoch 0, Batch: 60, Loss: 0.692395
Train - Epoch 0, Batch: 70, Loss: 0.691937
Train - Epoch 0, Batch: 80, Loss: 0.691298
Train - Epoch 0, Batch: 90, Loss: 0.691598
Train - Epoch 0, Batch: 100, Loss: 0.691793
Train - Epoch 0, Batch: 110, Loss: 0.692165
Train - Epoch 0, Batch: 120, Loss: 0.690955
Train - Epoch 0, Batch: 130, Loss: 0.690499
Train - Epoch 0, Batch: 140, Loss: 0.690640
Train - Epoch 0, Batch: 150, Loss: 0.689907
Train - Epoch 0, Batch: 160, Loss: 0.690089
Train - Epoch 0, Batch: 170, Loss: 0.688523
Train - Epoch 0, Batch: 180, Loss: 0.689051
Train - Epoch 0, Batch: 190, Loss: 0.689905
Train - Epoch 0, Batch: 200, Loss: 0.687957
Train - Epoch 0, Batch: 210, Loss: 0.688671
Train - Epoch 0, Batch: 220, Loss: 0.689283
Train - Epoch 0, Batch: 230, Loss: 0.688998
Train - Epoch 0, Batch: 240, Loss: 0.688783
Train - Epoch 0, Batch: 250, Loss: 0.688446
Train - Epoch 0, Batch: 260, Loss: 0.689400
Train - Epoch 0, Batch: 270, Loss: 0.687338
Train - Epoch 0, Batch: 280, Loss: 0.688278
Train - Epoch 0, Batch: 290, Loss: 0.687473
Train - Epoch 0, Batch: 300, Loss: 0.687648
Train - Epoch 0, Batch: 310, Loss: 0.688335
Train - Epoch 0, Batch: 320, Loss: 0.687367
Train - Epoch 0, Batch: 330, Loss: 0.686466
Train - Epoch 0, Batch: 340, Loss: 0.687521
Train - Epoch 0, Batch: 350, Loss: 0.687974
Train - Epoch 0, Batch: 360, Loss: 0.686737
Train - Epoch 0, Batch: 370, Loss: 0.688743
Train - Epoch 0, Batch: 380, Loss: 0.687739
Train - Epoch 0, Batch: 390, Loss: 0.687295
Train - Epoch 0, Batch: 400, Loss: 0.687346
Train - Epoch 0, Batch: 410, Loss: 0.687824
Train - Epoch 0, Batch: 420, Loss: 0.686773
Train - Epoch 0, Batch: 430, Loss: 0.687532
Train - Epoch 0, Batch: 440, Loss: 0.687855
Train - Epoch 0, Batch: 450, Loss: 0.686942
Train - Epoch 0, Batch: 460, Loss: 0.685816
Train - Epoch 0, Batch: 470, Loss: 0.688021
Train - Epoch 0, Batch: 480, Loss: 0.687249
Train - Epoch 0, Batch: 490, Loss: 0.687923
Train - Epoch 0, Batch: 500, Loss: 0.686279
Train - Epoch 0, Batch: 510, Loss: 0.687109
Train - Epoch 0, Batch: 520, Loss: 0.687814
Train - Epoch 0, Batch: 530, Loss: 0.687349
Train - Epoch 0, Batch: 540, Loss: 0.686785
Train - Epoch 0, Batch: 550, Loss: 0.686689
Train - Epoch 0, Batch: 560, Loss: 0.685439
Train - Epoch 0, Batch: 570, Loss: 0.687265
Train - Epoch 0, Batch: 580, Loss: 0.686485
Train - Epoch 0, Batch: 590, Loss: 0.685514
Train - Epoch 0, Batch: 600, Loss: 0.685683
Train - Epoch 0, Batch: 610, Loss: 0.686458
Train - Epoch 0, Batch: 620, Loss: 0.686818
Train - Epoch 0, Batch: 630, Loss: 0.686325
Train - Epoch 0, Batch: 640, Loss: 0.685636
Train - Epoch 1, Batch: 0, Loss: 0.685622
Train - Epoch 1, Batch: 10, Loss: 0.686883
Train - Epoch 1, Batch: 20, Loss: 0.687807
Train - Epoch 1, Batch: 30, Loss: 0.686531
Train - Epoch 1, Batch: 40, Loss: 0.686039
Train - Epoch 1, Batch: 50, Loss: 0.685733
Train - Epoch 1, Batch: 60, Loss: 0.685756
Train - Epoch 1, Batch: 70, Loss: 0.685711
Train - Epoch 1, Batch: 80, Loss: 0.684983
Train - Epoch 1, Batch: 90, Loss: 0.686317
Train - Epoch 1, Batch: 100, Loss: 0.686050
Train - Epoch 1, Batch: 110, Loss: 0.686415
Train - Epoch 1, Batch: 120, Loss: 0.685552
Train - Epoch 1, Batch: 130, Loss: 0.685901
Train - Epoch 1, Batch: 140, Loss: 0.686822
Train - Epoch 1, Batch: 150, Loss: 0.686862
Train - Epoch 1, Batch: 160, Loss: 0.686011
Train - Epoch 1, Batch: 170, Loss: 0.686088
Train - Epoch 1, Batch: 180, Loss: 0.685243
Train - Epoch 1, Batch: 190, Loss: 0.686063
Train - Epoch 1, Batch: 200, Loss: 0.684950
Train - Epoch 1, Batch: 210, Loss: 0.687428
Train - Epoch 1, Batch: 220, Loss: 0.685228
Train - Epoch 1, Batch: 230, Loss: 0.685931
Train - Epoch 1, Batch: 240, Loss: 0.685420
Train - Epoch 1, Batch: 250, Loss: 0.685915
Train - Epoch 1, Batch: 260, Loss: 0.685826
Train - Epoch 1, Batch: 270, Loss: 0.686089
Train - Epoch 1, Batch: 280, Loss: 0.686165
Train - Epoch 1, Batch: 290, Loss: 0.686246
Train - Epoch 1, Batch: 300, Loss: 0.685079
Train - Epoch 1, Batch: 310, Loss: 0.684042
Train - Epoch 1, Batch: 320, Loss: 0.685370
Train - Epoch 1, Batch: 330, Loss: 0.686108
Train - Epoch 1, Batch: 340, Loss: 0.686245
Train - Epoch 1, Batch: 350, Loss: 0.684778
Train - Epoch 1, Batch: 360, Loss: 0.685601
Train - Epoch 1, Batch: 370, Loss: 0.684425
Train - Epoch 1, Batch: 380, Loss: 0.684468
Train - Epoch 1, Batch: 390, Loss: 0.684633
Train - Epoch 1, Batch: 400, Loss: 0.685841
Train - Epoch 1, Batch: 410, Loss: 0.685400
Train - Epoch 1, Batch: 420, Loss: 0.686531
Train - Epoch 1, Batch: 430, Loss: 0.685942
Train - Epoch 1, Batch: 440, Loss: 0.685924
Train - Epoch 1, Batch: 450, Loss: 0.685129
Train - Epoch 1, Batch: 460, Loss: 0.686373
Train - Epoch 1, Batch: 470, Loss: 0.684076
Train - Epoch 1, Batch: 480, Loss: 0.685599
Train - Epoch 1, Batch: 490, Loss: 0.685238
Train - Epoch 1, Batch: 500, Loss: 0.684697
Train - Epoch 1, Batch: 510, Loss: 0.685276
Train - Epoch 1, Batch: 520, Loss: 0.685140
Train - Epoch 1, Batch: 530, Loss: 0.684844
Train - Epoch 1, Batch: 540, Loss: 0.685901
Train - Epoch 1, Batch: 550, Loss: 0.685085
Train - Epoch 1, Batch: 560, Loss: 0.683856
Train - Epoch 1, Batch: 570, Loss: 0.685608
Train - Epoch 1, Batch: 580, Loss: 0.685107
Train - Epoch 1, Batch: 590, Loss: 0.685652
Train - Epoch 1, Batch: 600, Loss: 0.684143
Train - Epoch 1, Batch: 610, Loss: 0.686062
Train - Epoch 1, Batch: 620, Loss: 0.684413
Train - Epoch 1, Batch: 630, Loss: 0.684926
Train - Epoch 1, Batch: 640, Loss: 0.685362
Train - Epoch 2, Batch: 0, Loss: 0.685224
Train - Epoch 2, Batch: 10, Loss: 0.686055
Train - Epoch 2, Batch: 20, Loss: 0.684289
Train - Epoch 2, Batch: 30, Loss: 0.685808
Train - Epoch 2, Batch: 40, Loss: 0.685225
Train - Epoch 2, Batch: 50, Loss: 0.683373
Train - Epoch 2, Batch: 60, Loss: 0.685595
Train - Epoch 2, Batch: 70, Loss: 0.684045
Train - Epoch 2, Batch: 80, Loss: 0.684289
Train - Epoch 2, Batch: 90, Loss: 0.685124
Train - Epoch 2, Batch: 100, Loss: 0.685616
Train - Epoch 2, Batch: 110, Loss: 0.684619
Train - Epoch 2, Batch: 120, Loss: 0.683947
Train - Epoch 2, Batch: 130, Loss: 0.684786
Train - Epoch 2, Batch: 140, Loss: 0.684090
Train - Epoch 2, Batch: 150, Loss: 0.686024
Train - Epoch 2, Batch: 160, Loss: 0.685137
Train - Epoch 2, Batch: 170, Loss: 0.683969
Train - Epoch 2, Batch: 180, Loss: 0.684902
Train - Epoch 2, Batch: 190, Loss: 0.684716
Train - Epoch 2, Batch: 200, Loss: 0.684785
Train - Epoch 2, Batch: 210, Loss: 0.683201
Train - Epoch 2, Batch: 220, Loss: 0.685696
Train - Epoch 2, Batch: 230, Loss: 0.684678
Train - Epoch 2, Batch: 240, Loss: 0.683996
Train - Epoch 2, Batch: 250, Loss: 0.685533
Train - Epoch 2, Batch: 260, Loss: 0.684898
Train - Epoch 2, Batch: 270, Loss: 0.684653
Train - Epoch 2, Batch: 280, Loss: 0.683667
Train - Epoch 2, Batch: 290, Loss: 0.683097
Train - Epoch 2, Batch: 300, Loss: 0.684068
Train - Epoch 2, Batch: 310, Loss: 0.684162
Train - Epoch 2, Batch: 320, Loss: 0.684245
Train - Epoch 2, Batch: 330, Loss: 0.684349
Train - Epoch 2, Batch: 340, Loss: 0.685814
Train - Epoch 2, Batch: 350, Loss: 0.685069
Train - Epoch 2, Batch: 360, Loss: 0.684012
Train - Epoch 2, Batch: 370, Loss: 0.685179
Train - Epoch 2, Batch: 380, Loss: 0.684881
Train - Epoch 2, Batch: 390, Loss: 0.684657
Train - Epoch 2, Batch: 400, Loss: 0.684516
Train - Epoch 2, Batch: 410, Loss: 0.684877
Train - Epoch 2, Batch: 420, Loss: 0.684607
Train - Epoch 2, Batch: 430, Loss: 0.683928
Train - Epoch 2, Batch: 440, Loss: 0.684183
Train - Epoch 2, Batch: 450, Loss: 0.684909
Train - Epoch 2, Batch: 460, Loss: 0.685569
Train - Epoch 2, Batch: 470, Loss: 0.684677
Train - Epoch 2, Batch: 480, Loss: 0.683638
Train - Epoch 2, Batch: 490, Loss: 0.684663
Train - Epoch 2, Batch: 500, Loss: 0.684918
Train - Epoch 2, Batch: 510, Loss: 0.684163
Train - Epoch 2, Batch: 520, Loss: 0.685269
Train - Epoch 2, Batch: 530, Loss: 0.684448
Train - Epoch 2, Batch: 540, Loss: 0.684114
Train - Epoch 2, Batch: 550, Loss: 0.683095/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685258
Train - Epoch 2, Batch: 570, Loss: 0.683518
Train - Epoch 2, Batch: 580, Loss: 0.683271
Train - Epoch 2, Batch: 590, Loss: 0.684075
Train - Epoch 2, Batch: 600, Loss: 0.683858
Train - Epoch 2, Batch: 610, Loss: 0.684129
Train - Epoch 2, Batch: 620, Loss: 0.685072
Train - Epoch 2, Batch: 630, Loss: 0.684280
Train - Epoch 2, Batch: 640, Loss: 0.684878
Train - Epoch 3, Batch: 0, Loss: 0.684126
Train - Epoch 3, Batch: 10, Loss: 0.684429
Train - Epoch 3, Batch: 20, Loss: 0.684281
Train - Epoch 3, Batch: 30, Loss: 0.685451
Train - Epoch 3, Batch: 40, Loss: 0.686467
Train - Epoch 3, Batch: 50, Loss: 0.683411
Train - Epoch 3, Batch: 60, Loss: 0.685106
Train - Epoch 3, Batch: 70, Loss: 0.685032
Train - Epoch 3, Batch: 80, Loss: 0.684401
Train - Epoch 3, Batch: 90, Loss: 0.684353
Train - Epoch 3, Batch: 100, Loss: 0.683824
Train - Epoch 3, Batch: 110, Loss: 0.684560
Train - Epoch 3, Batch: 120, Loss: 0.683779
Train - Epoch 3, Batch: 130, Loss: 0.683663
Train - Epoch 3, Batch: 140, Loss: 0.684806
Train - Epoch 3, Batch: 150, Loss: 0.684253
Train - Epoch 3, Batch: 160, Loss: 0.685390
Train - Epoch 3, Batch: 170, Loss: 0.682677
Train - Epoch 3, Batch: 180, Loss: 0.683773
Train - Epoch 3, Batch: 190, Loss: 0.684887
Train - Epoch 3, Batch: 200, Loss: 0.683738
Train - Epoch 3, Batch: 210, Loss: 0.684631
Train - Epoch 3, Batch: 220, Loss: 0.685235
Train - Epoch 3, Batch: 230, Loss: 0.684672
Train - Epoch 3, Batch: 240, Loss: 0.684498
Train - Epoch 3, Batch: 250, Loss: 0.684773
Train - Epoch 3, Batch: 260, Loss: 0.684810
Train - Epoch 3, Batch: 270, Loss: 0.683744
Train - Epoch 3, Batch: 280, Loss: 0.684501
Train - Epoch 3, Batch: 290, Loss: 0.684059
Train - Epoch 3, Batch: 300, Loss: 0.685073
Train - Epoch 3, Batch: 310, Loss: 0.685197
Train - Epoch 3, Batch: 320, Loss: 0.683375
Train - Epoch 3, Batch: 330, Loss: 0.684405
Train - Epoch 3, Batch: 340, Loss: 0.684248
Train - Epoch 3, Batch: 350, Loss: 0.682876
Train - Epoch 3, Batch: 360, Loss: 0.683699
Train - Epoch 3, Batch: 370, Loss: 0.683659
Train - Epoch 3, Batch: 380, Loss: 0.683890
Train - Epoch 3, Batch: 390, Loss: 0.684355
Train - Epoch 3, Batch: 400, Loss: 0.681572
Train - Epoch 3, Batch: 410, Loss: 0.684585
Train - Epoch 3, Batch: 420, Loss: 0.683682
Train - Epoch 3, Batch: 430, Loss: 0.684141
Train - Epoch 3, Batch: 440, Loss: 0.683013
Train - Epoch 3, Batch: 450, Loss: 0.685473
Train - Epoch 3, Batch: 460, Loss: 0.684351
Train - Epoch 3, Batch: 470, Loss: 0.683176
Train - Epoch 3, Batch: 480, Loss: 0.683896
Train - Epoch 3, Batch: 490, Loss: 0.683918
Train - Epoch 3, Batch: 500, Loss: 0.684339
Train - Epoch 3, Batch: 510, Loss: 0.683441
Train - Epoch 3, Batch: 520, Loss: 0.683645
Train - Epoch 3, Batch: 530, Loss: 0.683364
Train - Epoch 3, Batch: 540, Loss: 0.684001
Train - Epoch 3, Batch: 550, Loss: 0.683228
Train - Epoch 3, Batch: 560, Loss: 0.682914
Train - Epoch 3, Batch: 570, Loss: 0.684298
Train - Epoch 3, Batch: 580, Loss: 0.683590
Train - Epoch 3, Batch: 590, Loss: 0.683747
Train - Epoch 3, Batch: 600, Loss: 0.682563
Train - Epoch 3, Batch: 610, Loss: 0.684829
Train - Epoch 3, Batch: 620, Loss: 0.683961
Train - Epoch 3, Batch: 630, Loss: 0.684328
Train - Epoch 3, Batch: 640, Loss: 0.684234
training_time:: 7.526188850402832
training time full:: 7.5262291431427
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.554880
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 21000
training time is 5.94192099571228
overhead:: 0
overhead2:: 0
time_baseline:: 5.946520805358887
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554884
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.14859890937805176
overhead3:: 0.1960616111755371
overhead4:: 0.78169846534729
overhead5:: 0
time_provenance:: 2.7707276344299316
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1613459587097168
overhead3:: 0.21904587745666504
overhead4:: 0.8988497257232666
overhead5:: 0
time_provenance:: 2.891935110092163
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22885513305664062
overhead3:: 0.3321354389190674
overhead4:: 1.1216588020324707
overhead5:: 0
time_provenance:: 3.233121871948242
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24853181838989258
overhead3:: 0.372342586517334
overhead4:: 1.1991925239562988
overhead5:: 0
time_provenance:: 3.2974939346313477
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24286174774169922
overhead3:: 0.32723307609558105
overhead4:: 1.370114803314209
overhead5:: 0
time_provenance:: 4.133234977722168
curr_diff: 0 tensor(6.3949e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3949e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554882
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.23153162002563477
overhead3:: 0.32834887504577637
overhead4:: 1.2922484874725342
overhead5:: 0
time_provenance:: 3.455831527709961
curr_diff: 0 tensor(6.3383e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3383e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554882
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3068366050720215
overhead3:: 0.4608345031738281
overhead4:: 1.5553791522979736
overhead5:: 0
time_provenance:: 4.156756401062012
curr_diff: 0 tensor(6.2653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2653e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554882
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.30591845512390137
overhead3:: 0.4457559585571289
overhead4:: 1.5892424583435059
overhead5:: 0
time_provenance:: 3.8329224586486816
curr_diff: 0 tensor(6.1672e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1672e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554882
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.29224634170532227
overhead3:: 0.41774511337280273
overhead4:: 1.7721939086914062
overhead5:: 0
time_provenance:: 4.084061622619629
curr_diff: 0 tensor(4.6714e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6714e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554890
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3044619560241699
overhead3:: 0.43472933769226074
overhead4:: 1.8062758445739746
overhead5:: 0
time_provenance:: 4.085164308547974
curr_diff: 0 tensor(3.9251e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9251e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554896
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.342071533203125
overhead3:: 0.47072935104370117
overhead4:: 1.8744053840637207
overhead5:: 0
time_provenance:: 4.2228546142578125
curr_diff: 0 tensor(5.3271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3271e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554874
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.42497992515563965
overhead3:: 0.6288876533508301
overhead4:: 2.140594482421875
overhead5:: 0
time_provenance:: 4.726582288742065
curr_diff: 0 tensor(4.4857e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4857e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554890
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.40722203254699707
overhead3:: 0.564617395401001
overhead4:: 2.358414888381958
overhead5:: 0
time_provenance:: 4.922290802001953
curr_diff: 0 tensor(2.7002e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7002e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4527158737182617
overhead3:: 0.6095905303955078
overhead4:: 2.60158371925354
overhead5:: 0
time_provenance:: 5.4143712520599365
curr_diff: 0 tensor(2.6782e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6782e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4601776599884033
overhead3:: 0.6703641414642334
overhead4:: 2.588613748550415
overhead5:: 0
time_provenance:: 5.2300732135772705
curr_diff: 0 tensor(2.6468e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6468e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.5209076404571533
overhead3:: 0.7685718536376953
overhead4:: 2.3670544624328613
overhead5:: 0
time_provenance:: 5.160959482192993
curr_diff: 0 tensor(2.6048e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6048e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554888
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7852327823638916
overhead3:: 1.1092231273651123
overhead4:: 3.688270330429077
overhead5:: 0
time_provenance:: 6.2036051750183105
curr_diff: 0 tensor(2.9206e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9206e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554884
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.697063
Train - Epoch 0, Batch: 10, Loss: 0.696492
Train - Epoch 0, Batch: 20, Loss: 0.693771
Train - Epoch 0, Batch: 30, Loss: 0.692624
Train - Epoch 0, Batch: 40, Loss: 0.691899
Train - Epoch 0, Batch: 50, Loss: 0.691564
Train - Epoch 0, Batch: 60, Loss: 0.691189
Train - Epoch 0, Batch: 70, Loss: 0.691154
Train - Epoch 0, Batch: 80, Loss: 0.691776
Train - Epoch 0, Batch: 90, Loss: 0.690339
Train - Epoch 0, Batch: 100, Loss: 0.690318
Train - Epoch 0, Batch: 110, Loss: 0.690758
Train - Epoch 0, Batch: 120, Loss: 0.689561
Train - Epoch 0, Batch: 130, Loss: 0.689904
Train - Epoch 0, Batch: 140, Loss: 0.689495
Train - Epoch 0, Batch: 150, Loss: 0.689684
Train - Epoch 0, Batch: 160, Loss: 0.688865
Train - Epoch 0, Batch: 170, Loss: 0.688942
Train - Epoch 0, Batch: 180, Loss: 0.688849
Train - Epoch 0, Batch: 190, Loss: 0.688889
Train - Epoch 0, Batch: 200, Loss: 0.689707
Train - Epoch 0, Batch: 210, Loss: 0.688151
Train - Epoch 0, Batch: 220, Loss: 0.688853
Train - Epoch 0, Batch: 230, Loss: 0.689367
Train - Epoch 0, Batch: 240, Loss: 0.688575
Train - Epoch 0, Batch: 250, Loss: 0.687846
Train - Epoch 0, Batch: 260, Loss: 0.687348
Train - Epoch 0, Batch: 270, Loss: 0.688863
Train - Epoch 0, Batch: 280, Loss: 0.688628
Train - Epoch 0, Batch: 290, Loss: 0.689030
Train - Epoch 0, Batch: 300, Loss: 0.689434
Train - Epoch 0, Batch: 310, Loss: 0.688466
Train - Epoch 0, Batch: 320, Loss: 0.688759
Train - Epoch 0, Batch: 330, Loss: 0.687464
Train - Epoch 0, Batch: 340, Loss: 0.687878
Train - Epoch 0, Batch: 350, Loss: 0.689048
Train - Epoch 0, Batch: 360, Loss: 0.688025
Train - Epoch 0, Batch: 370, Loss: 0.688191
Train - Epoch 0, Batch: 380, Loss: 0.688536
Train - Epoch 0, Batch: 390, Loss: 0.687488
Train - Epoch 0, Batch: 400, Loss: 0.688029
Train - Epoch 0, Batch: 410, Loss: 0.688079
Train - Epoch 0, Batch: 420, Loss: 0.687810
Train - Epoch 0, Batch: 430, Loss: 0.686762
Train - Epoch 0, Batch: 440, Loss: 0.688424
Train - Epoch 0, Batch: 450, Loss: 0.687273
Train - Epoch 0, Batch: 460, Loss: 0.687303
Train - Epoch 0, Batch: 470, Loss: 0.687102
Train - Epoch 0, Batch: 480, Loss: 0.688013
Train - Epoch 0, Batch: 490, Loss: 0.686962
Train - Epoch 0, Batch: 500, Loss: 0.687524
Train - Epoch 0, Batch: 510, Loss: 0.686886
Train - Epoch 0, Batch: 520, Loss: 0.687618
Train - Epoch 0, Batch: 530, Loss: 0.686776
Train - Epoch 0, Batch: 540, Loss: 0.687344
Train - Epoch 0, Batch: 550, Loss: 0.687567
Train - Epoch 0, Batch: 560, Loss: 0.687595
Train - Epoch 0, Batch: 570, Loss: 0.686824
Train - Epoch 0, Batch: 580, Loss: 0.687556
Train - Epoch 0, Batch: 590, Loss: 0.687150
Train - Epoch 0, Batch: 600, Loss: 0.686593
Train - Epoch 0, Batch: 610, Loss: 0.686499
Train - Epoch 0, Batch: 620, Loss: 0.686687
Train - Epoch 0, Batch: 630, Loss: 0.686618
Train - Epoch 0, Batch: 640, Loss: 0.686894
Train - Epoch 1, Batch: 0, Loss: 0.688340
Train - Epoch 1, Batch: 10, Loss: 0.687052
Train - Epoch 1, Batch: 20, Loss: 0.687177
Train - Epoch 1, Batch: 30, Loss: 0.686536
Train - Epoch 1, Batch: 40, Loss: 0.686286
Train - Epoch 1, Batch: 50, Loss: 0.687035
Train - Epoch 1, Batch: 60, Loss: 0.687391
Train - Epoch 1, Batch: 70, Loss: 0.686348
Train - Epoch 1, Batch: 80, Loss: 0.685490
Train - Epoch 1, Batch: 90, Loss: 0.686282
Train - Epoch 1, Batch: 100, Loss: 0.686335
Train - Epoch 1, Batch: 110, Loss: 0.686863
Train - Epoch 1, Batch: 120, Loss: 0.687290
Train - Epoch 1, Batch: 130, Loss: 0.687384
Train - Epoch 1, Batch: 140, Loss: 0.685839
Train - Epoch 1, Batch: 150, Loss: 0.685496
Train - Epoch 1, Batch: 160, Loss: 0.686968
Train - Epoch 1, Batch: 170, Loss: 0.687055
Train - Epoch 1, Batch: 180, Loss: 0.686865
Train - Epoch 1, Batch: 190, Loss: 0.686350
Train - Epoch 1, Batch: 200, Loss: 0.687239
Train - Epoch 1, Batch: 210, Loss: 0.686648
Train - Epoch 1, Batch: 220, Loss: 0.687052
Train - Epoch 1, Batch: 230, Loss: 0.686964
Train - Epoch 1, Batch: 240, Loss: 0.687624
Train - Epoch 1, Batch: 250, Loss: 0.686043
Train - Epoch 1, Batch: 260, Loss: 0.686548
Train - Epoch 1, Batch: 270, Loss: 0.686807
Train - Epoch 1, Batch: 280, Loss: 0.686810
Train - Epoch 1, Batch: 290, Loss: 0.686098
Train - Epoch 1, Batch: 300, Loss: 0.685444
Train - Epoch 1, Batch: 310, Loss: 0.685824
Train - Epoch 1, Batch: 320, Loss: 0.686243
Train - Epoch 1, Batch: 330, Loss: 0.686433
Train - Epoch 1, Batch: 340, Loss: 0.685246
Train - Epoch 1, Batch: 350, Loss: 0.685402
Train - Epoch 1, Batch: 360, Loss: 0.685592
Train - Epoch 1, Batch: 370, Loss: 0.686035
Train - Epoch 1, Batch: 380, Loss: 0.686504
Train - Epoch 1, Batch: 390, Loss: 0.685992
Train - Epoch 1, Batch: 400, Loss: 0.685331
Train - Epoch 1, Batch: 410, Loss: 0.686117
Train - Epoch 1, Batch: 420, Loss: 0.684837
Train - Epoch 1, Batch: 430, Loss: 0.685680
Train - Epoch 1, Batch: 440, Loss: 0.685161
Train - Epoch 1, Batch: 450, Loss: 0.686009
Train - Epoch 1, Batch: 460, Loss: 0.686222
Train - Epoch 1, Batch: 470, Loss: 0.686219
Train - Epoch 1, Batch: 480, Loss: 0.686191
Train - Epoch 1, Batch: 490, Loss: 0.685245
Train - Epoch 1, Batch: 500, Loss: 0.686644
Train - Epoch 1, Batch: 510, Loss: 0.685669
Train - Epoch 1, Batch: 520, Loss: 0.685046
Train - Epoch 1, Batch: 530, Loss: 0.685548
Train - Epoch 1, Batch: 540, Loss: 0.686361
Train - Epoch 1, Batch: 550, Loss: 0.686687
Train - Epoch 1, Batch: 560, Loss: 0.686166
Train - Epoch 1, Batch: 570, Loss: 0.686115
Train - Epoch 1, Batch: 580, Loss: 0.685466
Train - Epoch 1, Batch: 590, Loss: 0.685020
Train - Epoch 1, Batch: 600, Loss: 0.686402
Train - Epoch 1, Batch: 610, Loss: 0.686275
Train - Epoch 1, Batch: 620, Loss: 0.686030
Train - Epoch 1, Batch: 630, Loss: 0.685921
Train - Epoch 1, Batch: 640, Loss: 0.685016
Train - Epoch 2, Batch: 0, Loss: 0.685981
Train - Epoch 2, Batch: 10, Loss: 0.684348
Train - Epoch 2, Batch: 20, Loss: 0.685320
Train - Epoch 2, Batch: 30, Loss: 0.685346
Train - Epoch 2, Batch: 40, Loss: 0.684291
Train - Epoch 2, Batch: 50, Loss: 0.685844
Train - Epoch 2, Batch: 60, Loss: 0.686199
Train - Epoch 2, Batch: 70, Loss: 0.685460
Train - Epoch 2, Batch: 80, Loss: 0.685393
Train - Epoch 2, Batch: 90, Loss: 0.684638
Train - Epoch 2, Batch: 100, Loss: 0.685842
Train - Epoch 2, Batch: 110, Loss: 0.684206
Train - Epoch 2, Batch: 120, Loss: 0.684019
Train - Epoch 2, Batch: 130, Loss: 0.685353
Train - Epoch 2, Batch: 140, Loss: 0.684700
Train - Epoch 2, Batch: 150, Loss: 0.684893
Train - Epoch 2, Batch: 160, Loss: 0.685715
Train - Epoch 2, Batch: 170, Loss: 0.684970
Train - Epoch 2, Batch: 180, Loss: 0.685552
Train - Epoch 2, Batch: 190, Loss: 0.685084
Train - Epoch 2, Batch: 200, Loss: 0.683779
Train - Epoch 2, Batch: 210, Loss: 0.684748
Train - Epoch 2, Batch: 220, Loss: 0.684528
Train - Epoch 2, Batch: 230, Loss: 0.685791
Train - Epoch 2, Batch: 240, Loss: 0.684605
Train - Epoch 2, Batch: 250, Loss: 0.684567
Train - Epoch 2, Batch: 260, Loss: 0.684088
Train - Epoch 2, Batch: 270, Loss: 0.684366
Train - Epoch 2, Batch: 280, Loss: 0.685809
Train - Epoch 2, Batch: 290, Loss: 0.684346
Train - Epoch 2, Batch: 300, Loss: 0.684955
Train - Epoch 2, Batch: 310, Loss: 0.685294
Train - Epoch 2, Batch: 320, Loss: 0.683741
Train - Epoch 2, Batch: 330, Loss: 0.683478
Train - Epoch 2, Batch: 340, Loss: 0.682943
Train - Epoch 2, Batch: 350, Loss: 0.685086
Train - Epoch 2, Batch: 360, Loss: 0.683784
Train - Epoch 2, Batch: 370, Loss: 0.684949
Train - Epoch 2, Batch: 380, Loss: 0.683826
Train - Epoch 2, Batch: 390, Loss: 0.686086
Train - Epoch 2, Batch: 400, Loss: 0.683996
Train - Epoch 2, Batch: 410, Loss: 0.685100
Train - Epoch 2, Batch: 420, Loss: 0.686190
Train - Epoch 2, Batch: 430, Loss: 0.684446
Train - Epoch 2, Batch: 440, Loss: 0.685169
Train - Epoch 2, Batch: 450, Loss: 0.685342
Train - Epoch 2, Batch: 460, Loss: 0.685030
Train - Epoch 2, Batch: 470, Loss: 0.684385
Train - Epoch 2, Batch: 480, Loss: 0.684625
Train - Epoch 2, Batch: 490, Loss: 0.685777
Train - Epoch 2, Batch: 500, Loss: 0.685423
Train - Epoch 2, Batch: 510, Loss: 0.684612
Train - Epoch 2, Batch: 520, Loss: 0.683852
Train - Epoch 2, Batch: 530, Loss: 0.684908
Train - Epoch 2, Batch: 540, Loss: 0.684471
Train - Epoch 2, Batch: 550, Loss: 0.684086/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684433
Train - Epoch 2, Batch: 570, Loss: 0.685389
Train - Epoch 2, Batch: 580, Loss: 0.684862
Train - Epoch 2, Batch: 590, Loss: 0.684631
Train - Epoch 2, Batch: 600, Loss: 0.684227
Train - Epoch 2, Batch: 610, Loss: 0.685108
Train - Epoch 2, Batch: 620, Loss: 0.684561
Train - Epoch 2, Batch: 630, Loss: 0.685863
Train - Epoch 2, Batch: 640, Loss: 0.685945
Train - Epoch 3, Batch: 0, Loss: 0.685656
Train - Epoch 3, Batch: 10, Loss: 0.684548
Train - Epoch 3, Batch: 20, Loss: 0.684401
Train - Epoch 3, Batch: 30, Loss: 0.683962
Train - Epoch 3, Batch: 40, Loss: 0.685184
Train - Epoch 3, Batch: 50, Loss: 0.684428
Train - Epoch 3, Batch: 60, Loss: 0.684907
Train - Epoch 3, Batch: 70, Loss: 0.685012
Train - Epoch 3, Batch: 80, Loss: 0.683029
Train - Epoch 3, Batch: 90, Loss: 0.685748
Train - Epoch 3, Batch: 100, Loss: 0.685819
Train - Epoch 3, Batch: 110, Loss: 0.685457
Train - Epoch 3, Batch: 120, Loss: 0.684404
Train - Epoch 3, Batch: 130, Loss: 0.684141
Train - Epoch 3, Batch: 140, Loss: 0.684894
Train - Epoch 3, Batch: 150, Loss: 0.684688
Train - Epoch 3, Batch: 160, Loss: 0.684686
Train - Epoch 3, Batch: 170, Loss: 0.683814
Train - Epoch 3, Batch: 180, Loss: 0.684660
Train - Epoch 3, Batch: 190, Loss: 0.685221
Train - Epoch 3, Batch: 200, Loss: 0.682409
Train - Epoch 3, Batch: 210, Loss: 0.685154
Train - Epoch 3, Batch: 220, Loss: 0.684706
Train - Epoch 3, Batch: 230, Loss: 0.683431
Train - Epoch 3, Batch: 240, Loss: 0.686347
Train - Epoch 3, Batch: 250, Loss: 0.684935
Train - Epoch 3, Batch: 260, Loss: 0.684433
Train - Epoch 3, Batch: 270, Loss: 0.683056
Train - Epoch 3, Batch: 280, Loss: 0.684788
Train - Epoch 3, Batch: 290, Loss: 0.685060
Train - Epoch 3, Batch: 300, Loss: 0.684809
Train - Epoch 3, Batch: 310, Loss: 0.684509
Train - Epoch 3, Batch: 320, Loss: 0.684269
Train - Epoch 3, Batch: 330, Loss: 0.683642
Train - Epoch 3, Batch: 340, Loss: 0.682972
Train - Epoch 3, Batch: 350, Loss: 0.683894
Train - Epoch 3, Batch: 360, Loss: 0.683123
Train - Epoch 3, Batch: 370, Loss: 0.682316
Train - Epoch 3, Batch: 380, Loss: 0.684322
Train - Epoch 3, Batch: 390, Loss: 0.684332
Train - Epoch 3, Batch: 400, Loss: 0.684818
Train - Epoch 3, Batch: 410, Loss: 0.684149
Train - Epoch 3, Batch: 420, Loss: 0.684222
Train - Epoch 3, Batch: 430, Loss: 0.684089
Train - Epoch 3, Batch: 440, Loss: 0.683503
Train - Epoch 3, Batch: 450, Loss: 0.682858
Train - Epoch 3, Batch: 460, Loss: 0.684188
Train - Epoch 3, Batch: 470, Loss: 0.684162
Train - Epoch 3, Batch: 480, Loss: 0.684666
Train - Epoch 3, Batch: 490, Loss: 0.683811
Train - Epoch 3, Batch: 500, Loss: 0.684489
Train - Epoch 3, Batch: 510, Loss: 0.685442
Train - Epoch 3, Batch: 520, Loss: 0.683253
Train - Epoch 3, Batch: 530, Loss: 0.683946
Train - Epoch 3, Batch: 540, Loss: 0.683244
Train - Epoch 3, Batch: 550, Loss: 0.683380
Train - Epoch 3, Batch: 560, Loss: 0.684513
Train - Epoch 3, Batch: 570, Loss: 0.683609
Train - Epoch 3, Batch: 580, Loss: 0.682914
Train - Epoch 3, Batch: 590, Loss: 0.683323
Train - Epoch 3, Batch: 600, Loss: 0.684516
Train - Epoch 3, Batch: 610, Loss: 0.684130
Train - Epoch 3, Batch: 620, Loss: 0.682433
Train - Epoch 3, Batch: 630, Loss: 0.683831
Train - Epoch 3, Batch: 640, Loss: 0.683113
training_time:: 7.665886878967285
training time full:: 7.6659252643585205
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.551502
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 21000
training time is 5.538191080093384
overhead:: 0
overhead2:: 0
time_baseline:: 5.541291952133179
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551476
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.15868353843688965
overhead3:: 0.21341371536254883
overhead4:: 0.7410430908203125
overhead5:: 0
time_provenance:: 2.8501176834106445
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551476
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.17427563667297363
overhead3:: 0.25786256790161133
overhead4:: 0.961932897567749
overhead5:: 0
time_provenance:: 2.983532428741455
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551476
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.20426201820373535
overhead3:: 0.28148388862609863
overhead4:: 1.0840952396392822
overhead5:: 0
time_provenance:: 3.175794839859009
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551476
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24993562698364258
overhead3:: 0.35983824729919434
overhead4:: 1.282458782196045
overhead5:: 0
time_provenance:: 3.4309194087982178
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551474
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.23293590545654297
overhead3:: 0.3338663578033447
overhead4:: 1.240464687347412
overhead5:: 0
time_provenance:: 3.623483180999756
curr_diff: 0 tensor(6.7624e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7624e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551482
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2720968723297119
overhead3:: 0.365856409072876
overhead4:: 1.5779054164886475
overhead5:: 0
time_provenance:: 4.469545364379883
curr_diff: 0 tensor(6.7277e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7277e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551482
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2533133029937744
overhead3:: 0.33510589599609375
overhead4:: 1.4030187129974365
overhead5:: 0
time_provenance:: 3.536067485809326
curr_diff: 0 tensor(6.6402e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6402e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551482
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3009035587310791
overhead3:: 0.43692612648010254
overhead4:: 1.6317765712738037
overhead5:: 0
time_provenance:: 4.1244823932647705
curr_diff: 0 tensor(6.6083e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6083e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551482
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.28432440757751465
overhead3:: 0.39621782302856445
overhead4:: 1.7126107215881348
overhead5:: 0
time_provenance:: 4.008584499359131
curr_diff: 0 tensor(3.6860e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6860e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3247218132019043
overhead3:: 0.44270825386047363
overhead4:: 1.715641975402832
overhead5:: 0
time_provenance:: 4.044803619384766
curr_diff: 0 tensor(4.5721e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5721e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3373992443084717
overhead3:: 0.45960545539855957
overhead4:: 1.8271434307098389
overhead5:: 0
time_provenance:: 4.1225810050964355
curr_diff: 0 tensor(4.7884e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7884e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551468
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.35661816596984863
overhead3:: 0.5106813907623291
overhead4:: 1.9603393077850342
overhead5:: 0
time_provenance:: 4.3373494148254395
curr_diff: 0 tensor(3.5943e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5943e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.558673620223999
overhead3:: 0.7596733570098877
overhead4:: 2.910797119140625
overhead5:: 0
time_provenance:: 6.542483329772949
curr_diff: 0 tensor(3.0429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4330918788909912
overhead3:: 0.5887196063995361
overhead4:: 2.3829915523529053
overhead5:: 0
time_provenance:: 5.004623651504517
curr_diff: 0 tensor(3.0342e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0342e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4792368412017822
overhead3:: 0.6872828006744385
overhead4:: 2.422752857208252
overhead5:: 0
time_provenance:: 5.1281890869140625
curr_diff: 0 tensor(2.9986e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9986e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4399878978729248
overhead3:: 0.6137990951538086
overhead4:: 2.4904870986938477
overhead5:: 0
time_provenance:: 4.999338865280151
curr_diff: 0 tensor(2.9736e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9736e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551478
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 21000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7964918613433838
overhead3:: 1.1240556240081787
overhead4:: 3.7013750076293945
overhead5:: 0
time_provenance:: 6.244109630584717
curr_diff: 0 tensor(2.3396e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3396e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551476
deletion rate:: 0.01
python3 generate_rand_ids 0.01  higgs 0
tensor([7864321, 5242887, 5505031,  ..., 4718586, 1572859, 1835006])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.695082
Train - Epoch 0, Batch: 10, Loss: 0.691769
Train - Epoch 0, Batch: 20, Loss: 0.691129
Train - Epoch 0, Batch: 30, Loss: 0.691252
Train - Epoch 0, Batch: 40, Loss: 0.690818
Train - Epoch 0, Batch: 50, Loss: 0.690398
Train - Epoch 0, Batch: 60, Loss: 0.690360
Train - Epoch 0, Batch: 70, Loss: 0.690739
Train - Epoch 0, Batch: 80, Loss: 0.689844
Train - Epoch 0, Batch: 90, Loss: 0.690584
Train - Epoch 0, Batch: 100, Loss: 0.688786
Train - Epoch 0, Batch: 110, Loss: 0.688758
Train - Epoch 0, Batch: 120, Loss: 0.688273
Train - Epoch 0, Batch: 130, Loss: 0.689659
Train - Epoch 0, Batch: 140, Loss: 0.689556
Train - Epoch 0, Batch: 150, Loss: 0.688976
Train - Epoch 0, Batch: 160, Loss: 0.688073
Train - Epoch 0, Batch: 170, Loss: 0.689298
Train - Epoch 0, Batch: 180, Loss: 0.689135
Train - Epoch 0, Batch: 190, Loss: 0.689277
Train - Epoch 0, Batch: 200, Loss: 0.688822
Train - Epoch 0, Batch: 210, Loss: 0.688275
Train - Epoch 0, Batch: 220, Loss: 0.688430
Train - Epoch 0, Batch: 230, Loss: 0.688982
Train - Epoch 0, Batch: 240, Loss: 0.688132
Train - Epoch 0, Batch: 250, Loss: 0.688551
Train - Epoch 0, Batch: 260, Loss: 0.688733
Train - Epoch 0, Batch: 270, Loss: 0.687090
Train - Epoch 0, Batch: 280, Loss: 0.688069
Train - Epoch 0, Batch: 290, Loss: 0.687612
Train - Epoch 0, Batch: 300, Loss: 0.688946
Train - Epoch 0, Batch: 310, Loss: 0.687934
Train - Epoch 0, Batch: 320, Loss: 0.688515
Train - Epoch 0, Batch: 330, Loss: 0.687777
Train - Epoch 0, Batch: 340, Loss: 0.688027
Train - Epoch 0, Batch: 350, Loss: 0.688252
Train - Epoch 0, Batch: 360, Loss: 0.686814
Train - Epoch 0, Batch: 370, Loss: 0.688420
Train - Epoch 0, Batch: 380, Loss: 0.688418
Train - Epoch 0, Batch: 390, Loss: 0.688526
Train - Epoch 0, Batch: 400, Loss: 0.687089
Train - Epoch 0, Batch: 410, Loss: 0.687698
Train - Epoch 0, Batch: 420, Loss: 0.687669
Train - Epoch 0, Batch: 430, Loss: 0.687137
Train - Epoch 0, Batch: 440, Loss: 0.688237
Train - Epoch 0, Batch: 450, Loss: 0.687228
Train - Epoch 0, Batch: 460, Loss: 0.687031
Train - Epoch 0, Batch: 470, Loss: 0.687090
Train - Epoch 0, Batch: 480, Loss: 0.687958
Train - Epoch 0, Batch: 490, Loss: 0.687592
Train - Epoch 0, Batch: 500, Loss: 0.688032
Train - Epoch 0, Batch: 510, Loss: 0.687525
Train - Epoch 0, Batch: 520, Loss: 0.687835
Train - Epoch 0, Batch: 530, Loss: 0.686314
Train - Epoch 0, Batch: 540, Loss: 0.685958
Train - Epoch 0, Batch: 550, Loss: 0.686088
Train - Epoch 0, Batch: 560, Loss: 0.687912
Train - Epoch 0, Batch: 570, Loss: 0.685749
Train - Epoch 0, Batch: 580, Loss: 0.686037
Train - Epoch 0, Batch: 590, Loss: 0.687375
Train - Epoch 0, Batch: 600, Loss: 0.687834
Train - Epoch 0, Batch: 610, Loss: 0.687384
Train - Epoch 0, Batch: 620, Loss: 0.687100
Train - Epoch 0, Batch: 630, Loss: 0.688135
Train - Epoch 0, Batch: 640, Loss: 0.685856
Train - Epoch 1, Batch: 0, Loss: 0.687768
Train - Epoch 1, Batch: 10, Loss: 0.686923
Train - Epoch 1, Batch: 20, Loss: 0.686434
Train - Epoch 1, Batch: 30, Loss: 0.687459
Train - Epoch 1, Batch: 40, Loss: 0.687027
Train - Epoch 1, Batch: 50, Loss: 0.686641
Train - Epoch 1, Batch: 60, Loss: 0.687135
Train - Epoch 1, Batch: 70, Loss: 0.686349
Train - Epoch 1, Batch: 80, Loss: 0.688298
Train - Epoch 1, Batch: 90, Loss: 0.687273
Train - Epoch 1, Batch: 100, Loss: 0.685726
Train - Epoch 1, Batch: 110, Loss: 0.686404
Train - Epoch 1, Batch: 120, Loss: 0.687021
Train - Epoch 1, Batch: 130, Loss: 0.685784
Train - Epoch 1, Batch: 140, Loss: 0.685830
Train - Epoch 1, Batch: 150, Loss: 0.686506
Train - Epoch 1, Batch: 160, Loss: 0.687256
Train - Epoch 1, Batch: 170, Loss: 0.686081
Train - Epoch 1, Batch: 180, Loss: 0.687623
Train - Epoch 1, Batch: 190, Loss: 0.686995
Train - Epoch 1, Batch: 200, Loss: 0.686989
Train - Epoch 1, Batch: 210, Loss: 0.686928
Train - Epoch 1, Batch: 220, Loss: 0.686069
Train - Epoch 1, Batch: 230, Loss: 0.686033
Train - Epoch 1, Batch: 240, Loss: 0.687239
Train - Epoch 1, Batch: 250, Loss: 0.685248
Train - Epoch 1, Batch: 260, Loss: 0.686241
Train - Epoch 1, Batch: 270, Loss: 0.685748
Train - Epoch 1, Batch: 280, Loss: 0.686589
Train - Epoch 1, Batch: 290, Loss: 0.685509
Train - Epoch 1, Batch: 300, Loss: 0.685695
Train - Epoch 1, Batch: 310, Loss: 0.686714
Train - Epoch 1, Batch: 320, Loss: 0.686833
Train - Epoch 1, Batch: 330, Loss: 0.686156
Train - Epoch 1, Batch: 340, Loss: 0.686684
Train - Epoch 1, Batch: 350, Loss: 0.686450
Train - Epoch 1, Batch: 360, Loss: 0.686348
Train - Epoch 1, Batch: 370, Loss: 0.685698
Train - Epoch 1, Batch: 380, Loss: 0.687679
Train - Epoch 1, Batch: 390, Loss: 0.686738
Train - Epoch 1, Batch: 400, Loss: 0.686205
Train - Epoch 1, Batch: 410, Loss: 0.686456
Train - Epoch 1, Batch: 420, Loss: 0.686416
Train - Epoch 1, Batch: 430, Loss: 0.685181
Train - Epoch 1, Batch: 440, Loss: 0.686043
Train - Epoch 1, Batch: 450, Loss: 0.685677
Train - Epoch 1, Batch: 460, Loss: 0.686294
Train - Epoch 1, Batch: 470, Loss: 0.685896
Train - Epoch 1, Batch: 480, Loss: 0.685408
Train - Epoch 1, Batch: 490, Loss: 0.686326
Train - Epoch 1, Batch: 500, Loss: 0.686525
Train - Epoch 1, Batch: 510, Loss: 0.684999
Train - Epoch 1, Batch: 520, Loss: 0.686077
Train - Epoch 1, Batch: 530, Loss: 0.686670
Train - Epoch 1, Batch: 540, Loss: 0.686030
Train - Epoch 1, Batch: 550, Loss: 0.684570
Train - Epoch 1, Batch: 560, Loss: 0.686039
Train - Epoch 1, Batch: 570, Loss: 0.684698
Train - Epoch 1, Batch: 580, Loss: 0.685382
Train - Epoch 1, Batch: 590, Loss: 0.684789
Train - Epoch 1, Batch: 600, Loss: 0.684817
Train - Epoch 1, Batch: 610, Loss: 0.686781
Train - Epoch 1, Batch: 620, Loss: 0.685628
Train - Epoch 1, Batch: 630, Loss: 0.686428
Train - Epoch 1, Batch: 640, Loss: 0.684924
Train - Epoch 2, Batch: 0, Loss: 0.686195
Train - Epoch 2, Batch: 10, Loss: 0.686685
Train - Epoch 2, Batch: 20, Loss: 0.684529
Train - Epoch 2, Batch: 30, Loss: 0.685108
Train - Epoch 2, Batch: 40, Loss: 0.684913
Train - Epoch 2, Batch: 50, Loss: 0.686230
Train - Epoch 2, Batch: 60, Loss: 0.684942
Train - Epoch 2, Batch: 70, Loss: 0.684538
Train - Epoch 2, Batch: 80, Loss: 0.685937
Train - Epoch 2, Batch: 90, Loss: 0.685195
Train - Epoch 2, Batch: 100, Loss: 0.685271
Train - Epoch 2, Batch: 110, Loss: 0.685806
Train - Epoch 2, Batch: 120, Loss: 0.684292
Train - Epoch 2, Batch: 130, Loss: 0.686501
Train - Epoch 2, Batch: 140, Loss: 0.684430
Train - Epoch 2, Batch: 150, Loss: 0.684228
Train - Epoch 2, Batch: 160, Loss: 0.685173
Train - Epoch 2, Batch: 170, Loss: 0.686758
Train - Epoch 2, Batch: 180, Loss: 0.685226
Train - Epoch 2, Batch: 190, Loss: 0.686187
Train - Epoch 2, Batch: 200, Loss: 0.685572
Train - Epoch 2, Batch: 210, Loss: 0.685509
Train - Epoch 2, Batch: 220, Loss: 0.684738
Train - Epoch 2, Batch: 230, Loss: 0.685646
Train - Epoch 2, Batch: 240, Loss: 0.685142
Train - Epoch 2, Batch: 250, Loss: 0.683736
Train - Epoch 2, Batch: 260, Loss: 0.683485
Train - Epoch 2, Batch: 270, Loss: 0.686026
Train - Epoch 2, Batch: 280, Loss: 0.685064
Train - Epoch 2, Batch: 290, Loss: 0.685186
Train - Epoch 2, Batch: 300, Loss: 0.682712
Train - Epoch 2, Batch: 310, Loss: 0.684008
Train - Epoch 2, Batch: 320, Loss: 0.685269
Train - Epoch 2, Batch: 330, Loss: 0.684614
Train - Epoch 2, Batch: 340, Loss: 0.685681
Train - Epoch 2, Batch: 350, Loss: 0.684360
Train - Epoch 2, Batch: 360, Loss: 0.684085
Train - Epoch 2, Batch: 370, Loss: 0.684044
Train - Epoch 2, Batch: 380, Loss: 0.685408
Train - Epoch 2, Batch: 390, Loss: 0.684544
Train - Epoch 2, Batch: 400, Loss: 0.684395
Train - Epoch 2, Batch: 410, Loss: 0.684555
Train - Epoch 2, Batch: 420, Loss: 0.685628
Train - Epoch 2, Batch: 430, Loss: 0.684755
Train - Epoch 2, Batch: 440, Loss: 0.683781
Train - Epoch 2, Batch: 450, Loss: 0.685151
Train - Epoch 2, Batch: 460, Loss: 0.684051
Train - Epoch 2, Batch: 470, Loss: 0.684943
Train - Epoch 2, Batch: 480, Loss: 0.685534
Train - Epoch 2, Batch: 490, Loss: 0.684227
Train - Epoch 2, Batch: 500, Loss: 0.684061
Train - Epoch 2, Batch: 510, Loss: 0.685133
Train - Epoch 2, Batch: 520, Loss: 0.683649
Train - Epoch 2, Batch: 530, Loss: 0.685288
Train - Epoch 2, Batch: 540, Loss: 0.683756
Train - Epoch 2, Batch: 550, Loss: 0.685111/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684996
Train - Epoch 2, Batch: 570, Loss: 0.684735
Train - Epoch 2, Batch: 580, Loss: 0.686528
Train - Epoch 2, Batch: 590, Loss: 0.683987
Train - Epoch 2, Batch: 600, Loss: 0.684998
Train - Epoch 2, Batch: 610, Loss: 0.684736
Train - Epoch 2, Batch: 620, Loss: 0.684494
Train - Epoch 2, Batch: 630, Loss: 0.684807
Train - Epoch 2, Batch: 640, Loss: 0.684440
Train - Epoch 3, Batch: 0, Loss: 0.684852
Train - Epoch 3, Batch: 10, Loss: 0.683832
Train - Epoch 3, Batch: 20, Loss: 0.683899
Train - Epoch 3, Batch: 30, Loss: 0.684487
Train - Epoch 3, Batch: 40, Loss: 0.684396
Train - Epoch 3, Batch: 50, Loss: 0.683808
Train - Epoch 3, Batch: 60, Loss: 0.684396
Train - Epoch 3, Batch: 70, Loss: 0.684671
Train - Epoch 3, Batch: 80, Loss: 0.684772
Train - Epoch 3, Batch: 90, Loss: 0.683963
Train - Epoch 3, Batch: 100, Loss: 0.684935
Train - Epoch 3, Batch: 110, Loss: 0.684254
Train - Epoch 3, Batch: 120, Loss: 0.684511
Train - Epoch 3, Batch: 130, Loss: 0.684309
Train - Epoch 3, Batch: 140, Loss: 0.683272
Train - Epoch 3, Batch: 150, Loss: 0.685833
Train - Epoch 3, Batch: 160, Loss: 0.684787
Train - Epoch 3, Batch: 170, Loss: 0.685655
Train - Epoch 3, Batch: 180, Loss: 0.684696
Train - Epoch 3, Batch: 190, Loss: 0.685782
Train - Epoch 3, Batch: 200, Loss: 0.683456
Train - Epoch 3, Batch: 210, Loss: 0.683598
Train - Epoch 3, Batch: 220, Loss: 0.684073
Train - Epoch 3, Batch: 230, Loss: 0.684336
Train - Epoch 3, Batch: 240, Loss: 0.684521
Train - Epoch 3, Batch: 250, Loss: 0.683886
Train - Epoch 3, Batch: 260, Loss: 0.684366
Train - Epoch 3, Batch: 270, Loss: 0.682956
Train - Epoch 3, Batch: 280, Loss: 0.684461
Train - Epoch 3, Batch: 290, Loss: 0.683622
Train - Epoch 3, Batch: 300, Loss: 0.684071
Train - Epoch 3, Batch: 310, Loss: 0.682710
Train - Epoch 3, Batch: 320, Loss: 0.684252
Train - Epoch 3, Batch: 330, Loss: 0.684137
Train - Epoch 3, Batch: 340, Loss: 0.683437
Train - Epoch 3, Batch: 350, Loss: 0.682766
Train - Epoch 3, Batch: 360, Loss: 0.683426
Train - Epoch 3, Batch: 370, Loss: 0.683878
Train - Epoch 3, Batch: 380, Loss: 0.683576
Train - Epoch 3, Batch: 390, Loss: 0.683889
Train - Epoch 3, Batch: 400, Loss: 0.684662
Train - Epoch 3, Batch: 410, Loss: 0.684119
Train - Epoch 3, Batch: 420, Loss: 0.683612
Train - Epoch 3, Batch: 430, Loss: 0.683570
Train - Epoch 3, Batch: 440, Loss: 0.683085
Train - Epoch 3, Batch: 450, Loss: 0.684181
Train - Epoch 3, Batch: 460, Loss: 0.684882
Train - Epoch 3, Batch: 470, Loss: 0.684619
Train - Epoch 3, Batch: 480, Loss: 0.684225
Train - Epoch 3, Batch: 490, Loss: 0.683831
Train - Epoch 3, Batch: 500, Loss: 0.684669
Train - Epoch 3, Batch: 510, Loss: 0.683786
Train - Epoch 3, Batch: 520, Loss: 0.684017
Train - Epoch 3, Batch: 530, Loss: 0.684316
Train - Epoch 3, Batch: 540, Loss: 0.684538
Train - Epoch 3, Batch: 550, Loss: 0.683832
Train - Epoch 3, Batch: 560, Loss: 0.684003
Train - Epoch 3, Batch: 570, Loss: 0.684676
Train - Epoch 3, Batch: 580, Loss: 0.682748
Train - Epoch 3, Batch: 590, Loss: 0.684827
Train - Epoch 3, Batch: 600, Loss: 0.683841
Train - Epoch 3, Batch: 610, Loss: 0.684311
Train - Epoch 3, Batch: 620, Loss: 0.684296
Train - Epoch 3, Batch: 630, Loss: 0.683576
Train - Epoch 3, Batch: 640, Loss: 0.682873
training_time:: 7.676108121871948
training time full:: 7.676147937774658
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.552276
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 105000
training time is 4.651823043823242
overhead:: 0
overhead2:: 0
time_baseline:: 4.655131578445435
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552270
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.18022942543029785
overhead3:: 0.2884852886199951
overhead4:: 0.9156894683837891
overhead5:: 0
time_provenance:: 3.180136203765869
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552274
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1905677318572998
overhead3:: 0.24789786338806152
overhead4:: 0.8983407020568848
overhead5:: 0
time_provenance:: 3.1157336235046387
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552274
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2032618522644043
overhead3:: 0.2912907600402832
overhead4:: 1.1397778987884521
overhead5:: 0
time_provenance:: 3.320204973220825
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552274
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.21431756019592285
overhead3:: 0.28812313079833984
overhead4:: 1.2102046012878418
overhead5:: 0
time_provenance:: 3.335536479949951
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552274
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22437596321105957
overhead3:: 0.3181443214416504
overhead4:: 1.2987799644470215
overhead5:: 0
time_provenance:: 3.564708948135376
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552278
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.27417612075805664
overhead3:: 0.38025712966918945
overhead4:: 1.488877296447754
overhead5:: 0
time_provenance:: 4.41200852394104
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552278
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.26349759101867676
overhead3:: 0.3555295467376709
overhead4:: 1.4117724895477295
overhead5:: 0
time_provenance:: 3.6542253494262695
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552278
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3454916477203369
overhead3:: 0.4903905391693115
overhead4:: 1.6474521160125732
overhead5:: 0
time_provenance:: 4.6198461055755615
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552278
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3192636966705322
overhead3:: 0.4871950149536133
overhead4:: 1.8174386024475098
overhead5:: 0
time_provenance:: 4.343372106552124
curr_diff: 0 tensor(6.3575e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3575e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552260
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.31613779067993164
overhead3:: 0.4318075180053711
overhead4:: 1.8611993789672852
overhead5:: 0
time_provenance:: 4.27850341796875
curr_diff: 0 tensor(8.8283e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8283e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552260
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3816111087799072
overhead3:: 0.5489857196807861
overhead4:: 2.015052080154419
overhead5:: 0
time_provenance:: 4.590490341186523
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552236
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.35543322563171387
overhead3:: 0.4927799701690674
overhead4:: 2.0219626426696777
overhead5:: 0
time_provenance:: 4.448232412338257
curr_diff: 0 tensor(6.2600e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2600e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552260
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.40177273750305176
overhead3:: 0.5583865642547607
overhead4:: 2.405747652053833
overhead5:: 0
time_provenance:: 5.033067226409912
curr_diff: 0 tensor(4.0246e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0246e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552276
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4246551990509033
overhead3:: 0.6021339893341064
overhead4:: 2.3224995136260986
overhead5:: 0
time_provenance:: 4.952832937240601
curr_diff: 0 tensor(3.9538e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9538e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552276
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4684765338897705
overhead3:: 0.6279301643371582
overhead4:: 2.5229032039642334
overhead5:: 0
time_provenance:: 5.224561929702759
curr_diff: 0 tensor(3.9404e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9404e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552276
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.46349430084228516
overhead3:: 0.6493275165557861
overhead4:: 2.5136373043060303
overhead5:: 0
time_provenance:: 5.170678377151489
curr_diff: 0 tensor(3.9086e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9086e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552276
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7610783576965332
overhead3:: 1.0332403182983398
overhead4:: 3.608100175857544
overhead5:: 0
time_provenance:: 6.098373889923096
curr_diff: 0 tensor(2.6929e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6929e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552270
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.693310
Train - Epoch 0, Batch: 10, Loss: 0.690523
Train - Epoch 0, Batch: 20, Loss: 0.691005
Train - Epoch 0, Batch: 30, Loss: 0.689464
Train - Epoch 0, Batch: 40, Loss: 0.689700
Train - Epoch 0, Batch: 50, Loss: 0.689691
Train - Epoch 0, Batch: 60, Loss: 0.689383
Train - Epoch 0, Batch: 70, Loss: 0.689217
Train - Epoch 0, Batch: 80, Loss: 0.689878
Train - Epoch 0, Batch: 90, Loss: 0.688462
Train - Epoch 0, Batch: 100, Loss: 0.689590
Train - Epoch 0, Batch: 110, Loss: 0.687382
Train - Epoch 0, Batch: 120, Loss: 0.688519
Train - Epoch 0, Batch: 130, Loss: 0.687440
Train - Epoch 0, Batch: 140, Loss: 0.686918
Train - Epoch 0, Batch: 150, Loss: 0.687527
Train - Epoch 0, Batch: 160, Loss: 0.688212
Train - Epoch 0, Batch: 170, Loss: 0.687420
Train - Epoch 0, Batch: 180, Loss: 0.687681
Train - Epoch 0, Batch: 190, Loss: 0.688399
Train - Epoch 0, Batch: 200, Loss: 0.687227
Train - Epoch 0, Batch: 210, Loss: 0.687578
Train - Epoch 0, Batch: 220, Loss: 0.687506
Train - Epoch 0, Batch: 230, Loss: 0.688364
Train - Epoch 0, Batch: 240, Loss: 0.688125
Train - Epoch 0, Batch: 250, Loss: 0.687004
Train - Epoch 0, Batch: 260, Loss: 0.687602
Train - Epoch 0, Batch: 270, Loss: 0.685600
Train - Epoch 0, Batch: 280, Loss: 0.686726
Train - Epoch 0, Batch: 290, Loss: 0.687408
Train - Epoch 0, Batch: 300, Loss: 0.687836
Train - Epoch 0, Batch: 310, Loss: 0.686874
Train - Epoch 0, Batch: 320, Loss: 0.686501
Train - Epoch 0, Batch: 330, Loss: 0.687323
Train - Epoch 0, Batch: 340, Loss: 0.686835
Train - Epoch 0, Batch: 350, Loss: 0.686188
Train - Epoch 0, Batch: 360, Loss: 0.686084
Train - Epoch 0, Batch: 370, Loss: 0.687032
Train - Epoch 0, Batch: 380, Loss: 0.687524
Train - Epoch 0, Batch: 390, Loss: 0.685870
Train - Epoch 0, Batch: 400, Loss: 0.685913
Train - Epoch 0, Batch: 410, Loss: 0.686075
Train - Epoch 0, Batch: 420, Loss: 0.687306
Train - Epoch 0, Batch: 430, Loss: 0.686019
Train - Epoch 0, Batch: 440, Loss: 0.685952
Train - Epoch 0, Batch: 450, Loss: 0.685968
Train - Epoch 0, Batch: 460, Loss: 0.687278
Train - Epoch 0, Batch: 470, Loss: 0.685716
Train - Epoch 0, Batch: 480, Loss: 0.686038
Train - Epoch 0, Batch: 490, Loss: 0.686305
Train - Epoch 0, Batch: 500, Loss: 0.685157
Train - Epoch 0, Batch: 510, Loss: 0.685859
Train - Epoch 0, Batch: 520, Loss: 0.686128
Train - Epoch 0, Batch: 530, Loss: 0.686446
Train - Epoch 0, Batch: 540, Loss: 0.685191
Train - Epoch 0, Batch: 550, Loss: 0.686598
Train - Epoch 0, Batch: 560, Loss: 0.687003
Train - Epoch 0, Batch: 570, Loss: 0.687292
Train - Epoch 0, Batch: 580, Loss: 0.684510
Train - Epoch 0, Batch: 590, Loss: 0.685968
Train - Epoch 0, Batch: 600, Loss: 0.684934
Train - Epoch 0, Batch: 610, Loss: 0.685649
Train - Epoch 0, Batch: 620, Loss: 0.685463
Train - Epoch 0, Batch: 630, Loss: 0.688587
Train - Epoch 0, Batch: 640, Loss: 0.685843
Train - Epoch 1, Batch: 0, Loss: 0.685732
Train - Epoch 1, Batch: 10, Loss: 0.687178
Train - Epoch 1, Batch: 20, Loss: 0.687129
Train - Epoch 1, Batch: 30, Loss: 0.686114
Train - Epoch 1, Batch: 40, Loss: 0.687053
Train - Epoch 1, Batch: 50, Loss: 0.685457
Train - Epoch 1, Batch: 60, Loss: 0.685907
Train - Epoch 1, Batch: 70, Loss: 0.685526
Train - Epoch 1, Batch: 80, Loss: 0.685998
Train - Epoch 1, Batch: 90, Loss: 0.686009
Train - Epoch 1, Batch: 100, Loss: 0.685777
Train - Epoch 1, Batch: 110, Loss: 0.685622
Train - Epoch 1, Batch: 120, Loss: 0.686094
Train - Epoch 1, Batch: 130, Loss: 0.685971
Train - Epoch 1, Batch: 140, Loss: 0.685819
Train - Epoch 1, Batch: 150, Loss: 0.685802
Train - Epoch 1, Batch: 160, Loss: 0.685815
Train - Epoch 1, Batch: 170, Loss: 0.685941
Train - Epoch 1, Batch: 180, Loss: 0.685769
Train - Epoch 1, Batch: 190, Loss: 0.684796
Train - Epoch 1, Batch: 200, Loss: 0.686049
Train - Epoch 1, Batch: 210, Loss: 0.685450
Train - Epoch 1, Batch: 220, Loss: 0.686018
Train - Epoch 1, Batch: 230, Loss: 0.685434
Train - Epoch 1, Batch: 240, Loss: 0.684740
Train - Epoch 1, Batch: 250, Loss: 0.685150
Train - Epoch 1, Batch: 260, Loss: 0.685644
Train - Epoch 1, Batch: 270, Loss: 0.685843
Train - Epoch 1, Batch: 280, Loss: 0.686223
Train - Epoch 1, Batch: 290, Loss: 0.684090
Train - Epoch 1, Batch: 300, Loss: 0.686328
Train - Epoch 1, Batch: 310, Loss: 0.685112
Train - Epoch 1, Batch: 320, Loss: 0.685265
Train - Epoch 1, Batch: 330, Loss: 0.685441
Train - Epoch 1, Batch: 340, Loss: 0.684219
Train - Epoch 1, Batch: 350, Loss: 0.685052
Train - Epoch 1, Batch: 360, Loss: 0.684653
Train - Epoch 1, Batch: 370, Loss: 0.685834
Train - Epoch 1, Batch: 380, Loss: 0.684960
Train - Epoch 1, Batch: 390, Loss: 0.685770
Train - Epoch 1, Batch: 400, Loss: 0.685770
Train - Epoch 1, Batch: 410, Loss: 0.685234
Train - Epoch 1, Batch: 420, Loss: 0.686412
Train - Epoch 1, Batch: 430, Loss: 0.684598
Train - Epoch 1, Batch: 440, Loss: 0.684268
Train - Epoch 1, Batch: 450, Loss: 0.683953
Train - Epoch 1, Batch: 460, Loss: 0.684250
Train - Epoch 1, Batch: 470, Loss: 0.685764
Train - Epoch 1, Batch: 480, Loss: 0.685731
Train - Epoch 1, Batch: 490, Loss: 0.684078
Train - Epoch 1, Batch: 500, Loss: 0.684614
Train - Epoch 1, Batch: 510, Loss: 0.685130
Train - Epoch 1, Batch: 520, Loss: 0.684349
Train - Epoch 1, Batch: 530, Loss: 0.684909
Train - Epoch 1, Batch: 540, Loss: 0.685219
Train - Epoch 1, Batch: 550, Loss: 0.684124
Train - Epoch 1, Batch: 560, Loss: 0.685849
Train - Epoch 1, Batch: 570, Loss: 0.684042
Train - Epoch 1, Batch: 580, Loss: 0.685547
Train - Epoch 1, Batch: 590, Loss: 0.684631
Train - Epoch 1, Batch: 600, Loss: 0.684812
Train - Epoch 1, Batch: 610, Loss: 0.685681
Train - Epoch 1, Batch: 620, Loss: 0.685773
Train - Epoch 1, Batch: 630, Loss: 0.685276
Train - Epoch 1, Batch: 640, Loss: 0.686238
Train - Epoch 2, Batch: 0, Loss: 0.684767
Train - Epoch 2, Batch: 10, Loss: 0.684221
Train - Epoch 2, Batch: 20, Loss: 0.685346
Train - Epoch 2, Batch: 30, Loss: 0.685121
Train - Epoch 2, Batch: 40, Loss: 0.685094
Train - Epoch 2, Batch: 50, Loss: 0.684730
Train - Epoch 2, Batch: 60, Loss: 0.685044
Train - Epoch 2, Batch: 70, Loss: 0.683978
Train - Epoch 2, Batch: 80, Loss: 0.684106
Train - Epoch 2, Batch: 90, Loss: 0.685019
Train - Epoch 2, Batch: 100, Loss: 0.684077
Train - Epoch 2, Batch: 110, Loss: 0.684193
Train - Epoch 2, Batch: 120, Loss: 0.685267
Train - Epoch 2, Batch: 130, Loss: 0.685870
Train - Epoch 2, Batch: 140, Loss: 0.684885
Train - Epoch 2, Batch: 150, Loss: 0.686003
Train - Epoch 2, Batch: 160, Loss: 0.685297
Train - Epoch 2, Batch: 170, Loss: 0.683879
Train - Epoch 2, Batch: 180, Loss: 0.684121
Train - Epoch 2, Batch: 190, Loss: 0.685288
Train - Epoch 2, Batch: 200, Loss: 0.684394
Train - Epoch 2, Batch: 210, Loss: 0.685379
Train - Epoch 2, Batch: 220, Loss: 0.684887
Train - Epoch 2, Batch: 230, Loss: 0.685135
Train - Epoch 2, Batch: 240, Loss: 0.683972
Train - Epoch 2, Batch: 250, Loss: 0.684149
Train - Epoch 2, Batch: 260, Loss: 0.684099
Train - Epoch 2, Batch: 270, Loss: 0.684839
Train - Epoch 2, Batch: 280, Loss: 0.685215
Train - Epoch 2, Batch: 290, Loss: 0.684594
Train - Epoch 2, Batch: 300, Loss: 0.684602
Train - Epoch 2, Batch: 310, Loss: 0.684494
Train - Epoch 2, Batch: 320, Loss: 0.683472
Train - Epoch 2, Batch: 330, Loss: 0.684350
Train - Epoch 2, Batch: 340, Loss: 0.685107
Train - Epoch 2, Batch: 350, Loss: 0.685918
Train - Epoch 2, Batch: 360, Loss: 0.684067
Train - Epoch 2, Batch: 370, Loss: 0.683673
Train - Epoch 2, Batch: 380, Loss: 0.686225
Train - Epoch 2, Batch: 390, Loss: 0.685245
Train - Epoch 2, Batch: 400, Loss: 0.684764
Train - Epoch 2, Batch: 410, Loss: 0.683467
Train - Epoch 2, Batch: 420, Loss: 0.684622
Train - Epoch 2, Batch: 430, Loss: 0.684424
Train - Epoch 2, Batch: 440, Loss: 0.685423
Train - Epoch 2, Batch: 450, Loss: 0.683407
Train - Epoch 2, Batch: 460, Loss: 0.684975
Train - Epoch 2, Batch: 470, Loss: 0.684516
Train - Epoch 2, Batch: 480, Loss: 0.683894
Train - Epoch 2, Batch: 490, Loss: 0.684567
Train - Epoch 2, Batch: 500, Loss: 0.684337
Train - Epoch 2, Batch: 510, Loss: 0.683445
Train - Epoch 2, Batch: 520, Loss: 0.684064
Train - Epoch 2, Batch: 530, Loss: 0.683933
Train - Epoch 2, Batch: 540, Loss: 0.684452
Train - Epoch 2, Batch: 550, Loss: 0.684025/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684289
Train - Epoch 2, Batch: 570, Loss: 0.684681
Train - Epoch 2, Batch: 580, Loss: 0.683970
Train - Epoch 2, Batch: 590, Loss: 0.684520
Train - Epoch 2, Batch: 600, Loss: 0.683163
Train - Epoch 2, Batch: 610, Loss: 0.684078
Train - Epoch 2, Batch: 620, Loss: 0.683111
Train - Epoch 2, Batch: 630, Loss: 0.685504
Train - Epoch 2, Batch: 640, Loss: 0.684479
Train - Epoch 3, Batch: 0, Loss: 0.684553
Train - Epoch 3, Batch: 10, Loss: 0.684784
Train - Epoch 3, Batch: 20, Loss: 0.684060
Train - Epoch 3, Batch: 30, Loss: 0.683911
Train - Epoch 3, Batch: 40, Loss: 0.683990
Train - Epoch 3, Batch: 50, Loss: 0.684344
Train - Epoch 3, Batch: 60, Loss: 0.683543
Train - Epoch 3, Batch: 70, Loss: 0.685875
Train - Epoch 3, Batch: 80, Loss: 0.685495
Train - Epoch 3, Batch: 90, Loss: 0.684438
Train - Epoch 3, Batch: 100, Loss: 0.685741
Train - Epoch 3, Batch: 110, Loss: 0.684565
Train - Epoch 3, Batch: 120, Loss: 0.682831
Train - Epoch 3, Batch: 130, Loss: 0.683433
Train - Epoch 3, Batch: 140, Loss: 0.683398
Train - Epoch 3, Batch: 150, Loss: 0.684089
Train - Epoch 3, Batch: 160, Loss: 0.686087
Train - Epoch 3, Batch: 170, Loss: 0.682314
Train - Epoch 3, Batch: 180, Loss: 0.684112
Train - Epoch 3, Batch: 190, Loss: 0.684313
Train - Epoch 3, Batch: 200, Loss: 0.684025
Train - Epoch 3, Batch: 210, Loss: 0.684798
Train - Epoch 3, Batch: 220, Loss: 0.684386
Train - Epoch 3, Batch: 230, Loss: 0.684230
Train - Epoch 3, Batch: 240, Loss: 0.684462
Train - Epoch 3, Batch: 250, Loss: 0.684504
Train - Epoch 3, Batch: 260, Loss: 0.684881
Train - Epoch 3, Batch: 270, Loss: 0.683933
Train - Epoch 3, Batch: 280, Loss: 0.684284
Train - Epoch 3, Batch: 290, Loss: 0.682826
Train - Epoch 3, Batch: 300, Loss: 0.682831
Train - Epoch 3, Batch: 310, Loss: 0.684056
Train - Epoch 3, Batch: 320, Loss: 0.684303
Train - Epoch 3, Batch: 330, Loss: 0.683848
Train - Epoch 3, Batch: 340, Loss: 0.683813
Train - Epoch 3, Batch: 350, Loss: 0.683477
Train - Epoch 3, Batch: 360, Loss: 0.684715
Train - Epoch 3, Batch: 370, Loss: 0.683050
Train - Epoch 3, Batch: 380, Loss: 0.684764
Train - Epoch 3, Batch: 390, Loss: 0.683818
Train - Epoch 3, Batch: 400, Loss: 0.684567
Train - Epoch 3, Batch: 410, Loss: 0.683810
Train - Epoch 3, Batch: 420, Loss: 0.684675
Train - Epoch 3, Batch: 430, Loss: 0.684919
Train - Epoch 3, Batch: 440, Loss: 0.684527
Train - Epoch 3, Batch: 450, Loss: 0.683563
Train - Epoch 3, Batch: 460, Loss: 0.683704
Train - Epoch 3, Batch: 470, Loss: 0.684724
Train - Epoch 3, Batch: 480, Loss: 0.683380
Train - Epoch 3, Batch: 490, Loss: 0.682579
Train - Epoch 3, Batch: 500, Loss: 0.683906
Train - Epoch 3, Batch: 510, Loss: 0.684559
Train - Epoch 3, Batch: 520, Loss: 0.684009
Train - Epoch 3, Batch: 530, Loss: 0.684547
Train - Epoch 3, Batch: 540, Loss: 0.682810
Train - Epoch 3, Batch: 550, Loss: 0.683492
Train - Epoch 3, Batch: 560, Loss: 0.684443
Train - Epoch 3, Batch: 570, Loss: 0.683336
Train - Epoch 3, Batch: 580, Loss: 0.683449
Train - Epoch 3, Batch: 590, Loss: 0.683760
Train - Epoch 3, Batch: 600, Loss: 0.682241
Train - Epoch 3, Batch: 610, Loss: 0.683455
Train - Epoch 3, Batch: 620, Loss: 0.683402
Train - Epoch 3, Batch: 630, Loss: 0.683245
Train - Epoch 3, Batch: 640, Loss: 0.683213
training_time:: 7.835225582122803
training time full:: 7.83526611328125
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000042, Accuracy: 0.556110
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 105000
training time is 5.464434862136841
overhead:: 0
overhead2:: 0
time_baseline:: 5.467568397521973
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556124
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.14663028717041016
overhead3:: 0.19812917709350586
overhead4:: 0.8186588287353516
overhead5:: 0
time_provenance:: 2.9610235691070557
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.18706369400024414
overhead3:: 0.2706434726715088
overhead4:: 0.9744625091552734
overhead5:: 0
time_provenance:: 3.1485390663146973
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1964123249053955
overhead3:: 0.2625291347503662
overhead4:: 1.048936128616333
overhead5:: 0
time_provenance:: 3.157052993774414
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.21510624885559082
overhead3:: 0.2827162742614746
overhead4:: 1.1551227569580078
overhead5:: 0
time_provenance:: 3.233847141265869
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.23331737518310547
overhead3:: 0.31586313247680664
overhead4:: 1.2574448585510254
overhead5:: 0
time_provenance:: 3.566335678100586
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2485971450805664
overhead3:: 0.34825801849365234
overhead4:: 1.3545567989349365
overhead5:: 0
time_provenance:: 3.6240458488464355
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24414277076721191
overhead3:: 0.33020877838134766
overhead4:: 1.3968372344970703
overhead5:: 0
time_provenance:: 3.6032145023345947
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.34012317657470703
overhead3:: 0.5279006958007812
overhead4:: 1.7194852828979492
overhead5:: 0
time_provenance:: 4.177534103393555
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556120
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.30373239517211914
overhead3:: 0.43943095207214355
overhead4:: 1.7500858306884766
overhead5:: 0
time_provenance:: 4.201387166976929
curr_diff: 0 tensor(7.9035e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9035e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556110
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.30782103538513184
overhead3:: 0.4174020290374756
overhead4:: 1.8457577228546143
overhead5:: 0
time_provenance:: 4.220235586166382
curr_diff: 0 tensor(7.0988e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0988e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556130
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3584935665130615
overhead3:: 0.47186994552612305
overhead4:: 1.9229390621185303
overhead5:: 0
time_provenance:: 4.399517059326172
curr_diff: 0 tensor(4.8975e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8975e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556126
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.33780837059020996
overhead3:: 0.46925783157348633
overhead4:: 1.9668054580688477
overhead5:: 0
time_provenance:: 4.363228797912598
curr_diff: 0 tensor(7.7834e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7834e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556110
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4259307384490967
overhead3:: 0.6295788288116455
overhead4:: 2.501852035522461
overhead5:: 0
time_provenance:: 5.247954368591309
curr_diff: 0 tensor(4.3377e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3377e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556132
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.511838436126709
overhead3:: 0.6864016056060791
overhead4:: 2.856421709060669
overhead5:: 0
time_provenance:: 6.315805435180664
curr_diff: 0 tensor(4.3310e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3310e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556132
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4467771053314209
overhead3:: 0.5724103450775146
overhead4:: 2.470452308654785
overhead5:: 0
time_provenance:: 5.098870754241943
curr_diff: 0 tensor(4.2970e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2970e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556130
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.45148205757141113
overhead3:: 0.6082777976989746
overhead4:: 2.5968821048736572
overhead5:: 0
time_provenance:: 5.271620512008667
curr_diff: 0 tensor(4.2663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2663e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556132
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.770359992980957
overhead3:: 1.056591272354126
overhead4:: 3.5919063091278076
overhead5:: 0
time_provenance:: 6.099451780319214
curr_diff: 0 tensor(2.6160e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6160e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.556124
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.703079
Train - Epoch 0, Batch: 10, Loss: 0.697350
Train - Epoch 0, Batch: 20, Loss: 0.697476
Train - Epoch 0, Batch: 30, Loss: 0.695060
Train - Epoch 0, Batch: 40, Loss: 0.694438
Train - Epoch 0, Batch: 50, Loss: 0.695402
Train - Epoch 0, Batch: 60, Loss: 0.694568
Train - Epoch 0, Batch: 70, Loss: 0.693929
Train - Epoch 0, Batch: 80, Loss: 0.692761
Train - Epoch 0, Batch: 90, Loss: 0.692624
Train - Epoch 0, Batch: 100, Loss: 0.691791
Train - Epoch 0, Batch: 110, Loss: 0.692755
Train - Epoch 0, Batch: 120, Loss: 0.692428
Train - Epoch 0, Batch: 130, Loss: 0.692384
Train - Epoch 0, Batch: 140, Loss: 0.691736
Train - Epoch 0, Batch: 150, Loss: 0.691191
Train - Epoch 0, Batch: 160, Loss: 0.690451
Train - Epoch 0, Batch: 170, Loss: 0.691021
Train - Epoch 0, Batch: 180, Loss: 0.691520
Train - Epoch 0, Batch: 190, Loss: 0.690983
Train - Epoch 0, Batch: 200, Loss: 0.690805
Train - Epoch 0, Batch: 210, Loss: 0.690374
Train - Epoch 0, Batch: 220, Loss: 0.690324
Train - Epoch 0, Batch: 230, Loss: 0.690742
Train - Epoch 0, Batch: 240, Loss: 0.689831
Train - Epoch 0, Batch: 250, Loss: 0.690460
Train - Epoch 0, Batch: 260, Loss: 0.690160
Train - Epoch 0, Batch: 270, Loss: 0.690821
Train - Epoch 0, Batch: 280, Loss: 0.689571
Train - Epoch 0, Batch: 290, Loss: 0.689404
Train - Epoch 0, Batch: 300, Loss: 0.689363
Train - Epoch 0, Batch: 310, Loss: 0.690318
Train - Epoch 0, Batch: 320, Loss: 0.690020
Train - Epoch 0, Batch: 330, Loss: 0.687924
Train - Epoch 0, Batch: 340, Loss: 0.689522
Train - Epoch 0, Batch: 350, Loss: 0.688967
Train - Epoch 0, Batch: 360, Loss: 0.688703
Train - Epoch 0, Batch: 370, Loss: 0.689226
Train - Epoch 0, Batch: 380, Loss: 0.689382
Train - Epoch 0, Batch: 390, Loss: 0.688616
Train - Epoch 0, Batch: 400, Loss: 0.688451
Train - Epoch 0, Batch: 410, Loss: 0.688302
Train - Epoch 0, Batch: 420, Loss: 0.689447
Train - Epoch 0, Batch: 430, Loss: 0.689231
Train - Epoch 0, Batch: 440, Loss: 0.689645
Train - Epoch 0, Batch: 450, Loss: 0.688198
Train - Epoch 0, Batch: 460, Loss: 0.688710
Train - Epoch 0, Batch: 470, Loss: 0.688337
Train - Epoch 0, Batch: 480, Loss: 0.688505
Train - Epoch 0, Batch: 490, Loss: 0.688279
Train - Epoch 0, Batch: 500, Loss: 0.687197
Train - Epoch 0, Batch: 510, Loss: 0.688443
Train - Epoch 0, Batch: 520, Loss: 0.688788
Train - Epoch 0, Batch: 530, Loss: 0.688403
Train - Epoch 0, Batch: 540, Loss: 0.688279
Train - Epoch 0, Batch: 550, Loss: 0.688664
Train - Epoch 0, Batch: 560, Loss: 0.687789
Train - Epoch 0, Batch: 570, Loss: 0.688428
Train - Epoch 0, Batch: 580, Loss: 0.688555
Train - Epoch 0, Batch: 590, Loss: 0.688519
Train - Epoch 0, Batch: 600, Loss: 0.687290
Train - Epoch 0, Batch: 610, Loss: 0.687740
Train - Epoch 0, Batch: 620, Loss: 0.687766
Train - Epoch 0, Batch: 630, Loss: 0.687603
Train - Epoch 0, Batch: 640, Loss: 0.687762
Train - Epoch 1, Batch: 0, Loss: 0.687784
Train - Epoch 1, Batch: 10, Loss: 0.688616
Train - Epoch 1, Batch: 20, Loss: 0.688362
Train - Epoch 1, Batch: 30, Loss: 0.688196
Train - Epoch 1, Batch: 40, Loss: 0.687516
Train - Epoch 1, Batch: 50, Loss: 0.688011
Train - Epoch 1, Batch: 60, Loss: 0.687193
Train - Epoch 1, Batch: 70, Loss: 0.687086
Train - Epoch 1, Batch: 80, Loss: 0.688020
Train - Epoch 1, Batch: 90, Loss: 0.687172
Train - Epoch 1, Batch: 100, Loss: 0.686878
Train - Epoch 1, Batch: 110, Loss: 0.687098
Train - Epoch 1, Batch: 120, Loss: 0.686947
Train - Epoch 1, Batch: 130, Loss: 0.687612
Train - Epoch 1, Batch: 140, Loss: 0.688148
Train - Epoch 1, Batch: 150, Loss: 0.686413
Train - Epoch 1, Batch: 160, Loss: 0.686109
Train - Epoch 1, Batch: 170, Loss: 0.688552
Train - Epoch 1, Batch: 180, Loss: 0.688094
Train - Epoch 1, Batch: 190, Loss: 0.687675
Train - Epoch 1, Batch: 200, Loss: 0.686311
Train - Epoch 1, Batch: 210, Loss: 0.686503
Train - Epoch 1, Batch: 220, Loss: 0.688720
Train - Epoch 1, Batch: 230, Loss: 0.687951
Train - Epoch 1, Batch: 240, Loss: 0.687686
Train - Epoch 1, Batch: 250, Loss: 0.687936
Train - Epoch 1, Batch: 260, Loss: 0.687246
Train - Epoch 1, Batch: 270, Loss: 0.686263
Train - Epoch 1, Batch: 280, Loss: 0.687328
Train - Epoch 1, Batch: 290, Loss: 0.686320
Train - Epoch 1, Batch: 300, Loss: 0.686940
Train - Epoch 1, Batch: 310, Loss: 0.686327
Train - Epoch 1, Batch: 320, Loss: 0.686554
Train - Epoch 1, Batch: 330, Loss: 0.687569
Train - Epoch 1, Batch: 340, Loss: 0.686214
Train - Epoch 1, Batch: 350, Loss: 0.686880
Train - Epoch 1, Batch: 360, Loss: 0.686321
Train - Epoch 1, Batch: 370, Loss: 0.687041
Train - Epoch 1, Batch: 380, Loss: 0.686480
Train - Epoch 1, Batch: 390, Loss: 0.687319
Train - Epoch 1, Batch: 400, Loss: 0.686115
Train - Epoch 1, Batch: 410, Loss: 0.686707
Train - Epoch 1, Batch: 420, Loss: 0.685768
Train - Epoch 1, Batch: 430, Loss: 0.686153
Train - Epoch 1, Batch: 440, Loss: 0.686718
Train - Epoch 1, Batch: 450, Loss: 0.687504
Train - Epoch 1, Batch: 460, Loss: 0.686121
Train - Epoch 1, Batch: 470, Loss: 0.687200
Train - Epoch 1, Batch: 480, Loss: 0.686347
Train - Epoch 1, Batch: 490, Loss: 0.687090
Train - Epoch 1, Batch: 500, Loss: 0.686974
Train - Epoch 1, Batch: 510, Loss: 0.686292
Train - Epoch 1, Batch: 520, Loss: 0.686431
Train - Epoch 1, Batch: 530, Loss: 0.684852
Train - Epoch 1, Batch: 540, Loss: 0.686534
Train - Epoch 1, Batch: 550, Loss: 0.686529
Train - Epoch 1, Batch: 560, Loss: 0.685861
Train - Epoch 1, Batch: 570, Loss: 0.687152
Train - Epoch 1, Batch: 580, Loss: 0.686023
Train - Epoch 1, Batch: 590, Loss: 0.685504
Train - Epoch 1, Batch: 600, Loss: 0.685079
Train - Epoch 1, Batch: 610, Loss: 0.685889
Train - Epoch 1, Batch: 620, Loss: 0.686447
Train - Epoch 1, Batch: 630, Loss: 0.686679
Train - Epoch 1, Batch: 640, Loss: 0.684868
Train - Epoch 2, Batch: 0, Loss: 0.686970
Train - Epoch 2, Batch: 10, Loss: 0.684805
Train - Epoch 2, Batch: 20, Loss: 0.685534
Train - Epoch 2, Batch: 30, Loss: 0.685713
Train - Epoch 2, Batch: 40, Loss: 0.685733
Train - Epoch 2, Batch: 50, Loss: 0.685060
Train - Epoch 2, Batch: 60, Loss: 0.686246
Train - Epoch 2, Batch: 70, Loss: 0.685887
Train - Epoch 2, Batch: 80, Loss: 0.685752
Train - Epoch 2, Batch: 90, Loss: 0.685491
Train - Epoch 2, Batch: 100, Loss: 0.685530
Train - Epoch 2, Batch: 110, Loss: 0.684973
Train - Epoch 2, Batch: 120, Loss: 0.685973
Train - Epoch 2, Batch: 130, Loss: 0.685801
Train - Epoch 2, Batch: 140, Loss: 0.684563
Train - Epoch 2, Batch: 150, Loss: 0.685804
Train - Epoch 2, Batch: 160, Loss: 0.684878
Train - Epoch 2, Batch: 170, Loss: 0.685066
Train - Epoch 2, Batch: 180, Loss: 0.685797
Train - Epoch 2, Batch: 190, Loss: 0.686033
Train - Epoch 2, Batch: 200, Loss: 0.685346
Train - Epoch 2, Batch: 210, Loss: 0.684676
Train - Epoch 2, Batch: 220, Loss: 0.685609
Train - Epoch 2, Batch: 230, Loss: 0.686452
Train - Epoch 2, Batch: 240, Loss: 0.685170
Train - Epoch 2, Batch: 250, Loss: 0.684986
Train - Epoch 2, Batch: 260, Loss: 0.685138
Train - Epoch 2, Batch: 270, Loss: 0.686218
Train - Epoch 2, Batch: 280, Loss: 0.685961
Train - Epoch 2, Batch: 290, Loss: 0.686397
Train - Epoch 2, Batch: 300, Loss: 0.685889
Train - Epoch 2, Batch: 310, Loss: 0.685629
Train - Epoch 2, Batch: 320, Loss: 0.685000
Train - Epoch 2, Batch: 330, Loss: 0.686374
Train - Epoch 2, Batch: 340, Loss: 0.685595
Train - Epoch 2, Batch: 350, Loss: 0.685153
Train - Epoch 2, Batch: 360, Loss: 0.686240
Train - Epoch 2, Batch: 370, Loss: 0.685311
Train - Epoch 2, Batch: 380, Loss: 0.685582
Train - Epoch 2, Batch: 390, Loss: 0.686038
Train - Epoch 2, Batch: 400, Loss: 0.685987
Train - Epoch 2, Batch: 410, Loss: 0.684218
Train - Epoch 2, Batch: 420, Loss: 0.685438
Train - Epoch 2, Batch: 430, Loss: 0.685692
Train - Epoch 2, Batch: 440, Loss: 0.685784
Train - Epoch 2, Batch: 450, Loss: 0.683926
Train - Epoch 2, Batch: 460, Loss: 0.684312
Train - Epoch 2, Batch: 470, Loss: 0.684786
Train - Epoch 2, Batch: 480, Loss: 0.684404
Train - Epoch 2, Batch: 490, Loss: 0.685334
Train - Epoch 2, Batch: 500, Loss: 0.686873
Train - Epoch 2, Batch: 510, Loss: 0.685575
Train - Epoch 2, Batch: 520, Loss: 0.684641
Train - Epoch 2, Batch: 530, Loss: 0.685682
Train - Epoch 2, Batch: 540, Loss: 0.685296
Train - Epoch 2, Batch: 550, Loss: 0.685035/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.686275
Train - Epoch 2, Batch: 570, Loss: 0.684961
Train - Epoch 2, Batch: 580, Loss: 0.684940
Train - Epoch 2, Batch: 590, Loss: 0.685927
Train - Epoch 2, Batch: 600, Loss: 0.686060
Train - Epoch 2, Batch: 610, Loss: 0.685011
Train - Epoch 2, Batch: 620, Loss: 0.685804
Train - Epoch 2, Batch: 630, Loss: 0.684200
Train - Epoch 2, Batch: 640, Loss: 0.685982
Train - Epoch 3, Batch: 0, Loss: 0.684341
Train - Epoch 3, Batch: 10, Loss: 0.684342
Train - Epoch 3, Batch: 20, Loss: 0.684811
Train - Epoch 3, Batch: 30, Loss: 0.685468
Train - Epoch 3, Batch: 40, Loss: 0.685228
Train - Epoch 3, Batch: 50, Loss: 0.685820
Train - Epoch 3, Batch: 60, Loss: 0.685374
Train - Epoch 3, Batch: 70, Loss: 0.686447
Train - Epoch 3, Batch: 80, Loss: 0.685173
Train - Epoch 3, Batch: 90, Loss: 0.685201
Train - Epoch 3, Batch: 100, Loss: 0.685481
Train - Epoch 3, Batch: 110, Loss: 0.684038
Train - Epoch 3, Batch: 120, Loss: 0.685204
Train - Epoch 3, Batch: 130, Loss: 0.684633
Train - Epoch 3, Batch: 140, Loss: 0.685454
Train - Epoch 3, Batch: 150, Loss: 0.685224
Train - Epoch 3, Batch: 160, Loss: 0.685061
Train - Epoch 3, Batch: 170, Loss: 0.685181
Train - Epoch 3, Batch: 180, Loss: 0.685030
Train - Epoch 3, Batch: 190, Loss: 0.684352
Train - Epoch 3, Batch: 200, Loss: 0.684307
Train - Epoch 3, Batch: 210, Loss: 0.684671
Train - Epoch 3, Batch: 220, Loss: 0.684930
Train - Epoch 3, Batch: 230, Loss: 0.685335
Train - Epoch 3, Batch: 240, Loss: 0.685078
Train - Epoch 3, Batch: 250, Loss: 0.684276
Train - Epoch 3, Batch: 260, Loss: 0.685024
Train - Epoch 3, Batch: 270, Loss: 0.685740
Train - Epoch 3, Batch: 280, Loss: 0.685730
Train - Epoch 3, Batch: 290, Loss: 0.684432
Train - Epoch 3, Batch: 300, Loss: 0.683531
Train - Epoch 3, Batch: 310, Loss: 0.684734
Train - Epoch 3, Batch: 320, Loss: 0.683998
Train - Epoch 3, Batch: 330, Loss: 0.684592
Train - Epoch 3, Batch: 340, Loss: 0.683572
Train - Epoch 3, Batch: 350, Loss: 0.683067
Train - Epoch 3, Batch: 360, Loss: 0.685459
Train - Epoch 3, Batch: 370, Loss: 0.683867
Train - Epoch 3, Batch: 380, Loss: 0.684269
Train - Epoch 3, Batch: 390, Loss: 0.684307
Train - Epoch 3, Batch: 400, Loss: 0.684437
Train - Epoch 3, Batch: 410, Loss: 0.684638
Train - Epoch 3, Batch: 420, Loss: 0.683864
Train - Epoch 3, Batch: 430, Loss: 0.684455
Train - Epoch 3, Batch: 440, Loss: 0.684000
Train - Epoch 3, Batch: 450, Loss: 0.684467
Train - Epoch 3, Batch: 460, Loss: 0.684959
Train - Epoch 3, Batch: 470, Loss: 0.684971
Train - Epoch 3, Batch: 480, Loss: 0.684763
Train - Epoch 3, Batch: 490, Loss: 0.683827
Train - Epoch 3, Batch: 500, Loss: 0.684575
Train - Epoch 3, Batch: 510, Loss: 0.683032
Train - Epoch 3, Batch: 520, Loss: 0.683867
Train - Epoch 3, Batch: 530, Loss: 0.684365
Train - Epoch 3, Batch: 540, Loss: 0.683436
Train - Epoch 3, Batch: 550, Loss: 0.685085
Train - Epoch 3, Batch: 560, Loss: 0.684189
Train - Epoch 3, Batch: 570, Loss: 0.684542
Train - Epoch 3, Batch: 580, Loss: 0.683291
Train - Epoch 3, Batch: 590, Loss: 0.684356
Train - Epoch 3, Batch: 600, Loss: 0.683658
Train - Epoch 3, Batch: 610, Loss: 0.684187
Train - Epoch 3, Batch: 620, Loss: 0.683889
Train - Epoch 3, Batch: 630, Loss: 0.683320
Train - Epoch 3, Batch: 640, Loss: 0.685708
training_time:: 7.5483949184417725
training time full:: 7.548438787460327
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.553360
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 105000
training time is 4.993201732635498
overhead:: 0
overhead2:: 0
time_baseline:: 4.997352123260498
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.15163826942443848
overhead3:: 0.21511101722717285
overhead4:: 0.7461411952972412
overhead5:: 0
time_provenance:: 2.8186352252960205
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553210
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.20000529289245605
overhead3:: 0.2861325740814209
overhead4:: 0.9302260875701904
overhead5:: 0
time_provenance:: 3.0662407875061035
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553208
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2276134490966797
overhead3:: 0.34413862228393555
overhead4:: 1.159210205078125
overhead5:: 0
time_provenance:: 3.384465217590332
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553210
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2517738342285156
overhead3:: 0.3690152168273926
overhead4:: 1.2915732860565186
overhead5:: 0
time_provenance:: 3.5134365558624268
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553206
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22126483917236328
overhead3:: 0.3088359832763672
overhead4:: 1.3074314594268799
overhead5:: 0
time_provenance:: 3.841254711151123
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553252
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2370467185974121
overhead3:: 0.3305842876434326
overhead4:: 1.3166041374206543
overhead5:: 0
time_provenance:: 3.594912052154541
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553252
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3179938793182373
overhead3:: 0.45175933837890625
overhead4:: 1.657975673675537
overhead5:: 0
time_provenance:: 4.701421737670898
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553252
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2735099792480469
overhead3:: 0.34905576705932617
overhead4:: 1.464172124862671
overhead5:: 0
time_provenance:: 3.692955255508423
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553252
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3061060905456543
overhead3:: 0.42868566513061523
overhead4:: 1.8118360042572021
overhead5:: 0
time_provenance:: 4.252885103225708
curr_diff: 0 tensor(7.4188e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4188e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553276
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.30934882164001465
overhead3:: 0.4229550361633301
overhead4:: 1.8540570735931396
overhead5:: 0
time_provenance:: 4.262299060821533
curr_diff: 0 tensor(6.2210e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2210e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553270
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.43618202209472656
overhead3:: 0.635939359664917
overhead4:: 2.2044713497161865
overhead5:: 0
time_provenance:: 5.348082780838013
curr_diff: 0 tensor(6.3445e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3445e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553238
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3531215190887451
overhead3:: 0.49021029472351074
overhead4:: 1.9614930152893066
overhead5:: 0
time_provenance:: 4.338592767715454
curr_diff: 0 tensor(7.2675e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2675e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553276
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3895750045776367
overhead3:: 0.5457096099853516
overhead4:: 2.359086513519287
overhead5:: 0
time_provenance:: 4.964820384979248
curr_diff: 0 tensor(3.6977e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6977e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.41161656379699707
overhead3:: 0.5562081336975098
overhead4:: 2.4548394680023193
overhead5:: 0
time_provenance:: 5.081945896148682
curr_diff: 0 tensor(3.6384e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6384e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.40967679023742676
overhead3:: 0.5765142440795898
overhead4:: 2.4719748497009277
overhead5:: 0
time_provenance:: 5.028999090194702
curr_diff: 0 tensor(3.6139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4798576831817627
overhead3:: 0.6595356464385986
overhead4:: 2.3442397117614746
overhead5:: 0
time_provenance:: 5.057390928268433
curr_diff: 0 tensor(3.5368e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5368e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.831080436706543
overhead3:: 1.1443431377410889
overhead4:: 3.716843366622925
overhead5:: 0
time_provenance:: 6.40447735786438
curr_diff: 0 tensor(3.2924e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2924e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553266
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.695760
Train - Epoch 0, Batch: 10, Loss: 0.695284
Train - Epoch 0, Batch: 20, Loss: 0.693846
Train - Epoch 0, Batch: 30, Loss: 0.692998
Train - Epoch 0, Batch: 40, Loss: 0.691565
Train - Epoch 0, Batch: 50, Loss: 0.692993
Train - Epoch 0, Batch: 60, Loss: 0.691159
Train - Epoch 0, Batch: 70, Loss: 0.691493
Train - Epoch 0, Batch: 80, Loss: 0.690927
Train - Epoch 0, Batch: 90, Loss: 0.690491
Train - Epoch 0, Batch: 100, Loss: 0.690377
Train - Epoch 0, Batch: 110, Loss: 0.689822
Train - Epoch 0, Batch: 120, Loss: 0.690335
Train - Epoch 0, Batch: 130, Loss: 0.688616
Train - Epoch 0, Batch: 140, Loss: 0.689700
Train - Epoch 0, Batch: 150, Loss: 0.689251
Train - Epoch 0, Batch: 160, Loss: 0.689271
Train - Epoch 0, Batch: 170, Loss: 0.688629
Train - Epoch 0, Batch: 180, Loss: 0.689236
Train - Epoch 0, Batch: 190, Loss: 0.689354
Train - Epoch 0, Batch: 200, Loss: 0.688140
Train - Epoch 0, Batch: 210, Loss: 0.687507
Train - Epoch 0, Batch: 220, Loss: 0.688760
Train - Epoch 0, Batch: 230, Loss: 0.688193
Train - Epoch 0, Batch: 240, Loss: 0.688475
Train - Epoch 0, Batch: 250, Loss: 0.687658
Train - Epoch 0, Batch: 260, Loss: 0.687419
Train - Epoch 0, Batch: 270, Loss: 0.688807
Train - Epoch 0, Batch: 280, Loss: 0.688395
Train - Epoch 0, Batch: 290, Loss: 0.687534
Train - Epoch 0, Batch: 300, Loss: 0.688061
Train - Epoch 0, Batch: 310, Loss: 0.687244
Train - Epoch 0, Batch: 320, Loss: 0.687078
Train - Epoch 0, Batch: 330, Loss: 0.687189
Train - Epoch 0, Batch: 340, Loss: 0.686911
Train - Epoch 0, Batch: 350, Loss: 0.687114
Train - Epoch 0, Batch: 360, Loss: 0.686708
Train - Epoch 0, Batch: 370, Loss: 0.687836
Train - Epoch 0, Batch: 380, Loss: 0.686646
Train - Epoch 0, Batch: 390, Loss: 0.686567
Train - Epoch 0, Batch: 400, Loss: 0.687385
Train - Epoch 0, Batch: 410, Loss: 0.686585
Train - Epoch 0, Batch: 420, Loss: 0.685370
Train - Epoch 0, Batch: 430, Loss: 0.687241
Train - Epoch 0, Batch: 440, Loss: 0.685911
Train - Epoch 0, Batch: 450, Loss: 0.687977
Train - Epoch 0, Batch: 460, Loss: 0.687063
Train - Epoch 0, Batch: 470, Loss: 0.687488
Train - Epoch 0, Batch: 480, Loss: 0.687143
Train - Epoch 0, Batch: 490, Loss: 0.686344
Train - Epoch 0, Batch: 500, Loss: 0.686314
Train - Epoch 0, Batch: 510, Loss: 0.685988
Train - Epoch 0, Batch: 520, Loss: 0.687860
Train - Epoch 0, Batch: 530, Loss: 0.686567
Train - Epoch 0, Batch: 540, Loss: 0.686407
Train - Epoch 0, Batch: 550, Loss: 0.685989
Train - Epoch 0, Batch: 560, Loss: 0.686744
Train - Epoch 0, Batch: 570, Loss: 0.686710
Train - Epoch 0, Batch: 580, Loss: 0.685913
Train - Epoch 0, Batch: 590, Loss: 0.687288
Train - Epoch 0, Batch: 600, Loss: 0.686350
Train - Epoch 0, Batch: 610, Loss: 0.685996
Train - Epoch 0, Batch: 620, Loss: 0.685886
Train - Epoch 0, Batch: 630, Loss: 0.686450
Train - Epoch 0, Batch: 640, Loss: 0.685762
Train - Epoch 1, Batch: 0, Loss: 0.687055
Train - Epoch 1, Batch: 10, Loss: 0.686012
Train - Epoch 1, Batch: 20, Loss: 0.685320
Train - Epoch 1, Batch: 30, Loss: 0.686391
Train - Epoch 1, Batch: 40, Loss: 0.685324
Train - Epoch 1, Batch: 50, Loss: 0.685319
Train - Epoch 1, Batch: 60, Loss: 0.685898
Train - Epoch 1, Batch: 70, Loss: 0.685873
Train - Epoch 1, Batch: 80, Loss: 0.685953
Train - Epoch 1, Batch: 90, Loss: 0.685868
Train - Epoch 1, Batch: 100, Loss: 0.686174
Train - Epoch 1, Batch: 110, Loss: 0.686695
Train - Epoch 1, Batch: 120, Loss: 0.686435
Train - Epoch 1, Batch: 130, Loss: 0.684760
Train - Epoch 1, Batch: 140, Loss: 0.685481
Train - Epoch 1, Batch: 150, Loss: 0.685694
Train - Epoch 1, Batch: 160, Loss: 0.684928
Train - Epoch 1, Batch: 170, Loss: 0.686833
Train - Epoch 1, Batch: 180, Loss: 0.686567
Train - Epoch 1, Batch: 190, Loss: 0.686375
Train - Epoch 1, Batch: 200, Loss: 0.686054
Train - Epoch 1, Batch: 210, Loss: 0.685814
Train - Epoch 1, Batch: 220, Loss: 0.685731
Train - Epoch 1, Batch: 230, Loss: 0.685986
Train - Epoch 1, Batch: 240, Loss: 0.685890
Train - Epoch 1, Batch: 250, Loss: 0.685656
Train - Epoch 1, Batch: 260, Loss: 0.685040
Train - Epoch 1, Batch: 270, Loss: 0.686633
Train - Epoch 1, Batch: 280, Loss: 0.685285
Train - Epoch 1, Batch: 290, Loss: 0.684898
Train - Epoch 1, Batch: 300, Loss: 0.685296
Train - Epoch 1, Batch: 310, Loss: 0.686967
Train - Epoch 1, Batch: 320, Loss: 0.685357
Train - Epoch 1, Batch: 330, Loss: 0.684866
Train - Epoch 1, Batch: 340, Loss: 0.685673
Train - Epoch 1, Batch: 350, Loss: 0.684773
Train - Epoch 1, Batch: 360, Loss: 0.684701
Train - Epoch 1, Batch: 370, Loss: 0.685679
Train - Epoch 1, Batch: 380, Loss: 0.686502
Train - Epoch 1, Batch: 390, Loss: 0.685958
Train - Epoch 1, Batch: 400, Loss: 0.684917
Train - Epoch 1, Batch: 410, Loss: 0.685469
Train - Epoch 1, Batch: 420, Loss: 0.685474
Train - Epoch 1, Batch: 430, Loss: 0.685297
Train - Epoch 1, Batch: 440, Loss: 0.685018
Train - Epoch 1, Batch: 450, Loss: 0.685765
Train - Epoch 1, Batch: 460, Loss: 0.684753
Train - Epoch 1, Batch: 470, Loss: 0.685363
Train - Epoch 1, Batch: 480, Loss: 0.685453
Train - Epoch 1, Batch: 490, Loss: 0.683458
Train - Epoch 1, Batch: 500, Loss: 0.685247
Train - Epoch 1, Batch: 510, Loss: 0.684977
Train - Epoch 1, Batch: 520, Loss: 0.685087
Train - Epoch 1, Batch: 530, Loss: 0.684492
Train - Epoch 1, Batch: 540, Loss: 0.683532
Train - Epoch 1, Batch: 550, Loss: 0.683178
Train - Epoch 1, Batch: 560, Loss: 0.684531
Train - Epoch 1, Batch: 570, Loss: 0.684584
Train - Epoch 1, Batch: 580, Loss: 0.685535
Train - Epoch 1, Batch: 590, Loss: 0.685844
Train - Epoch 1, Batch: 600, Loss: 0.684639
Train - Epoch 1, Batch: 610, Loss: 0.685407
Train - Epoch 1, Batch: 620, Loss: 0.685778
Train - Epoch 1, Batch: 630, Loss: 0.684620
Train - Epoch 1, Batch: 640, Loss: 0.685563
Train - Epoch 2, Batch: 0, Loss: 0.684177
Train - Epoch 2, Batch: 10, Loss: 0.685672
Train - Epoch 2, Batch: 20, Loss: 0.685787
Train - Epoch 2, Batch: 30, Loss: 0.683691
Train - Epoch 2, Batch: 40, Loss: 0.685078
Train - Epoch 2, Batch: 50, Loss: 0.686121
Train - Epoch 2, Batch: 60, Loss: 0.685624
Train - Epoch 2, Batch: 70, Loss: 0.684752
Train - Epoch 2, Batch: 80, Loss: 0.684674
Train - Epoch 2, Batch: 90, Loss: 0.684223
Train - Epoch 2, Batch: 100, Loss: 0.684326
Train - Epoch 2, Batch: 110, Loss: 0.684652
Train - Epoch 2, Batch: 120, Loss: 0.684851
Train - Epoch 2, Batch: 130, Loss: 0.684714
Train - Epoch 2, Batch: 140, Loss: 0.684576
Train - Epoch 2, Batch: 150, Loss: 0.684938
Train - Epoch 2, Batch: 160, Loss: 0.685345
Train - Epoch 2, Batch: 170, Loss: 0.685286
Train - Epoch 2, Batch: 180, Loss: 0.685384
Train - Epoch 2, Batch: 190, Loss: 0.684715
Train - Epoch 2, Batch: 200, Loss: 0.683893
Train - Epoch 2, Batch: 210, Loss: 0.684108
Train - Epoch 2, Batch: 220, Loss: 0.684814
Train - Epoch 2, Batch: 230, Loss: 0.685730
Train - Epoch 2, Batch: 240, Loss: 0.684625
Train - Epoch 2, Batch: 250, Loss: 0.683718
Train - Epoch 2, Batch: 260, Loss: 0.685085
Train - Epoch 2, Batch: 270, Loss: 0.684557
Train - Epoch 2, Batch: 280, Loss: 0.684792
Train - Epoch 2, Batch: 290, Loss: 0.684062
Train - Epoch 2, Batch: 300, Loss: 0.684441
Train - Epoch 2, Batch: 310, Loss: 0.683838
Train - Epoch 2, Batch: 320, Loss: 0.684712
Train - Epoch 2, Batch: 330, Loss: 0.684981
Train - Epoch 2, Batch: 340, Loss: 0.685066
Train - Epoch 2, Batch: 350, Loss: 0.684639
Train - Epoch 2, Batch: 360, Loss: 0.684416
Train - Epoch 2, Batch: 370, Loss: 0.685597
Train - Epoch 2, Batch: 380, Loss: 0.684402
Train - Epoch 2, Batch: 390, Loss: 0.684514
Train - Epoch 2, Batch: 400, Loss: 0.684576
Train - Epoch 2, Batch: 410, Loss: 0.685762
Train - Epoch 2, Batch: 420, Loss: 0.685029
Train - Epoch 2, Batch: 430, Loss: 0.684474
Train - Epoch 2, Batch: 440, Loss: 0.684508
Train - Epoch 2, Batch: 450, Loss: 0.683735
Train - Epoch 2, Batch: 460, Loss: 0.685052
Train - Epoch 2, Batch: 470, Loss: 0.684116
Train - Epoch 2, Batch: 480, Loss: 0.684548
Train - Epoch 2, Batch: 490, Loss: 0.683594
Train - Epoch 2, Batch: 500, Loss: 0.684058
Train - Epoch 2, Batch: 510, Loss: 0.684031
Train - Epoch 2, Batch: 520, Loss: 0.683976
Train - Epoch 2, Batch: 530, Loss: 0.684009
Train - Epoch 2, Batch: 540, Loss: 0.683545
Train - Epoch 2, Batch: 550, Loss: 0.684580/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683759
Train - Epoch 2, Batch: 570, Loss: 0.684002
Train - Epoch 2, Batch: 580, Loss: 0.684657
Train - Epoch 2, Batch: 590, Loss: 0.683628
Train - Epoch 2, Batch: 600, Loss: 0.684891
Train - Epoch 2, Batch: 610, Loss: 0.683814
Train - Epoch 2, Batch: 620, Loss: 0.683436
Train - Epoch 2, Batch: 630, Loss: 0.684036
Train - Epoch 2, Batch: 640, Loss: 0.684322
Train - Epoch 3, Batch: 0, Loss: 0.684124
Train - Epoch 3, Batch: 10, Loss: 0.684555
Train - Epoch 3, Batch: 20, Loss: 0.685037
Train - Epoch 3, Batch: 30, Loss: 0.685029
Train - Epoch 3, Batch: 40, Loss: 0.684252
Train - Epoch 3, Batch: 50, Loss: 0.684018
Train - Epoch 3, Batch: 60, Loss: 0.685085
Train - Epoch 3, Batch: 70, Loss: 0.684253
Train - Epoch 3, Batch: 80, Loss: 0.683424
Train - Epoch 3, Batch: 90, Loss: 0.684279
Train - Epoch 3, Batch: 100, Loss: 0.685122
Train - Epoch 3, Batch: 110, Loss: 0.684634
Train - Epoch 3, Batch: 120, Loss: 0.684138
Train - Epoch 3, Batch: 130, Loss: 0.682732
Train - Epoch 3, Batch: 140, Loss: 0.684507
Train - Epoch 3, Batch: 150, Loss: 0.683424
Train - Epoch 3, Batch: 160, Loss: 0.684064
Train - Epoch 3, Batch: 170, Loss: 0.684395
Train - Epoch 3, Batch: 180, Loss: 0.684258
Train - Epoch 3, Batch: 190, Loss: 0.682910
Train - Epoch 3, Batch: 200, Loss: 0.683870
Train - Epoch 3, Batch: 210, Loss: 0.684167
Train - Epoch 3, Batch: 220, Loss: 0.683600
Train - Epoch 3, Batch: 230, Loss: 0.684115
Train - Epoch 3, Batch: 240, Loss: 0.683124
Train - Epoch 3, Batch: 250, Loss: 0.684458
Train - Epoch 3, Batch: 260, Loss: 0.684509
Train - Epoch 3, Batch: 270, Loss: 0.683655
Train - Epoch 3, Batch: 280, Loss: 0.683519
Train - Epoch 3, Batch: 290, Loss: 0.684588
Train - Epoch 3, Batch: 300, Loss: 0.683473
Train - Epoch 3, Batch: 310, Loss: 0.684205
Train - Epoch 3, Batch: 320, Loss: 0.685031
Train - Epoch 3, Batch: 330, Loss: 0.684266
Train - Epoch 3, Batch: 340, Loss: 0.683640
Train - Epoch 3, Batch: 350, Loss: 0.682800
Train - Epoch 3, Batch: 360, Loss: 0.684370
Train - Epoch 3, Batch: 370, Loss: 0.684633
Train - Epoch 3, Batch: 380, Loss: 0.684217
Train - Epoch 3, Batch: 390, Loss: 0.684353
Train - Epoch 3, Batch: 400, Loss: 0.683964
Train - Epoch 3, Batch: 410, Loss: 0.685334
Train - Epoch 3, Batch: 420, Loss: 0.683966
Train - Epoch 3, Batch: 430, Loss: 0.683934
Train - Epoch 3, Batch: 440, Loss: 0.683995
Train - Epoch 3, Batch: 450, Loss: 0.683482
Train - Epoch 3, Batch: 460, Loss: 0.684296
Train - Epoch 3, Batch: 470, Loss: 0.683594
Train - Epoch 3, Batch: 480, Loss: 0.682858
Train - Epoch 3, Batch: 490, Loss: 0.685192
Train - Epoch 3, Batch: 500, Loss: 0.684632
Train - Epoch 3, Batch: 510, Loss: 0.683456
Train - Epoch 3, Batch: 520, Loss: 0.683588
Train - Epoch 3, Batch: 530, Loss: 0.682814
Train - Epoch 3, Batch: 540, Loss: 0.685044
Train - Epoch 3, Batch: 550, Loss: 0.682959
Train - Epoch 3, Batch: 560, Loss: 0.684455
Train - Epoch 3, Batch: 570, Loss: 0.684346
Train - Epoch 3, Batch: 580, Loss: 0.684026
Train - Epoch 3, Batch: 590, Loss: 0.683072
Train - Epoch 3, Batch: 600, Loss: 0.684246
Train - Epoch 3, Batch: 610, Loss: 0.683858
Train - Epoch 3, Batch: 620, Loss: 0.684193
Train - Epoch 3, Batch: 630, Loss: 0.684676
Train - Epoch 3, Batch: 640, Loss: 0.683997
training_time:: 7.891185283660889
training time full:: 7.89122462272644
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000042, Accuracy: 0.556000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 105000
training time is 5.035380840301514
overhead:: 0
overhead2:: 0
time_baseline:: 5.038606405258179
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555966
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1484522819519043
overhead3:: 0.19598817825317383
overhead4:: 0.8101003170013428
overhead5:: 0
time_provenance:: 2.928647756576538
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555946
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.1890721321105957
overhead3:: 0.2659726142883301
overhead4:: 0.9727404117584229
overhead5:: 0
time_provenance:: 3.1917030811309814
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555946
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.20377850532531738
overhead3:: 0.2841792106628418
overhead4:: 1.0567572116851807
overhead5:: 0
time_provenance:: 3.2606637477874756
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555946
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.23450398445129395
overhead3:: 0.322864294052124
overhead4:: 1.182741641998291
overhead5:: 0
time_provenance:: 3.3637664318084717
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555944
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.21429824829101562
overhead3:: 0.2961127758026123
overhead4:: 1.2764568328857422
overhead5:: 0
time_provenance:: 3.4891741275787354
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555950
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.25978946685791016
overhead3:: 0.360379695892334
overhead4:: 1.4528477191925049
overhead5:: 0
time_provenance:: 4.248539209365845
curr_diff: 0 tensor(9.7481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7481e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555950
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.24712061882019043
overhead3:: 0.3235349655151367
overhead4:: 1.3753859996795654
overhead5:: 0
time_provenance:: 3.576507329940796
curr_diff: 0 tensor(9.6673e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6673e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555950
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2960505485534668
overhead3:: 0.42359423637390137
overhead4:: 1.5823075771331787
overhead5:: 0
time_provenance:: 3.8760249614715576
curr_diff: 0 tensor(9.4218e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4218e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555952
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.28785037994384766
overhead3:: 0.38852858543395996
overhead4:: 1.5689988136291504
overhead5:: 0
time_provenance:: 3.8352103233337402
curr_diff: 0 tensor(5.8414e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8414e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555954
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3056635856628418
overhead3:: 0.42786478996276855
overhead4:: 1.7754437923431396
overhead5:: 0
time_provenance:: 4.163158655166626
curr_diff: 0 tensor(5.2938e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2938e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555964
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3734285831451416
overhead3:: 0.5622847080230713
overhead4:: 2.042550563812256
overhead5:: 0
time_provenance:: 4.608454942703247
curr_diff: 0 tensor(4.4908e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4908e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555944
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.36403775215148926
overhead3:: 0.5019075870513916
overhead4:: 2.002948045730591
overhead5:: 0
time_provenance:: 4.443565845489502
curr_diff: 0 tensor(5.6302e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6302e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555954
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4250509738922119
overhead3:: 0.5778005123138428
overhead4:: 2.450673818588257
overhead5:: 0
time_provenance:: 5.267113447189331
curr_diff: 0 tensor(3.4782e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4782e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555956
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.42824244499206543
overhead3:: 0.5688803195953369
overhead4:: 2.3887810707092285
overhead5:: 0
time_provenance:: 5.037800312042236
curr_diff: 0 tensor(3.4041e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4041e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555956
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.4593791961669922
overhead3:: 0.6240253448486328
overhead4:: 2.385507583618164
overhead5:: 0
time_provenance:: 5.067727327346802
curr_diff: 0 tensor(3.3449e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3449e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555956
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.492473840713501
overhead3:: 0.7146916389465332
overhead4:: 2.6080455780029297
overhead5:: 0
time_provenance:: 5.386920928955078
curr_diff: 0 tensor(3.3036e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3036e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555956
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.8164060115814209
overhead3:: 1.1693620681762695
overhead4:: 3.8250696659088135
overhead5:: 0
time_provenance:: 6.524647951126099
curr_diff: 0 tensor(3.0167e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0167e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.555966
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.731431
Train - Epoch 0, Batch: 10, Loss: 0.703231
Train - Epoch 0, Batch: 20, Loss: 0.699686
Train - Epoch 0, Batch: 30, Loss: 0.699501
Train - Epoch 0, Batch: 40, Loss: 0.696930
Train - Epoch 0, Batch: 50, Loss: 0.696218
Train - Epoch 0, Batch: 60, Loss: 0.695419
Train - Epoch 0, Batch: 70, Loss: 0.693619
Train - Epoch 0, Batch: 80, Loss: 0.695274
Train - Epoch 0, Batch: 90, Loss: 0.693863
Train - Epoch 0, Batch: 100, Loss: 0.694257
Train - Epoch 0, Batch: 110, Loss: 0.693780
Train - Epoch 0, Batch: 120, Loss: 0.692675
Train - Epoch 0, Batch: 130, Loss: 0.692736
Train - Epoch 0, Batch: 140, Loss: 0.692140
Train - Epoch 0, Batch: 150, Loss: 0.690874
Train - Epoch 0, Batch: 160, Loss: 0.691479
Train - Epoch 0, Batch: 170, Loss: 0.690134
Train - Epoch 0, Batch: 180, Loss: 0.690580
Train - Epoch 0, Batch: 190, Loss: 0.691133
Train - Epoch 0, Batch: 200, Loss: 0.690123
Train - Epoch 0, Batch: 210, Loss: 0.690180
Train - Epoch 0, Batch: 220, Loss: 0.690343
Train - Epoch 0, Batch: 230, Loss: 0.690566
Train - Epoch 0, Batch: 240, Loss: 0.689234
Train - Epoch 0, Batch: 250, Loss: 0.689711
Train - Epoch 0, Batch: 260, Loss: 0.689538
Train - Epoch 0, Batch: 270, Loss: 0.689529
Train - Epoch 0, Batch: 280, Loss: 0.688084
Train - Epoch 0, Batch: 290, Loss: 0.689626
Train - Epoch 0, Batch: 300, Loss: 0.690168
Train - Epoch 0, Batch: 310, Loss: 0.689263
Train - Epoch 0, Batch: 320, Loss: 0.689169
Train - Epoch 0, Batch: 330, Loss: 0.689046
Train - Epoch 0, Batch: 340, Loss: 0.689013
Train - Epoch 0, Batch: 350, Loss: 0.689143
Train - Epoch 0, Batch: 360, Loss: 0.688782
Train - Epoch 0, Batch: 370, Loss: 0.689450
Train - Epoch 0, Batch: 380, Loss: 0.689118
Train - Epoch 0, Batch: 390, Loss: 0.688950
Train - Epoch 0, Batch: 400, Loss: 0.688391
Train - Epoch 0, Batch: 410, Loss: 0.687797
Train - Epoch 0, Batch: 420, Loss: 0.687707
Train - Epoch 0, Batch: 430, Loss: 0.687764
Train - Epoch 0, Batch: 440, Loss: 0.687762
Train - Epoch 0, Batch: 450, Loss: 0.688260
Train - Epoch 0, Batch: 460, Loss: 0.688772
Train - Epoch 0, Batch: 470, Loss: 0.688524
Train - Epoch 0, Batch: 480, Loss: 0.687009
Train - Epoch 0, Batch: 490, Loss: 0.687296
Train - Epoch 0, Batch: 500, Loss: 0.688831
Train - Epoch 0, Batch: 510, Loss: 0.688117
Train - Epoch 0, Batch: 520, Loss: 0.688503
Train - Epoch 0, Batch: 530, Loss: 0.687099
Train - Epoch 0, Batch: 540, Loss: 0.689172
Train - Epoch 0, Batch: 550, Loss: 0.688001
Train - Epoch 0, Batch: 560, Loss: 0.687177
Train - Epoch 0, Batch: 570, Loss: 0.687937
Train - Epoch 0, Batch: 580, Loss: 0.687743
Train - Epoch 0, Batch: 590, Loss: 0.687291
Train - Epoch 0, Batch: 600, Loss: 0.687210
Train - Epoch 0, Batch: 610, Loss: 0.686463
Train - Epoch 0, Batch: 620, Loss: 0.687215
Train - Epoch 0, Batch: 630, Loss: 0.686126
Train - Epoch 0, Batch: 640, Loss: 0.687286
Train - Epoch 1, Batch: 0, Loss: 0.687202
Train - Epoch 1, Batch: 10, Loss: 0.688538
Train - Epoch 1, Batch: 20, Loss: 0.687078
Train - Epoch 1, Batch: 30, Loss: 0.687822
Train - Epoch 1, Batch: 40, Loss: 0.687708
Train - Epoch 1, Batch: 50, Loss: 0.686427
Train - Epoch 1, Batch: 60, Loss: 0.686219
Train - Epoch 1, Batch: 70, Loss: 0.687920
Train - Epoch 1, Batch: 80, Loss: 0.687656
Train - Epoch 1, Batch: 90, Loss: 0.687692
Train - Epoch 1, Batch: 100, Loss: 0.686238
Train - Epoch 1, Batch: 110, Loss: 0.686906
Train - Epoch 1, Batch: 120, Loss: 0.688007
Train - Epoch 1, Batch: 130, Loss: 0.685953
Train - Epoch 1, Batch: 140, Loss: 0.687134
Train - Epoch 1, Batch: 150, Loss: 0.687585
Train - Epoch 1, Batch: 160, Loss: 0.686731
Train - Epoch 1, Batch: 170, Loss: 0.687443
Train - Epoch 1, Batch: 180, Loss: 0.686827
Train - Epoch 1, Batch: 190, Loss: 0.685351
Train - Epoch 1, Batch: 200, Loss: 0.686602
Train - Epoch 1, Batch: 210, Loss: 0.686474
Train - Epoch 1, Batch: 220, Loss: 0.685325
Train - Epoch 1, Batch: 230, Loss: 0.685884
Train - Epoch 1, Batch: 240, Loss: 0.685943
Train - Epoch 1, Batch: 250, Loss: 0.685769
Train - Epoch 1, Batch: 260, Loss: 0.686234
Train - Epoch 1, Batch: 270, Loss: 0.685771
Train - Epoch 1, Batch: 280, Loss: 0.686846
Train - Epoch 1, Batch: 290, Loss: 0.686405
Train - Epoch 1, Batch: 300, Loss: 0.686902
Train - Epoch 1, Batch: 310, Loss: 0.685026
Train - Epoch 1, Batch: 320, Loss: 0.686920
Train - Epoch 1, Batch: 330, Loss: 0.685553
Train - Epoch 1, Batch: 340, Loss: 0.684542
Train - Epoch 1, Batch: 350, Loss: 0.685369
Train - Epoch 1, Batch: 360, Loss: 0.687377
Train - Epoch 1, Batch: 370, Loss: 0.685782
Train - Epoch 1, Batch: 380, Loss: 0.686668
Train - Epoch 1, Batch: 390, Loss: 0.685565
Train - Epoch 1, Batch: 400, Loss: 0.685900
Train - Epoch 1, Batch: 410, Loss: 0.685867
Train - Epoch 1, Batch: 420, Loss: 0.685849
Train - Epoch 1, Batch: 430, Loss: 0.685817
Train - Epoch 1, Batch: 440, Loss: 0.686310
Train - Epoch 1, Batch: 450, Loss: 0.686460
Train - Epoch 1, Batch: 460, Loss: 0.686109
Train - Epoch 1, Batch: 470, Loss: 0.684789
Train - Epoch 1, Batch: 480, Loss: 0.686302
Train - Epoch 1, Batch: 490, Loss: 0.686745
Train - Epoch 1, Batch: 500, Loss: 0.685514
Train - Epoch 1, Batch: 510, Loss: 0.687005
Train - Epoch 1, Batch: 520, Loss: 0.685986
Train - Epoch 1, Batch: 530, Loss: 0.685558
Train - Epoch 1, Batch: 540, Loss: 0.685278
Train - Epoch 1, Batch: 550, Loss: 0.686256
Train - Epoch 1, Batch: 560, Loss: 0.685877
Train - Epoch 1, Batch: 570, Loss: 0.685957
Train - Epoch 1, Batch: 580, Loss: 0.686208
Train - Epoch 1, Batch: 590, Loss: 0.685168
Train - Epoch 1, Batch: 600, Loss: 0.685489
Train - Epoch 1, Batch: 610, Loss: 0.684822
Train - Epoch 1, Batch: 620, Loss: 0.685783
Train - Epoch 1, Batch: 630, Loss: 0.685350
Train - Epoch 1, Batch: 640, Loss: 0.685599
Train - Epoch 2, Batch: 0, Loss: 0.685289
Train - Epoch 2, Batch: 10, Loss: 0.685370
Train - Epoch 2, Batch: 20, Loss: 0.686557
Train - Epoch 2, Batch: 30, Loss: 0.683751
Train - Epoch 2, Batch: 40, Loss: 0.685986
Train - Epoch 2, Batch: 50, Loss: 0.683854
Train - Epoch 2, Batch: 60, Loss: 0.686023
Train - Epoch 2, Batch: 70, Loss: 0.684988
Train - Epoch 2, Batch: 80, Loss: 0.684846
Train - Epoch 2, Batch: 90, Loss: 0.684978
Train - Epoch 2, Batch: 100, Loss: 0.684461
Train - Epoch 2, Batch: 110, Loss: 0.684956
Train - Epoch 2, Batch: 120, Loss: 0.685725
Train - Epoch 2, Batch: 130, Loss: 0.684855
Train - Epoch 2, Batch: 140, Loss: 0.686119
Train - Epoch 2, Batch: 150, Loss: 0.685286
Train - Epoch 2, Batch: 160, Loss: 0.684614
Train - Epoch 2, Batch: 170, Loss: 0.683899
Train - Epoch 2, Batch: 180, Loss: 0.685858
Train - Epoch 2, Batch: 190, Loss: 0.684732
Train - Epoch 2, Batch: 200, Loss: 0.684405
Train - Epoch 2, Batch: 210, Loss: 0.683886
Train - Epoch 2, Batch: 220, Loss: 0.685322
Train - Epoch 2, Batch: 230, Loss: 0.684035
Train - Epoch 2, Batch: 240, Loss: 0.686042
Train - Epoch 2, Batch: 250, Loss: 0.684154
Train - Epoch 2, Batch: 260, Loss: 0.685112
Train - Epoch 2, Batch: 270, Loss: 0.685177
Train - Epoch 2, Batch: 280, Loss: 0.686442
Train - Epoch 2, Batch: 290, Loss: 0.685300
Train - Epoch 2, Batch: 300, Loss: 0.684848
Train - Epoch 2, Batch: 310, Loss: 0.684498
Train - Epoch 2, Batch: 320, Loss: 0.686065
Train - Epoch 2, Batch: 330, Loss: 0.684443
Train - Epoch 2, Batch: 340, Loss: 0.685471
Train - Epoch 2, Batch: 350, Loss: 0.686021
Train - Epoch 2, Batch: 360, Loss: 0.685234
Train - Epoch 2, Batch: 370, Loss: 0.684446
Train - Epoch 2, Batch: 380, Loss: 0.684874
Train - Epoch 2, Batch: 390, Loss: 0.684450
Train - Epoch 2, Batch: 400, Loss: 0.686136
Train - Epoch 2, Batch: 410, Loss: 0.684478
Train - Epoch 2, Batch: 420, Loss: 0.684916
Train - Epoch 2, Batch: 430, Loss: 0.685557
Train - Epoch 2, Batch: 440, Loss: 0.684910
Train - Epoch 2, Batch: 450, Loss: 0.683969
Train - Epoch 2, Batch: 460, Loss: 0.684778
Train - Epoch 2, Batch: 470, Loss: 0.684629
Train - Epoch 2, Batch: 480, Loss: 0.685163
Train - Epoch 2, Batch: 490, Loss: 0.685008
Train - Epoch 2, Batch: 500, Loss: 0.684154
Train - Epoch 2, Batch: 510, Loss: 0.685110
Train - Epoch 2, Batch: 520, Loss: 0.685380
Train - Epoch 2, Batch: 530, Loss: 0.685579
Train - Epoch 2, Batch: 540, Loss: 0.685184
Train - Epoch 2, Batch: 550, Loss: 0.685346/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684701
Train - Epoch 2, Batch: 570, Loss: 0.684748
Train - Epoch 2, Batch: 580, Loss: 0.685459
Train - Epoch 2, Batch: 590, Loss: 0.685305
Train - Epoch 2, Batch: 600, Loss: 0.683874
Train - Epoch 2, Batch: 610, Loss: 0.684781
Train - Epoch 2, Batch: 620, Loss: 0.684230
Train - Epoch 2, Batch: 630, Loss: 0.684775
Train - Epoch 2, Batch: 640, Loss: 0.686130
Train - Epoch 3, Batch: 0, Loss: 0.684946
Train - Epoch 3, Batch: 10, Loss: 0.684440
Train - Epoch 3, Batch: 20, Loss: 0.684946
Train - Epoch 3, Batch: 30, Loss: 0.683299
Train - Epoch 3, Batch: 40, Loss: 0.684193
Train - Epoch 3, Batch: 50, Loss: 0.686003
Train - Epoch 3, Batch: 60, Loss: 0.683710
Train - Epoch 3, Batch: 70, Loss: 0.684514
Train - Epoch 3, Batch: 80, Loss: 0.684559
Train - Epoch 3, Batch: 90, Loss: 0.683582
Train - Epoch 3, Batch: 100, Loss: 0.683628
Train - Epoch 3, Batch: 110, Loss: 0.685062
Train - Epoch 3, Batch: 120, Loss: 0.684616
Train - Epoch 3, Batch: 130, Loss: 0.685595
Train - Epoch 3, Batch: 140, Loss: 0.684126
Train - Epoch 3, Batch: 150, Loss: 0.684335
Train - Epoch 3, Batch: 160, Loss: 0.684147
Train - Epoch 3, Batch: 170, Loss: 0.684584
Train - Epoch 3, Batch: 180, Loss: 0.683959
Train - Epoch 3, Batch: 190, Loss: 0.683909
Train - Epoch 3, Batch: 200, Loss: 0.684650
Train - Epoch 3, Batch: 210, Loss: 0.683852
Train - Epoch 3, Batch: 220, Loss: 0.684664
Train - Epoch 3, Batch: 230, Loss: 0.685205
Train - Epoch 3, Batch: 240, Loss: 0.684875
Train - Epoch 3, Batch: 250, Loss: 0.684764
Train - Epoch 3, Batch: 260, Loss: 0.684959
Train - Epoch 3, Batch: 270, Loss: 0.683283
Train - Epoch 3, Batch: 280, Loss: 0.685422
Train - Epoch 3, Batch: 290, Loss: 0.683287
Train - Epoch 3, Batch: 300, Loss: 0.684879
Train - Epoch 3, Batch: 310, Loss: 0.684427
Train - Epoch 3, Batch: 320, Loss: 0.685218
Train - Epoch 3, Batch: 330, Loss: 0.684950
Train - Epoch 3, Batch: 340, Loss: 0.682768
Train - Epoch 3, Batch: 350, Loss: 0.683586
Train - Epoch 3, Batch: 360, Loss: 0.684909
Train - Epoch 3, Batch: 370, Loss: 0.682561
Train - Epoch 3, Batch: 380, Loss: 0.682833
Train - Epoch 3, Batch: 390, Loss: 0.684207
Train - Epoch 3, Batch: 400, Loss: 0.684675
Train - Epoch 3, Batch: 410, Loss: 0.683024
Train - Epoch 3, Batch: 420, Loss: 0.684535
Train - Epoch 3, Batch: 430, Loss: 0.685276
Train - Epoch 3, Batch: 440, Loss: 0.684826
Train - Epoch 3, Batch: 450, Loss: 0.684052
Train - Epoch 3, Batch: 460, Loss: 0.684058
Train - Epoch 3, Batch: 470, Loss: 0.684099
Train - Epoch 3, Batch: 480, Loss: 0.683924
Train - Epoch 3, Batch: 490, Loss: 0.683570
Train - Epoch 3, Batch: 500, Loss: 0.683860
Train - Epoch 3, Batch: 510, Loss: 0.684225
Train - Epoch 3, Batch: 520, Loss: 0.683113
Train - Epoch 3, Batch: 530, Loss: 0.684327
Train - Epoch 3, Batch: 540, Loss: 0.683370
Train - Epoch 3, Batch: 550, Loss: 0.685107
Train - Epoch 3, Batch: 560, Loss: 0.683628
Train - Epoch 3, Batch: 570, Loss: 0.683770
Train - Epoch 3, Batch: 580, Loss: 0.684682
Train - Epoch 3, Batch: 590, Loss: 0.682739
Train - Epoch 3, Batch: 600, Loss: 0.684593
Train - Epoch 3, Batch: 610, Loss: 0.685030
Train - Epoch 3, Batch: 620, Loss: 0.683405
Train - Epoch 3, Batch: 630, Loss: 0.684166
Train - Epoch 3, Batch: 640, Loss: 0.682861
training_time:: 7.842150688171387
training time full:: 7.842193126678467
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000042, Accuracy: 0.551400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta data size:: 105000
training time is 4.558536767959595
overhead:: 0
overhead2:: 0
time_baseline:: 4.561389446258545
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 10
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.14630866050720215
overhead3:: 0.20293712615966797
overhead4:: 0.7550265789031982
overhead5:: 0
time_provenance:: 2.850703239440918
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551438
period:: 10
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.19667720794677734
overhead3:: 0.2874891757965088
overhead4:: 1.0284087657928467
overhead5:: 0
time_provenance:: 3.1887853145599365
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551438
period:: 10
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.2170581817626953
overhead3:: 0.3132462501525879
overhead4:: 1.0840296745300293
overhead5:: 0
time_provenance:: 3.285736560821533
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551440
period:: 10
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22341394424438477
overhead3:: 0.29078102111816406
overhead4:: 1.1483981609344482
overhead5:: 0
time_provenance:: 3.2378437519073486
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551442
period:: 5
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.21718239784240723
overhead3:: 0.31163620948791504
overhead4:: 1.3092474937438965
overhead5:: 0
time_provenance:: 3.5729358196258545
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551380
period:: 5
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.22311925888061523
overhead3:: 0.3118143081665039
overhead4:: 1.3585572242736816
overhead5:: 0
time_provenance:: 3.5584845542907715
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551380
period:: 5
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.27906155586242676
overhead3:: 0.3844490051269531
overhead4:: 1.4546785354614258
overhead5:: 0
time_provenance:: 3.865368127822876
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551382
period:: 5
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.30826830863952637
overhead3:: 0.45117712020874023
overhead4:: 1.665926218032837
overhead5:: 0
time_provenance:: 4.021728754043579
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551382
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 3 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.31429624557495117
overhead3:: 0.44899630546569824
overhead4:: 1.8080995082855225
overhead5:: 0
time_provenance:: 4.314148187637329
curr_diff: 0 tensor(5.9981e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9981e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551418
period:: 3
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 3 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3546266555786133
overhead3:: 0.5213227272033691
overhead4:: 1.9629976749420166
overhead5:: 0
time_provenance:: 4.519671201705933
curr_diff: 0 tensor(5.6605e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6605e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 3
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 3 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3648974895477295
overhead3:: 0.518540620803833
overhead4:: 1.9917380809783936
overhead5:: 0
time_provenance:: 4.546675205230713
curr_diff: 0 tensor(6.3027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551412
period:: 3
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 3 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.3949894905090332
overhead3:: 0.5869755744934082
overhead4:: 2.150681257247925
overhead5:: 0
time_provenance:: 4.716854810714722
curr_diff: 0 tensor(5.6519e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6519e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551420
period:: 2
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance3_lr.py 300 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.5078918933868408
overhead3:: 0.7134685516357422
overhead4:: 2.6085011959075928
overhead5:: 0
time_provenance:: 6.015622138977051
curr_diff: 0 tensor(3.6230e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6230e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 2
init_iters:: 400
incremental updates::
python3 incremental_updates_provenance3_lr.py 400 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.39802980422973633
overhead3:: 0.5401272773742676
overhead4:: 2.467283010482788
overhead5:: 0
time_provenance:: 5.01962685585022
curr_diff: 0 tensor(3.5968e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5968e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 2
init_iters:: 500
incremental updates::
python3 incremental_updates_provenance3_lr.py 500 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.44000673294067383
overhead3:: 0.6133975982666016
overhead4:: 2.604837417602539
overhead5:: 0
time_provenance:: 5.266310453414917
curr_diff: 0 tensor(3.5805e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5805e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 2
init_iters:: 600
incremental updates::
python3 incremental_updates_provenance3_lr.py 600 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.5071499347686768
overhead3:: 0.6867103576660156
overhead4:: 2.8039724826812744
overhead5:: 0
time_provenance:: 5.925130367279053
curr_diff: 0 tensor(3.5561e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5561e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 105000
max_epoch:: 4
overhead:: 0
overhead2:: 0.7769820690155029
overhead3:: 1.080974817276001
overhead4:: 3.6922149658203125
overhead5:: 0
time_provenance:: 6.262340784072876
curr_diff: 0 tensor(2.3185e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3185e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.551422
