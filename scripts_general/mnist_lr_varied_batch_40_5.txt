period::5
init_iters::40
varied deletion rate::
varied number of samples::
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  MNIST5 1
tensor([58838])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.647536277771
training time full:: 108.65817594528198
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.7494478225708
time_baseline:: 106.01991963386536
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 64.58712530136108
curr_diff: 0 tensor(1.1424e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1424e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 111.05762314796448
training time full:: 111.06937384605408
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 108.40638852119446
time_baseline:: 108.67788100242615
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.87967348098755
curr_diff: 0 tensor(1.1424e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1424e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.56344962120056
training time full:: 109.57603168487549
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.62114667892456
time_baseline:: 106.89053702354431
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.67923545837402
curr_diff: 0 tensor(1.1424e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1424e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.08719062805176
training time full:: 109.09888958930969
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.39605402946472
time_baseline:: 106.66809225082397
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.48084473609924
curr_diff: 0 tensor(1.1424e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1424e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.20314621925354
training time full:: 110.21552968025208
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.10218024253845
time_baseline:: 105.37328433990479
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.033889055252075
curr_diff: 0 tensor(1.1424e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1424e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.94337201118469
training time full:: 52.9505136013031
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.719560861587524
time_baseline:: 48.85370492935181
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.01735758781433
curr_diff: 0 tensor(5.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.42522740364075
training time full:: 52.431490659713745
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.35606670379639
time_baseline:: 48.49708008766174
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.880971431732178
curr_diff: 0 tensor(5.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.64549970626831
training time full:: 52.65163612365723
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 49.30021119117737
time_baseline:: 49.43762230873108
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.909059524536133
curr_diff: 0 tensor(5.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.83618402481079
training time full:: 52.84247589111328
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.89064288139343
time_baseline:: 48.027263879776
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.873160123825073
curr_diff: 0 tensor(5.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.62789869308472
training time full:: 52.63414549827576
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.60098838806152
time_baseline:: 48.74196457862854
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.419865131378174
curr_diff: 0 tensor(5.2029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.48824954032898
training time full:: 27.49074101448059
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.088709592819214
time_baseline:: 24.160293340682983
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.13656735420227
curr_diff: 0 tensor(1.1431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.295416116714478
training time full:: 27.297885417938232
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.359424829483032
time_baseline:: 24.431336879730225
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.077188730239868
curr_diff: 0 tensor(1.1431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.495759963989258
training time full:: 27.498318910598755
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.255502462387085
time_baseline:: 24.32750105857849
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.031301736831665
curr_diff: 0 tensor(1.1431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.06716012954712
training time full:: 27.0694260597229
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.14741611480713
time_baseline:: 24.21925687789917
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 14.735928535461426
curr_diff: 0 tensor(1.1431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.095382928848267
training time full:: 27.09751868247986
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.416036128997803
time_baseline:: 24.487184762954712
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 14.994597911834717
curr_diff: 0 tensor(1.1431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.141739845275879
training time full:: 6.1417787075042725
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0256898403167725
time_baseline:: 5.043052673339844
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.5779738426208496
curr_diff: 0 tensor(2.1131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.903800010681152
training time full:: 5.903838396072388
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.199206352233887
time_baseline:: 5.2200846672058105
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.558082103729248
curr_diff: 0 tensor(2.1131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.802534580230713
training time full:: 5.8025736808776855
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.289375305175781
time_baseline:: 5.306745290756226
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.5762734413146973
curr_diff: 0 tensor(2.1131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.778920888900757
training time full:: 5.7789599895477295
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.141460180282593
time_baseline:: 5.159636497497559
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6390912532806396
curr_diff: 0 tensor(2.1131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.818898916244507
training time full:: 5.818936347961426
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.223408937454224
time_baseline:: 5.240994215011597
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.5647835731506348
curr_diff: 0 tensor(2.1131e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1131e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6402225494384766
training time full:: 1.6402592658996582
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.422631025314331
time_baseline:: 1.4274892807006836
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.155818223953247
curr_diff: 0 tensor(1.8787e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8787e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6712188720703125
training time full:: 1.671255111694336
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4192118644714355
time_baseline:: 1.4240541458129883
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1406629085540771
curr_diff: 0 tensor(1.8787e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8787e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6482529640197754
training time full:: 1.6482901573181152
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4906771183013916
time_baseline:: 1.4954993724822998
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1354506015777588
curr_diff: 0 tensor(1.8787e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8787e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.659256935119629
training time full:: 1.6592931747436523
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4389064311981201
time_baseline:: 1.443746566772461
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1546287536621094
curr_diff: 0 tensor(1.8787e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8787e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.642836332321167
training time full:: 1.6428725719451904
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4885962009429932
time_baseline:: 1.4934501647949219
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1313095092773438
curr_diff: 0 tensor(1.8787e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8787e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  MNIST5 0
tensor([ 3119, 58838, 38775])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.10106301307678
training time full:: 109.11108636856079
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.46537828445435
time_baseline:: 105.73401665687561
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.20649242401123
curr_diff: 0 tensor(2.5692e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5692e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.61304903030396
training time full:: 109.62449836730957
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.54183554649353
time_baseline:: 107.812020778656
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 64.30035901069641
curr_diff: 0 tensor(2.5692e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5692e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.65553784370422
training time full:: 109.66917896270752
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.2554349899292
time_baseline:: 106.52959370613098
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 61.91006517410278
curr_diff: 0 tensor(2.5692e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5692e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.07668423652649
training time full:: 110.08929181098938
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.12799406051636
time_baseline:: 107.39776015281677
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.65935301780701
curr_diff: 0 tensor(2.5692e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5692e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 113.16616702079773
training time full:: 113.1787223815918
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.95461440086365
time_baseline:: 106.22427988052368
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.42634963989258
curr_diff: 0 tensor(2.5692e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5692e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.53656625747681
training time full:: 52.543708086013794
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.95952844619751
time_baseline:: 48.093876361846924
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.03605842590332
curr_diff: 0 tensor(1.7482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.70040488243103
training time full:: 52.70655131340027
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.18545699119568
time_baseline:: 48.32190132141113
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.678908109664917
curr_diff: 0 tensor(1.7482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.33924984931946
training time full:: 52.345370054244995
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.857332706451416
time_baseline:: 47.99364638328552
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.73226284980774
curr_diff: 0 tensor(1.7482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.29013204574585
training time full:: 53.296212673187256
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.255126953125
time_baseline:: 48.39161825180054
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.931782245635986
curr_diff: 0 tensor(1.7482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.222816705703735
training time full:: 52.229063272476196
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.12614297866821
time_baseline:: 48.26141691207886
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.246676445007324
curr_diff: 0 tensor(1.7482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.325026750564575
training time full:: 27.327494144439697
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.39146590232849
time_baseline:: 24.465351343154907
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.129819869995117
curr_diff: 0 tensor(2.6651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.104843854904175
training time full:: 27.10698628425598
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.085235595703125
time_baseline:: 24.156999349594116
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.228396892547607
curr_diff: 0 tensor(2.6651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.250926971435547
training time full:: 27.25311851501465
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.309365272521973
time_baseline:: 24.379945039749146
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.125915765762329
curr_diff: 0 tensor(2.6651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.61138939857483
training time full:: 27.613579750061035
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.391987085342407
time_baseline:: 24.463216543197632
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.006065845489502
curr_diff: 0 tensor(2.6651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.223737001419067
training time full:: 27.22589683532715
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.1523756980896
time_baseline:: 24.22637152671814
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.162542581558228
curr_diff: 0 tensor(2.6651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.803266525268555
training time full:: 5.80330491065979
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.297999382019043
time_baseline:: 5.3158416748046875
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.627546548843384
curr_diff: 0 tensor(4.3466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.853270530700684
training time full:: 5.853308916091919
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0387351512908936
time_baseline:: 5.05649471282959
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7790944576263428
curr_diff: 0 tensor(4.3466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.950496673583984
training time full:: 5.9505369663238525
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.3449437618255615
time_baseline:: 5.364876985549927
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.627469778060913
curr_diff: 0 tensor(4.3466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.752702951431274
training time full:: 5.7527406215667725
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.132084131240845
time_baseline:: 5.149883270263672
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.611466884613037
curr_diff: 0 tensor(4.3466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.01166296005249
training time full:: 6.011701345443726
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.29010272026062
time_baseline:: 5.307588338851929
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.5873606204986572
curr_diff: 0 tensor(4.3466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3466e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6888420581817627
training time full:: 1.6888797283172607
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4404418468475342
time_baseline:: 1.4453306198120117
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1827244758605957
curr_diff: 0 tensor(4.5380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6761751174926758
training time full:: 1.6762094497680664
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4548931121826172
time_baseline:: 1.4597561359405518
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1171090602874756
curr_diff: 0 tensor(4.5380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7123363018035889
training time full:: 1.712371826171875
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4254229068756104
time_baseline:: 1.430267095565796
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1665372848510742
curr_diff: 0 tensor(4.5380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.799436330795288
training time full:: 1.7994725704193115
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4577233791351318
time_baseline:: 1.4625656604766846
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1198105812072754
curr_diff: 0 tensor(4.5380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.677734375
training time full:: 1.6777722835540771
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4962797164916992
time_baseline:: 1.501143455505371
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1170225143432617
curr_diff: 0 tensor(4.5380e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5380e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  MNIST5 0
tensor([47952, 34253, 58838, 38775, 18477,  3119])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.52585911750793
training time full:: 108.53585314750671
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.9503128528595
time_baseline:: 107.22130084037781
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.83681511878967
curr_diff: 0 tensor(3.4483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.06178641319275
training time full:: 110.0732901096344
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.67430472373962
time_baseline:: 106.94704270362854
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 61.95970892906189
curr_diff: 0 tensor(3.4483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.28374123573303
training time full:: 110.29598355293274
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.21338844299316
time_baseline:: 106.48240280151367
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.036137104034424
curr_diff: 0 tensor(3.4483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.54378318786621
training time full:: 109.55564665794373
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.93400526046753
time_baseline:: 106.2063136100769
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.170527935028076
curr_diff: 0 tensor(3.4483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.96401476860046
training time full:: 108.97576975822449
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.11512470245361
time_baseline:: 106.38553524017334
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.6611053943634
curr_diff: 0 tensor(3.4483e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4483e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.98005485534668
training time full:: 52.987122774124146
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.57798433303833
time_baseline:: 48.71489477157593
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.998175859451294
curr_diff: 0 tensor(1.8179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 51.95454668998718
training time full:: 51.960692405700684
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.07475423812866
time_baseline:: 48.21654772758484
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.941951751708984
curr_diff: 0 tensor(1.8179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.030921459198
training time full:: 52.03715658187866
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.18507766723633
time_baseline:: 48.32016134262085
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.553991317749023
curr_diff: 0 tensor(1.8179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.06653833389282
training time full:: 52.07291078567505
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.02631187438965
time_baseline:: 48.1674530506134
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.56849241256714
curr_diff: 0 tensor(1.8179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.740737438201904
training time full:: 52.7468798160553
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.71185517311096
time_baseline:: 48.85053324699402
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.433697938919067
curr_diff: 0 tensor(1.8179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.470081329345703
training time full:: 27.472513675689697
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.1191143989563
time_baseline:: 24.190857648849487
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.019395589828491
curr_diff: 0 tensor(3.0636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.504690408706665
training time full:: 27.507237911224365
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.691837787628174
time_baseline:: 24.762357711791992
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.02582836151123
curr_diff: 0 tensor(3.0636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.512643098831177
training time full:: 27.51495337486267
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.37400531768799
time_baseline:: 24.444406986236572
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.148529052734375
curr_diff: 0 tensor(3.0636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.409890174865723
training time full:: 27.41204833984375
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.928258419036865
time_baseline:: 23.999381065368652
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.507011651992798
curr_diff: 0 tensor(3.0636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.304891109466553
training time full:: 27.307023525238037
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.305262327194214
time_baseline:: 24.379486799240112
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.255875587463379
curr_diff: 0 tensor(3.0636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.921654224395752
training time full:: 5.921691417694092
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.305879592895508
time_baseline:: 5.323617935180664
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6617496013641357
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.827929735183716
training time full:: 5.827969312667847
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.984493017196655
time_baseline:: 5.002025842666626
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.682584524154663
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8360137939453125
training time full:: 5.8360536098480225
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.2624499797821045
time_baseline:: 5.2796690464019775
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6463394165039062
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.778193712234497
training time full:: 5.7782323360443115
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.181225299835205
time_baseline:: 5.199472904205322
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6722757816314697
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.81396484375
training time full:: 5.814003944396973
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.150514364242554
time_baseline:: 5.167834758758545
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6174681186676025
curr_diff: 0 tensor(7.3674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6877620220184326
training time full:: 1.687800645828247
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.407573938369751
time_baseline:: 1.4122834205627441
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1479687690734863
curr_diff: 0 tensor(8.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6632139682769775
training time full:: 1.6632492542266846
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4308221340179443
time_baseline:: 1.4357717037200928
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1638383865356445
curr_diff: 0 tensor(8.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.68764328956604
training time full:: 1.6876792907714844
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4139213562011719
time_baseline:: 1.4187462329864502
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1374597549438477
curr_diff: 0 tensor(8.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6773061752319336
training time full:: 1.6773433685302734
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4560682773590088
time_baseline:: 1.4610562324523926
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1340141296386719
curr_diff: 0 tensor(8.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.634810209274292
training time full:: 1.6348485946655273
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4345433712005615
time_baseline:: 1.4394042491912842
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1202971935272217
curr_diff: 0 tensor(8.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  MNIST5 0
tensor([18852,  3781, 34253, 18477,  3119, 47952, 48857, 14005, 58838, 38775,
        21113, 40667])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.41520428657532
training time full:: 108.42518401145935
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.47338819503784
time_baseline:: 105.74729490280151
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 64.19254398345947
curr_diff: 0 tensor(4.6403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6403e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.08117699623108
training time full:: 109.09244585037231
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.94953155517578
time_baseline:: 107.22245383262634
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.10750079154968
curr_diff: 0 tensor(4.6403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6403e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.9918692111969
training time full:: 111.00391983985901
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.29425597190857
time_baseline:: 106.5723249912262
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.16938757896423
curr_diff: 0 tensor(4.6403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6403e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.12880849838257
training time full:: 110.14104676246643
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.51774549484253
time_baseline:: 106.78831386566162
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.522913455963135
curr_diff: 0 tensor(4.6403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6403e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.8636314868927
training time full:: 109.87565326690674
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.41865515708923
time_baseline:: 105.68930506706238
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.72196912765503
curr_diff: 0 tensor(4.6403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6403e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.0111083984375
training time full:: 53.018129110336304
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.576764822006226
time_baseline:: 48.71250009536743
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.81194305419922
curr_diff: 0 tensor(2.4442e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4442e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.668479919433594
training time full:: 52.674800395965576
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.88929605484009
time_baseline:: 49.02801823616028
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.689892768859863
curr_diff: 0 tensor(2.4442e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4442e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.76568627357483
training time full:: 52.77181696891785
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.35077357292175
time_baseline:: 48.48619604110718
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.988704681396484
curr_diff: 0 tensor(2.4442e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4442e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.57423949241638
training time full:: 52.580347776412964
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.512216567993164
time_baseline:: 48.653602838516235
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.772412061691284
curr_diff: 0 tensor(2.4442e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4442e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.70235562324524
training time full:: 52.70863199234009
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.269713163375854
time_baseline:: 48.40483498573303
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.62577724456787
curr_diff: 0 tensor(2.4442e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4442e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.382272958755493
training time full:: 27.384826183319092
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.38548469543457
time_baseline:: 24.456401348114014
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.267679929733276
curr_diff: 0 tensor(3.3883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3883e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.49848961830139
training time full:: 27.500947952270508
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.595768928527832
time_baseline:: 24.667166233062744
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.161373853683472
curr_diff: 0 tensor(3.3883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3883e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.48317241668701
training time full:: 27.485445022583008
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.593870639801025
time_baseline:: 24.664182901382446
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.236100196838379
curr_diff: 0 tensor(3.3883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3883e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.097672939300537
training time full:: 27.0998592376709
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.417354583740234
time_baseline:: 24.4898464679718
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.197709798812866
curr_diff: 0 tensor(3.3883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3883e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.176409006118774
training time full:: 27.178586959838867
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.279383659362793
time_baseline:: 24.34991955757141
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 14.954063177108765
curr_diff: 0 tensor(3.3883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3883e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.878321170806885
training time full:: 5.878358602523804
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.044282913208008
time_baseline:: 5.061527967453003
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6733696460723877
curr_diff: 0 tensor(9.0425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9002768993377686
training time full:: 5.900315761566162
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.097347259521484
time_baseline:: 5.115040063858032
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6264657974243164
curr_diff: 0 tensor(9.0425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.906267166137695
training time full:: 5.906304121017456
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.023292064666748
time_baseline:: 5.041608810424805
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6079788208007812
curr_diff: 0 tensor(9.0425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.779282808303833
training time full:: 5.779321908950806
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.150699853897095
time_baseline:: 5.170375347137451
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6524386405944824
curr_diff: 0 tensor(9.0425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.839224576950073
training time full:: 5.839261531829834
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.2439329624176025
time_baseline:: 5.26398491859436
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.696237564086914
curr_diff: 0 tensor(9.0425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6445977687835693
training time full:: 1.6446328163146973
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4403204917907715
time_baseline:: 1.4451191425323486
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1432805061340332
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.724006175994873
training time full:: 1.7240417003631592
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4417357444763184
time_baseline:: 1.4464716911315918
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.160872220993042
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7094786167144775
training time full:: 1.7095162868499756
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4610350131988525
time_baseline:: 1.4658944606781006
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1627006530761719
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.684136152267456
training time full:: 1.6841731071472168
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.454909324645996
time_baseline:: 1.4598171710968018
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2635273933410645
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.673283576965332
training time full:: 1.6733205318450928
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4166626930236816
time_baseline:: 1.421506404876709
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1643507480621338
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  MNIST5 0
tensor([49348,  3781, 32006, 34253, 25677, 10574, 47952, 19280, 58838, 33494,
        19224, 48857, 26394, 40667, 55643,  2083, 18852, 30756, 38695, 21111,
        20139, 18477,  3119, 14005, 30517, 38775, 29814, 21113, 10365, 59838])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.2874813079834
training time full:: 108.29753971099854
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.55558633804321
time_baseline:: 105.82775521278381
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.13986921310425
curr_diff: 0 tensor(7.3643e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3643e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.62120938301086
training time full:: 108.63265252113342
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.3440682888031
time_baseline:: 106.61384129524231
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.109591007232666
curr_diff: 0 tensor(7.3643e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3643e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.46880054473877
training time full:: 108.48037719726562
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.04541659355164
time_baseline:: 107.31640434265137
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.834494829177856
curr_diff: 0 tensor(7.3643e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3643e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.98531746864319
training time full:: 109.99679851531982
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.16471076011658
time_baseline:: 106.43493485450745
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.64074444770813
curr_diff: 0 tensor(7.3643e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3643e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.41055393218994
training time full:: 110.42246317863464
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.23817825317383
time_baseline:: 106.51029086112976
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.29359483718872
curr_diff: 0 tensor(7.3643e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3643e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.404404163360596
training time full:: 52.4114887714386
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.65769720077515
time_baseline:: 48.792616844177246
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.747344493865967
curr_diff: 0 tensor(3.0741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.14794301986694
training time full:: 52.15408539772034
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.61330270767212
time_baseline:: 48.747949838638306
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.87427854537964
curr_diff: 0 tensor(3.0741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.34839153289795
training time full:: 52.3546097278595
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.91322326660156
time_baseline:: 49.04851984977722
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.056894063949585
curr_diff: 0 tensor(3.0741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.50183939933777
training time full:: 52.508151054382324
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.38789343833923
time_baseline:: 48.52932000160217
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.73559021949768
curr_diff: 0 tensor(3.0741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.39788341522217
training time full:: 52.404147148132324
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.227744579315186
time_baseline:: 48.36475443840027
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.47538733482361
curr_diff: 0 tensor(3.0741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875700
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.224371433258057
training time full:: 27.226796627044678
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.203970909118652
time_baseline:: 24.275739669799805
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.113371133804321
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.448018074035645
training time full:: 27.450519800186157
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.373475074768066
time_baseline:: 24.44373917579651
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 14.96001148223877
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.146753549575806
training time full:: 27.148939847946167
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.35613203048706
time_baseline:: 24.42953872680664
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.083933591842651
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.59754991531372
training time full:: 27.599684476852417
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.703556060791016
time_baseline:: 24.775928258895874
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.216354370117188
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.30409860610962
training time full:: 27.306293487548828
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.16930079460144
time_baseline:: 24.23998475074768
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.189926862716675
curr_diff: 0 tensor(6.0064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.801578044891357
training time full:: 5.801615476608276
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.057156085968018
time_baseline:: 5.074761629104614
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6999566555023193
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.913365602493286
training time full:: 5.913402557373047
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.131114721298218
time_baseline:: 5.148406982421875
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.687361478805542
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.797105550765991
training time full:: 5.797145366668701
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.031564474105835
time_baseline:: 5.049468517303467
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6629858016967773
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.861800909042358
training time full:: 5.861839056015015
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.157801628112793
time_baseline:: 5.175268650054932
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7279064655303955
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.859293699264526
training time full:: 5.8593316078186035
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1093950271606445
time_baseline:: 5.12673807144165
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7112393379211426
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.664642095565796
training time full:: 1.6646771430969238
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4647114276885986
time_baseline:: 1.4695158004760742
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1956322193145752
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7137632369995117
training time full:: 1.7137997150421143
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4317991733551025
time_baseline:: 1.4366755485534668
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1904621124267578
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6624548435211182
training time full:: 1.6624901294708252
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4434926509857178
time_baseline:: 1.4484608173370361
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1886053085327148
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6712665557861328
training time full:: 1.6713058948516846
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4174344539642334
time_baseline:: 1.422400712966919
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1655621528625488
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6529476642608643
training time full:: 1.6529853343963623
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.5003373622894287
time_baseline:: 1.5053229331970215
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1731133460998535
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
deletion rate:: 0.001
python3 generate_rand_ids 0.001  MNIST5 0
tensor([35972, 32006, 38775, 55312, 31377,  4886, 19224, 26394, 54554, 44957,
        53153,  2083, 18852, 30756, 40869, 38695,  3498, 20139, 18477,  3119,
         2100, 14005, 30517, 37045, 28726, 23095, 58172, 59838, 58049,  7491,
        49348,  3781, 34253, 25677, 10574, 47952, 19280, 55245, 47316, 58838,
        33494, 48599, 48857, 49625, 55643, 40667, 25309, 15843, 58471, 20585,
         1003, 51696, 55409, 58485, 29814, 21111, 21113, 31100, 10365, 59007])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.82322144508362
training time full:: 108.83341526985168
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.84776020050049
time_baseline:: 106.1210367679596
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.587400674819946
curr_diff: 0 tensor(1.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.61853218078613
training time full:: 109.63052868843079
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.17358303070068
time_baseline:: 105.4439754486084
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.627931356430054
curr_diff: 0 tensor(1.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.89804339408875
training time full:: 108.90981459617615
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.04716897010803
time_baseline:: 106.31932187080383
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.77931213378906
curr_diff: 0 tensor(1.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.92582607269287
training time full:: 109.93766832351685
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.68946838378906
time_baseline:: 105.96327471733093
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.22660994529724
curr_diff: 0 tensor(1.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.88094401359558
training time full:: 109.89264178276062
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.9352388381958
time_baseline:: 107.20324540138245
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.828794717788696
curr_diff: 0 tensor(1.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.695258378982544
training time full:: 52.70239853858948
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.41691613197327
time_baseline:: 48.5526385307312
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.814363956451416
curr_diff: 0 tensor(4.7440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.43033146858215
training time full:: 52.43659853935242
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.50439500808716
time_baseline:: 48.64320230484009
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.891484260559082
curr_diff: 0 tensor(4.7440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.42705988883972
training time full:: 52.43346452713013
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.308005571365356
time_baseline:: 48.444275856018066
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.57292079925537
curr_diff: 0 tensor(4.7440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.573073387145996
training time full:: 52.57927441596985
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.63543772697449
time_baseline:: 48.77223253250122
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.732906341552734
curr_diff: 0 tensor(4.7440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.276193141937256
training time full:: 52.28253436088562
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.15270709991455
time_baseline:: 48.288121461868286
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.806200742721558
curr_diff: 0 tensor(4.7440e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7440e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.331162691116333
training time full:: 27.333744764328003
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.445560932159424
time_baseline:: 24.516831159591675
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.234900951385498
curr_diff: 0 tensor(6.6465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 26.996964693069458
training time full:: 26.999311923980713
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.28001832962036
time_baseline:: 24.351574182510376
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.475663661956787
curr_diff: 0 tensor(6.6465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.305060863494873
training time full:: 27.307244300842285
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.3499493598938
time_baseline:: 24.420864820480347
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.284976959228516
curr_diff: 0 tensor(6.6465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.157764196395874
training time full:: 27.1599063873291
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.520498037338257
time_baseline:: 24.59437370300293
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.321258306503296
curr_diff: 0 tensor(6.6465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.41053056716919
training time full:: 27.41267967224121
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.513035535812378
time_baseline:: 24.584194898605347
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.19650387763977
curr_diff: 0 tensor(6.6465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.770032644271851
training time full:: 5.770070552825928
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.058981657028198
time_baseline:: 5.077201843261719
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.635848045349121
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.818447828292847
training time full:: 5.818486452102661
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.08346700668335
time_baseline:: 5.100739240646362
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.754791498184204
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.794417381286621
training time full:: 5.7944560050964355
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1192896366119385
time_baseline:: 5.137116432189941
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.8021559715270996
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.774303913116455
training time full:: 5.774343729019165
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.184201240539551
time_baseline:: 5.201987981796265
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6403987407684326
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.969494819641113
training time full:: 5.969532012939453
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.041855335235596
time_baseline:: 5.060128450393677
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6519954204559326
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7237579822540283
training time full:: 1.7237918376922607
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4148454666137695
time_baseline:: 1.4198622703552246
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2040715217590332
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6287040710449219
training time full:: 1.6287403106689453
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.459486722946167
time_baseline:: 1.4641847610473633
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1716861724853516
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6861169338226318
training time full:: 1.686152696609497
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.419987678527832
time_baseline:: 1.4251272678375244
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1752581596374512
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6765568256378174
training time full:: 1.6765928268432617
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4474706649780273
time_baseline:: 1.4524292945861816
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1618685722351074
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6597557067871094
training time full:: 1.6597929000854492
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4755196571350098
time_baseline:: 1.4804399013519287
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1935741901397705
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
deletion rate:: 0.002
python3 generate_rand_ids 0.002  MNIST5 0
tensor([ 5634,  3075, 32006, 43527, 53512, 44555, 23309, 43534, 55312,  4886,
        19224, 33560, 54554, 26394, 28444, 58396, 12831,  2083, 30756, 49957,
        38695, 49964, 18477, 41773,  3119, 14128,  2100, 30517, 28726, 23095,
         5172, 28470, 45364, 27191, 58172, 55353, 45116, 18753,  7491,  7494,
        36167, 58443, 25677, 10574, 58190, 47952, 19280, 31057, 55643,  5471,
        50531, 58471, 20585, 59501, 55409, 58485, 29814, 21111, 38775, 21113,
         3958, 40054, 31100, 10365, 31353, 59007, 35972, 33159, 33160, 39305,
        19595, 31377, 44957, 53153, 54689, 18852, 40869, 30629, 25766,  3498,
        20139, 50091, 14005, 37045, 34997, 16822, 39608, 25789, 59838, 58049,
        46529, 57795, 49348,  3781, 46020,  1223,   715, 34253, 55245, 50382,
        47316, 41172, 58838, 33494, 48599, 48857, 49625, 40667, 25309, 21474,
        15843, 30691, 31974, 33002,  1003, 36079, 51696,  1778, 33780, 22262])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.59879422187805
training time full:: 108.608962059021
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.17212843894958
time_baseline:: 106.43827247619629
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.90272784233093
curr_diff: 0 tensor(1.7072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.96445393562317
training time full:: 108.97566223144531
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.12607955932617
time_baseline:: 105.39340496063232
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.41960597038269
curr_diff: 0 tensor(1.7072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.36841678619385
training time full:: 109.38037657737732
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.65721011161804
time_baseline:: 105.92693281173706
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.11652326583862
curr_diff: 0 tensor(1.7072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.75736618041992
training time full:: 109.76894855499268
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.07861351966858
time_baseline:: 106.35173177719116
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.0741240978241
curr_diff: 0 tensor(1.7072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.32208800315857
training time full:: 109.33416700363159
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.06491136550903
time_baseline:: 107.33392190933228
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.79384398460388
curr_diff: 0 tensor(1.7072e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7072e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.15183067321777
training time full:: 53.159106969833374
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.21097111701965
time_baseline:: 48.34617233276367
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.968761920928955
curr_diff: 0 tensor(6.2989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.142250061035156
training time full:: 53.14851641654968
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.73607563972473
time_baseline:: 47.872212171554565
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.516899585723877
curr_diff: 0 tensor(6.2989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.53666663169861
training time full:: 52.54299020767212
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.22287106513977
time_baseline:: 48.360398054122925
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.81415057182312
curr_diff: 0 tensor(6.2989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.6273672580719
training time full:: 52.63366389274597
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.35696530342102
time_baseline:: 48.49125838279724
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.70518207550049
curr_diff: 0 tensor(6.2989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.08924055099487
training time full:: 53.095449924468994
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.04456901550293
time_baseline:: 48.180657386779785
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.913790702819824
curr_diff: 0 tensor(6.2989e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2989e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.704347133636475
training time full:: 27.706863164901733
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.397392511367798
time_baseline:: 24.469847917556763
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.207643508911133
curr_diff: 0 tensor(9.8139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.577500581741333
training time full:: 27.5798282623291
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.100159406661987
time_baseline:: 24.170239448547363
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.119529008865356
curr_diff: 0 tensor(9.8139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.305408716201782
training time full:: 27.30757784843445
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.25296664237976
time_baseline:: 24.327234029769897
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.153213500976562
curr_diff: 0 tensor(9.8139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.16979694366455
training time full:: 27.171989917755127
provenance prepare time:: 2.86102294921875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.438493013381958
time_baseline:: 24.50952124595642
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.057289123535156
curr_diff: 0 tensor(9.8139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.468095541000366
training time full:: 27.470266342163086
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.517447471618652
time_baseline:: 24.58822202682495
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.083937644958496
curr_diff: 0 tensor(9.8139e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8139e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876400
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9210240840911865
training time full:: 5.92106556892395
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.996487140655518
time_baseline:: 5.013966798782349
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7520225048065186
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.845579147338867
training time full:: 5.845618963241577
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.98813271522522
time_baseline:: 5.005757808685303
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6782121658325195
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.817570686340332
training time full:: 5.8176093101501465
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.114381313323975
time_baseline:: 5.132121562957764
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.707144260406494
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.951122283935547
training time full:: 5.951160907745361
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.133150815963745
time_baseline:: 5.150547742843628
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7161362171173096
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.040576934814453
training time full:: 6.040616035461426
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.077142715454102
time_baseline:: 5.094877004623413
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.6986775398254395
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7245683670043945
training time full:: 1.7246062755584717
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3739757537841797
time_baseline:: 1.3787693977355957
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1924540996551514
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873300
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6707324981689453
training time full:: 1.670769453048706
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.429356575012207
time_baseline:: 1.4342410564422607
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2489430904388428
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873300
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6587684154510498
training time full:: 1.6588046550750732
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3668887615203857
time_baseline:: 1.3716790676116943
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1798856258392334
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873300
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7254621982574463
training time full:: 1.7254998683929443
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3631591796875
time_baseline:: 1.3679733276367188
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2810606956481934
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873300
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6871159076690674
training time full:: 1.687157154083252
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3857247829437256
time_baseline:: 1.3904569149017334
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2031095027923584
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873300
deletion rate:: 0.01
python3 generate_rand_ids 0.01  MNIST5 0
tensor([10242,  2050, 18435, 20485, 14343, 59399, 47118, 55312, 36881,  8214,
           22,  2083, 30756, 57382, 36905, 49196, 18477, 53298, 12339,  2100,
        26674, 28726, 22581, 59446, 55353, 22578,  4152, 45116, 41029, 49222,
        53318,  6218,  6223, 22616, 55385, 59480, 51291, 32856, 28774, 10343,
        20585, 59501, 55409,  8306, 39027, 32890, 10365, 20610, 57474, 28804,
        36997,  4230, 37003, 57487, 41105, 39065, 32922, 22685,  8350, 22687,
          162, 32933, 45224,   169, 51379, 37045, 34997, 16570, 59582, 49348,
        59594,  2258, 47316, 41172, 16597, 30934, 39129, 39137, 41187, 57574,
        22758, 35047, 33002, 35055, 22768, 51441,  6386, 18672, 33011, 33016,
        33026, 53512, 20744, 45322, 53524, 39195, 20764, 55580, 18723, 47395,
        22824, 45361, 45364,   318, 49472, 18753, 51520, 10562, 18755,  8515,
        10574, 55631, 31057, 24914, 57687, 26970, 55643, 37212, 12636, 22885,
        14703,  2415, 31100,  2431, 24962, 29060, 33159, 33160, 39305, 49543,
        59785, 47499, 59797,  6549, 20890, 59802, 10655, 22945, 18852, 16807,
        55724,  2477, 49580, 49587, 29108, 16822, 53688, 41401, 59838, 29119,
        59841, 57795, 41413,  2504, 31180, 12748, 45521, 22997, 49625, 18914,
        51696,  4602, 31226, 49659, 59907, 43527, 43534, 31248, 21011,  8727,
        51739,  4638, 12831,   544, 47649, 18978, 18980,  8744, 59946, 39469,
        47662, 12849, 43571, 23095, 27191,  6713, 23098, 25149, 55870, 16959,
        41539, 29254, 19014, 55880, 59978, 19025, 53846, 31321, 21084, 41568,
        45666, 57956, 19050, 51819,  6762,  2669, 39533, 21111, 31353, 21113,
        29307, 45692, 21119, 37504, 21122, 35462, 14985, 14989, 31377, 41618,
        58003,   662, 53916, 55969,  6826,   685, 29361,  8882, 51890, 39608,
        19136, 58049, 25281, 21187, 49859, 39621, 51910, 15043, 45763, 23242,
          715, 23248, 10961,   723, 43732,  6869, 33494, 35542, 25309,  2782,
        49889, 47845, 13030, 43764,  4856, 41721, 43770, 51970, 23309, 11022,
        47887, 37647, 39698, 54037,  4886, 49941, 33560, 19224,  4891, 11038,
        49953, 49957,  4905, 49964, 41773, 15149,   820, 23350, 45880, 58172,
          831, 29507, 33611, 15180,  2893, 58190, 47952, 19280, 29520, 43859,
        15188, 23381, 35668,   852, 11092, 56159, 21345, 29537,  7012, 58213,
        17257,  2924, 31604,   884,  4986, 54143,  2950, 33678, 41880, 17307,
         9118, 52127, 27554, 50091, 25520, 54200, 43965, 46014, 43967, 19393,
        29634, 31683, 46020,  5060, 33730, 43979, 19404, 52172, 54225, 54235,
        29661, 21474, 27623,  1003, 33780,  7156, 56308, 35834, 48122, 33791,
        54273, 37890,  3075, 58372, 56324, 58396, 37917, 56355, 44071,  3119,
        13360, 27699,  5172, 42039,  9272, 48186, 19514, 33853, 58443, 25677,
        19533, 11344, 42067,  3155, 42070,  1113, 31836, 54367, 19551, 50276,
        58471, 52329,  5231, 40049, 58485, 29814, 40054, 29813, 23671,  9347,
        35972, 35973, 19595, 58507, 52364, 11406, 21646, 11407, 17552,  3222,
         3230, 48289, 46244, 25766,  3238, 40103, 25772, 48302, 54454, 48312,
        23740, 25789,  7360, 15557,  1223, 31944, 11465, 50382, 31974, 36079,
        19697, 36095, 52482, 32006, 42258,  1302, 46359, 40217, 54554, 34080,
        23849, 46378, 19759, 50480, 36145, 32048, 40242, 30004, 48436, 36148,
         5431,  1341, 11584,  7491, 38211,  7494, 36167, 40264,  3402,  7501,
        19796,  9560, 30044, 32092,  5471, 25953, 34145, 50531, 58722, 46445,
        42364, 17789, 30078, 52614, 11656, 58767,  7578, 54689, 58786,  1445,
        50601,  3498, 36266,  5548,  9645, 44464, 11704, 46521, 32189, 46529,
        34253, 15821, 34260, 58838, 48599, 44507, 21985, 42466, 15843, 44516,
        44519, 36329, 48623, 34290, 19965,  5634, 24073, 44555, 42509, 50704,
        36385, 54821,  9766, 15916, 48685, 34350, 28208, 22068,  9784, 52793,
         5690, 36411, 58939, 15934, 32329, 44621, 18002,  7766, 18006,  1627,
        26210,  5731, 42600, 24181, 59007,  7808,  1665, 50822, 52878, 32401,
        34454, 16035, 44707,  1699, 20139, 57003, 24240, 14005,  7866,  3771,
        48830, 50883,  3781, 48857, 40667, 36577,  3810, 52963,  5872,  1778,
        12018, 22262, 59131, 44797,  5886, 57088, 46853, 30477, 44817, 18194,
        57111, 26394, 22298, 28444, 30492, 18206, 38695, 10024,  5928, 14128,
        22320,  3892, 30517, 28470, 32569, 44860, 14141, 14142, 32576, 36679,
        36696, 28510, 26468, 10086, 34664, 40810, 46955,  3958, 38775, 26500,
        12167, 59272, 53139, 44957, 10142, 14238, 53153, 40869, 30629,  1980,
        55231,  6084, 55245, 26574, 57303, 30691, 30699, 26606,  8180, 10233])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.60003280639648
training time full:: 109.61005330085754
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 104.44069027900696
time_baseline:: 104.70688343048096
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.48564338684082
curr_diff: 0 tensor(4.1244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.31840968132019
training time full:: 109.3298614025116
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.1371853351593
time_baseline:: 106.40615701675415
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.41671109199524
curr_diff: 0 tensor(4.1244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.4850263595581
training time full:: 109.49674534797668
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.4529013633728
time_baseline:: 105.72031903266907
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.55189561843872
curr_diff: 0 tensor(4.1244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.62130832672119
training time full:: 109.63315105438232
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 104.81265997886658
time_baseline:: 105.08227157592773
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 63.203702211380005
curr_diff: 0 tensor(4.1244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.64115309715271
training time full:: 110.65292692184448
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.21164846420288
time_baseline:: 105.48045134544373
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 62.88075399398804
curr_diff: 0 tensor(4.1244e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1244e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874900
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.8885715007782
training time full:: 52.89558219909668
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.176700830459595
time_baseline:: 48.31279802322388
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.407028913497925
curr_diff: 0 tensor(8.7221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.60768961906433
training time full:: 52.61379933357239
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.424821615219116
time_baseline:: 48.55871391296387
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.343992471694946
curr_diff: 0 tensor(8.7221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.469924211502075
training time full:: 52.47616958618164
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.38940739631653
time_baseline:: 48.578041553497314
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.846132040023804
curr_diff: 0 tensor(8.7221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 51.787436723709106
training time full:: 51.7936692237854
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.71143412590027
time_baseline:: 47.847163915634155
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.31784987449646
curr_diff: 0 tensor(8.7221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.23017454147339
training time full:: 52.23653841018677
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.543816804885864
time_baseline:: 48.67971730232239
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.348713636398315
curr_diff: 0 tensor(8.7221e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7221e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0131, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0131, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.377686500549316
training time full:: 27.380134105682373
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.9367094039917
time_baseline:: 24.007845401763916
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.599621295928955
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.40945315361023
training time full:: 27.41188931465149
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.171406269073486
time_baseline:: 24.241464614868164
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.410784721374512
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.25481867790222
training time full:: 27.257038354873657
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.41786003112793
time_baseline:: 24.488059759140015
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.751493692398071
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.532066106796265
training time full:: 27.534252882003784
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.929324865341187
time_baseline:: 24.000975847244263
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.545555114746094
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.265784978866577
training time full:: 27.267940282821655
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.84652829170227
time_baseline:: 23.91570281982422
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 15.359441995620728
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.852872133255005
training time full:: 5.852911472320557
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.050714015960693
time_baseline:: 5.068141937255859
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.77083420753479
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.891576528549194
training time full:: 5.8916144371032715
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.071995258331299
time_baseline:: 5.08952784538269
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.714869499206543
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9529783725738525
training time full:: 5.953017473220825
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.04358696937561
time_baseline:: 5.0612287521362305
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7845826148986816
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.866622686386108
training time full:: 5.866662263870239
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.178697824478149
time_baseline:: 5.19597864151001
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.774822950363159
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.856825590133667
training time full:: 5.856865644454956
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.011727333068848
time_baseline:: 5.029100656509399
curr_diff: 0 tensor(0.0132, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0132, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.7688088417053223
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875100
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7288727760314941
training time full:: 1.7289092540740967
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3493437767028809
time_baseline:: 1.3542251586914062
curr_diff: 0 tensor(0.0134, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0134, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2032365798950195
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6466875076293945
training time full:: 1.6467230319976807
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3645586967468262
time_baseline:: 1.369518756866455
curr_diff: 0 tensor(0.0134, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0134, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1986725330352783
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6814649105072021
training time full:: 1.6815016269683838
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.345548391342163
time_baseline:: 1.3504691123962402
curr_diff: 0 tensor(0.0134, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0134, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1871328353881836
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.738534688949585
training time full:: 1.7385706901550293
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3644373416900635
time_baseline:: 1.369368553161621
curr_diff: 0 tensor(0.0134, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0134, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.2234041690826416
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6551189422607422
training time full:: 1.6551547050476074
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4137170314788818
time_baseline:: 1.4275763034820557
curr_diff: 0 tensor(0.0134, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0134, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1783716678619385
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
