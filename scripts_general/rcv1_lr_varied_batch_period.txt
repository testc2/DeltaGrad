period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression rcv1 16384 200 3
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  rcv1 1
torch.Size([20242, 47236])
tensor([10510])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693032
Train - Epoch 1, Batch: 0, Loss: 0.692659
Train - Epoch 2, Batch: 0, Loss: 0.692268
Train - Epoch 3, Batch: 0, Loss: 0.691873
Train - Epoch 4, Batch: 0, Loss: 0.691554
Train - Epoch 5, Batch: 0, Loss: 0.691166
Train - Epoch 6, Batch: 0, Loss: 0.690792
Train - Epoch 7, Batch: 0, Loss: 0.690386
Train - Epoch 8, Batch: 0, Loss: 0.690083
Train - Epoch 9, Batch: 0, Loss: 0.689677
Train - Epoch 10, Batch: 0, Loss: 0.689340
Train - Epoch 11, Batch: 0, Loss: 0.688978
Train - Epoch 12, Batch: 0, Loss: 0.688589
Train - Epoch 13, Batch: 0, Loss: 0.688253
Train - Epoch 14, Batch: 0, Loss: 0.687928
Train - Epoch 15, Batch: 0, Loss: 0.687510
Train - Epoch 16, Batch: 0, Loss: 0.687103
Train - Epoch 17, Batch: 0, Loss: 0.686770
Train - Epoch 18, Batch: 0, Loss: 0.686397
Train - Epoch 19, Batch: 0, Loss: 0.685950
Train - Epoch 20, Batch: 0, Loss: 0.685656
Train - Epoch 21, Batch: 0, Loss: 0.685338
Train - Epoch 22, Batch: 0, Loss: 0.685045
Train - Epoch 23, Batch: 0, Loss: 0.684568
Train - Epoch 24, Batch: 0, Loss: 0.684281
Train - Epoch 25, Batch: 0, Loss: 0.683872
Train - Epoch 26, Batch: 0, Loss: 0.683576
Train - Epoch 27, Batch: 0, Loss: 0.683249
Train - Epoch 28, Batch: 0, Loss: 0.682894
Train - Epoch 29, Batch: 0, Loss: 0.682539
Train - Epoch 30, Batch: 0, Loss: 0.682189
Train - Epoch 31, Batch: 0, Loss: 0.681926
Train - Epoch 32, Batch: 0, Loss: 0.681426
Train - Epoch 33, Batch: 0, Loss: 0.681136
Train - Epoch 34, Batch: 0, Loss: 0.680740
Train - Epoch 35, Batch: 0, Loss: 0.680367
Train - Epoch 36, Batch: 0, Loss: 0.680061
Train - Epoch 37, Batch: 0, Loss: 0.679806
Train - Epoch 38, Batch: 0, Loss: 0.679431
Train - Epoch 39, Batch: 0, Loss: 0.679102
Train - Epoch 40, Batch: 0, Loss: 0.678799
Train - Epoch 41, Batch: 0, Loss: 0.678334
Train - Epoch 42, Batch: 0, Loss: 0.678125
Train - Epoch 43, Batch: 0, Loss: 0.677609
Train - Epoch 44, Batch: 0, Loss: 0.677160
Train - Epoch 45, Batch: 0, Loss: 0.676881
Train - Epoch 46, Batch: 0, Loss: 0.676667
Train - Epoch 47, Batch: 0, Loss: 0.676387
Train - Epoch 48, Batch: 0, Loss: 0.675988
Train - Epoch 49, Batch: 0, Loss: 0.675603
Train - Epoch 50, Batch: 0, Loss: 0.675123
Train - Epoch 51, Batch: 0, Loss: 0.675007
Train - Epoch 52, Batch: 0, Loss: 0.674745
Train - Epoch 53, Batch: 0, Loss: 0.674203
Train - Epoch 54, Batch: 0, Loss: 0.673973
Train - Epoch 55, Batch: 0, Loss: 0.673878
Train - Epoch 56, Batch: 0, Loss: 0.673388
Train - Epoch 57, Batch: 0, Loss: 0.673027
Train - Epoch 58, Batch: 0, Loss: 0.672566
Train - Epoch 59, Batch: 0, Loss: 0.672356
Train - Epoch 60, Batch: 0, Loss: 0.671873
Train - Epoch 61, Batch: 0, Loss: 0.671688
Train - Epoch 62, Batch: 0, Loss: 0.671274
Train - Epoch 63, Batch: 0, Loss: 0.671062
Train - Epoch 64, Batch: 0, Loss: 0.670578
Train - Epoch 65, Batch: 0, Loss: 0.670438
Train - Epoch 66, Batch: 0, Loss: 0.669898
Train - Epoch 67, Batch: 0, Loss: 0.669850
Train - Epoch 68, Batch: 0, Loss: 0.669697
Train - Epoch 69, Batch: 0, Loss: 0.668968
Train - Epoch 70, Batch: 0, Loss: 0.668850
Train - Epoch 71, Batch: 0, Loss: 0.668446
Train - Epoch 72, Batch: 0, Loss: 0.668068
Train - Epoch 73, Batch: 0, Loss: 0.667922
Train - Epoch 74, Batch: 0, Loss: 0.667581
Train - Epoch 75, Batch: 0, Loss: 0.667046
Train - Epoch 76, Batch: 0, Loss: 0.666669
Train - Epoch 77, Batch: 0, Loss: 0.666447
Train - Epoch 78, Batch: 0, Loss: 0.666309
Train - Epoch 79, Batch: 0, Loss: 0.665981
Train - Epoch 80, Batch: 0, Loss: 0.665528
Train - Epoch 81, Batch: 0, Loss: 0.665635
Train - Epoch 82, Batch: 0, Loss: 0.664942
Train - Epoch 83, Batch: 0, Loss: 0.664647
Train - Epoch 84, Batch: 0, Loss: 0.664280
Train - Epoch 85, Batch: 0, Loss: 0.664155
Train - Epoch 86, Batch: 0, Loss: 0.663620
Train - Epoch 87, Batch: 0, Loss: 0.663309
Train - Epoch 88, Batch: 0, Loss: 0.663054
Train - Epoch 89, Batch: 0, Loss: 0.662734
Train - Epoch 90, Batch: 0, Loss: 0.662628
Train - Epoch 91, Batch: 0, Loss: 0.662440
Train - Epoch 92, Batch: 0, Loss: 0.661523
Train - Epoch 93, Batch: 0, Loss: 0.661384
Train - Epoch 94, Batch: 0, Loss: 0.661292
Train - Epoch 95, Batch: 0, Loss: 0.660957
Train - Epoch 96, Batch: 0, Loss: 0.660885
Train - Epoch 97, Batch: 0, Loss: 0.660740
Train - Epoch 98, Batch: 0, Loss: 0.660391
Train - Epoch 99, Batch: 0, Loss: 0.659995
Train - Epoch 100, Batch: 0, Loss: 0.659468
Train - Epoch 101, Batch: 0, Loss: 0.659507
Train - Epoch 102, Batch: 0, Loss: 0.658670
Train - Epoch 103, Batch: 0, Loss: 0.658588
Train - Epoch 104, Batch: 0, Loss: 0.658266
Train - Epoch 105, Batch: 0, Loss: 0.658507
Train - Epoch 106, Batch: 0, Loss: 0.657867
Train - Epoch 107, Batch: 0, Loss: 0.657532
Train - Epoch 108, Batch: 0, Loss: 0.657080
Train - Epoch 109, Batch: 0, Loss: 0.656770
Train - Epoch 110, Batch: 0, Loss: 0.656461
Train - Epoch 111, Batch: 0, Loss: 0.656181
Train - Epoch 112, Batch: 0, Loss: 0.655524
Train - Epoch 113, Batch: 0, Loss: 0.655664
Train - Epoch 114, Batch: 0, Loss: 0.655511
Train - Epoch 115, Batch: 0, Loss: 0.655239
Train - Epoch 116, Batch: 0, Loss: 0.654942
Train - Epoch 117, Batch: 0, Loss: 0.654555
Train - Epoch 118, Batch: 0, Loss: 0.654713
Train - Epoch 119, Batch: 0, Loss: 0.654142
Train - Epoch 120, Batch: 0, Loss: 0.653551
Train - Epoch 121, Batch: 0, Loss: 0.653567
Train - Epoch 122, Batch: 0, Loss: 0.653129
Train - Epoch 123, Batch: 0, Loss: 0.652927
Train - Epoch 124, Batch: 0, Loss: 0.652128
Train - Epoch 125, Batch: 0, Loss: 0.652419
Train - Epoch 126, Batch: 0, Loss: 0.651829
Train - Epoch 127, Batch: 0, Loss: 0.652114
Train - Epoch 128, Batch: 0, Loss: 0.651681
Train - Epoch 129, Batch: 0, Loss: 0.650530
Train - Epoch 130, Batch: 0, Loss: 0.650666
Train - Epoch 131, Batch: 0, Loss: 0.650723
Train - Epoch 132, Batch: 0, Loss: 0.650557
Train - Epoch 133, Batch: 0, Loss: 0.649999
Train - Epoch 134, Batch: 0, Loss: 0.649795
Train - Epoch 135, Batch: 0, Loss: 0.649687
Train - Epoch 136, Batch: 0, Loss: 0.649164
Train - Epoch 137, Batch: 0, Loss: 0.649026
Train - Epoch 138, Batch: 0, Loss: 0.648538
Train - Epoch 139, Batch: 0, Loss: 0.648049
Train - Epoch 140, Batch: 0, Loss: 0.647944
Train - Epoch 141, Batch: 0, Loss: 0.648027
Train - Epoch 142, Batch: 0, Loss: 0.647355
Train - Epoch 143, Batch: 0, Loss: 0.646850
Train - Epoch 144, Batch: 0, Loss: 0.646883
Train - Epoch 145, Batch: 0, Loss: 0.646272
Train - Epoch 146, Batch: 0, Loss: 0.646457
Train - Epoch 147, Batch: 0, Loss: 0.646326
Train - Epoch 148, Batch: 0, Loss: 0.645872
Train - Epoch 149, Batch: 0, Loss: 0.645983
Train - Epoch 150, Batch: 0, Loss: 0.645624
Train - Epoch 151, Batch: 0, Loss: 0.644896
Train - Epoch 152, Batch: 0, Loss: 0.644937
Train - Epoch 153, Batch: 0, Loss: 0.644376
Train - Epoch 154, Batch: 0, Loss: 0.644006
Train - Epoch 155, Batch: 0, Loss: 0.643346
Train - Epoch 156, Batch: 0, Loss: 0.644260
Train - Epoch 157, Batch: 0, Loss: 0.643811
Train - Epoch 158, Batch: 0, Loss: 0.643341
Train - Epoch 159, Batch: 0, Loss: 0.642775
Train - Epoch 160, Batch: 0, Loss: 0.642710
Train - Epoch 161, Batch: 0, Loss: 0.642408
Train - Epoch 162, Batch: 0, Loss: 0.642047
Train - Epoch 163, Batch: 0, Loss: 0.641461
Train - Epoch 164, Batch: 0, Loss: 0.641873
Train - Epoch 165, Batch: 0, Loss: 0.641281
Train - Epoch 166, Batch: 0, Loss: 0.641123
Train - Epoch 167, Batch: 0, Loss: 0.641124
Train - Epoch 168, Batch: 0, Loss: 0.640555
Train - Epoch 169, Batch: 0, Loss: 0.640379
Train - Epoch 170, Batch: 0, Loss: 0.639994
Train - Epoch 171, Batch: 0, Loss: 0.639759
Train - Epoch 172, Batch: 0, Loss: 0.639365
Train - Epoch 173, Batch: 0, Loss: 0.639118
Train - Epoch 174, Batch: 0, Loss: 0.639029
Train - Epoch 175, Batch: 0, Loss: 0.638401
Train - Epoch 176, Batch: 0, Loss: 0.638737
Train - Epoch 177, Batch: 0, Loss: 0.638673
Train - Epoch 178, Batch: 0, Loss: 0.638034
Train - Epoch 179, Batch: 0, Loss: 0.637902
Train - Epoch 180, Batch: 0, Loss: 0.637094
Train - Epoch 181, Batch: 0, Loss: 0.637334
Train - Epoch 182, Batch: 0, Loss: 0.637109
Train - Epoch 183, Batch: 0, Loss: 0.636612
Train - Epoch 184, Batch: 0, Loss: 0.636757
Train - Epoch 185, Batch: 0, Loss: 0.636330
Train - Epoch 186, Batch: 0, Loss: 0.636038/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635539
Train - Epoch 188, Batch: 0, Loss: 0.635121
Train - Epoch 189, Batch: 0, Loss: 0.636006
Train - Epoch 190, Batch: 0, Loss: 0.635542
Train - Epoch 191, Batch: 0, Loss: 0.635060
Train - Epoch 192, Batch: 0, Loss: 0.634327
Train - Epoch 193, Batch: 0, Loss: 0.634126
Train - Epoch 194, Batch: 0, Loss: 0.634447
Train - Epoch 195, Batch: 0, Loss: 0.633296
Train - Epoch 196, Batch: 0, Loss: 0.633106
Train - Epoch 197, Batch: 0, Loss: 0.633405
Train - Epoch 198, Batch: 0, Loss: 0.632997
Train - Epoch 199, Batch: 0, Loss: 0.632904
training_time:: 351.0330638885498
training time full:: 351.0331325531006
provenance prepare time:: 7.867813110351562e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926835
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.73792576789856
overhead:: 0
overhead2:: 0.2142322063446045
overhead3:: 0
time_baseline:: 275.73821091651917
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926835
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624856)
RCV1 Test Avg. Accuracy:: 0.9224341931417082
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.01288151741027832
overhead3:: 0.12392187118530273
overhead4:: 33.44315147399902
overhead5:: 0
memory usage:: 26584731648
time_provenance:: 44.40089559555054
curr_diff: 0 tensor(4.2572e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2572e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926835
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.00005_10200
tensor(624856)
RCV1 Test Avg. Accuracy:: 0.9224341931417082
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693328
Train - Epoch 1, Batch: 0, Loss: 0.692959
Train - Epoch 2, Batch: 0, Loss: 0.692557
Train - Epoch 3, Batch: 0, Loss: 0.692182
Train - Epoch 4, Batch: 0, Loss: 0.691807
Train - Epoch 5, Batch: 0, Loss: 0.691444
Train - Epoch 6, Batch: 0, Loss: 0.691077
Train - Epoch 7, Batch: 0, Loss: 0.690700
Train - Epoch 8, Batch: 0, Loss: 0.690345
Train - Epoch 9, Batch: 0, Loss: 0.690009
Train - Epoch 10, Batch: 0, Loss: 0.689596
Train - Epoch 11, Batch: 0, Loss: 0.689201
Train - Epoch 12, Batch: 0, Loss: 0.688777
Train - Epoch 13, Batch: 0, Loss: 0.688505
Train - Epoch 14, Batch: 0, Loss: 0.688092
Train - Epoch 15, Batch: 0, Loss: 0.687784
Train - Epoch 16, Batch: 0, Loss: 0.687437
Train - Epoch 17, Batch: 0, Loss: 0.687072
Train - Epoch 18, Batch: 0, Loss: 0.686706
Train - Epoch 19, Batch: 0, Loss: 0.686363
Train - Epoch 20, Batch: 0, Loss: 0.685914
Train - Epoch 21, Batch: 0, Loss: 0.685627
Train - Epoch 22, Batch: 0, Loss: 0.685232
Train - Epoch 23, Batch: 0, Loss: 0.684890
Train - Epoch 24, Batch: 0, Loss: 0.684578
Train - Epoch 25, Batch: 0, Loss: 0.684183
Train - Epoch 26, Batch: 0, Loss: 0.683977
Train - Epoch 27, Batch: 0, Loss: 0.683544
Train - Epoch 28, Batch: 0, Loss: 0.683031
Train - Epoch 29, Batch: 0, Loss: 0.682818
Train - Epoch 30, Batch: 0, Loss: 0.682513
Train - Epoch 31, Batch: 0, Loss: 0.682114
Train - Epoch 32, Batch: 0, Loss: 0.681745
Train - Epoch 33, Batch: 0, Loss: 0.681381
Train - Epoch 34, Batch: 0, Loss: 0.680900
Train - Epoch 35, Batch: 0, Loss: 0.680706
Train - Epoch 36, Batch: 0, Loss: 0.680312
Train - Epoch 37, Batch: 0, Loss: 0.680054
Train - Epoch 38, Batch: 0, Loss: 0.679680
Train - Epoch 39, Batch: 0, Loss: 0.679230
Train - Epoch 40, Batch: 0, Loss: 0.678947
Train - Epoch 41, Batch: 0, Loss: 0.678464
Train - Epoch 42, Batch: 0, Loss: 0.678245
Train - Epoch 43, Batch: 0, Loss: 0.677883
Train - Epoch 44, Batch: 0, Loss: 0.677635
Train - Epoch 45, Batch: 0, Loss: 0.677280
Train - Epoch 46, Batch: 0, Loss: 0.676927
Train - Epoch 47, Batch: 0, Loss: 0.676649
Train - Epoch 48, Batch: 0, Loss: 0.676296
Train - Epoch 49, Batch: 0, Loss: 0.675856
Train - Epoch 50, Batch: 0, Loss: 0.675580
Train - Epoch 51, Batch: 0, Loss: 0.675211
Train - Epoch 52, Batch: 0, Loss: 0.674808
Train - Epoch 53, Batch: 0, Loss: 0.674511
Train - Epoch 54, Batch: 0, Loss: 0.674206
Train - Epoch 55, Batch: 0, Loss: 0.673849
Train - Epoch 56, Batch: 0, Loss: 0.673706
Train - Epoch 57, Batch: 0, Loss: 0.673257
Train - Epoch 58, Batch: 0, Loss: 0.672971
Train - Epoch 59, Batch: 0, Loss: 0.672630
Train - Epoch 60, Batch: 0, Loss: 0.672337
Train - Epoch 61, Batch: 0, Loss: 0.671952
Train - Epoch 62, Batch: 0, Loss: 0.671521
Train - Epoch 63, Batch: 0, Loss: 0.671138
Train - Epoch 64, Batch: 0, Loss: 0.671024
Train - Epoch 65, Batch: 0, Loss: 0.670790
Train - Epoch 66, Batch: 0, Loss: 0.670323
Train - Epoch 67, Batch: 0, Loss: 0.670121
Train - Epoch 68, Batch: 0, Loss: 0.669746
Train - Epoch 69, Batch: 0, Loss: 0.669404
Train - Epoch 70, Batch: 0, Loss: 0.669306
Train - Epoch 71, Batch: 0, Loss: 0.668659
Train - Epoch 72, Batch: 0, Loss: 0.668133
Train - Epoch 73, Batch: 0, Loss: 0.668107
Train - Epoch 74, Batch: 0, Loss: 0.668083
Train - Epoch 75, Batch: 0, Loss: 0.667708
Train - Epoch 76, Batch: 0, Loss: 0.667161
Train - Epoch 77, Batch: 0, Loss: 0.666672
Train - Epoch 78, Batch: 0, Loss: 0.666918
Train - Epoch 79, Batch: 0, Loss: 0.666147
Train - Epoch 80, Batch: 0, Loss: 0.665721
Train - Epoch 81, Batch: 0, Loss: 0.665534
Train - Epoch 82, Batch: 0, Loss: 0.665467
Train - Epoch 83, Batch: 0, Loss: 0.665170
Train - Epoch 84, Batch: 0, Loss: 0.664775
Train - Epoch 85, Batch: 0, Loss: 0.664166
Train - Epoch 86, Batch: 0, Loss: 0.663727
Train - Epoch 87, Batch: 0, Loss: 0.663635
Train - Epoch 88, Batch: 0, Loss: 0.663402
Train - Epoch 89, Batch: 0, Loss: 0.663046
Train - Epoch 90, Batch: 0, Loss: 0.662498
Train - Epoch 91, Batch: 0, Loss: 0.662436
Train - Epoch 92, Batch: 0, Loss: 0.661885
Train - Epoch 93, Batch: 0, Loss: 0.661732
Train - Epoch 94, Batch: 0, Loss: 0.661810
Train - Epoch 95, Batch: 0, Loss: 0.661412
Train - Epoch 96, Batch: 0, Loss: 0.661116
Train - Epoch 97, Batch: 0, Loss: 0.660312
Train - Epoch 98, Batch: 0, Loss: 0.660275
Train - Epoch 99, Batch: 0, Loss: 0.659871
Train - Epoch 100, Batch: 0, Loss: 0.659437
Train - Epoch 101, Batch: 0, Loss: 0.659487
Train - Epoch 102, Batch: 0, Loss: 0.658952
Train - Epoch 103, Batch: 0, Loss: 0.658884
Train - Epoch 104, Batch: 0, Loss: 0.658523
Train - Epoch 105, Batch: 0, Loss: 0.658065
Train - Epoch 106, Batch: 0, Loss: 0.657620
Train - Epoch 107, Batch: 0, Loss: 0.657470
Train - Epoch 108, Batch: 0, Loss: 0.657754
Train - Epoch 109, Batch: 0, Loss: 0.657163
Train - Epoch 110, Batch: 0, Loss: 0.656703
Train - Epoch 111, Batch: 0, Loss: 0.656479
Train - Epoch 112, Batch: 0, Loss: 0.656014
Train - Epoch 113, Batch: 0, Loss: 0.656261
Train - Epoch 114, Batch: 0, Loss: 0.655509
Train - Epoch 115, Batch: 0, Loss: 0.655159
Train - Epoch 116, Batch: 0, Loss: 0.655182
Train - Epoch 117, Batch: 0, Loss: 0.654695
Train - Epoch 118, Batch: 0, Loss: 0.654769
Train - Epoch 119, Batch: 0, Loss: 0.653998
Train - Epoch 120, Batch: 0, Loss: 0.653870
Train - Epoch 121, Batch: 0, Loss: 0.653812
Train - Epoch 122, Batch: 0, Loss: 0.653507
Train - Epoch 123, Batch: 0, Loss: 0.652921
Train - Epoch 124, Batch: 0, Loss: 0.652740
Train - Epoch 125, Batch: 0, Loss: 0.652612
Train - Epoch 126, Batch: 0, Loss: 0.652287
Train - Epoch 127, Batch: 0, Loss: 0.651932
Train - Epoch 128, Batch: 0, Loss: 0.651461
Train - Epoch 129, Batch: 0, Loss: 0.651472
Train - Epoch 130, Batch: 0, Loss: 0.650842
Train - Epoch 131, Batch: 0, Loss: 0.650931
Train - Epoch 132, Batch: 0, Loss: 0.650231
Train - Epoch 133, Batch: 0, Loss: 0.650331
Train - Epoch 134, Batch: 0, Loss: 0.649976
Train - Epoch 135, Batch: 0, Loss: 0.649766
Train - Epoch 136, Batch: 0, Loss: 0.649697
Train - Epoch 137, Batch: 0, Loss: 0.649439
Train - Epoch 138, Batch: 0, Loss: 0.648647
Train - Epoch 139, Batch: 0, Loss: 0.648514
Train - Epoch 140, Batch: 0, Loss: 0.648019
Train - Epoch 141, Batch: 0, Loss: 0.648172
Train - Epoch 142, Batch: 0, Loss: 0.647893
Train - Epoch 143, Batch: 0, Loss: 0.647484
Train - Epoch 144, Batch: 0, Loss: 0.646861
Train - Epoch 145, Batch: 0, Loss: 0.647208
Train - Epoch 146, Batch: 0, Loss: 0.646630
Train - Epoch 147, Batch: 0, Loss: 0.646381
Train - Epoch 148, Batch: 0, Loss: 0.646141
Train - Epoch 149, Batch: 0, Loss: 0.646062
Train - Epoch 150, Batch: 0, Loss: 0.645480
Train - Epoch 151, Batch: 0, Loss: 0.645261
Train - Epoch 152, Batch: 0, Loss: 0.644781
Train - Epoch 153, Batch: 0, Loss: 0.645229
Train - Epoch 154, Batch: 0, Loss: 0.644124
Train - Epoch 155, Batch: 0, Loss: 0.643961
Train - Epoch 156, Batch: 0, Loss: 0.643778
Train - Epoch 157, Batch: 0, Loss: 0.643584
Train - Epoch 158, Batch: 0, Loss: 0.643637
Train - Epoch 159, Batch: 0, Loss: 0.642579
Train - Epoch 160, Batch: 0, Loss: 0.643005
Train - Epoch 161, Batch: 0, Loss: 0.642679
Train - Epoch 162, Batch: 0, Loss: 0.642585
Train - Epoch 163, Batch: 0, Loss: 0.641861
Train - Epoch 164, Batch: 0, Loss: 0.641881
Train - Epoch 165, Batch: 0, Loss: 0.641863
Train - Epoch 166, Batch: 0, Loss: 0.640861
Train - Epoch 167, Batch: 0, Loss: 0.641200
Train - Epoch 168, Batch: 0, Loss: 0.640995
Train - Epoch 169, Batch: 0, Loss: 0.640023
Train - Epoch 170, Batch: 0, Loss: 0.639993
Train - Epoch 171, Batch: 0, Loss: 0.640001
Train - Epoch 172, Batch: 0, Loss: 0.639257
Train - Epoch 173, Batch: 0, Loss: 0.639134
Train - Epoch 174, Batch: 0, Loss: 0.638857
Train - Epoch 175, Batch: 0, Loss: 0.638893
Train - Epoch 176, Batch: 0, Loss: 0.638796
Train - Epoch 177, Batch: 0, Loss: 0.638521
Train - Epoch 178, Batch: 0, Loss: 0.638726
Train - Epoch 179, Batch: 0, Loss: 0.638695
Train - Epoch 180, Batch: 0, Loss: 0.637755
Train - Epoch 181, Batch: 0, Loss: 0.637723
Train - Epoch 182, Batch: 0, Loss: 0.637336
Train - Epoch 183, Batch: 0, Loss: 0.636865
Train - Epoch 184, Batch: 0, Loss: 0.636714
Train - Epoch 185, Batch: 0, Loss: 0.636626
Train - Epoch 186, Batch: 0, Loss: 0.635934/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635702
Train - Epoch 188, Batch: 0, Loss: 0.635700
Train - Epoch 189, Batch: 0, Loss: 0.635232
Train - Epoch 190, Batch: 0, Loss: 0.635706
Train - Epoch 191, Batch: 0, Loss: 0.634556
Train - Epoch 192, Batch: 0, Loss: 0.634351
Train - Epoch 193, Batch: 0, Loss: 0.635286
Train - Epoch 194, Batch: 0, Loss: 0.633838
Train - Epoch 195, Batch: 0, Loss: 0.633822
Train - Epoch 196, Batch: 0, Loss: 0.633932
Train - Epoch 197, Batch: 0, Loss: 0.633644
Train - Epoch 198, Batch: 0, Loss: 0.633557
Train - Epoch 199, Batch: 0, Loss: 0.632776
training_time:: 351.0570652484894
training time full:: 351.0571298599243
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000063, Accuracy: 0.927428
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.50491428375244
overhead:: 0
overhead2:: 0.21074914932250977
overhead3:: 0
time_baseline:: 275.50520157814026
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927428
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624725)
RCV1 Test Avg. Accuracy:: 0.9222408063785155
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.01103973388671875
overhead3:: 0.12195181846618652
overhead4:: 33.4955997467041
overhead5:: 0
memory usage:: 26587619328
time_provenance:: 44.80038094520569
curr_diff: 0 tensor(4.7507e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7507e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927428
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.00005_10200
tensor(624725)
RCV1 Test Avg. Accuracy:: 0.9222408063785155
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693015
Train - Epoch 1, Batch: 0, Loss: 0.692641
Train - Epoch 2, Batch: 0, Loss: 0.692277
Train - Epoch 3, Batch: 0, Loss: 0.691899
Train - Epoch 4, Batch: 0, Loss: 0.691529
Train - Epoch 5, Batch: 0, Loss: 0.691146
Train - Epoch 6, Batch: 0, Loss: 0.690784
Train - Epoch 7, Batch: 0, Loss: 0.690413
Train - Epoch 8, Batch: 0, Loss: 0.690025
Train - Epoch 9, Batch: 0, Loss: 0.689640
Train - Epoch 10, Batch: 0, Loss: 0.689313
Train - Epoch 11, Batch: 0, Loss: 0.688955
Train - Epoch 12, Batch: 0, Loss: 0.688557
Train - Epoch 13, Batch: 0, Loss: 0.688249
Train - Epoch 14, Batch: 0, Loss: 0.687810
Train - Epoch 15, Batch: 0, Loss: 0.687526
Train - Epoch 16, Batch: 0, Loss: 0.687112
Train - Epoch 17, Batch: 0, Loss: 0.686722
Train - Epoch 18, Batch: 0, Loss: 0.686371
Train - Epoch 19, Batch: 0, Loss: 0.685954
Train - Epoch 20, Batch: 0, Loss: 0.685631
Train - Epoch 21, Batch: 0, Loss: 0.685237
Train - Epoch 22, Batch: 0, Loss: 0.684959
Train - Epoch 23, Batch: 0, Loss: 0.684718
Train - Epoch 24, Batch: 0, Loss: 0.684282
Train - Epoch 25, Batch: 0, Loss: 0.683855
Train - Epoch 26, Batch: 0, Loss: 0.683579
Train - Epoch 27, Batch: 0, Loss: 0.683237
Train - Epoch 28, Batch: 0, Loss: 0.682826
Train - Epoch 29, Batch: 0, Loss: 0.682446
Train - Epoch 30, Batch: 0, Loss: 0.682024
Train - Epoch 31, Batch: 0, Loss: 0.681795
Train - Epoch 32, Batch: 0, Loss: 0.681474
Train - Epoch 33, Batch: 0, Loss: 0.681017
Train - Epoch 34, Batch: 0, Loss: 0.680554
Train - Epoch 35, Batch: 0, Loss: 0.680514
Train - Epoch 36, Batch: 0, Loss: 0.680053
Train - Epoch 37, Batch: 0, Loss: 0.679771
Train - Epoch 38, Batch: 0, Loss: 0.679331
Train - Epoch 39, Batch: 0, Loss: 0.679088
Train - Epoch 40, Batch: 0, Loss: 0.678606
Train - Epoch 41, Batch: 0, Loss: 0.678197
Train - Epoch 42, Batch: 0, Loss: 0.677920
Train - Epoch 43, Batch: 0, Loss: 0.677764
Train - Epoch 44, Batch: 0, Loss: 0.677360
Train - Epoch 45, Batch: 0, Loss: 0.676917
Train - Epoch 46, Batch: 0, Loss: 0.676634
Train - Epoch 47, Batch: 0, Loss: 0.676365
Train - Epoch 48, Batch: 0, Loss: 0.675819
Train - Epoch 49, Batch: 0, Loss: 0.675677
Train - Epoch 50, Batch: 0, Loss: 0.675299
Train - Epoch 51, Batch: 0, Loss: 0.675097
Train - Epoch 52, Batch: 0, Loss: 0.674553
Train - Epoch 53, Batch: 0, Loss: 0.674395
Train - Epoch 54, Batch: 0, Loss: 0.673817
Train - Epoch 55, Batch: 0, Loss: 0.673732
Train - Epoch 56, Batch: 0, Loss: 0.673215
Train - Epoch 57, Batch: 0, Loss: 0.673045
Train - Epoch 58, Batch: 0, Loss: 0.672686
Train - Epoch 59, Batch: 0, Loss: 0.672187
Train - Epoch 60, Batch: 0, Loss: 0.671990
Train - Epoch 61, Batch: 0, Loss: 0.671749
Train - Epoch 62, Batch: 0, Loss: 0.671449
Train - Epoch 63, Batch: 0, Loss: 0.670963
Train - Epoch 64, Batch: 0, Loss: 0.670739
Train - Epoch 65, Batch: 0, Loss: 0.670417
Train - Epoch 66, Batch: 0, Loss: 0.670051
Train - Epoch 67, Batch: 0, Loss: 0.670117
Train - Epoch 68, Batch: 0, Loss: 0.669643
Train - Epoch 69, Batch: 0, Loss: 0.669136
Train - Epoch 70, Batch: 0, Loss: 0.668740
Train - Epoch 71, Batch: 0, Loss: 0.668505
Train - Epoch 72, Batch: 0, Loss: 0.667966
Train - Epoch 73, Batch: 0, Loss: 0.668204
Train - Epoch 74, Batch: 0, Loss: 0.667541
Train - Epoch 75, Batch: 0, Loss: 0.667128
Train - Epoch 76, Batch: 0, Loss: 0.666943
Train - Epoch 77, Batch: 0, Loss: 0.666343
Train - Epoch 78, Batch: 0, Loss: 0.666371
Train - Epoch 79, Batch: 0, Loss: 0.666066
Train - Epoch 80, Batch: 0, Loss: 0.665573
Train - Epoch 81, Batch: 0, Loss: 0.665175
Train - Epoch 82, Batch: 0, Loss: 0.665004
Train - Epoch 83, Batch: 0, Loss: 0.664837
Train - Epoch 84, Batch: 0, Loss: 0.664424
Train - Epoch 85, Batch: 0, Loss: 0.663857
Train - Epoch 86, Batch: 0, Loss: 0.663156
Train - Epoch 87, Batch: 0, Loss: 0.663170
Train - Epoch 88, Batch: 0, Loss: 0.663119
Train - Epoch 89, Batch: 0, Loss: 0.663046
Train - Epoch 90, Batch: 0, Loss: 0.662427
Train - Epoch 91, Batch: 0, Loss: 0.661860
Train - Epoch 92, Batch: 0, Loss: 0.662199
Train - Epoch 93, Batch: 0, Loss: 0.661446
Train - Epoch 94, Batch: 0, Loss: 0.661181
Train - Epoch 95, Batch: 0, Loss: 0.661075
Train - Epoch 96, Batch: 0, Loss: 0.660790
Train - Epoch 97, Batch: 0, Loss: 0.660120
Train - Epoch 98, Batch: 0, Loss: 0.659899
Train - Epoch 99, Batch: 0, Loss: 0.659944
Train - Epoch 100, Batch: 0, Loss: 0.659449
Train - Epoch 101, Batch: 0, Loss: 0.659497
Train - Epoch 102, Batch: 0, Loss: 0.658960
Train - Epoch 103, Batch: 0, Loss: 0.658337
Train - Epoch 104, Batch: 0, Loss: 0.658030
Train - Epoch 105, Batch: 0, Loss: 0.658229
Train - Epoch 106, Batch: 0, Loss: 0.657758
Train - Epoch 107, Batch: 0, Loss: 0.657465
Train - Epoch 108, Batch: 0, Loss: 0.657114
Train - Epoch 109, Batch: 0, Loss: 0.656859
Train - Epoch 110, Batch: 0, Loss: 0.656400
Train - Epoch 111, Batch: 0, Loss: 0.656139
Train - Epoch 112, Batch: 0, Loss: 0.655745
Train - Epoch 113, Batch: 0, Loss: 0.655631
Train - Epoch 114, Batch: 0, Loss: 0.655392
Train - Epoch 115, Batch: 0, Loss: 0.654958
Train - Epoch 116, Batch: 0, Loss: 0.654444
Train - Epoch 117, Batch: 0, Loss: 0.655058
Train - Epoch 118, Batch: 0, Loss: 0.653867
Train - Epoch 119, Batch: 0, Loss: 0.653935
Train - Epoch 120, Batch: 0, Loss: 0.653600
Train - Epoch 121, Batch: 0, Loss: 0.653542
Train - Epoch 122, Batch: 0, Loss: 0.653084
Train - Epoch 123, Batch: 0, Loss: 0.652428
Train - Epoch 124, Batch: 0, Loss: 0.652307
Train - Epoch 125, Batch: 0, Loss: 0.652220
Train - Epoch 126, Batch: 0, Loss: 0.651853
Train - Epoch 127, Batch: 0, Loss: 0.651375
Train - Epoch 128, Batch: 0, Loss: 0.651203
Train - Epoch 129, Batch: 0, Loss: 0.650998
Train - Epoch 130, Batch: 0, Loss: 0.650811
Train - Epoch 131, Batch: 0, Loss: 0.650297
Train - Epoch 132, Batch: 0, Loss: 0.649645
Train - Epoch 133, Batch: 0, Loss: 0.649803
Train - Epoch 134, Batch: 0, Loss: 0.649441
Train - Epoch 135, Batch: 0, Loss: 0.649497
Train - Epoch 136, Batch: 0, Loss: 0.649313
Train - Epoch 137, Batch: 0, Loss: 0.648845
Train - Epoch 138, Batch: 0, Loss: 0.648517
Train - Epoch 139, Batch: 0, Loss: 0.648095
Train - Epoch 140, Batch: 0, Loss: 0.648253
Train - Epoch 141, Batch: 0, Loss: 0.647721
Train - Epoch 142, Batch: 0, Loss: 0.647228
Train - Epoch 143, Batch: 0, Loss: 0.646557
Train - Epoch 144, Batch: 0, Loss: 0.646842
Train - Epoch 145, Batch: 0, Loss: 0.647038
Train - Epoch 146, Batch: 0, Loss: 0.646317
Train - Epoch 147, Batch: 0, Loss: 0.646340
Train - Epoch 148, Batch: 0, Loss: 0.645705
Train - Epoch 149, Batch: 0, Loss: 0.645838
Train - Epoch 150, Batch: 0, Loss: 0.645163
Train - Epoch 151, Batch: 0, Loss: 0.644752
Train - Epoch 152, Batch: 0, Loss: 0.645032
Train - Epoch 153, Batch: 0, Loss: 0.644818
Train - Epoch 154, Batch: 0, Loss: 0.644321
Train - Epoch 155, Batch: 0, Loss: 0.644471
Train - Epoch 156, Batch: 0, Loss: 0.643320
Train - Epoch 157, Batch: 0, Loss: 0.643870
Train - Epoch 158, Batch: 0, Loss: 0.643452
Train - Epoch 159, Batch: 0, Loss: 0.642830
Train - Epoch 160, Batch: 0, Loss: 0.642939
Train - Epoch 161, Batch: 0, Loss: 0.642517
Train - Epoch 162, Batch: 0, Loss: 0.641751
Train - Epoch 163, Batch: 0, Loss: 0.641434
Train - Epoch 164, Batch: 0, Loss: 0.641438
Train - Epoch 165, Batch: 0, Loss: 0.641503
Train - Epoch 166, Batch: 0, Loss: 0.640926
Train - Epoch 167, Batch: 0, Loss: 0.639906
Train - Epoch 168, Batch: 0, Loss: 0.640365
Train - Epoch 169, Batch: 0, Loss: 0.639780
Train - Epoch 170, Batch: 0, Loss: 0.640321
Train - Epoch 171, Batch: 0, Loss: 0.639497
Train - Epoch 172, Batch: 0, Loss: 0.639887
Train - Epoch 173, Batch: 0, Loss: 0.638910
Train - Epoch 174, Batch: 0, Loss: 0.638919
Train - Epoch 175, Batch: 0, Loss: 0.638909
Train - Epoch 176, Batch: 0, Loss: 0.638701
Train - Epoch 177, Batch: 0, Loss: 0.638373
Train - Epoch 178, Batch: 0, Loss: 0.637843
Train - Epoch 179, Batch: 0, Loss: 0.638230
Train - Epoch 180, Batch: 0, Loss: 0.637890
Train - Epoch 181, Batch: 0, Loss: 0.637661
Train - Epoch 182, Batch: 0, Loss: 0.636919
Train - Epoch 183, Batch: 0, Loss: 0.636723
Train - Epoch 184, Batch: 0, Loss: 0.636545
Train - Epoch 185, Batch: 0, Loss: 0.635529
Train - Epoch 186, Batch: 0, Loss: 0.635786/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635679
Train - Epoch 188, Batch: 0, Loss: 0.636175
Train - Epoch 189, Batch: 0, Loss: 0.634873
Train - Epoch 190, Batch: 0, Loss: 0.634706
Train - Epoch 191, Batch: 0, Loss: 0.634677
Train - Epoch 192, Batch: 0, Loss: 0.635124
Train - Epoch 193, Batch: 0, Loss: 0.634222
Train - Epoch 194, Batch: 0, Loss: 0.634049
Train - Epoch 195, Batch: 0, Loss: 0.633737
Train - Epoch 196, Batch: 0, Loss: 0.632991
Train - Epoch 197, Batch: 0, Loss: 0.633725
Train - Epoch 198, Batch: 0, Loss: 0.632762
Train - Epoch 199, Batch: 0, Loss: 0.632537
training_time:: 351.2998208999634
training time full:: 351.2998960018158
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000063, Accuracy: 0.927626
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.8180637359619
overhead:: 0
overhead2:: 0.21860837936401367
overhead3:: 0
time_baseline:: 275.8183524608612
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927478
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624945)
RCV1 Test Avg. Accuracy:: 0.9225655780418925
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.01309514045715332
overhead3:: 0.12423229217529297
overhead4:: 33.63849401473999
overhead5:: 0
memory usage:: 26617995264
time_provenance:: 44.60793685913086
curr_diff: 0 tensor(4.8851e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8851e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927478
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.00005_10200
tensor(624945)
RCV1 Test Avg. Accuracy:: 0.9225655780418925
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  rcv1 0
tensor([ 4244, 10510])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693098
Train - Epoch 1, Batch: 0, Loss: 0.692696
Train - Epoch 2, Batch: 0, Loss: 0.692337
Train - Epoch 3, Batch: 0, Loss: 0.691939
Train - Epoch 4, Batch: 0, Loss: 0.691612
Train - Epoch 5, Batch: 0, Loss: 0.691206
Train - Epoch 6, Batch: 0, Loss: 0.690844
Train - Epoch 7, Batch: 0, Loss: 0.690437
Train - Epoch 8, Batch: 0, Loss: 0.690116
Train - Epoch 9, Batch: 0, Loss: 0.689724
Train - Epoch 10, Batch: 0, Loss: 0.689370
Train - Epoch 11, Batch: 0, Loss: 0.689020
Train - Epoch 12, Batch: 0, Loss: 0.688650
Train - Epoch 13, Batch: 0, Loss: 0.688300
Train - Epoch 14, Batch: 0, Loss: 0.687934
Train - Epoch 15, Batch: 0, Loss: 0.687588
Train - Epoch 16, Batch: 0, Loss: 0.687138
Train - Epoch 17, Batch: 0, Loss: 0.686826
Train - Epoch 18, Batch: 0, Loss: 0.686431
Train - Epoch 19, Batch: 0, Loss: 0.686027
Train - Epoch 20, Batch: 0, Loss: 0.685696
Train - Epoch 21, Batch: 0, Loss: 0.685392
Train - Epoch 22, Batch: 0, Loss: 0.685081
Train - Epoch 23, Batch: 0, Loss: 0.684629
Train - Epoch 24, Batch: 0, Loss: 0.684323
Train - Epoch 25, Batch: 0, Loss: 0.683931
Train - Epoch 26, Batch: 0, Loss: 0.683635
Train - Epoch 27, Batch: 0, Loss: 0.683298
Train - Epoch 28, Batch: 0, Loss: 0.682952
Train - Epoch 29, Batch: 0, Loss: 0.682588
Train - Epoch 30, Batch: 0, Loss: 0.682226
Train - Epoch 31, Batch: 0, Loss: 0.681982
Train - Epoch 32, Batch: 0, Loss: 0.681486
Train - Epoch 33, Batch: 0, Loss: 0.681168
Train - Epoch 34, Batch: 0, Loss: 0.680800
Train - Epoch 35, Batch: 0, Loss: 0.680422
Train - Epoch 36, Batch: 0, Loss: 0.680129
Train - Epoch 37, Batch: 0, Loss: 0.679863
Train - Epoch 38, Batch: 0, Loss: 0.679470
Train - Epoch 39, Batch: 0, Loss: 0.679133
Train - Epoch 40, Batch: 0, Loss: 0.678833
Train - Epoch 41, Batch: 0, Loss: 0.678363
Train - Epoch 42, Batch: 0, Loss: 0.678178
Train - Epoch 43, Batch: 0, Loss: 0.677647
Train - Epoch 44, Batch: 0, Loss: 0.677211
Train - Epoch 45, Batch: 0, Loss: 0.676931
Train - Epoch 46, Batch: 0, Loss: 0.676703
Train - Epoch 47, Batch: 0, Loss: 0.676416
Train - Epoch 48, Batch: 0, Loss: 0.676047
Train - Epoch 49, Batch: 0, Loss: 0.675637
Train - Epoch 50, Batch: 0, Loss: 0.675171
Train - Epoch 51, Batch: 0, Loss: 0.675029
Train - Epoch 52, Batch: 0, Loss: 0.674775
Train - Epoch 53, Batch: 0, Loss: 0.674262
Train - Epoch 54, Batch: 0, Loss: 0.674032
Train - Epoch 55, Batch: 0, Loss: 0.673935
Train - Epoch 56, Batch: 0, Loss: 0.673429
Train - Epoch 57, Batch: 0, Loss: 0.673047
Train - Epoch 58, Batch: 0, Loss: 0.672593
Train - Epoch 59, Batch: 0, Loss: 0.672412
Train - Epoch 60, Batch: 0, Loss: 0.671913
Train - Epoch 61, Batch: 0, Loss: 0.671758
Train - Epoch 62, Batch: 0, Loss: 0.671312
Train - Epoch 63, Batch: 0, Loss: 0.671114
Train - Epoch 64, Batch: 0, Loss: 0.670613
Train - Epoch 65, Batch: 0, Loss: 0.670482
Train - Epoch 66, Batch: 0, Loss: 0.669952
Train - Epoch 67, Batch: 0, Loss: 0.669876
Train - Epoch 68, Batch: 0, Loss: 0.669759
Train - Epoch 69, Batch: 0, Loss: 0.669007
Train - Epoch 70, Batch: 0, Loss: 0.668921
Train - Epoch 71, Batch: 0, Loss: 0.668502
Train - Epoch 72, Batch: 0, Loss: 0.668114
Train - Epoch 73, Batch: 0, Loss: 0.667924
Train - Epoch 74, Batch: 0, Loss: 0.667610
Train - Epoch 75, Batch: 0, Loss: 0.667075
Train - Epoch 76, Batch: 0, Loss: 0.666714
Train - Epoch 77, Batch: 0, Loss: 0.666511
Train - Epoch 78, Batch: 0, Loss: 0.666341
Train - Epoch 79, Batch: 0, Loss: 0.666019
Train - Epoch 80, Batch: 0, Loss: 0.665570
Train - Epoch 81, Batch: 0, Loss: 0.665678
Train - Epoch 82, Batch: 0, Loss: 0.664989
Train - Epoch 83, Batch: 0, Loss: 0.664697
Train - Epoch 84, Batch: 0, Loss: 0.664336
Train - Epoch 85, Batch: 0, Loss: 0.664231
Train - Epoch 86, Batch: 0, Loss: 0.663665
Train - Epoch 87, Batch: 0, Loss: 0.663360
Train - Epoch 88, Batch: 0, Loss: 0.663088
Train - Epoch 89, Batch: 0, Loss: 0.662770
Train - Epoch 90, Batch: 0, Loss: 0.662694
Train - Epoch 91, Batch: 0, Loss: 0.662465
Train - Epoch 92, Batch: 0, Loss: 0.661593
Train - Epoch 93, Batch: 0, Loss: 0.661409
Train - Epoch 94, Batch: 0, Loss: 0.661317
Train - Epoch 95, Batch: 0, Loss: 0.661005
Train - Epoch 96, Batch: 0, Loss: 0.660912
Train - Epoch 97, Batch: 0, Loss: 0.660785
Train - Epoch 98, Batch: 0, Loss: 0.660407
Train - Epoch 99, Batch: 0, Loss: 0.660031
Train - Epoch 100, Batch: 0, Loss: 0.659482
Train - Epoch 101, Batch: 0, Loss: 0.659556
Train - Epoch 102, Batch: 0, Loss: 0.658695
Train - Epoch 103, Batch: 0, Loss: 0.658640
Train - Epoch 104, Batch: 0, Loss: 0.658308
Train - Epoch 105, Batch: 0, Loss: 0.658542
Train - Epoch 106, Batch: 0, Loss: 0.657920
Train - Epoch 107, Batch: 0, Loss: 0.657576
Train - Epoch 108, Batch: 0, Loss: 0.657076
Train - Epoch 109, Batch: 0, Loss: 0.656810
Train - Epoch 110, Batch: 0, Loss: 0.656478
Train - Epoch 111, Batch: 0, Loss: 0.656229
Train - Epoch 112, Batch: 0, Loss: 0.655535
Train - Epoch 113, Batch: 0, Loss: 0.655710
Train - Epoch 114, Batch: 0, Loss: 0.655553
Train - Epoch 115, Batch: 0, Loss: 0.655261
Train - Epoch 116, Batch: 0, Loss: 0.654970
Train - Epoch 117, Batch: 0, Loss: 0.654581
Train - Epoch 118, Batch: 0, Loss: 0.654744
Train - Epoch 119, Batch: 0, Loss: 0.654203
Train - Epoch 120, Batch: 0, Loss: 0.653576
Train - Epoch 121, Batch: 0, Loss: 0.653622
Train - Epoch 122, Batch: 0, Loss: 0.653179
Train - Epoch 123, Batch: 0, Loss: 0.652969
Train - Epoch 124, Batch: 0, Loss: 0.652159
Train - Epoch 125, Batch: 0, Loss: 0.652473
Train - Epoch 126, Batch: 0, Loss: 0.651843
Train - Epoch 127, Batch: 0, Loss: 0.652162
Train - Epoch 128, Batch: 0, Loss: 0.651709
Train - Epoch 129, Batch: 0, Loss: 0.650574
Train - Epoch 130, Batch: 0, Loss: 0.650704
Train - Epoch 131, Batch: 0, Loss: 0.650764
Train - Epoch 132, Batch: 0, Loss: 0.650585
Train - Epoch 133, Batch: 0, Loss: 0.650051
Train - Epoch 134, Batch: 0, Loss: 0.649840
Train - Epoch 135, Batch: 0, Loss: 0.649739
Train - Epoch 136, Batch: 0, Loss: 0.649205
Train - Epoch 137, Batch: 0, Loss: 0.649056
Train - Epoch 138, Batch: 0, Loss: 0.648583
Train - Epoch 139, Batch: 0, Loss: 0.648060
Train - Epoch 140, Batch: 0, Loss: 0.647967
Train - Epoch 141, Batch: 0, Loss: 0.648041
Train - Epoch 142, Batch: 0, Loss: 0.647374
Train - Epoch 143, Batch: 0, Loss: 0.646889
Train - Epoch 144, Batch: 0, Loss: 0.646930
Train - Epoch 145, Batch: 0, Loss: 0.646313
Train - Epoch 146, Batch: 0, Loss: 0.646493
Train - Epoch 147, Batch: 0, Loss: 0.646382
Train - Epoch 148, Batch: 0, Loss: 0.645902
Train - Epoch 149, Batch: 0, Loss: 0.646020
Train - Epoch 150, Batch: 0, Loss: 0.645634
Train - Epoch 151, Batch: 0, Loss: 0.644943
Train - Epoch 152, Batch: 0, Loss: 0.644978
Train - Epoch 153, Batch: 0, Loss: 0.644412
Train - Epoch 154, Batch: 0, Loss: 0.644068
Train - Epoch 155, Batch: 0, Loss: 0.643376
Train - Epoch 156, Batch: 0, Loss: 0.644288
Train - Epoch 157, Batch: 0, Loss: 0.643870
Train - Epoch 158, Batch: 0, Loss: 0.643397
Train - Epoch 159, Batch: 0, Loss: 0.642819
Train - Epoch 160, Batch: 0, Loss: 0.642734
Train - Epoch 161, Batch: 0, Loss: 0.642449
Train - Epoch 162, Batch: 0, Loss: 0.642075
Train - Epoch 163, Batch: 0, Loss: 0.641498
Train - Epoch 164, Batch: 0, Loss: 0.641899
Train - Epoch 165, Batch: 0, Loss: 0.641325
Train - Epoch 166, Batch: 0, Loss: 0.641163
Train - Epoch 167, Batch: 0, Loss: 0.641149
Train - Epoch 168, Batch: 0, Loss: 0.640590
Train - Epoch 169, Batch: 0, Loss: 0.640398
Train - Epoch 170, Batch: 0, Loss: 0.640019
Train - Epoch 171, Batch: 0, Loss: 0.639785
Train - Epoch 172, Batch: 0, Loss: 0.639431
Train - Epoch 173, Batch: 0, Loss: 0.639169
Train - Epoch 174, Batch: 0, Loss: 0.639079
Train - Epoch 175, Batch: 0, Loss: 0.638433
Train - Epoch 176, Batch: 0, Loss: 0.638794
Train - Epoch 177, Batch: 0, Loss: 0.638701
Train - Epoch 178, Batch: 0, Loss: 0.638052
Train - Epoch 179, Batch: 0, Loss: 0.637959
Train - Epoch 180, Batch: 0, Loss: 0.637131
Train - Epoch 181, Batch: 0, Loss: 0.637363
Train - Epoch 182, Batch: 0, Loss: 0.637139
Train - Epoch 183, Batch: 0, Loss: 0.636656
Train - Epoch 184, Batch: 0, Loss: 0.636771
Train - Epoch 185, Batch: 0, Loss: 0.636376
Train - Epoch 186, Batch: 0, Loss: 0.636073/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635576
Train - Epoch 188, Batch: 0, Loss: 0.635159
Train - Epoch 189, Batch: 0, Loss: 0.636056
Train - Epoch 190, Batch: 0, Loss: 0.635582
Train - Epoch 191, Batch: 0, Loss: 0.635104
Train - Epoch 192, Batch: 0, Loss: 0.634355
Train - Epoch 193, Batch: 0, Loss: 0.634153
Train - Epoch 194, Batch: 0, Loss: 0.634486
Train - Epoch 195, Batch: 0, Loss: 0.633329
Train - Epoch 196, Batch: 0, Loss: 0.633150
Train - Epoch 197, Batch: 0, Loss: 0.633447
Train - Epoch 198, Batch: 0, Loss: 0.633015
Train - Epoch 199, Batch: 0, Loss: 0.632953
training_time:: 350.43275690078735
training time full:: 350.4328238964081
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926539
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.4638624191284
overhead:: 0
overhead2:: 0.30502891540527344
overhead3:: 0
time_baseline:: 275.46420645713806
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926588
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624623)
RCV1 Test Avg. Accuracy:: 0.9220902304254952
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.01878213882446289
overhead3:: 0.12183260917663574
overhead4:: 33.4293110370636
overhead5:: 0
memory usage:: 26591899648
time_provenance:: 45.03083419799805
curr_diff: 0 tensor(6.1417e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1417e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926588
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.0001_10200
tensor(624623)
RCV1 Test Avg. Accuracy:: 0.9220902304254952
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693224
Train - Epoch 1, Batch: 0, Loss: 0.692868
Train - Epoch 2, Batch: 0, Loss: 0.692485
Train - Epoch 3, Batch: 0, Loss: 0.692091
Train - Epoch 4, Batch: 0, Loss: 0.691742
Train - Epoch 5, Batch: 0, Loss: 0.691341
Train - Epoch 6, Batch: 0, Loss: 0.690949
Train - Epoch 7, Batch: 0, Loss: 0.690589
Train - Epoch 8, Batch: 0, Loss: 0.690239
Train - Epoch 9, Batch: 0, Loss: 0.689922
Train - Epoch 10, Batch: 0, Loss: 0.689517
Train - Epoch 11, Batch: 0, Loss: 0.689094
Train - Epoch 12, Batch: 0, Loss: 0.688716
Train - Epoch 13, Batch: 0, Loss: 0.688477
Train - Epoch 14, Batch: 0, Loss: 0.688025
Train - Epoch 15, Batch: 0, Loss: 0.687702
Train - Epoch 16, Batch: 0, Loss: 0.687340
Train - Epoch 17, Batch: 0, Loss: 0.686987
Train - Epoch 18, Batch: 0, Loss: 0.686648
Train - Epoch 19, Batch: 0, Loss: 0.686289
Train - Epoch 20, Batch: 0, Loss: 0.685857
Train - Epoch 21, Batch: 0, Loss: 0.685514
Train - Epoch 22, Batch: 0, Loss: 0.685114
Train - Epoch 23, Batch: 0, Loss: 0.684780
Train - Epoch 24, Batch: 0, Loss: 0.684496
Train - Epoch 25, Batch: 0, Loss: 0.684075
Train - Epoch 26, Batch: 0, Loss: 0.683882
Train - Epoch 27, Batch: 0, Loss: 0.683476
Train - Epoch 28, Batch: 0, Loss: 0.682978
Train - Epoch 29, Batch: 0, Loss: 0.682722
Train - Epoch 30, Batch: 0, Loss: 0.682423
Train - Epoch 31, Batch: 0, Loss: 0.682044
Train - Epoch 32, Batch: 0, Loss: 0.681655
Train - Epoch 33, Batch: 0, Loss: 0.681271
Train - Epoch 34, Batch: 0, Loss: 0.680796
Train - Epoch 35, Batch: 0, Loss: 0.680631
Train - Epoch 36, Batch: 0, Loss: 0.680237
Train - Epoch 37, Batch: 0, Loss: 0.679973
Train - Epoch 38, Batch: 0, Loss: 0.679619
Train - Epoch 39, Batch: 0, Loss: 0.679155
Train - Epoch 40, Batch: 0, Loss: 0.678871
Train - Epoch 41, Batch: 0, Loss: 0.678382
Train - Epoch 42, Batch: 0, Loss: 0.678157
Train - Epoch 43, Batch: 0, Loss: 0.677777
Train - Epoch 44, Batch: 0, Loss: 0.677547
Train - Epoch 45, Batch: 0, Loss: 0.677193
Train - Epoch 46, Batch: 0, Loss: 0.676863
Train - Epoch 47, Batch: 0, Loss: 0.676564
Train - Epoch 48, Batch: 0, Loss: 0.676217
Train - Epoch 49, Batch: 0, Loss: 0.675791
Train - Epoch 50, Batch: 0, Loss: 0.675494
Train - Epoch 51, Batch: 0, Loss: 0.675124
Train - Epoch 52, Batch: 0, Loss: 0.674731
Train - Epoch 53, Batch: 0, Loss: 0.674452
Train - Epoch 54, Batch: 0, Loss: 0.674133
Train - Epoch 55, Batch: 0, Loss: 0.673753
Train - Epoch 56, Batch: 0, Loss: 0.673651
Train - Epoch 57, Batch: 0, Loss: 0.673180
Train - Epoch 58, Batch: 0, Loss: 0.672886
Train - Epoch 59, Batch: 0, Loss: 0.672595
Train - Epoch 60, Batch: 0, Loss: 0.672233
Train - Epoch 61, Batch: 0, Loss: 0.671869
Train - Epoch 62, Batch: 0, Loss: 0.671463
Train - Epoch 63, Batch: 0, Loss: 0.671043
Train - Epoch 64, Batch: 0, Loss: 0.670962
Train - Epoch 65, Batch: 0, Loss: 0.670722
Train - Epoch 66, Batch: 0, Loss: 0.670213
Train - Epoch 67, Batch: 0, Loss: 0.670035
Train - Epoch 68, Batch: 0, Loss: 0.669680
Train - Epoch 69, Batch: 0, Loss: 0.669327
Train - Epoch 70, Batch: 0, Loss: 0.669246
Train - Epoch 71, Batch: 0, Loss: 0.668591
Train - Epoch 72, Batch: 0, Loss: 0.668053
Train - Epoch 73, Batch: 0, Loss: 0.668047
Train - Epoch 74, Batch: 0, Loss: 0.667990
Train - Epoch 75, Batch: 0, Loss: 0.667619
Train - Epoch 76, Batch: 0, Loss: 0.667077
Train - Epoch 77, Batch: 0, Loss: 0.666596
Train - Epoch 78, Batch: 0, Loss: 0.666824
Train - Epoch 79, Batch: 0, Loss: 0.666050
Train - Epoch 80, Batch: 0, Loss: 0.665662
Train - Epoch 81, Batch: 0, Loss: 0.665471
Train - Epoch 82, Batch: 0, Loss: 0.665412
Train - Epoch 83, Batch: 0, Loss: 0.665107
Train - Epoch 84, Batch: 0, Loss: 0.664706
Train - Epoch 85, Batch: 0, Loss: 0.664098
Train - Epoch 86, Batch: 0, Loss: 0.663685
Train - Epoch 87, Batch: 0, Loss: 0.663536
Train - Epoch 88, Batch: 0, Loss: 0.663339
Train - Epoch 89, Batch: 0, Loss: 0.662955
Train - Epoch 90, Batch: 0, Loss: 0.662458
Train - Epoch 91, Batch: 0, Loss: 0.662343
Train - Epoch 92, Batch: 0, Loss: 0.661799
Train - Epoch 93, Batch: 0, Loss: 0.661661
Train - Epoch 94, Batch: 0, Loss: 0.661732
Train - Epoch 95, Batch: 0, Loss: 0.661343
Train - Epoch 96, Batch: 0, Loss: 0.661025
Train - Epoch 97, Batch: 0, Loss: 0.660222
Train - Epoch 98, Batch: 0, Loss: 0.660176
Train - Epoch 99, Batch: 0, Loss: 0.659821
Train - Epoch 100, Batch: 0, Loss: 0.659369
Train - Epoch 101, Batch: 0, Loss: 0.659422
Train - Epoch 102, Batch: 0, Loss: 0.658873
Train - Epoch 103, Batch: 0, Loss: 0.658777
Train - Epoch 104, Batch: 0, Loss: 0.658444
Train - Epoch 105, Batch: 0, Loss: 0.657973
Train - Epoch 106, Batch: 0, Loss: 0.657563
Train - Epoch 107, Batch: 0, Loss: 0.657411
Train - Epoch 108, Batch: 0, Loss: 0.657691
Train - Epoch 109, Batch: 0, Loss: 0.657081
Train - Epoch 110, Batch: 0, Loss: 0.656602
Train - Epoch 111, Batch: 0, Loss: 0.656423
Train - Epoch 112, Batch: 0, Loss: 0.655966
Train - Epoch 113, Batch: 0, Loss: 0.656199
Train - Epoch 114, Batch: 0, Loss: 0.655431
Train - Epoch 115, Batch: 0, Loss: 0.655089
Train - Epoch 116, Batch: 0, Loss: 0.655143
Train - Epoch 117, Batch: 0, Loss: 0.654625
Train - Epoch 118, Batch: 0, Loss: 0.654678
Train - Epoch 119, Batch: 0, Loss: 0.653920
Train - Epoch 120, Batch: 0, Loss: 0.653774
Train - Epoch 121, Batch: 0, Loss: 0.653740
Train - Epoch 122, Batch: 0, Loss: 0.653429
Train - Epoch 123, Batch: 0, Loss: 0.652848
Train - Epoch 124, Batch: 0, Loss: 0.652639
Train - Epoch 125, Batch: 0, Loss: 0.652531
Train - Epoch 126, Batch: 0, Loss: 0.652203
Train - Epoch 127, Batch: 0, Loss: 0.651862
Train - Epoch 128, Batch: 0, Loss: 0.651403
Train - Epoch 129, Batch: 0, Loss: 0.651416
Train - Epoch 130, Batch: 0, Loss: 0.650778
Train - Epoch 131, Batch: 0, Loss: 0.650872
Train - Epoch 132, Batch: 0, Loss: 0.650153
Train - Epoch 133, Batch: 0, Loss: 0.650265
Train - Epoch 134, Batch: 0, Loss: 0.649919
Train - Epoch 135, Batch: 0, Loss: 0.649689
Train - Epoch 136, Batch: 0, Loss: 0.649652
Train - Epoch 137, Batch: 0, Loss: 0.649366
Train - Epoch 138, Batch: 0, Loss: 0.648576
Train - Epoch 139, Batch: 0, Loss: 0.648458
Train - Epoch 140, Batch: 0, Loss: 0.647969
Train - Epoch 141, Batch: 0, Loss: 0.648086
Train - Epoch 142, Batch: 0, Loss: 0.647853
Train - Epoch 143, Batch: 0, Loss: 0.647387
Train - Epoch 144, Batch: 0, Loss: 0.646798
Train - Epoch 145, Batch: 0, Loss: 0.647167
Train - Epoch 146, Batch: 0, Loss: 0.646553
Train - Epoch 147, Batch: 0, Loss: 0.646300
Train - Epoch 148, Batch: 0, Loss: 0.646107
Train - Epoch 149, Batch: 0, Loss: 0.645974
Train - Epoch 150, Batch: 0, Loss: 0.645438
Train - Epoch 151, Batch: 0, Loss: 0.645223
Train - Epoch 152, Batch: 0, Loss: 0.644723
Train - Epoch 153, Batch: 0, Loss: 0.645184
Train - Epoch 154, Batch: 0, Loss: 0.644064
Train - Epoch 155, Batch: 0, Loss: 0.643905
Train - Epoch 156, Batch: 0, Loss: 0.643689
Train - Epoch 157, Batch: 0, Loss: 0.643521
Train - Epoch 158, Batch: 0, Loss: 0.643585
Train - Epoch 159, Batch: 0, Loss: 0.642517
Train - Epoch 160, Batch: 0, Loss: 0.642969
Train - Epoch 161, Batch: 0, Loss: 0.642611
Train - Epoch 162, Batch: 0, Loss: 0.642516
Train - Epoch 163, Batch: 0, Loss: 0.641790
Train - Epoch 164, Batch: 0, Loss: 0.641826
Train - Epoch 165, Batch: 0, Loss: 0.641772
Train - Epoch 166, Batch: 0, Loss: 0.640822
Train - Epoch 167, Batch: 0, Loss: 0.641143
Train - Epoch 168, Batch: 0, Loss: 0.640918
Train - Epoch 169, Batch: 0, Loss: 0.639946
Train - Epoch 170, Batch: 0, Loss: 0.639928
Train - Epoch 171, Batch: 0, Loss: 0.639957
Train - Epoch 172, Batch: 0, Loss: 0.639201
Train - Epoch 173, Batch: 0, Loss: 0.639088
Train - Epoch 174, Batch: 0, Loss: 0.638820
Train - Epoch 175, Batch: 0, Loss: 0.638857
Train - Epoch 176, Batch: 0, Loss: 0.638748
Train - Epoch 177, Batch: 0, Loss: 0.638476
Train - Epoch 178, Batch: 0, Loss: 0.638646
Train - Epoch 179, Batch: 0, Loss: 0.638634
Train - Epoch 180, Batch: 0, Loss: 0.637724
Train - Epoch 181, Batch: 0, Loss: 0.637648
Train - Epoch 182, Batch: 0, Loss: 0.637276
Train - Epoch 183, Batch: 0, Loss: 0.636803
Train - Epoch 184, Batch: 0, Loss: 0.636649
Train - Epoch 185, Batch: 0, Loss: 0.636562
Train - Epoch 186, Batch: 0, Loss: 0.635887/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635631
Train - Epoch 188, Batch: 0, Loss: 0.635643
Train - Epoch 189, Batch: 0, Loss: 0.635171
Train - Epoch 190, Batch: 0, Loss: 0.635656
Train - Epoch 191, Batch: 0, Loss: 0.634490
Train - Epoch 192, Batch: 0, Loss: 0.634290
Train - Epoch 193, Batch: 0, Loss: 0.635230
Train - Epoch 194, Batch: 0, Loss: 0.633790
Train - Epoch 195, Batch: 0, Loss: 0.633747
Train - Epoch 196, Batch: 0, Loss: 0.633887
Train - Epoch 197, Batch: 0, Loss: 0.633570
Train - Epoch 198, Batch: 0, Loss: 0.633488
Train - Epoch 199, Batch: 0, Loss: 0.632712
training_time:: 351.0811891555786
training time full:: 351.0812556743622
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926687
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 276.1865532398224
overhead:: 0
overhead2:: 0.31394100189208984
overhead3:: 0
time_baseline:: 276.1868848800659
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926638
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624543)
RCV1 Test Avg. Accuracy:: 0.9219721316388125
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.01709270477294922
overhead3:: 0.12180304527282715
overhead4:: 33.53101825714111
overhead5:: 0
memory usage:: 26601738240
time_provenance:: 45.56793165206909
curr_diff: 0 tensor(6.2338e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2338e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926638
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.0001_10200
tensor(624543)
RCV1 Test Avg. Accuracy:: 0.9219721316388125
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693155
Train - Epoch 1, Batch: 0, Loss: 0.692782
Train - Epoch 2, Batch: 0, Loss: 0.692420
Train - Epoch 3, Batch: 0, Loss: 0.692040
Train - Epoch 4, Batch: 0, Loss: 0.691661
Train - Epoch 5, Batch: 0, Loss: 0.691314
Train - Epoch 6, Batch: 0, Loss: 0.690932
Train - Epoch 7, Batch: 0, Loss: 0.690589
Train - Epoch 8, Batch: 0, Loss: 0.690195
Train - Epoch 9, Batch: 0, Loss: 0.689803
Train - Epoch 10, Batch: 0, Loss: 0.689474
Train - Epoch 11, Batch: 0, Loss: 0.689137
Train - Epoch 12, Batch: 0, Loss: 0.688693
Train - Epoch 13, Batch: 0, Loss: 0.688403
Train - Epoch 14, Batch: 0, Loss: 0.687941
Train - Epoch 15, Batch: 0, Loss: 0.687691
Train - Epoch 16, Batch: 0, Loss: 0.687273
Train - Epoch 17, Batch: 0, Loss: 0.686891
Train - Epoch 18, Batch: 0, Loss: 0.686531
Train - Epoch 19, Batch: 0, Loss: 0.686111
Train - Epoch 20, Batch: 0, Loss: 0.685768
Train - Epoch 21, Batch: 0, Loss: 0.685393
Train - Epoch 22, Batch: 0, Loss: 0.685130
Train - Epoch 23, Batch: 0, Loss: 0.684853
Train - Epoch 24, Batch: 0, Loss: 0.684446
Train - Epoch 25, Batch: 0, Loss: 0.684005
Train - Epoch 26, Batch: 0, Loss: 0.683730
Train - Epoch 27, Batch: 0, Loss: 0.683376
Train - Epoch 28, Batch: 0, Loss: 0.682978
Train - Epoch 29, Batch: 0, Loss: 0.682609
Train - Epoch 30, Batch: 0, Loss: 0.682198
Train - Epoch 31, Batch: 0, Loss: 0.681982
Train - Epoch 32, Batch: 0, Loss: 0.681608
Train - Epoch 33, Batch: 0, Loss: 0.681164
Train - Epoch 34, Batch: 0, Loss: 0.680711
Train - Epoch 35, Batch: 0, Loss: 0.680665
Train - Epoch 36, Batch: 0, Loss: 0.680179
Train - Epoch 37, Batch: 0, Loss: 0.679909
Train - Epoch 38, Batch: 0, Loss: 0.679475
Train - Epoch 39, Batch: 0, Loss: 0.679211
Train - Epoch 40, Batch: 0, Loss: 0.678790
Train - Epoch 41, Batch: 0, Loss: 0.678342
Train - Epoch 42, Batch: 0, Loss: 0.678059
Train - Epoch 43, Batch: 0, Loss: 0.677924
Train - Epoch 44, Batch: 0, Loss: 0.677541
Train - Epoch 45, Batch: 0, Loss: 0.677053
Train - Epoch 46, Batch: 0, Loss: 0.676770
Train - Epoch 47, Batch: 0, Loss: 0.676475
Train - Epoch 48, Batch: 0, Loss: 0.675978
Train - Epoch 49, Batch: 0, Loss: 0.675831
Train - Epoch 50, Batch: 0, Loss: 0.675429
Train - Epoch 51, Batch: 0, Loss: 0.675225
Train - Epoch 52, Batch: 0, Loss: 0.674700
Train - Epoch 53, Batch: 0, Loss: 0.674539
Train - Epoch 54, Batch: 0, Loss: 0.673963
Train - Epoch 55, Batch: 0, Loss: 0.673883
Train - Epoch 56, Batch: 0, Loss: 0.673360
Train - Epoch 57, Batch: 0, Loss: 0.673166
Train - Epoch 58, Batch: 0, Loss: 0.672819
Train - Epoch 59, Batch: 0, Loss: 0.672310
Train - Epoch 60, Batch: 0, Loss: 0.672126
Train - Epoch 61, Batch: 0, Loss: 0.671881
Train - Epoch 62, Batch: 0, Loss: 0.671598
Train - Epoch 63, Batch: 0, Loss: 0.671085
Train - Epoch 64, Batch: 0, Loss: 0.670876
Train - Epoch 65, Batch: 0, Loss: 0.670589
Train - Epoch 66, Batch: 0, Loss: 0.670193
Train - Epoch 67, Batch: 0, Loss: 0.670230
Train - Epoch 68, Batch: 0, Loss: 0.669796
Train - Epoch 69, Batch: 0, Loss: 0.669267
Train - Epoch 70, Batch: 0, Loss: 0.668889
Train - Epoch 71, Batch: 0, Loss: 0.668636
Train - Epoch 72, Batch: 0, Loss: 0.668108
Train - Epoch 73, Batch: 0, Loss: 0.668324
Train - Epoch 74, Batch: 0, Loss: 0.667684
Train - Epoch 75, Batch: 0, Loss: 0.667267
Train - Epoch 76, Batch: 0, Loss: 0.667087
Train - Epoch 77, Batch: 0, Loss: 0.666477
Train - Epoch 78, Batch: 0, Loss: 0.666504
Train - Epoch 79, Batch: 0, Loss: 0.666203
Train - Epoch 80, Batch: 0, Loss: 0.665683
Train - Epoch 81, Batch: 0, Loss: 0.665344
Train - Epoch 82, Batch: 0, Loss: 0.665135
Train - Epoch 83, Batch: 0, Loss: 0.664955
Train - Epoch 84, Batch: 0, Loss: 0.664573
Train - Epoch 85, Batch: 0, Loss: 0.663987
Train - Epoch 86, Batch: 0, Loss: 0.663286
Train - Epoch 87, Batch: 0, Loss: 0.663308
Train - Epoch 88, Batch: 0, Loss: 0.663263
Train - Epoch 89, Batch: 0, Loss: 0.663177
Train - Epoch 90, Batch: 0, Loss: 0.662563
Train - Epoch 91, Batch: 0, Loss: 0.662011
Train - Epoch 92, Batch: 0, Loss: 0.662344
Train - Epoch 93, Batch: 0, Loss: 0.661601
Train - Epoch 94, Batch: 0, Loss: 0.661285
Train - Epoch 95, Batch: 0, Loss: 0.661163
Train - Epoch 96, Batch: 0, Loss: 0.660924
Train - Epoch 97, Batch: 0, Loss: 0.660230
Train - Epoch 98, Batch: 0, Loss: 0.660035
Train - Epoch 99, Batch: 0, Loss: 0.660058
Train - Epoch 100, Batch: 0, Loss: 0.659591
Train - Epoch 101, Batch: 0, Loss: 0.659618
Train - Epoch 102, Batch: 0, Loss: 0.659062
Train - Epoch 103, Batch: 0, Loss: 0.658458
Train - Epoch 104, Batch: 0, Loss: 0.658188
Train - Epoch 105, Batch: 0, Loss: 0.658344
Train - Epoch 106, Batch: 0, Loss: 0.657872
Train - Epoch 107, Batch: 0, Loss: 0.657583
Train - Epoch 108, Batch: 0, Loss: 0.657233
Train - Epoch 109, Batch: 0, Loss: 0.656979
Train - Epoch 110, Batch: 0, Loss: 0.656538
Train - Epoch 111, Batch: 0, Loss: 0.656298
Train - Epoch 112, Batch: 0, Loss: 0.655872
Train - Epoch 113, Batch: 0, Loss: 0.655767
Train - Epoch 114, Batch: 0, Loss: 0.655517
Train - Epoch 115, Batch: 0, Loss: 0.655095
Train - Epoch 116, Batch: 0, Loss: 0.654574
Train - Epoch 117, Batch: 0, Loss: 0.655181
Train - Epoch 118, Batch: 0, Loss: 0.654000
Train - Epoch 119, Batch: 0, Loss: 0.654048
Train - Epoch 120, Batch: 0, Loss: 0.653734
Train - Epoch 121, Batch: 0, Loss: 0.653665
Train - Epoch 122, Batch: 0, Loss: 0.653221
Train - Epoch 123, Batch: 0, Loss: 0.652540
Train - Epoch 124, Batch: 0, Loss: 0.652418
Train - Epoch 125, Batch: 0, Loss: 0.652352
Train - Epoch 126, Batch: 0, Loss: 0.651994
Train - Epoch 127, Batch: 0, Loss: 0.651525
Train - Epoch 128, Batch: 0, Loss: 0.651364
Train - Epoch 129, Batch: 0, Loss: 0.651127
Train - Epoch 130, Batch: 0, Loss: 0.650920
Train - Epoch 131, Batch: 0, Loss: 0.650418
Train - Epoch 132, Batch: 0, Loss: 0.649792
Train - Epoch 133, Batch: 0, Loss: 0.649958
Train - Epoch 134, Batch: 0, Loss: 0.649539
Train - Epoch 135, Batch: 0, Loss: 0.649608
Train - Epoch 136, Batch: 0, Loss: 0.649411
Train - Epoch 137, Batch: 0, Loss: 0.648963
Train - Epoch 138, Batch: 0, Loss: 0.648652
Train - Epoch 139, Batch: 0, Loss: 0.648207
Train - Epoch 140, Batch: 0, Loss: 0.648363
Train - Epoch 141, Batch: 0, Loss: 0.647827
Train - Epoch 142, Batch: 0, Loss: 0.647322
Train - Epoch 143, Batch: 0, Loss: 0.646655
Train - Epoch 144, Batch: 0, Loss: 0.646966
Train - Epoch 145, Batch: 0, Loss: 0.647136
Train - Epoch 146, Batch: 0, Loss: 0.646396
Train - Epoch 147, Batch: 0, Loss: 0.646454
Train - Epoch 148, Batch: 0, Loss: 0.645830
Train - Epoch 149, Batch: 0, Loss: 0.645982
Train - Epoch 150, Batch: 0, Loss: 0.645283
Train - Epoch 151, Batch: 0, Loss: 0.644886
Train - Epoch 152, Batch: 0, Loss: 0.645103
Train - Epoch 153, Batch: 0, Loss: 0.644962
Train - Epoch 154, Batch: 0, Loss: 0.644429
Train - Epoch 155, Batch: 0, Loss: 0.644609
Train - Epoch 156, Batch: 0, Loss: 0.643444
Train - Epoch 157, Batch: 0, Loss: 0.643995
Train - Epoch 158, Batch: 0, Loss: 0.643570
Train - Epoch 159, Batch: 0, Loss: 0.642941
Train - Epoch 160, Batch: 0, Loss: 0.643061
Train - Epoch 161, Batch: 0, Loss: 0.642641
Train - Epoch 162, Batch: 0, Loss: 0.641863
Train - Epoch 163, Batch: 0, Loss: 0.641536
Train - Epoch 164, Batch: 0, Loss: 0.641550
Train - Epoch 165, Batch: 0, Loss: 0.641620
Train - Epoch 166, Batch: 0, Loss: 0.641046
Train - Epoch 167, Batch: 0, Loss: 0.640017
Train - Epoch 168, Batch: 0, Loss: 0.640485
Train - Epoch 169, Batch: 0, Loss: 0.639890
Train - Epoch 170, Batch: 0, Loss: 0.640433
Train - Epoch 171, Batch: 0, Loss: 0.639620
Train - Epoch 172, Batch: 0, Loss: 0.640023
Train - Epoch 173, Batch: 0, Loss: 0.639016
Train - Epoch 174, Batch: 0, Loss: 0.639029
Train - Epoch 175, Batch: 0, Loss: 0.639007
Train - Epoch 176, Batch: 0, Loss: 0.638810
Train - Epoch 177, Batch: 0, Loss: 0.638493
Train - Epoch 178, Batch: 0, Loss: 0.637948
Train - Epoch 179, Batch: 0, Loss: 0.638321
Train - Epoch 180, Batch: 0, Loss: 0.638011
Train - Epoch 181, Batch: 0, Loss: 0.637766
Train - Epoch 182, Batch: 0, Loss: 0.637000
Train - Epoch 183, Batch: 0, Loss: 0.636825
Train - Epoch 184, Batch: 0, Loss: 0.636657
Train - Epoch 185, Batch: 0, Loss: 0.635644
Train - Epoch 186, Batch: 0, Loss: 0.635892/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635776
Train - Epoch 188, Batch: 0, Loss: 0.636282
Train - Epoch 189, Batch: 0, Loss: 0.634968
Train - Epoch 190, Batch: 0, Loss: 0.634812
Train - Epoch 191, Batch: 0, Loss: 0.634795
Train - Epoch 192, Batch: 0, Loss: 0.635243
Train - Epoch 193, Batch: 0, Loss: 0.634354
Train - Epoch 194, Batch: 0, Loss: 0.634144
Train - Epoch 195, Batch: 0, Loss: 0.633846
Train - Epoch 196, Batch: 0, Loss: 0.633090
Train - Epoch 197, Batch: 0, Loss: 0.633857
Train - Epoch 198, Batch: 0, Loss: 0.632859
Train - Epoch 199, Batch: 0, Loss: 0.632650
training_time:: 350.0019021034241
training time full:: 350.0019700527191
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926687
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.7498872280121
overhead:: 0
overhead2:: 0.30357837677001953
overhead3:: 0
time_baseline:: 275.75021600723267
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926687
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624882)
RCV1 Test Avg. Accuracy:: 0.92247257524738
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.020157575607299805
overhead3:: 0.12068605422973633
overhead4:: 33.622037410736084
overhead5:: 0
memory usage:: 26622103552
time_provenance:: 45.64972472190857
curr_diff: 0 tensor(6.5316e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5316e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926687
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.0001_10200
tensor(624882)
RCV1 Test Avg. Accuracy:: 0.92247257524738
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  rcv1 0
tensor([  353, 16611,  8325, 17576,  2986, 10510, 17426,  4244,  9244, 12191])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693193
Train - Epoch 1, Batch: 0, Loss: 0.692808
Train - Epoch 2, Batch: 0, Loss: 0.692432
Train - Epoch 3, Batch: 0, Loss: 0.692072
Train - Epoch 4, Batch: 0, Loss: 0.691698
Train - Epoch 5, Batch: 0, Loss: 0.691330
Train - Epoch 6, Batch: 0, Loss: 0.690976
Train - Epoch 7, Batch: 0, Loss: 0.690571
Train - Epoch 8, Batch: 0, Loss: 0.690240
Train - Epoch 9, Batch: 0, Loss: 0.689849
Train - Epoch 10, Batch: 0, Loss: 0.689505
Train - Epoch 11, Batch: 0, Loss: 0.689134
Train - Epoch 12, Batch: 0, Loss: 0.688742
Train - Epoch 13, Batch: 0, Loss: 0.688384
Train - Epoch 14, Batch: 0, Loss: 0.688072
Train - Epoch 15, Batch: 0, Loss: 0.687711
Train - Epoch 16, Batch: 0, Loss: 0.687269
Train - Epoch 17, Batch: 0, Loss: 0.686960
Train - Epoch 18, Batch: 0, Loss: 0.686561
Train - Epoch 19, Batch: 0, Loss: 0.686130
Train - Epoch 20, Batch: 0, Loss: 0.685807
Train - Epoch 21, Batch: 0, Loss: 0.685488
Train - Epoch 22, Batch: 0, Loss: 0.685186
Train - Epoch 23, Batch: 0, Loss: 0.684750
Train - Epoch 24, Batch: 0, Loss: 0.684443
Train - Epoch 25, Batch: 0, Loss: 0.684008
Train - Epoch 26, Batch: 0, Loss: 0.683750
Train - Epoch 27, Batch: 0, Loss: 0.683415
Train - Epoch 28, Batch: 0, Loss: 0.683092
Train - Epoch 29, Batch: 0, Loss: 0.682714
Train - Epoch 30, Batch: 0, Loss: 0.682325
Train - Epoch 31, Batch: 0, Loss: 0.682120
Train - Epoch 32, Batch: 0, Loss: 0.681608
Train - Epoch 33, Batch: 0, Loss: 0.681278
Train - Epoch 34, Batch: 0, Loss: 0.680889
Train - Epoch 35, Batch: 0, Loss: 0.680517
Train - Epoch 36, Batch: 0, Loss: 0.680244
Train - Epoch 37, Batch: 0, Loss: 0.680008
Train - Epoch 38, Batch: 0, Loss: 0.679584
Train - Epoch 39, Batch: 0, Loss: 0.679269
Train - Epoch 40, Batch: 0, Loss: 0.678924
Train - Epoch 41, Batch: 0, Loss: 0.678492
Train - Epoch 42, Batch: 0, Loss: 0.678293
Train - Epoch 43, Batch: 0, Loss: 0.677757
Train - Epoch 44, Batch: 0, Loss: 0.677321
Train - Epoch 45, Batch: 0, Loss: 0.677052
Train - Epoch 46, Batch: 0, Loss: 0.676824
Train - Epoch 47, Batch: 0, Loss: 0.676533
Train - Epoch 48, Batch: 0, Loss: 0.676115
Train - Epoch 49, Batch: 0, Loss: 0.675743
Train - Epoch 50, Batch: 0, Loss: 0.675277
Train - Epoch 51, Batch: 0, Loss: 0.675191
Train - Epoch 52, Batch: 0, Loss: 0.674888
Train - Epoch 53, Batch: 0, Loss: 0.674364
Train - Epoch 54, Batch: 0, Loss: 0.674166
Train - Epoch 55, Batch: 0, Loss: 0.674023
Train - Epoch 56, Batch: 0, Loss: 0.673537
Train - Epoch 57, Batch: 0, Loss: 0.673181
Train - Epoch 58, Batch: 0, Loss: 0.672681
Train - Epoch 59, Batch: 0, Loss: 0.672494
Train - Epoch 60, Batch: 0, Loss: 0.672013
Train - Epoch 61, Batch: 0, Loss: 0.671838
Train - Epoch 62, Batch: 0, Loss: 0.671427
Train - Epoch 63, Batch: 0, Loss: 0.671238
Train - Epoch 64, Batch: 0, Loss: 0.670729
Train - Epoch 65, Batch: 0, Loss: 0.670591
Train - Epoch 66, Batch: 0, Loss: 0.670082
Train - Epoch 67, Batch: 0, Loss: 0.669979
Train - Epoch 68, Batch: 0, Loss: 0.669882
Train - Epoch 69, Batch: 0, Loss: 0.669128
Train - Epoch 70, Batch: 0, Loss: 0.669009
Train - Epoch 71, Batch: 0, Loss: 0.668590
Train - Epoch 72, Batch: 0, Loss: 0.668224
Train - Epoch 73, Batch: 0, Loss: 0.668054
Train - Epoch 74, Batch: 0, Loss: 0.667696
Train - Epoch 75, Batch: 0, Loss: 0.667183
Train - Epoch 76, Batch: 0, Loss: 0.666812
Train - Epoch 77, Batch: 0, Loss: 0.666592
Train - Epoch 78, Batch: 0, Loss: 0.666431
Train - Epoch 79, Batch: 0, Loss: 0.666091
Train - Epoch 80, Batch: 0, Loss: 0.665676
Train - Epoch 81, Batch: 0, Loss: 0.665779
Train - Epoch 82, Batch: 0, Loss: 0.665063
Train - Epoch 83, Batch: 0, Loss: 0.664808
Train - Epoch 84, Batch: 0, Loss: 0.664417
Train - Epoch 85, Batch: 0, Loss: 0.664338
Train - Epoch 86, Batch: 0, Loss: 0.663768
Train - Epoch 87, Batch: 0, Loss: 0.663463
Train - Epoch 88, Batch: 0, Loss: 0.663191
Train - Epoch 89, Batch: 0, Loss: 0.662896
Train - Epoch 90, Batch: 0, Loss: 0.662799
Train - Epoch 91, Batch: 0, Loss: 0.662561
Train - Epoch 92, Batch: 0, Loss: 0.661713
Train - Epoch 93, Batch: 0, Loss: 0.661539
Train - Epoch 94, Batch: 0, Loss: 0.661419
Train - Epoch 95, Batch: 0, Loss: 0.661110
Train - Epoch 96, Batch: 0, Loss: 0.661007
Train - Epoch 97, Batch: 0, Loss: 0.660869
Train - Epoch 98, Batch: 0, Loss: 0.660515
Train - Epoch 99, Batch: 0, Loss: 0.660124
Train - Epoch 100, Batch: 0, Loss: 0.659588
Train - Epoch 101, Batch: 0, Loss: 0.659637
Train - Epoch 102, Batch: 0, Loss: 0.658794
Train - Epoch 103, Batch: 0, Loss: 0.658746
Train - Epoch 104, Batch: 0, Loss: 0.658418
Train - Epoch 105, Batch: 0, Loss: 0.658641
Train - Epoch 106, Batch: 0, Loss: 0.658014
Train - Epoch 107, Batch: 0, Loss: 0.657669
Train - Epoch 108, Batch: 0, Loss: 0.657200
Train - Epoch 109, Batch: 0, Loss: 0.656912
Train - Epoch 110, Batch: 0, Loss: 0.656604
Train - Epoch 111, Batch: 0, Loss: 0.656297
Train - Epoch 112, Batch: 0, Loss: 0.655672
Train - Epoch 113, Batch: 0, Loss: 0.655813
Train - Epoch 114, Batch: 0, Loss: 0.655634
Train - Epoch 115, Batch: 0, Loss: 0.655353
Train - Epoch 116, Batch: 0, Loss: 0.655070
Train - Epoch 117, Batch: 0, Loss: 0.654707
Train - Epoch 118, Batch: 0, Loss: 0.654821
Train - Epoch 119, Batch: 0, Loss: 0.654275
Train - Epoch 120, Batch: 0, Loss: 0.653689
Train - Epoch 121, Batch: 0, Loss: 0.653712
Train - Epoch 122, Batch: 0, Loss: 0.653252
Train - Epoch 123, Batch: 0, Loss: 0.653069
Train - Epoch 124, Batch: 0, Loss: 0.652253
Train - Epoch 125, Batch: 0, Loss: 0.652546
Train - Epoch 126, Batch: 0, Loss: 0.651953
Train - Epoch 127, Batch: 0, Loss: 0.652235
Train - Epoch 128, Batch: 0, Loss: 0.651798
Train - Epoch 129, Batch: 0, Loss: 0.650657
Train - Epoch 130, Batch: 0, Loss: 0.650784
Train - Epoch 131, Batch: 0, Loss: 0.650850
Train - Epoch 132, Batch: 0, Loss: 0.650646
Train - Epoch 133, Batch: 0, Loss: 0.650148
Train - Epoch 134, Batch: 0, Loss: 0.649930
Train - Epoch 135, Batch: 0, Loss: 0.649821
Train - Epoch 136, Batch: 0, Loss: 0.649288
Train - Epoch 137, Batch: 0, Loss: 0.649145
Train - Epoch 138, Batch: 0, Loss: 0.648642
Train - Epoch 139, Batch: 0, Loss: 0.648178
Train - Epoch 140, Batch: 0, Loss: 0.648077
Train - Epoch 141, Batch: 0, Loss: 0.648150
Train - Epoch 142, Batch: 0, Loss: 0.647463
Train - Epoch 143, Batch: 0, Loss: 0.646986
Train - Epoch 144, Batch: 0, Loss: 0.647017
Train - Epoch 145, Batch: 0, Loss: 0.646402
Train - Epoch 146, Batch: 0, Loss: 0.646560
Train - Epoch 147, Batch: 0, Loss: 0.646453
Train - Epoch 148, Batch: 0, Loss: 0.645983
Train - Epoch 149, Batch: 0, Loss: 0.646118
Train - Epoch 150, Batch: 0, Loss: 0.645744
Train - Epoch 151, Batch: 0, Loss: 0.645012
Train - Epoch 152, Batch: 0, Loss: 0.645081
Train - Epoch 153, Batch: 0, Loss: 0.644506
Train - Epoch 154, Batch: 0, Loss: 0.644156
Train - Epoch 155, Batch: 0, Loss: 0.643472
Train - Epoch 156, Batch: 0, Loss: 0.644379
Train - Epoch 157, Batch: 0, Loss: 0.643944
Train - Epoch 158, Batch: 0, Loss: 0.643462
Train - Epoch 159, Batch: 0, Loss: 0.642890
Train - Epoch 160, Batch: 0, Loss: 0.642829
Train - Epoch 161, Batch: 0, Loss: 0.642554
Train - Epoch 162, Batch: 0, Loss: 0.642146
Train - Epoch 163, Batch: 0, Loss: 0.641592
Train - Epoch 164, Batch: 0, Loss: 0.641994
Train - Epoch 165, Batch: 0, Loss: 0.641405
Train - Epoch 166, Batch: 0, Loss: 0.641265
Train - Epoch 167, Batch: 0, Loss: 0.641252
Train - Epoch 168, Batch: 0, Loss: 0.640688
Train - Epoch 169, Batch: 0, Loss: 0.640507
Train - Epoch 170, Batch: 0, Loss: 0.640103
Train - Epoch 171, Batch: 0, Loss: 0.639880
Train - Epoch 172, Batch: 0, Loss: 0.639509
Train - Epoch 173, Batch: 0, Loss: 0.639241
Train - Epoch 174, Batch: 0, Loss: 0.639158
Train - Epoch 175, Batch: 0, Loss: 0.638532
Train - Epoch 176, Batch: 0, Loss: 0.638888
Train - Epoch 177, Batch: 0, Loss: 0.638768
Train - Epoch 178, Batch: 0, Loss: 0.638141
Train - Epoch 179, Batch: 0, Loss: 0.637999
Train - Epoch 180, Batch: 0, Loss: 0.637201
Train - Epoch 181, Batch: 0, Loss: 0.637452
Train - Epoch 182, Batch: 0, Loss: 0.637232
Train - Epoch 183, Batch: 0, Loss: 0.636735
Train - Epoch 184, Batch: 0, Loss: 0.636854
Train - Epoch 185, Batch: 0, Loss: 0.636450
Train - Epoch 186, Batch: 0, Loss: 0.636141/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635666
Train - Epoch 188, Batch: 0, Loss: 0.635227
Train - Epoch 189, Batch: 0, Loss: 0.636137
Train - Epoch 190, Batch: 0, Loss: 0.635663
Train - Epoch 191, Batch: 0, Loss: 0.635188
Train - Epoch 192, Batch: 0, Loss: 0.634437
Train - Epoch 193, Batch: 0, Loss: 0.634215
Train - Epoch 194, Batch: 0, Loss: 0.634568
Train - Epoch 195, Batch: 0, Loss: 0.633420
Train - Epoch 196, Batch: 0, Loss: 0.633218
Train - Epoch 197, Batch: 0, Loss: 0.633548
Train - Epoch 198, Batch: 0, Loss: 0.633105
Train - Epoch 199, Batch: 0, Loss: 0.633037
training_time:: 349.7082452774048
training time full:: 349.7083110809326
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000063, Accuracy: 0.927379
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.5708200931549
overhead:: 0
overhead2:: 0.393756628036499
overhead3:: 0
time_baseline:: 275.5711817741394
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927329
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624750)
RCV1 Test Avg. Accuracy:: 0.9222777122493537
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.020756244659423828
overhead3:: 0.11755871772766113
overhead4:: 33.195369720458984
overhead5:: 0
memory usage:: 26647728128
time_provenance:: 45.57089304924011
curr_diff: 0 tensor(1.1644e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1644e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927329
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.0005_10200
tensor(624749)
RCV1 Test Avg. Accuracy:: 0.9222762360145202
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693108
Train - Epoch 1, Batch: 0, Loss: 0.692747
Train - Epoch 2, Batch: 0, Loss: 0.692332
Train - Epoch 3, Batch: 0, Loss: 0.691986
Train - Epoch 4, Batch: 0, Loss: 0.691638
Train - Epoch 5, Batch: 0, Loss: 0.691275
Train - Epoch 6, Batch: 0, Loss: 0.690851
Train - Epoch 7, Batch: 0, Loss: 0.690511
Train - Epoch 8, Batch: 0, Loss: 0.690142
Train - Epoch 9, Batch: 0, Loss: 0.689777
Train - Epoch 10, Batch: 0, Loss: 0.689391
Train - Epoch 11, Batch: 0, Loss: 0.688998
Train - Epoch 12, Batch: 0, Loss: 0.688627
Train - Epoch 13, Batch: 0, Loss: 0.688321
Train - Epoch 14, Batch: 0, Loss: 0.687936
Train - Epoch 15, Batch: 0, Loss: 0.687604
Train - Epoch 16, Batch: 0, Loss: 0.687252
Train - Epoch 17, Batch: 0, Loss: 0.686872
Train - Epoch 18, Batch: 0, Loss: 0.686520
Train - Epoch 19, Batch: 0, Loss: 0.686185
Train - Epoch 20, Batch: 0, Loss: 0.685735
Train - Epoch 21, Batch: 0, Loss: 0.685411
Train - Epoch 22, Batch: 0, Loss: 0.685041
Train - Epoch 23, Batch: 0, Loss: 0.684684
Train - Epoch 24, Batch: 0, Loss: 0.684394
Train - Epoch 25, Batch: 0, Loss: 0.683976
Train - Epoch 26, Batch: 0, Loss: 0.683799
Train - Epoch 27, Batch: 0, Loss: 0.683394
Train - Epoch 28, Batch: 0, Loss: 0.682845
Train - Epoch 29, Batch: 0, Loss: 0.682630
Train - Epoch 30, Batch: 0, Loss: 0.682309
Train - Epoch 31, Batch: 0, Loss: 0.681926
Train - Epoch 32, Batch: 0, Loss: 0.681546
Train - Epoch 33, Batch: 0, Loss: 0.681173
Train - Epoch 34, Batch: 0, Loss: 0.680698
Train - Epoch 35, Batch: 0, Loss: 0.680522
Train - Epoch 36, Batch: 0, Loss: 0.680132
Train - Epoch 37, Batch: 0, Loss: 0.679895
Train - Epoch 38, Batch: 0, Loss: 0.679503
Train - Epoch 39, Batch: 0, Loss: 0.679070
Train - Epoch 40, Batch: 0, Loss: 0.678777
Train - Epoch 41, Batch: 0, Loss: 0.678287
Train - Epoch 42, Batch: 0, Loss: 0.678028
Train - Epoch 43, Batch: 0, Loss: 0.677671
Train - Epoch 44, Batch: 0, Loss: 0.677456
Train - Epoch 45, Batch: 0, Loss: 0.677117
Train - Epoch 46, Batch: 0, Loss: 0.676761
Train - Epoch 47, Batch: 0, Loss: 0.676451
Train - Epoch 48, Batch: 0, Loss: 0.676092
Train - Epoch 49, Batch: 0, Loss: 0.675696
Train - Epoch 50, Batch: 0, Loss: 0.675388
Train - Epoch 51, Batch: 0, Loss: 0.675059
Train - Epoch 52, Batch: 0, Loss: 0.674641
Train - Epoch 53, Batch: 0, Loss: 0.674356
Train - Epoch 54, Batch: 0, Loss: 0.674046
Train - Epoch 55, Batch: 0, Loss: 0.673669
Train - Epoch 56, Batch: 0, Loss: 0.673538
Train - Epoch 57, Batch: 0, Loss: 0.673099
Train - Epoch 58, Batch: 0, Loss: 0.672806
Train - Epoch 59, Batch: 0, Loss: 0.672477
Train - Epoch 60, Batch: 0, Loss: 0.672152
Train - Epoch 61, Batch: 0, Loss: 0.671781
Train - Epoch 62, Batch: 0, Loss: 0.671348
Train - Epoch 63, Batch: 0, Loss: 0.670980
Train - Epoch 64, Batch: 0, Loss: 0.670863
Train - Epoch 65, Batch: 0, Loss: 0.670643
Train - Epoch 66, Batch: 0, Loss: 0.670136
Train - Epoch 67, Batch: 0, Loss: 0.669946
Train - Epoch 68, Batch: 0, Loss: 0.669582
Train - Epoch 69, Batch: 0, Loss: 0.669228
Train - Epoch 70, Batch: 0, Loss: 0.669126
Train - Epoch 71, Batch: 0, Loss: 0.668487
Train - Epoch 72, Batch: 0, Loss: 0.667982
Train - Epoch 73, Batch: 0, Loss: 0.667974
Train - Epoch 74, Batch: 0, Loss: 0.667917
Train - Epoch 75, Batch: 0, Loss: 0.667515
Train - Epoch 76, Batch: 0, Loss: 0.666994
Train - Epoch 77, Batch: 0, Loss: 0.666501
Train - Epoch 78, Batch: 0, Loss: 0.666746
Train - Epoch 79, Batch: 0, Loss: 0.665967
Train - Epoch 80, Batch: 0, Loss: 0.665538
Train - Epoch 81, Batch: 0, Loss: 0.665395
Train - Epoch 82, Batch: 0, Loss: 0.665330
Train - Epoch 83, Batch: 0, Loss: 0.664993
Train - Epoch 84, Batch: 0, Loss: 0.664608
Train - Epoch 85, Batch: 0, Loss: 0.663981
Train - Epoch 86, Batch: 0, Loss: 0.663589
Train - Epoch 87, Batch: 0, Loss: 0.663471
Train - Epoch 88, Batch: 0, Loss: 0.663251
Train - Epoch 89, Batch: 0, Loss: 0.662885
Train - Epoch 90, Batch: 0, Loss: 0.662339
Train - Epoch 91, Batch: 0, Loss: 0.662256
Train - Epoch 92, Batch: 0, Loss: 0.661708
Train - Epoch 93, Batch: 0, Loss: 0.661577
Train - Epoch 94, Batch: 0, Loss: 0.661660
Train - Epoch 95, Batch: 0, Loss: 0.661274
Train - Epoch 96, Batch: 0, Loss: 0.660961
Train - Epoch 97, Batch: 0, Loss: 0.660139
Train - Epoch 98, Batch: 0, Loss: 0.660103
Train - Epoch 99, Batch: 0, Loss: 0.659713
Train - Epoch 100, Batch: 0, Loss: 0.659271
Train - Epoch 101, Batch: 0, Loss: 0.659303
Train - Epoch 102, Batch: 0, Loss: 0.658793
Train - Epoch 103, Batch: 0, Loss: 0.658696
Train - Epoch 104, Batch: 0, Loss: 0.658359
Train - Epoch 105, Batch: 0, Loss: 0.657907
Train - Epoch 106, Batch: 0, Loss: 0.657458
Train - Epoch 107, Batch: 0, Loss: 0.657297
Train - Epoch 108, Batch: 0, Loss: 0.657610
Train - Epoch 109, Batch: 0, Loss: 0.656988
Train - Epoch 110, Batch: 0, Loss: 0.656533
Train - Epoch 111, Batch: 0, Loss: 0.656310
Train - Epoch 112, Batch: 0, Loss: 0.655873
Train - Epoch 113, Batch: 0, Loss: 0.656119
Train - Epoch 114, Batch: 0, Loss: 0.655361
Train - Epoch 115, Batch: 0, Loss: 0.655015
Train - Epoch 116, Batch: 0, Loss: 0.655077
Train - Epoch 117, Batch: 0, Loss: 0.654545
Train - Epoch 118, Batch: 0, Loss: 0.654598
Train - Epoch 119, Batch: 0, Loss: 0.653856
Train - Epoch 120, Batch: 0, Loss: 0.653725
Train - Epoch 121, Batch: 0, Loss: 0.653641
Train - Epoch 122, Batch: 0, Loss: 0.653365
Train - Epoch 123, Batch: 0, Loss: 0.652769
Train - Epoch 124, Batch: 0, Loss: 0.652585
Train - Epoch 125, Batch: 0, Loss: 0.652439
Train - Epoch 126, Batch: 0, Loss: 0.652136
Train - Epoch 127, Batch: 0, Loss: 0.651792
Train - Epoch 128, Batch: 0, Loss: 0.651309
Train - Epoch 129, Batch: 0, Loss: 0.651311
Train - Epoch 130, Batch: 0, Loss: 0.650723
Train - Epoch 131, Batch: 0, Loss: 0.650785
Train - Epoch 132, Batch: 0, Loss: 0.650067
Train - Epoch 133, Batch: 0, Loss: 0.650175
Train - Epoch 134, Batch: 0, Loss: 0.649843
Train - Epoch 135, Batch: 0, Loss: 0.649627
Train - Epoch 136, Batch: 0, Loss: 0.649548
Train - Epoch 137, Batch: 0, Loss: 0.649281
Train - Epoch 138, Batch: 0, Loss: 0.648508
Train - Epoch 139, Batch: 0, Loss: 0.648387
Train - Epoch 140, Batch: 0, Loss: 0.647873
Train - Epoch 141, Batch: 0, Loss: 0.648042
Train - Epoch 142, Batch: 0, Loss: 0.647771
Train - Epoch 143, Batch: 0, Loss: 0.647317
Train - Epoch 144, Batch: 0, Loss: 0.646722
Train - Epoch 145, Batch: 0, Loss: 0.647069
Train - Epoch 146, Batch: 0, Loss: 0.646476
Train - Epoch 147, Batch: 0, Loss: 0.646231
Train - Epoch 148, Batch: 0, Loss: 0.645999
Train - Epoch 149, Batch: 0, Loss: 0.645890
Train - Epoch 150, Batch: 0, Loss: 0.645348
Train - Epoch 151, Batch: 0, Loss: 0.645140
Train - Epoch 152, Batch: 0, Loss: 0.644629
Train - Epoch 153, Batch: 0, Loss: 0.645094
Train - Epoch 154, Batch: 0, Loss: 0.643993
Train - Epoch 155, Batch: 0, Loss: 0.643825
Train - Epoch 156, Batch: 0, Loss: 0.643626
Train - Epoch 157, Batch: 0, Loss: 0.643456
Train - Epoch 158, Batch: 0, Loss: 0.643493
Train - Epoch 159, Batch: 0, Loss: 0.642454
Train - Epoch 160, Batch: 0, Loss: 0.642890
Train - Epoch 161, Batch: 0, Loss: 0.642541
Train - Epoch 162, Batch: 0, Loss: 0.642443
Train - Epoch 163, Batch: 0, Loss: 0.641727
Train - Epoch 164, Batch: 0, Loss: 0.641739
Train - Epoch 165, Batch: 0, Loss: 0.641696
Train - Epoch 166, Batch: 0, Loss: 0.640709
Train - Epoch 167, Batch: 0, Loss: 0.641050
Train - Epoch 168, Batch: 0, Loss: 0.640864
Train - Epoch 169, Batch: 0, Loss: 0.639899
Train - Epoch 170, Batch: 0, Loss: 0.639867
Train - Epoch 171, Batch: 0, Loss: 0.639879
Train - Epoch 172, Batch: 0, Loss: 0.639114
Train - Epoch 173, Batch: 0, Loss: 0.638998
Train - Epoch 174, Batch: 0, Loss: 0.638736
Train - Epoch 175, Batch: 0, Loss: 0.638773
Train - Epoch 176, Batch: 0, Loss: 0.638675
Train - Epoch 177, Batch: 0, Loss: 0.638372
Train - Epoch 178, Batch: 0, Loss: 0.638564
Train - Epoch 179, Batch: 0, Loss: 0.638574
Train - Epoch 180, Batch: 0, Loss: 0.637627
Train - Epoch 181, Batch: 0, Loss: 0.637570
Train - Epoch 182, Batch: 0, Loss: 0.637211
Train - Epoch 183, Batch: 0, Loss: 0.636749
Train - Epoch 184, Batch: 0, Loss: 0.636596
Train - Epoch 185, Batch: 0, Loss: 0.636476
Train - Epoch 186, Batch: 0, Loss: 0.635826/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635574
Train - Epoch 188, Batch: 0, Loss: 0.635592
Train - Epoch 189, Batch: 0, Loss: 0.635091
Train - Epoch 190, Batch: 0, Loss: 0.635584
Train - Epoch 191, Batch: 0, Loss: 0.634414
Train - Epoch 192, Batch: 0, Loss: 0.634217
Train - Epoch 193, Batch: 0, Loss: 0.635147
Train - Epoch 194, Batch: 0, Loss: 0.633722
Train - Epoch 195, Batch: 0, Loss: 0.633705
Train - Epoch 196, Batch: 0, Loss: 0.633795
Train - Epoch 197, Batch: 0, Loss: 0.633494
Train - Epoch 198, Batch: 0, Loss: 0.633410
Train - Epoch 199, Batch: 0, Loss: 0.632629
training_time:: 350.6748912334442
training time full:: 350.674959897995
provenance prepare time:: 6.198883056640625e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926391
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.475058555603
overhead:: 0
overhead2:: 0.39229846000671387
overhead3:: 0
time_baseline:: 275.47543263435364
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926440
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624749)
RCV1 Test Avg. Accuracy:: 0.9222762360145202
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.022025108337402344
overhead3:: 0.11771893501281738
overhead4:: 32.99712085723877
overhead5:: 0
memory usage:: 26620694528
time_provenance:: 45.390949010849
curr_diff: 0 tensor(1.1515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1515e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926440
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.0005_10200
tensor(624749)
RCV1 Test Avg. Accuracy:: 0.9222762360145202
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693238
Train - Epoch 1, Batch: 0, Loss: 0.692839
Train - Epoch 2, Batch: 0, Loss: 0.692476
Train - Epoch 3, Batch: 0, Loss: 0.692109
Train - Epoch 4, Batch: 0, Loss: 0.691761
Train - Epoch 5, Batch: 0, Loss: 0.691373
Train - Epoch 6, Batch: 0, Loss: 0.691018
Train - Epoch 7, Batch: 0, Loss: 0.690641
Train - Epoch 8, Batch: 0, Loss: 0.690256
Train - Epoch 9, Batch: 0, Loss: 0.689856
Train - Epoch 10, Batch: 0, Loss: 0.689540
Train - Epoch 11, Batch: 0, Loss: 0.689198
Train - Epoch 12, Batch: 0, Loss: 0.688802
Train - Epoch 13, Batch: 0, Loss: 0.688462
Train - Epoch 14, Batch: 0, Loss: 0.688026
Train - Epoch 15, Batch: 0, Loss: 0.687764
Train - Epoch 16, Batch: 0, Loss: 0.687333
Train - Epoch 17, Batch: 0, Loss: 0.686927
Train - Epoch 18, Batch: 0, Loss: 0.686578
Train - Epoch 19, Batch: 0, Loss: 0.686184
Train - Epoch 20, Batch: 0, Loss: 0.685843
Train - Epoch 21, Batch: 0, Loss: 0.685440
Train - Epoch 22, Batch: 0, Loss: 0.685206
Train - Epoch 23, Batch: 0, Loss: 0.684926
Train - Epoch 24, Batch: 0, Loss: 0.684508
Train - Epoch 25, Batch: 0, Loss: 0.684086
Train - Epoch 26, Batch: 0, Loss: 0.683802
Train - Epoch 27, Batch: 0, Loss: 0.683424
Train - Epoch 28, Batch: 0, Loss: 0.683038
Train - Epoch 29, Batch: 0, Loss: 0.682659
Train - Epoch 30, Batch: 0, Loss: 0.682277
Train - Epoch 31, Batch: 0, Loss: 0.682014
Train - Epoch 32, Batch: 0, Loss: 0.681706
Train - Epoch 33, Batch: 0, Loss: 0.681226
Train - Epoch 34, Batch: 0, Loss: 0.680769
Train - Epoch 35, Batch: 0, Loss: 0.680719
Train - Epoch 36, Batch: 0, Loss: 0.680248
Train - Epoch 37, Batch: 0, Loss: 0.679998
Train - Epoch 38, Batch: 0, Loss: 0.679537
Train - Epoch 39, Batch: 0, Loss: 0.679312
Train - Epoch 40, Batch: 0, Loss: 0.678835
Train - Epoch 41, Batch: 0, Loss: 0.678395
Train - Epoch 42, Batch: 0, Loss: 0.678127
Train - Epoch 43, Batch: 0, Loss: 0.677989
Train - Epoch 44, Batch: 0, Loss: 0.677567
Train - Epoch 45, Batch: 0, Loss: 0.677104
Train - Epoch 46, Batch: 0, Loss: 0.676833
Train - Epoch 47, Batch: 0, Loss: 0.676548
Train - Epoch 48, Batch: 0, Loss: 0.676022
Train - Epoch 49, Batch: 0, Loss: 0.675866
Train - Epoch 50, Batch: 0, Loss: 0.675474
Train - Epoch 51, Batch: 0, Loss: 0.675297
Train - Epoch 52, Batch: 0, Loss: 0.674747
Train - Epoch 53, Batch: 0, Loss: 0.674609
Train - Epoch 54, Batch: 0, Loss: 0.673996
Train - Epoch 55, Batch: 0, Loss: 0.673933
Train - Epoch 56, Batch: 0, Loss: 0.673407
Train - Epoch 57, Batch: 0, Loss: 0.673241
Train - Epoch 58, Batch: 0, Loss: 0.672885
Train - Epoch 59, Batch: 0, Loss: 0.672383
Train - Epoch 60, Batch: 0, Loss: 0.672149
Train - Epoch 61, Batch: 0, Loss: 0.671959
Train - Epoch 62, Batch: 0, Loss: 0.671674
Train - Epoch 63, Batch: 0, Loss: 0.671168
Train - Epoch 64, Batch: 0, Loss: 0.670958
Train - Epoch 65, Batch: 0, Loss: 0.670630
Train - Epoch 66, Batch: 0, Loss: 0.670232
Train - Epoch 67, Batch: 0, Loss: 0.670306
Train - Epoch 68, Batch: 0, Loss: 0.669846
Train - Epoch 69, Batch: 0, Loss: 0.669319
Train - Epoch 70, Batch: 0, Loss: 0.668954
Train - Epoch 71, Batch: 0, Loss: 0.668686
Train - Epoch 72, Batch: 0, Loss: 0.668147
Train - Epoch 73, Batch: 0, Loss: 0.668362
Train - Epoch 74, Batch: 0, Loss: 0.667732
Train - Epoch 75, Batch: 0, Loss: 0.667323
Train - Epoch 76, Batch: 0, Loss: 0.667112
Train - Epoch 77, Batch: 0, Loss: 0.666517
Train - Epoch 78, Batch: 0, Loss: 0.666553
Train - Epoch 79, Batch: 0, Loss: 0.666258
Train - Epoch 80, Batch: 0, Loss: 0.665722
Train - Epoch 81, Batch: 0, Loss: 0.665380
Train - Epoch 82, Batch: 0, Loss: 0.665199
Train - Epoch 83, Batch: 0, Loss: 0.665031
Train - Epoch 84, Batch: 0, Loss: 0.664637
Train - Epoch 85, Batch: 0, Loss: 0.664026
Train - Epoch 86, Batch: 0, Loss: 0.663318
Train - Epoch 87, Batch: 0, Loss: 0.663367
Train - Epoch 88, Batch: 0, Loss: 0.663310
Train - Epoch 89, Batch: 0, Loss: 0.663243
Train - Epoch 90, Batch: 0, Loss: 0.662615
Train - Epoch 91, Batch: 0, Loss: 0.662061
Train - Epoch 92, Batch: 0, Loss: 0.662406
Train - Epoch 93, Batch: 0, Loss: 0.661654
Train - Epoch 94, Batch: 0, Loss: 0.661361
Train - Epoch 95, Batch: 0, Loss: 0.661238
Train - Epoch 96, Batch: 0, Loss: 0.660983
Train - Epoch 97, Batch: 0, Loss: 0.660275
Train - Epoch 98, Batch: 0, Loss: 0.660078
Train - Epoch 99, Batch: 0, Loss: 0.660109
Train - Epoch 100, Batch: 0, Loss: 0.659650
Train - Epoch 101, Batch: 0, Loss: 0.659673
Train - Epoch 102, Batch: 0, Loss: 0.659131
Train - Epoch 103, Batch: 0, Loss: 0.658535
Train - Epoch 104, Batch: 0, Loss: 0.658230
Train - Epoch 105, Batch: 0, Loss: 0.658414
Train - Epoch 106, Batch: 0, Loss: 0.657918
Train - Epoch 107, Batch: 0, Loss: 0.657647
Train - Epoch 108, Batch: 0, Loss: 0.657278
Train - Epoch 109, Batch: 0, Loss: 0.657053
Train - Epoch 110, Batch: 0, Loss: 0.656612
Train - Epoch 111, Batch: 0, Loss: 0.656338
Train - Epoch 112, Batch: 0, Loss: 0.655924
Train - Epoch 113, Batch: 0, Loss: 0.655830
Train - Epoch 114, Batch: 0, Loss: 0.655567
Train - Epoch 115, Batch: 0, Loss: 0.655143
Train - Epoch 116, Batch: 0, Loss: 0.654642
Train - Epoch 117, Batch: 0, Loss: 0.655235
Train - Epoch 118, Batch: 0, Loss: 0.654028
Train - Epoch 119, Batch: 0, Loss: 0.654101
Train - Epoch 120, Batch: 0, Loss: 0.653797
Train - Epoch 121, Batch: 0, Loss: 0.653730
Train - Epoch 122, Batch: 0, Loss: 0.653252
Train - Epoch 123, Batch: 0, Loss: 0.652598
Train - Epoch 124, Batch: 0, Loss: 0.652475
Train - Epoch 125, Batch: 0, Loss: 0.652381
Train - Epoch 126, Batch: 0, Loss: 0.652048
Train - Epoch 127, Batch: 0, Loss: 0.651567
Train - Epoch 128, Batch: 0, Loss: 0.651384
Train - Epoch 129, Batch: 0, Loss: 0.651175
Train - Epoch 130, Batch: 0, Loss: 0.650981
Train - Epoch 131, Batch: 0, Loss: 0.650464
Train - Epoch 132, Batch: 0, Loss: 0.649838
Train - Epoch 133, Batch: 0, Loss: 0.650024
Train - Epoch 134, Batch: 0, Loss: 0.649604
Train - Epoch 135, Batch: 0, Loss: 0.649665
Train - Epoch 136, Batch: 0, Loss: 0.649457
Train - Epoch 137, Batch: 0, Loss: 0.649017
Train - Epoch 138, Batch: 0, Loss: 0.648698
Train - Epoch 139, Batch: 0, Loss: 0.648270
Train - Epoch 140, Batch: 0, Loss: 0.648404
Train - Epoch 141, Batch: 0, Loss: 0.647877
Train - Epoch 142, Batch: 0, Loss: 0.647385
Train - Epoch 143, Batch: 0, Loss: 0.646715
Train - Epoch 144, Batch: 0, Loss: 0.647031
Train - Epoch 145, Batch: 0, Loss: 0.647202
Train - Epoch 146, Batch: 0, Loss: 0.646484
Train - Epoch 147, Batch: 0, Loss: 0.646501
Train - Epoch 148, Batch: 0, Loss: 0.645868
Train - Epoch 149, Batch: 0, Loss: 0.646013
Train - Epoch 150, Batch: 0, Loss: 0.645324
Train - Epoch 151, Batch: 0, Loss: 0.644914
Train - Epoch 152, Batch: 0, Loss: 0.645174
Train - Epoch 153, Batch: 0, Loss: 0.645008
Train - Epoch 154, Batch: 0, Loss: 0.644477
Train - Epoch 155, Batch: 0, Loss: 0.644632
Train - Epoch 156, Batch: 0, Loss: 0.643483
Train - Epoch 157, Batch: 0, Loss: 0.644035
Train - Epoch 158, Batch: 0, Loss: 0.643603
Train - Epoch 159, Batch: 0, Loss: 0.642984
Train - Epoch 160, Batch: 0, Loss: 0.643108
Train - Epoch 161, Batch: 0, Loss: 0.642689
Train - Epoch 162, Batch: 0, Loss: 0.641926
Train - Epoch 163, Batch: 0, Loss: 0.641586
Train - Epoch 164, Batch: 0, Loss: 0.641597
Train - Epoch 165, Batch: 0, Loss: 0.641643
Train - Epoch 166, Batch: 0, Loss: 0.641094
Train - Epoch 167, Batch: 0, Loss: 0.640061
Train - Epoch 168, Batch: 0, Loss: 0.640547
Train - Epoch 169, Batch: 0, Loss: 0.639927
Train - Epoch 170, Batch: 0, Loss: 0.640474
Train - Epoch 171, Batch: 0, Loss: 0.639649
Train - Epoch 172, Batch: 0, Loss: 0.640045
Train - Epoch 173, Batch: 0, Loss: 0.639064
Train - Epoch 174, Batch: 0, Loss: 0.639091
Train - Epoch 175, Batch: 0, Loss: 0.639058
Train - Epoch 176, Batch: 0, Loss: 0.638855
Train - Epoch 177, Batch: 0, Loss: 0.638555
Train - Epoch 178, Batch: 0, Loss: 0.637997
Train - Epoch 179, Batch: 0, Loss: 0.638374
Train - Epoch 180, Batch: 0, Loss: 0.638053
Train - Epoch 181, Batch: 0, Loss: 0.637814
Train - Epoch 182, Batch: 0, Loss: 0.637073
Train - Epoch 183, Batch: 0, Loss: 0.636866
Train - Epoch 184, Batch: 0, Loss: 0.636688
Train - Epoch 185, Batch: 0, Loss: 0.635695
Train - Epoch 186, Batch: 0, Loss: 0.635935/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635819
Train - Epoch 188, Batch: 0, Loss: 0.636336
Train - Epoch 189, Batch: 0, Loss: 0.635023
Train - Epoch 190, Batch: 0, Loss: 0.634847
Train - Epoch 191, Batch: 0, Loss: 0.634806
Train - Epoch 192, Batch: 0, Loss: 0.635283
Train - Epoch 193, Batch: 0, Loss: 0.634378
Train - Epoch 194, Batch: 0, Loss: 0.634198
Train - Epoch 195, Batch: 0, Loss: 0.633897
Train - Epoch 196, Batch: 0, Loss: 0.633119
Train - Epoch 197, Batch: 0, Loss: 0.633868
Train - Epoch 198, Batch: 0, Loss: 0.632898
Train - Epoch 199, Batch: 0, Loss: 0.632675
training_time:: 350.993625164032
training time full:: 350.9936902523041
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000063, Accuracy: 0.927478
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.1930046081543
overhead:: 0
overhead2:: 0.3942582607269287
overhead3:: 0
time_baseline:: 275.1933581829071
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927527
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(625025)
RCV1 Test Avg. Accuracy:: 0.9226836768285752
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.020711898803710938
overhead3:: 0.11646866798400879
overhead4:: 33.235477447509766
overhead5:: 0
memory usage:: 26610790400
time_provenance:: 45.63387632369995
curr_diff: 0 tensor(1.2112e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2112e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927527
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.0005_10200
tensor(625025)
RCV1 Test Avg. Accuracy:: 0.9226836768285752
deletion rate:: 0.001
python3 generate_rand_ids 0.001  rcv1 0
tensor([  353, 11298, 16611,   515,  8325,  4259, 16601, 17576,  5000,  2986,
        10510,  3598, 19664, 17426,  4244, 10357, 19769,  9244, 19582, 12191])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693160
Train - Epoch 1, Batch: 0, Loss: 0.692780
Train - Epoch 2, Batch: 0, Loss: 0.692428
Train - Epoch 3, Batch: 0, Loss: 0.692063
Train - Epoch 4, Batch: 0, Loss: 0.691688
Train - Epoch 5, Batch: 0, Loss: 0.691304
Train - Epoch 6, Batch: 0, Loss: 0.690955
Train - Epoch 7, Batch: 0, Loss: 0.690534
Train - Epoch 8, Batch: 0, Loss: 0.690208
Train - Epoch 9, Batch: 0, Loss: 0.689801
Train - Epoch 10, Batch: 0, Loss: 0.689464
Train - Epoch 11, Batch: 0, Loss: 0.689092
Train - Epoch 12, Batch: 0, Loss: 0.688713
Train - Epoch 13, Batch: 0, Loss: 0.688373
Train - Epoch 14, Batch: 0, Loss: 0.688033
Train - Epoch 15, Batch: 0, Loss: 0.687663
Train - Epoch 16, Batch: 0, Loss: 0.687235
Train - Epoch 17, Batch: 0, Loss: 0.686904
Train - Epoch 18, Batch: 0, Loss: 0.686529
Train - Epoch 19, Batch: 0, Loss: 0.686099
Train - Epoch 20, Batch: 0, Loss: 0.685778
Train - Epoch 21, Batch: 0, Loss: 0.685457
Train - Epoch 22, Batch: 0, Loss: 0.685171
Train - Epoch 23, Batch: 0, Loss: 0.684722
Train - Epoch 24, Batch: 0, Loss: 0.684381
Train - Epoch 25, Batch: 0, Loss: 0.683977
Train - Epoch 26, Batch: 0, Loss: 0.683724
Train - Epoch 27, Batch: 0, Loss: 0.683384
Train - Epoch 28, Batch: 0, Loss: 0.683029
Train - Epoch 29, Batch: 0, Loss: 0.682659
Train - Epoch 30, Batch: 0, Loss: 0.682308
Train - Epoch 31, Batch: 0, Loss: 0.682059
Train - Epoch 32, Batch: 0, Loss: 0.681554
Train - Epoch 33, Batch: 0, Loss: 0.681246
Train - Epoch 34, Batch: 0, Loss: 0.680866
Train - Epoch 35, Batch: 0, Loss: 0.680504
Train - Epoch 36, Batch: 0, Loss: 0.680225
Train - Epoch 37, Batch: 0, Loss: 0.679924
Train - Epoch 38, Batch: 0, Loss: 0.679559
Train - Epoch 39, Batch: 0, Loss: 0.679213
Train - Epoch 40, Batch: 0, Loss: 0.678934
Train - Epoch 41, Batch: 0, Loss: 0.678486
Train - Epoch 42, Batch: 0, Loss: 0.678272
Train - Epoch 43, Batch: 0, Loss: 0.677737
Train - Epoch 44, Batch: 0, Loss: 0.677281
Train - Epoch 45, Batch: 0, Loss: 0.677012
Train - Epoch 46, Batch: 0, Loss: 0.676802
Train - Epoch 47, Batch: 0, Loss: 0.676508
Train - Epoch 48, Batch: 0, Loss: 0.676108
Train - Epoch 49, Batch: 0, Loss: 0.675701
Train - Epoch 50, Batch: 0, Loss: 0.675257
Train - Epoch 51, Batch: 0, Loss: 0.675142
Train - Epoch 52, Batch: 0, Loss: 0.674856
Train - Epoch 53, Batch: 0, Loss: 0.674349
Train - Epoch 54, Batch: 0, Loss: 0.674120
Train - Epoch 55, Batch: 0, Loss: 0.674003
Train - Epoch 56, Batch: 0, Loss: 0.673497
Train - Epoch 57, Batch: 0, Loss: 0.673135
Train - Epoch 58, Batch: 0, Loss: 0.672729
Train - Epoch 59, Batch: 0, Loss: 0.672484
Train - Epoch 60, Batch: 0, Loss: 0.672002
Train - Epoch 61, Batch: 0, Loss: 0.671824
Train - Epoch 62, Batch: 0, Loss: 0.671392
Train - Epoch 63, Batch: 0, Loss: 0.671197
Train - Epoch 64, Batch: 0, Loss: 0.670707
Train - Epoch 65, Batch: 0, Loss: 0.670531
Train - Epoch 66, Batch: 0, Loss: 0.670020
Train - Epoch 67, Batch: 0, Loss: 0.669999
Train - Epoch 68, Batch: 0, Loss: 0.669844
Train - Epoch 69, Batch: 0, Loss: 0.669108
Train - Epoch 70, Batch: 0, Loss: 0.668975
Train - Epoch 71, Batch: 0, Loss: 0.668572
Train - Epoch 72, Batch: 0, Loss: 0.668181
Train - Epoch 73, Batch: 0, Loss: 0.667999
Train - Epoch 74, Batch: 0, Loss: 0.667688
Train - Epoch 75, Batch: 0, Loss: 0.667168
Train - Epoch 76, Batch: 0, Loss: 0.666816
Train - Epoch 77, Batch: 0, Loss: 0.666572
Train - Epoch 78, Batch: 0, Loss: 0.666412
Train - Epoch 79, Batch: 0, Loss: 0.666081
Train - Epoch 80, Batch: 0, Loss: 0.665645
Train - Epoch 81, Batch: 0, Loss: 0.665767
Train - Epoch 82, Batch: 0, Loss: 0.665063
Train - Epoch 83, Batch: 0, Loss: 0.664758
Train - Epoch 84, Batch: 0, Loss: 0.664414
Train - Epoch 85, Batch: 0, Loss: 0.664298
Train - Epoch 86, Batch: 0, Loss: 0.663738
Train - Epoch 87, Batch: 0, Loss: 0.663442
Train - Epoch 88, Batch: 0, Loss: 0.663168
Train - Epoch 89, Batch: 0, Loss: 0.662856
Train - Epoch 90, Batch: 0, Loss: 0.662735
Train - Epoch 91, Batch: 0, Loss: 0.662545
Train - Epoch 92, Batch: 0, Loss: 0.661659
Train - Epoch 93, Batch: 0, Loss: 0.661526
Train - Epoch 94, Batch: 0, Loss: 0.661387
Train - Epoch 95, Batch: 0, Loss: 0.661081
Train - Epoch 96, Batch: 0, Loss: 0.661012
Train - Epoch 97, Batch: 0, Loss: 0.660864
Train - Epoch 98, Batch: 0, Loss: 0.660488
Train - Epoch 99, Batch: 0, Loss: 0.660119
Train - Epoch 100, Batch: 0, Loss: 0.659570
Train - Epoch 101, Batch: 0, Loss: 0.659614
Train - Epoch 102, Batch: 0, Loss: 0.658771
Train - Epoch 103, Batch: 0, Loss: 0.658718
Train - Epoch 104, Batch: 0, Loss: 0.658396
Train - Epoch 105, Batch: 0, Loss: 0.658626
Train - Epoch 106, Batch: 0, Loss: 0.658001
Train - Epoch 107, Batch: 0, Loss: 0.657633
Train - Epoch 108, Batch: 0, Loss: 0.657191
Train - Epoch 109, Batch: 0, Loss: 0.656880
Train - Epoch 110, Batch: 0, Loss: 0.656573
Train - Epoch 111, Batch: 0, Loss: 0.656290
Train - Epoch 112, Batch: 0, Loss: 0.655623
Train - Epoch 113, Batch: 0, Loss: 0.655788
Train - Epoch 114, Batch: 0, Loss: 0.655626
Train - Epoch 115, Batch: 0, Loss: 0.655335
Train - Epoch 116, Batch: 0, Loss: 0.655029
Train - Epoch 117, Batch: 0, Loss: 0.654668
Train - Epoch 118, Batch: 0, Loss: 0.654823
Train - Epoch 119, Batch: 0, Loss: 0.654256
Train - Epoch 120, Batch: 0, Loss: 0.653657
Train - Epoch 121, Batch: 0, Loss: 0.653676
Train - Epoch 122, Batch: 0, Loss: 0.653233
Train - Epoch 123, Batch: 0, Loss: 0.653045
Train - Epoch 124, Batch: 0, Loss: 0.652225
Train - Epoch 125, Batch: 0, Loss: 0.652518
Train - Epoch 126, Batch: 0, Loss: 0.651932
Train - Epoch 127, Batch: 0, Loss: 0.652209
Train - Epoch 128, Batch: 0, Loss: 0.651777
Train - Epoch 129, Batch: 0, Loss: 0.650633
Train - Epoch 130, Batch: 0, Loss: 0.650780
Train - Epoch 131, Batch: 0, Loss: 0.650838
Train - Epoch 132, Batch: 0, Loss: 0.650633
Train - Epoch 133, Batch: 0, Loss: 0.650122
Train - Epoch 134, Batch: 0, Loss: 0.649913
Train - Epoch 135, Batch: 0, Loss: 0.649808
Train - Epoch 136, Batch: 0, Loss: 0.649292
Train - Epoch 137, Batch: 0, Loss: 0.649123
Train - Epoch 138, Batch: 0, Loss: 0.648624
Train - Epoch 139, Batch: 0, Loss: 0.648163
Train - Epoch 140, Batch: 0, Loss: 0.648058
Train - Epoch 141, Batch: 0, Loss: 0.648107
Train - Epoch 142, Batch: 0, Loss: 0.647441
Train - Epoch 143, Batch: 0, Loss: 0.646957
Train - Epoch 144, Batch: 0, Loss: 0.646978
Train - Epoch 145, Batch: 0, Loss: 0.646382
Train - Epoch 146, Batch: 0, Loss: 0.646556
Train - Epoch 147, Batch: 0, Loss: 0.646441
Train - Epoch 148, Batch: 0, Loss: 0.645960
Train - Epoch 149, Batch: 0, Loss: 0.646095
Train - Epoch 150, Batch: 0, Loss: 0.645704
Train - Epoch 151, Batch: 0, Loss: 0.644981
Train - Epoch 152, Batch: 0, Loss: 0.645045
Train - Epoch 153, Batch: 0, Loss: 0.644467
Train - Epoch 154, Batch: 0, Loss: 0.644141
Train - Epoch 155, Batch: 0, Loss: 0.643477
Train - Epoch 156, Batch: 0, Loss: 0.644366
Train - Epoch 157, Batch: 0, Loss: 0.643919
Train - Epoch 158, Batch: 0, Loss: 0.643445
Train - Epoch 159, Batch: 0, Loss: 0.642867
Train - Epoch 160, Batch: 0, Loss: 0.642818
Train - Epoch 161, Batch: 0, Loss: 0.642507
Train - Epoch 162, Batch: 0, Loss: 0.642140
Train - Epoch 163, Batch: 0, Loss: 0.641551
Train - Epoch 164, Batch: 0, Loss: 0.641961
Train - Epoch 165, Batch: 0, Loss: 0.641390
Train - Epoch 166, Batch: 0, Loss: 0.641235
Train - Epoch 167, Batch: 0, Loss: 0.641232
Train - Epoch 168, Batch: 0, Loss: 0.640660
Train - Epoch 169, Batch: 0, Loss: 0.640477
Train - Epoch 170, Batch: 0, Loss: 0.640095
Train - Epoch 171, Batch: 0, Loss: 0.639844
Train - Epoch 172, Batch: 0, Loss: 0.639497
Train - Epoch 173, Batch: 0, Loss: 0.639226
Train - Epoch 174, Batch: 0, Loss: 0.639125
Train - Epoch 175, Batch: 0, Loss: 0.638500
Train - Epoch 176, Batch: 0, Loss: 0.638850
Train - Epoch 177, Batch: 0, Loss: 0.638763
Train - Epoch 178, Batch: 0, Loss: 0.638140
Train - Epoch 179, Batch: 0, Loss: 0.637990
Train - Epoch 180, Batch: 0, Loss: 0.637208
Train - Epoch 181, Batch: 0, Loss: 0.637454
Train - Epoch 182, Batch: 0, Loss: 0.637204
Train - Epoch 183, Batch: 0, Loss: 0.636723
Train - Epoch 184, Batch: 0, Loss: 0.636838
Train - Epoch 185, Batch: 0, Loss: 0.636457
Train - Epoch 186, Batch: 0, Loss: 0.636140/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635650
Train - Epoch 188, Batch: 0, Loss: 0.635203
Train - Epoch 189, Batch: 0, Loss: 0.636094
Train - Epoch 190, Batch: 0, Loss: 0.635663
Train - Epoch 191, Batch: 0, Loss: 0.635166
Train - Epoch 192, Batch: 0, Loss: 0.634423
Train - Epoch 193, Batch: 0, Loss: 0.634212
Train - Epoch 194, Batch: 0, Loss: 0.634554
Train - Epoch 195, Batch: 0, Loss: 0.633411
Train - Epoch 196, Batch: 0, Loss: 0.633199
Train - Epoch 197, Batch: 0, Loss: 0.633518
Train - Epoch 198, Batch: 0, Loss: 0.633094
Train - Epoch 199, Batch: 0, Loss: 0.633023
training_time:: 350.7690315246582
training time full:: 350.7690966129303
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000063, Accuracy: 0.927082
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 276.4230315685272
overhead:: 0
overhead2:: 0.3961467742919922
overhead3:: 0
time_baseline:: 276.4233901500702
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624530)
RCV1 Test Avg. Accuracy:: 0.9219529405859767
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.022058486938476562
overhead3:: 0.12040066719055176
overhead4:: 32.50753140449524
overhead5:: 0
memory usage:: 26615156736
time_provenance:: 44.95336413383484
curr_diff: 0 tensor(2.4181e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4181e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.001_10200
tensor(624530)
RCV1 Test Avg. Accuracy:: 0.9219529405859767
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693105
Train - Epoch 1, Batch: 0, Loss: 0.692734
Train - Epoch 2, Batch: 0, Loss: 0.692322
Train - Epoch 3, Batch: 0, Loss: 0.691987
Train - Epoch 4, Batch: 0, Loss: 0.691652
Train - Epoch 5, Batch: 0, Loss: 0.691241
Train - Epoch 6, Batch: 0, Loss: 0.690855
Train - Epoch 7, Batch: 0, Loss: 0.690504
Train - Epoch 8, Batch: 0, Loss: 0.690144
Train - Epoch 9, Batch: 0, Loss: 0.689797
Train - Epoch 10, Batch: 0, Loss: 0.689377
Train - Epoch 11, Batch: 0, Loss: 0.688978
Train - Epoch 12, Batch: 0, Loss: 0.688609
Train - Epoch 13, Batch: 0, Loss: 0.688308
Train - Epoch 14, Batch: 0, Loss: 0.687923
Train - Epoch 15, Batch: 0, Loss: 0.687596
Train - Epoch 16, Batch: 0, Loss: 0.687226
Train - Epoch 17, Batch: 0, Loss: 0.686878
Train - Epoch 18, Batch: 0, Loss: 0.686506
Train - Epoch 19, Batch: 0, Loss: 0.686165
Train - Epoch 20, Batch: 0, Loss: 0.685727
Train - Epoch 21, Batch: 0, Loss: 0.685421
Train - Epoch 22, Batch: 0, Loss: 0.685015
Train - Epoch 23, Batch: 0, Loss: 0.684690
Train - Epoch 24, Batch: 0, Loss: 0.684393
Train - Epoch 25, Batch: 0, Loss: 0.683960
Train - Epoch 26, Batch: 0, Loss: 0.683770
Train - Epoch 27, Batch: 0, Loss: 0.683351
Train - Epoch 28, Batch: 0, Loss: 0.682823
Train - Epoch 29, Batch: 0, Loss: 0.682609
Train - Epoch 30, Batch: 0, Loss: 0.682289
Train - Epoch 31, Batch: 0, Loss: 0.681902
Train - Epoch 32, Batch: 0, Loss: 0.681544
Train - Epoch 33, Batch: 0, Loss: 0.681188
Train - Epoch 34, Batch: 0, Loss: 0.680696
Train - Epoch 35, Batch: 0, Loss: 0.680493
Train - Epoch 36, Batch: 0, Loss: 0.680136
Train - Epoch 37, Batch: 0, Loss: 0.679881
Train - Epoch 38, Batch: 0, Loss: 0.679490
Train - Epoch 39, Batch: 0, Loss: 0.679057
Train - Epoch 40, Batch: 0, Loss: 0.678773
Train - Epoch 41, Batch: 0, Loss: 0.678272
Train - Epoch 42, Batch: 0, Loss: 0.678053
Train - Epoch 43, Batch: 0, Loss: 0.677662
Train - Epoch 44, Batch: 0, Loss: 0.677447
Train - Epoch 45, Batch: 0, Loss: 0.677092
Train - Epoch 46, Batch: 0, Loss: 0.676742
Train - Epoch 47, Batch: 0, Loss: 0.676444
Train - Epoch 48, Batch: 0, Loss: 0.676108
Train - Epoch 49, Batch: 0, Loss: 0.675660
Train - Epoch 50, Batch: 0, Loss: 0.675391
Train - Epoch 51, Batch: 0, Loss: 0.675049
Train - Epoch 52, Batch: 0, Loss: 0.674637
Train - Epoch 53, Batch: 0, Loss: 0.674301
Train - Epoch 54, Batch: 0, Loss: 0.674023
Train - Epoch 55, Batch: 0, Loss: 0.673688
Train - Epoch 56, Batch: 0, Loss: 0.673534
Train - Epoch 57, Batch: 0, Loss: 0.673091
Train - Epoch 58, Batch: 0, Loss: 0.672773
Train - Epoch 59, Batch: 0, Loss: 0.672480
Train - Epoch 60, Batch: 0, Loss: 0.672136
Train - Epoch 61, Batch: 0, Loss: 0.671793
Train - Epoch 62, Batch: 0, Loss: 0.671321
Train - Epoch 63, Batch: 0, Loss: 0.670961
Train - Epoch 64, Batch: 0, Loss: 0.670830
Train - Epoch 65, Batch: 0, Loss: 0.670616
Train - Epoch 66, Batch: 0, Loss: 0.670126
Train - Epoch 67, Batch: 0, Loss: 0.669946
Train - Epoch 68, Batch: 0, Loss: 0.669559
Train - Epoch 69, Batch: 0, Loss: 0.669243
Train - Epoch 70, Batch: 0, Loss: 0.669117
Train - Epoch 71, Batch: 0, Loss: 0.668486
Train - Epoch 72, Batch: 0, Loss: 0.667974
Train - Epoch 73, Batch: 0, Loss: 0.667915
Train - Epoch 74, Batch: 0, Loss: 0.667886
Train - Epoch 75, Batch: 0, Loss: 0.667528
Train - Epoch 76, Batch: 0, Loss: 0.666989
Train - Epoch 77, Batch: 0, Loss: 0.666509
Train - Epoch 78, Batch: 0, Loss: 0.666726
Train - Epoch 79, Batch: 0, Loss: 0.665957
Train - Epoch 80, Batch: 0, Loss: 0.665556
Train - Epoch 81, Batch: 0, Loss: 0.665354
Train - Epoch 82, Batch: 0, Loss: 0.665305
Train - Epoch 83, Batch: 0, Loss: 0.664989
Train - Epoch 84, Batch: 0, Loss: 0.664589
Train - Epoch 85, Batch: 0, Loss: 0.663989
Train - Epoch 86, Batch: 0, Loss: 0.663572
Train - Epoch 87, Batch: 0, Loss: 0.663427
Train - Epoch 88, Batch: 0, Loss: 0.663235
Train - Epoch 89, Batch: 0, Loss: 0.662890
Train - Epoch 90, Batch: 0, Loss: 0.662301
Train - Epoch 91, Batch: 0, Loss: 0.662257
Train - Epoch 92, Batch: 0, Loss: 0.661694
Train - Epoch 93, Batch: 0, Loss: 0.661545
Train - Epoch 94, Batch: 0, Loss: 0.661613
Train - Epoch 95, Batch: 0, Loss: 0.661253
Train - Epoch 96, Batch: 0, Loss: 0.660956
Train - Epoch 97, Batch: 0, Loss: 0.660130
Train - Epoch 98, Batch: 0, Loss: 0.660117
Train - Epoch 99, Batch: 0, Loss: 0.659674
Train - Epoch 100, Batch: 0, Loss: 0.659246
Train - Epoch 101, Batch: 0, Loss: 0.659312
Train - Epoch 102, Batch: 0, Loss: 0.658770
Train - Epoch 103, Batch: 0, Loss: 0.658690
Train - Epoch 104, Batch: 0, Loss: 0.658348
Train - Epoch 105, Batch: 0, Loss: 0.657886
Train - Epoch 106, Batch: 0, Loss: 0.657456
Train - Epoch 107, Batch: 0, Loss: 0.657301
Train - Epoch 108, Batch: 0, Loss: 0.657595
Train - Epoch 109, Batch: 0, Loss: 0.656986
Train - Epoch 110, Batch: 0, Loss: 0.656516
Train - Epoch 111, Batch: 0, Loss: 0.656278
Train - Epoch 112, Batch: 0, Loss: 0.655862
Train - Epoch 113, Batch: 0, Loss: 0.656103
Train - Epoch 114, Batch: 0, Loss: 0.655345
Train - Epoch 115, Batch: 0, Loss: 0.654989
Train - Epoch 116, Batch: 0, Loss: 0.655020
Train - Epoch 117, Batch: 0, Loss: 0.654533
Train - Epoch 118, Batch: 0, Loss: 0.654580
Train - Epoch 119, Batch: 0, Loss: 0.653833
Train - Epoch 120, Batch: 0, Loss: 0.653719
Train - Epoch 121, Batch: 0, Loss: 0.653647
Train - Epoch 122, Batch: 0, Loss: 0.653341
Train - Epoch 123, Batch: 0, Loss: 0.652738
Train - Epoch 124, Batch: 0, Loss: 0.652563
Train - Epoch 125, Batch: 0, Loss: 0.652438
Train - Epoch 126, Batch: 0, Loss: 0.652115
Train - Epoch 127, Batch: 0, Loss: 0.651767
Train - Epoch 128, Batch: 0, Loss: 0.651296
Train - Epoch 129, Batch: 0, Loss: 0.651304
Train - Epoch 130, Batch: 0, Loss: 0.650711
Train - Epoch 131, Batch: 0, Loss: 0.650775
Train - Epoch 132, Batch: 0, Loss: 0.650068
Train - Epoch 133, Batch: 0, Loss: 0.650152
Train - Epoch 134, Batch: 0, Loss: 0.649814
Train - Epoch 135, Batch: 0, Loss: 0.649605
Train - Epoch 136, Batch: 0, Loss: 0.649543
Train - Epoch 137, Batch: 0, Loss: 0.649277
Train - Epoch 138, Batch: 0, Loss: 0.648485
Train - Epoch 139, Batch: 0, Loss: 0.648367
Train - Epoch 140, Batch: 0, Loss: 0.647848
Train - Epoch 141, Batch: 0, Loss: 0.648007
Train - Epoch 142, Batch: 0, Loss: 0.647752
Train - Epoch 143, Batch: 0, Loss: 0.647298
Train - Epoch 144, Batch: 0, Loss: 0.646687
Train - Epoch 145, Batch: 0, Loss: 0.647067
Train - Epoch 146, Batch: 0, Loss: 0.646451
Train - Epoch 147, Batch: 0, Loss: 0.646219
Train - Epoch 148, Batch: 0, Loss: 0.646000
Train - Epoch 149, Batch: 0, Loss: 0.645877
Train - Epoch 150, Batch: 0, Loss: 0.645325
Train - Epoch 151, Batch: 0, Loss: 0.645152
Train - Epoch 152, Batch: 0, Loss: 0.644619
Train - Epoch 153, Batch: 0, Loss: 0.645062
Train - Epoch 154, Batch: 0, Loss: 0.643961
Train - Epoch 155, Batch: 0, Loss: 0.643838
Train - Epoch 156, Batch: 0, Loss: 0.643593
Train - Epoch 157, Batch: 0, Loss: 0.643413
Train - Epoch 158, Batch: 0, Loss: 0.643469
Train - Epoch 159, Batch: 0, Loss: 0.642413
Train - Epoch 160, Batch: 0, Loss: 0.642852
Train - Epoch 161, Batch: 0, Loss: 0.642532
Train - Epoch 162, Batch: 0, Loss: 0.642416
Train - Epoch 163, Batch: 0, Loss: 0.641710
Train - Epoch 164, Batch: 0, Loss: 0.641714
Train - Epoch 165, Batch: 0, Loss: 0.641673
Train - Epoch 166, Batch: 0, Loss: 0.640699
Train - Epoch 167, Batch: 0, Loss: 0.641036
Train - Epoch 168, Batch: 0, Loss: 0.640809
Train - Epoch 169, Batch: 0, Loss: 0.639867
Train - Epoch 170, Batch: 0, Loss: 0.639839
Train - Epoch 171, Batch: 0, Loss: 0.639864
Train - Epoch 172, Batch: 0, Loss: 0.639090
Train - Epoch 173, Batch: 0, Loss: 0.638979
Train - Epoch 174, Batch: 0, Loss: 0.638722
Train - Epoch 175, Batch: 0, Loss: 0.638751
Train - Epoch 176, Batch: 0, Loss: 0.638648
Train - Epoch 177, Batch: 0, Loss: 0.638352
Train - Epoch 178, Batch: 0, Loss: 0.638563
Train - Epoch 179, Batch: 0, Loss: 0.638529
Train - Epoch 180, Batch: 0, Loss: 0.637616
Train - Epoch 181, Batch: 0, Loss: 0.637552
Train - Epoch 182, Batch: 0, Loss: 0.637181
Train - Epoch 183, Batch: 0, Loss: 0.636707
Train - Epoch 184, Batch: 0, Loss: 0.636577
Train - Epoch 185, Batch: 0, Loss: 0.636439
Train - Epoch 186, Batch: 0, Loss: 0.635810/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635541
Train - Epoch 188, Batch: 0, Loss: 0.635560
Train - Epoch 189, Batch: 0, Loss: 0.635070
Train - Epoch 190, Batch: 0, Loss: 0.635552
Train - Epoch 191, Batch: 0, Loss: 0.634401
Train - Epoch 192, Batch: 0, Loss: 0.634199
Train - Epoch 193, Batch: 0, Loss: 0.635150
Train - Epoch 194, Batch: 0, Loss: 0.633687
Train - Epoch 195, Batch: 0, Loss: 0.633672
Train - Epoch 196, Batch: 0, Loss: 0.633756
Train - Epoch 197, Batch: 0, Loss: 0.633506
Train - Epoch 198, Batch: 0, Loss: 0.633402
Train - Epoch 199, Batch: 0, Loss: 0.632644
training_time:: 351.10687828063965
training time full:: 351.1069438457489
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000063, Accuracy: 0.928021
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.91537141799927
overhead:: 0
overhead2:: 0.3931589126586914
overhead3:: 0
time_baseline:: 275.91573667526245
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.928268
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(625216)
RCV1 Test Avg. Accuracy:: 0.9229656376817799
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.020785808563232422
overhead3:: 0.11590194702148438
overhead4:: 32.763264179229736
overhead5:: 0
memory usage:: 26603401216
time_provenance:: 45.176684617996216
curr_diff: 0 tensor(2.1807e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1807e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.928268
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.001_10200
tensor(625216)
RCV1 Test Avg. Accuracy:: 0.9229656376817799
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693094
Train - Epoch 1, Batch: 0, Loss: 0.692709
Train - Epoch 2, Batch: 0, Loss: 0.692333
Train - Epoch 3, Batch: 0, Loss: 0.691962
Train - Epoch 4, Batch: 0, Loss: 0.691591
Train - Epoch 5, Batch: 0, Loss: 0.691236
Train - Epoch 6, Batch: 0, Loss: 0.690863
Train - Epoch 7, Batch: 0, Loss: 0.690496
Train - Epoch 8, Batch: 0, Loss: 0.690121
Train - Epoch 9, Batch: 0, Loss: 0.689711
Train - Epoch 10, Batch: 0, Loss: 0.689384
Train - Epoch 11, Batch: 0, Loss: 0.689042
Train - Epoch 12, Batch: 0, Loss: 0.688625
Train - Epoch 13, Batch: 0, Loss: 0.688305
Train - Epoch 14, Batch: 0, Loss: 0.687883
Train - Epoch 15, Batch: 0, Loss: 0.687622
Train - Epoch 16, Batch: 0, Loss: 0.687198
Train - Epoch 17, Batch: 0, Loss: 0.686789
Train - Epoch 18, Batch: 0, Loss: 0.686433
Train - Epoch 19, Batch: 0, Loss: 0.686053
Train - Epoch 20, Batch: 0, Loss: 0.685701
Train - Epoch 21, Batch: 0, Loss: 0.685310
Train - Epoch 22, Batch: 0, Loss: 0.685047
Train - Epoch 23, Batch: 0, Loss: 0.684796
Train - Epoch 24, Batch: 0, Loss: 0.684344
Train - Epoch 25, Batch: 0, Loss: 0.683928
Train - Epoch 26, Batch: 0, Loss: 0.683663
Train - Epoch 27, Batch: 0, Loss: 0.683283
Train - Epoch 28, Batch: 0, Loss: 0.682905
Train - Epoch 29, Batch: 0, Loss: 0.682532
Train - Epoch 30, Batch: 0, Loss: 0.682112
Train - Epoch 31, Batch: 0, Loss: 0.681868
Train - Epoch 32, Batch: 0, Loss: 0.681543
Train - Epoch 33, Batch: 0, Loss: 0.681111
Train - Epoch 34, Batch: 0, Loss: 0.680601
Train - Epoch 35, Batch: 0, Loss: 0.680595
Train - Epoch 36, Batch: 0, Loss: 0.680093
Train - Epoch 37, Batch: 0, Loss: 0.679834
Train - Epoch 38, Batch: 0, Loss: 0.679399
Train - Epoch 39, Batch: 0, Loss: 0.679133
Train - Epoch 40, Batch: 0, Loss: 0.678696
Train - Epoch 41, Batch: 0, Loss: 0.678272
Train - Epoch 42, Batch: 0, Loss: 0.677971
Train - Epoch 43, Batch: 0, Loss: 0.677832
Train - Epoch 44, Batch: 0, Loss: 0.677443
Train - Epoch 45, Batch: 0, Loss: 0.676975
Train - Epoch 46, Batch: 0, Loss: 0.676697
Train - Epoch 47, Batch: 0, Loss: 0.676430
Train - Epoch 48, Batch: 0, Loss: 0.675896
Train - Epoch 49, Batch: 0, Loss: 0.675734
Train - Epoch 50, Batch: 0, Loss: 0.675376
Train - Epoch 51, Batch: 0, Loss: 0.675126
Train - Epoch 52, Batch: 0, Loss: 0.674624
Train - Epoch 53, Batch: 0, Loss: 0.674438
Train - Epoch 54, Batch: 0, Loss: 0.673881
Train - Epoch 55, Batch: 0, Loss: 0.673779
Train - Epoch 56, Batch: 0, Loss: 0.673267
Train - Epoch 57, Batch: 0, Loss: 0.673087
Train - Epoch 58, Batch: 0, Loss: 0.672742
Train - Epoch 59, Batch: 0, Loss: 0.672239
Train - Epoch 60, Batch: 0, Loss: 0.672036
Train - Epoch 61, Batch: 0, Loss: 0.671805
Train - Epoch 62, Batch: 0, Loss: 0.671503
Train - Epoch 63, Batch: 0, Loss: 0.671017
Train - Epoch 64, Batch: 0, Loss: 0.670797
Train - Epoch 65, Batch: 0, Loss: 0.670475
Train - Epoch 66, Batch: 0, Loss: 0.670101
Train - Epoch 67, Batch: 0, Loss: 0.670163
Train - Epoch 68, Batch: 0, Loss: 0.669700
Train - Epoch 69, Batch: 0, Loss: 0.669209
Train - Epoch 70, Batch: 0, Loss: 0.668800
Train - Epoch 71, Batch: 0, Loss: 0.668570
Train - Epoch 72, Batch: 0, Loss: 0.668024
Train - Epoch 73, Batch: 0, Loss: 0.668234
Train - Epoch 74, Batch: 0, Loss: 0.667615
Train - Epoch 75, Batch: 0, Loss: 0.667191
Train - Epoch 76, Batch: 0, Loss: 0.666975
Train - Epoch 77, Batch: 0, Loss: 0.666389
Train - Epoch 78, Batch: 0, Loss: 0.666410
Train - Epoch 79, Batch: 0, Loss: 0.666118
Train - Epoch 80, Batch: 0, Loss: 0.665603
Train - Epoch 81, Batch: 0, Loss: 0.665241
Train - Epoch 82, Batch: 0, Loss: 0.665041
Train - Epoch 83, Batch: 0, Loss: 0.664855
Train - Epoch 84, Batch: 0, Loss: 0.664502
Train - Epoch 85, Batch: 0, Loss: 0.663891
Train - Epoch 86, Batch: 0, Loss: 0.663205
Train - Epoch 87, Batch: 0, Loss: 0.663233
Train - Epoch 88, Batch: 0, Loss: 0.663172
Train - Epoch 89, Batch: 0, Loss: 0.663094
Train - Epoch 90, Batch: 0, Loss: 0.662480
Train - Epoch 91, Batch: 0, Loss: 0.661919
Train - Epoch 92, Batch: 0, Loss: 0.662259
Train - Epoch 93, Batch: 0, Loss: 0.661495
Train - Epoch 94, Batch: 0, Loss: 0.661209
Train - Epoch 95, Batch: 0, Loss: 0.661098
Train - Epoch 96, Batch: 0, Loss: 0.660874
Train - Epoch 97, Batch: 0, Loss: 0.660153
Train - Epoch 98, Batch: 0, Loss: 0.659940
Train - Epoch 99, Batch: 0, Loss: 0.659969
Train - Epoch 100, Batch: 0, Loss: 0.659497
Train - Epoch 101, Batch: 0, Loss: 0.659565
Train - Epoch 102, Batch: 0, Loss: 0.659008
Train - Epoch 103, Batch: 0, Loss: 0.658404
Train - Epoch 104, Batch: 0, Loss: 0.658095
Train - Epoch 105, Batch: 0, Loss: 0.658264
Train - Epoch 106, Batch: 0, Loss: 0.657790
Train - Epoch 107, Batch: 0, Loss: 0.657495
Train - Epoch 108, Batch: 0, Loss: 0.657162
Train - Epoch 109, Batch: 0, Loss: 0.656907
Train - Epoch 110, Batch: 0, Loss: 0.656447
Train - Epoch 111, Batch: 0, Loss: 0.656191
Train - Epoch 112, Batch: 0, Loss: 0.655816
Train - Epoch 113, Batch: 0, Loss: 0.655690
Train - Epoch 114, Batch: 0, Loss: 0.655453
Train - Epoch 115, Batch: 0, Loss: 0.654999
Train - Epoch 116, Batch: 0, Loss: 0.654496
Train - Epoch 117, Batch: 0, Loss: 0.655124
Train - Epoch 118, Batch: 0, Loss: 0.653917
Train - Epoch 119, Batch: 0, Loss: 0.653961
Train - Epoch 120, Batch: 0, Loss: 0.653658
Train - Epoch 121, Batch: 0, Loss: 0.653602
Train - Epoch 122, Batch: 0, Loss: 0.653129
Train - Epoch 123, Batch: 0, Loss: 0.652458
Train - Epoch 124, Batch: 0, Loss: 0.652330
Train - Epoch 125, Batch: 0, Loss: 0.652281
Train - Epoch 126, Batch: 0, Loss: 0.651909
Train - Epoch 127, Batch: 0, Loss: 0.651450
Train - Epoch 128, Batch: 0, Loss: 0.651251
Train - Epoch 129, Batch: 0, Loss: 0.651069
Train - Epoch 130, Batch: 0, Loss: 0.650847
Train - Epoch 131, Batch: 0, Loss: 0.650340
Train - Epoch 132, Batch: 0, Loss: 0.649713
Train - Epoch 133, Batch: 0, Loss: 0.649879
Train - Epoch 134, Batch: 0, Loss: 0.649484
Train - Epoch 135, Batch: 0, Loss: 0.649545
Train - Epoch 136, Batch: 0, Loss: 0.649325
Train - Epoch 137, Batch: 0, Loss: 0.648893
Train - Epoch 138, Batch: 0, Loss: 0.648578
Train - Epoch 139, Batch: 0, Loss: 0.648153
Train - Epoch 140, Batch: 0, Loss: 0.648277
Train - Epoch 141, Batch: 0, Loss: 0.647772
Train - Epoch 142, Batch: 0, Loss: 0.647267
Train - Epoch 143, Batch: 0, Loss: 0.646590
Train - Epoch 144, Batch: 0, Loss: 0.646897
Train - Epoch 145, Batch: 0, Loss: 0.647086
Train - Epoch 146, Batch: 0, Loss: 0.646338
Train - Epoch 147, Batch: 0, Loss: 0.646386
Train - Epoch 148, Batch: 0, Loss: 0.645751
Train - Epoch 149, Batch: 0, Loss: 0.645892
Train - Epoch 150, Batch: 0, Loss: 0.645214
Train - Epoch 151, Batch: 0, Loss: 0.644801
Train - Epoch 152, Batch: 0, Loss: 0.645072
Train - Epoch 153, Batch: 0, Loss: 0.644891
Train - Epoch 154, Batch: 0, Loss: 0.644357
Train - Epoch 155, Batch: 0, Loss: 0.644525
Train - Epoch 156, Batch: 0, Loss: 0.643357
Train - Epoch 157, Batch: 0, Loss: 0.643926
Train - Epoch 158, Batch: 0, Loss: 0.643493
Train - Epoch 159, Batch: 0, Loss: 0.642873
Train - Epoch 160, Batch: 0, Loss: 0.642961
Train - Epoch 161, Batch: 0, Loss: 0.642567
Train - Epoch 162, Batch: 0, Loss: 0.641805
Train - Epoch 163, Batch: 0, Loss: 0.641450
Train - Epoch 164, Batch: 0, Loss: 0.641485
Train - Epoch 165, Batch: 0, Loss: 0.641565
Train - Epoch 166, Batch: 0, Loss: 0.640990
Train - Epoch 167, Batch: 0, Loss: 0.639951
Train - Epoch 168, Batch: 0, Loss: 0.640436
Train - Epoch 169, Batch: 0, Loss: 0.639812
Train - Epoch 170, Batch: 0, Loss: 0.640356
Train - Epoch 171, Batch: 0, Loss: 0.639553
Train - Epoch 172, Batch: 0, Loss: 0.639940
Train - Epoch 173, Batch: 0, Loss: 0.638940
Train - Epoch 174, Batch: 0, Loss: 0.638948
Train - Epoch 175, Batch: 0, Loss: 0.638954
Train - Epoch 176, Batch: 0, Loss: 0.638724
Train - Epoch 177, Batch: 0, Loss: 0.638419
Train - Epoch 178, Batch: 0, Loss: 0.637877
Train - Epoch 179, Batch: 0, Loss: 0.638252
Train - Epoch 180, Batch: 0, Loss: 0.637947
Train - Epoch 181, Batch: 0, Loss: 0.637688
Train - Epoch 182, Batch: 0, Loss: 0.636957
Train - Epoch 183, Batch: 0, Loss: 0.636762
Train - Epoch 184, Batch: 0, Loss: 0.636556
Train - Epoch 185, Batch: 0, Loss: 0.635555
Train - Epoch 186, Batch: 0, Loss: 0.635829/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635721
Train - Epoch 188, Batch: 0, Loss: 0.636222
Train - Epoch 189, Batch: 0, Loss: 0.634910
Train - Epoch 190, Batch: 0, Loss: 0.634729
Train - Epoch 191, Batch: 0, Loss: 0.634723
Train - Epoch 192, Batch: 0, Loss: 0.635153
Train - Epoch 193, Batch: 0, Loss: 0.634269
Train - Epoch 194, Batch: 0, Loss: 0.634103
Train - Epoch 195, Batch: 0, Loss: 0.633793
Train - Epoch 196, Batch: 0, Loss: 0.633015
Train - Epoch 197, Batch: 0, Loss: 0.633779
Train - Epoch 198, Batch: 0, Loss: 0.632783
Train - Epoch 199, Batch: 0, Loss: 0.632594
training_time:: 350.4045639038086
training time full:: 350.40464186668396
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926736
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 275.2490756511688
overhead:: 0
overhead2:: 0.3955719470977783
overhead3:: 0
time_baseline:: 275.24941968917847
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926736
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(625064)
RCV1 Test Avg. Accuracy:: 0.922741249987083
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.022370100021362305
overhead3:: 0.12140727043151855
overhead4:: 32.497172117233276
overhead5:: 0
memory usage:: 26613678080
time_provenance:: 44.9244499206543
curr_diff: 0 tensor(2.2722e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2722e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926736
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.001_10200
tensor(625064)
RCV1 Test Avg. Accuracy:: 0.922741249987083
deletion rate:: 0.01
python3 generate_rand_ids 0.01  rcv1 0
tensor([ 4096,   515,  8713,   523,  3598,  3089, 17426,  6169, 15898,  9244,
        13853, 12832,  5665, 11298, 17953,  4132, 16421,  2600, 12329,  4145,
         1585,  5689,    59, 14908,  8766,  6720,  5184,  3140, 15435,  4173,
        10319,   592, 14417,  1619, 19539, 15963,   607, 14944,  4706, 19046,
        10345,   105,  9833,  1133, 15472, 10357, 14453,  6775,  3193,  4218,
        19582,   641,  8325,  7303,  5768, 12425, 20106,  5772,  2188, 19086,
         8848, 11408, 19089,  4244,  6292, 19607,  4250, 12445, 16542,  4259,
         1187, 18596, 17576, 16554,  3244, 18608, 11954, 13491, 17588, 18101,
         5299,  2232,  9919,  3779, 11462, 11465, 19149, 19664, 10448,  4305,
        17617, 17112, 16601, 14042,  5851, 14044,  6882, 16611, 12515, 11491,
         4836,  4328,  6380, 19182,  6382,  1776, 17142, 19192, 10490, 20225,
        19715, 19203, 18184, 10510, 13073, 18194,  9491,  1812,  4369, 14104,
        19738,  8476, 11036,  1311, 19744, 15136, 12066,  2847, 10533, 19751,
         7463, 18226,  8499, 15670, 19769, 11065,  6464,  4933,  9547, 19788,
        11598,  5456, 16211,  9054,  4959, 16736,   353, 17249,  7010,  1381,
         7526,  9576,  6512, 10608,  1406,  3455,  7553,  7554,  8069,  9607,
         5000,  1418,  9103,  6034, 12691,  8600,  6040, 12186,   923, 12699,
        15259,  4510, 12191,  2979,  6053,  2986, 10668, 11693,  8620,  2481,
         5556, 15802,  8125,  6079,  9160,  1992, 11213,  6093, 13263, 18900,
         2009,  6108,   990,  1503,  7648, 17378,   485,  6632, 19441, 13299,
        10744,  7166])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693176
Train - Epoch 1, Batch: 0, Loss: 0.692809
Train - Epoch 2, Batch: 0, Loss: 0.692432
Train - Epoch 3, Batch: 0, Loss: 0.692033
Train - Epoch 4, Batch: 0, Loss: 0.691703
Train - Epoch 5, Batch: 0, Loss: 0.691316
Train - Epoch 6, Batch: 0, Loss: 0.690974
Train - Epoch 7, Batch: 0, Loss: 0.690544
Train - Epoch 8, Batch: 0, Loss: 0.690231
Train - Epoch 9, Batch: 0, Loss: 0.689826
Train - Epoch 10, Batch: 0, Loss: 0.689485
Train - Epoch 11, Batch: 0, Loss: 0.689131
Train - Epoch 12, Batch: 0, Loss: 0.688740
Train - Epoch 13, Batch: 0, Loss: 0.688374
Train - Epoch 14, Batch: 0, Loss: 0.688052
Train - Epoch 15, Batch: 0, Loss: 0.687694
Train - Epoch 16, Batch: 0, Loss: 0.687247
Train - Epoch 17, Batch: 0, Loss: 0.686927
Train - Epoch 18, Batch: 0, Loss: 0.686522
Train - Epoch 19, Batch: 0, Loss: 0.686112
Train - Epoch 20, Batch: 0, Loss: 0.685783
Train - Epoch 21, Batch: 0, Loss: 0.685475
Train - Epoch 22, Batch: 0, Loss: 0.685166
Train - Epoch 23, Batch: 0, Loss: 0.684741
Train - Epoch 24, Batch: 0, Loss: 0.684427
Train - Epoch 25, Batch: 0, Loss: 0.684010
Train - Epoch 26, Batch: 0, Loss: 0.683734
Train - Epoch 27, Batch: 0, Loss: 0.683401
Train - Epoch 28, Batch: 0, Loss: 0.683041
Train - Epoch 29, Batch: 0, Loss: 0.682694
Train - Epoch 30, Batch: 0, Loss: 0.682311
Train - Epoch 31, Batch: 0, Loss: 0.682090
Train - Epoch 32, Batch: 0, Loss: 0.681550
Train - Epoch 33, Batch: 0, Loss: 0.681266
Train - Epoch 34, Batch: 0, Loss: 0.680872
Train - Epoch 35, Batch: 0, Loss: 0.680502
Train - Epoch 36, Batch: 0, Loss: 0.680220
Train - Epoch 37, Batch: 0, Loss: 0.679947
Train - Epoch 38, Batch: 0, Loss: 0.679600
Train - Epoch 39, Batch: 0, Loss: 0.679236
Train - Epoch 40, Batch: 0, Loss: 0.678930
Train - Epoch 41, Batch: 0, Loss: 0.678469
Train - Epoch 42, Batch: 0, Loss: 0.678278
Train - Epoch 43, Batch: 0, Loss: 0.677736
Train - Epoch 44, Batch: 0, Loss: 0.677314
Train - Epoch 45, Batch: 0, Loss: 0.677040
Train - Epoch 46, Batch: 0, Loss: 0.676781
Train - Epoch 47, Batch: 0, Loss: 0.676548
Train - Epoch 48, Batch: 0, Loss: 0.676118
Train - Epoch 49, Batch: 0, Loss: 0.675759
Train - Epoch 50, Batch: 0, Loss: 0.675263
Train - Epoch 51, Batch: 0, Loss: 0.675128
Train - Epoch 52, Batch: 0, Loss: 0.674879
Train - Epoch 53, Batch: 0, Loss: 0.674339
Train - Epoch 54, Batch: 0, Loss: 0.674130
Train - Epoch 55, Batch: 0, Loss: 0.674013
Train - Epoch 56, Batch: 0, Loss: 0.673524
Train - Epoch 57, Batch: 0, Loss: 0.673128
Train - Epoch 58, Batch: 0, Loss: 0.672720
Train - Epoch 59, Batch: 0, Loss: 0.672514
Train - Epoch 60, Batch: 0, Loss: 0.672001
Train - Epoch 61, Batch: 0, Loss: 0.671819
Train - Epoch 62, Batch: 0, Loss: 0.671413
Train - Epoch 63, Batch: 0, Loss: 0.671201
Train - Epoch 64, Batch: 0, Loss: 0.670691
Train - Epoch 65, Batch: 0, Loss: 0.670561
Train - Epoch 66, Batch: 0, Loss: 0.670054
Train - Epoch 67, Batch: 0, Loss: 0.669965
Train - Epoch 68, Batch: 0, Loss: 0.669858
Train - Epoch 69, Batch: 0, Loss: 0.669128
Train - Epoch 70, Batch: 0, Loss: 0.669012
Train - Epoch 71, Batch: 0, Loss: 0.668597
Train - Epoch 72, Batch: 0, Loss: 0.668210
Train - Epoch 73, Batch: 0, Loss: 0.668034
Train - Epoch 74, Batch: 0, Loss: 0.667662
Train - Epoch 75, Batch: 0, Loss: 0.667191
Train - Epoch 76, Batch: 0, Loss: 0.666810
Train - Epoch 77, Batch: 0, Loss: 0.666602
Train - Epoch 78, Batch: 0, Loss: 0.666418
Train - Epoch 79, Batch: 0, Loss: 0.666120
Train - Epoch 80, Batch: 0, Loss: 0.665662
Train - Epoch 81, Batch: 0, Loss: 0.665776
Train - Epoch 82, Batch: 0, Loss: 0.665067
Train - Epoch 83, Batch: 0, Loss: 0.664770
Train - Epoch 84, Batch: 0, Loss: 0.664430
Train - Epoch 85, Batch: 0, Loss: 0.664317
Train - Epoch 86, Batch: 0, Loss: 0.663754
Train - Epoch 87, Batch: 0, Loss: 0.663448
Train - Epoch 88, Batch: 0, Loss: 0.663206
Train - Epoch 89, Batch: 0, Loss: 0.662902
Train - Epoch 90, Batch: 0, Loss: 0.662785
Train - Epoch 91, Batch: 0, Loss: 0.662527
Train - Epoch 92, Batch: 0, Loss: 0.661680
Train - Epoch 93, Batch: 0, Loss: 0.661531
Train - Epoch 94, Batch: 0, Loss: 0.661411
Train - Epoch 95, Batch: 0, Loss: 0.661100
Train - Epoch 96, Batch: 0, Loss: 0.661016
Train - Epoch 97, Batch: 0, Loss: 0.660871
Train - Epoch 98, Batch: 0, Loss: 0.660512
Train - Epoch 99, Batch: 0, Loss: 0.660123
Train - Epoch 100, Batch: 0, Loss: 0.659580
Train - Epoch 101, Batch: 0, Loss: 0.659604
Train - Epoch 102, Batch: 0, Loss: 0.658795
Train - Epoch 103, Batch: 0, Loss: 0.658731
Train - Epoch 104, Batch: 0, Loss: 0.658418
Train - Epoch 105, Batch: 0, Loss: 0.658642
Train - Epoch 106, Batch: 0, Loss: 0.658029
Train - Epoch 107, Batch: 0, Loss: 0.657667
Train - Epoch 108, Batch: 0, Loss: 0.657214
Train - Epoch 109, Batch: 0, Loss: 0.656915
Train - Epoch 110, Batch: 0, Loss: 0.656576
Train - Epoch 111, Batch: 0, Loss: 0.656299
Train - Epoch 112, Batch: 0, Loss: 0.655639
Train - Epoch 113, Batch: 0, Loss: 0.655795
Train - Epoch 114, Batch: 0, Loss: 0.655638
Train - Epoch 115, Batch: 0, Loss: 0.655359
Train - Epoch 116, Batch: 0, Loss: 0.655068
Train - Epoch 117, Batch: 0, Loss: 0.654683
Train - Epoch 118, Batch: 0, Loss: 0.654823
Train - Epoch 119, Batch: 0, Loss: 0.654283
Train - Epoch 120, Batch: 0, Loss: 0.653666
Train - Epoch 121, Batch: 0, Loss: 0.653701
Train - Epoch 122, Batch: 0, Loss: 0.653239
Train - Epoch 123, Batch: 0, Loss: 0.653052
Train - Epoch 124, Batch: 0, Loss: 0.652247
Train - Epoch 125, Batch: 0, Loss: 0.652536
Train - Epoch 126, Batch: 0, Loss: 0.651956
Train - Epoch 127, Batch: 0, Loss: 0.652252
Train - Epoch 128, Batch: 0, Loss: 0.651799
Train - Epoch 129, Batch: 0, Loss: 0.650648
Train - Epoch 130, Batch: 0, Loss: 0.650792
Train - Epoch 131, Batch: 0, Loss: 0.650829
Train - Epoch 132, Batch: 0, Loss: 0.650641
Train - Epoch 133, Batch: 0, Loss: 0.650145
Train - Epoch 134, Batch: 0, Loss: 0.649911
Train - Epoch 135, Batch: 0, Loss: 0.649821
Train - Epoch 136, Batch: 0, Loss: 0.649294
Train - Epoch 137, Batch: 0, Loss: 0.649142
Train - Epoch 138, Batch: 0, Loss: 0.648655
Train - Epoch 139, Batch: 0, Loss: 0.648160
Train - Epoch 140, Batch: 0, Loss: 0.648072
Train - Epoch 141, Batch: 0, Loss: 0.648145
Train - Epoch 142, Batch: 0, Loss: 0.647474
Train - Epoch 143, Batch: 0, Loss: 0.646986
Train - Epoch 144, Batch: 0, Loss: 0.646998
Train - Epoch 145, Batch: 0, Loss: 0.646375
Train - Epoch 146, Batch: 0, Loss: 0.646560
Train - Epoch 147, Batch: 0, Loss: 0.646432
Train - Epoch 148, Batch: 0, Loss: 0.645968
Train - Epoch 149, Batch: 0, Loss: 0.646103
Train - Epoch 150, Batch: 0, Loss: 0.645722
Train - Epoch 151, Batch: 0, Loss: 0.645016
Train - Epoch 152, Batch: 0, Loss: 0.645064
Train - Epoch 153, Batch: 0, Loss: 0.644491
Train - Epoch 154, Batch: 0, Loss: 0.644147
Train - Epoch 155, Batch: 0, Loss: 0.643457
Train - Epoch 156, Batch: 0, Loss: 0.644383
Train - Epoch 157, Batch: 0, Loss: 0.643949
Train - Epoch 158, Batch: 0, Loss: 0.643446
Train - Epoch 159, Batch: 0, Loss: 0.642878
Train - Epoch 160, Batch: 0, Loss: 0.642802
Train - Epoch 161, Batch: 0, Loss: 0.642539
Train - Epoch 162, Batch: 0, Loss: 0.642149
Train - Epoch 163, Batch: 0, Loss: 0.641559
Train - Epoch 164, Batch: 0, Loss: 0.641983
Train - Epoch 165, Batch: 0, Loss: 0.641393
Train - Epoch 166, Batch: 0, Loss: 0.641251
Train - Epoch 167, Batch: 0, Loss: 0.641247
Train - Epoch 168, Batch: 0, Loss: 0.640673
Train - Epoch 169, Batch: 0, Loss: 0.640487
Train - Epoch 170, Batch: 0, Loss: 0.640112
Train - Epoch 171, Batch: 0, Loss: 0.639857
Train - Epoch 172, Batch: 0, Loss: 0.639494
Train - Epoch 173, Batch: 0, Loss: 0.639234
Train - Epoch 174, Batch: 0, Loss: 0.639157
Train - Epoch 175, Batch: 0, Loss: 0.638520
Train - Epoch 176, Batch: 0, Loss: 0.638854
Train - Epoch 177, Batch: 0, Loss: 0.638777
Train - Epoch 178, Batch: 0, Loss: 0.638141
Train - Epoch 179, Batch: 0, Loss: 0.638007
Train - Epoch 180, Batch: 0, Loss: 0.637212
Train - Epoch 181, Batch: 0, Loss: 0.637457
Train - Epoch 182, Batch: 0, Loss: 0.637227
Train - Epoch 183, Batch: 0, Loss: 0.636727
Train - Epoch 184, Batch: 0, Loss: 0.636845
Train - Epoch 185, Batch: 0, Loss: 0.636457
Train - Epoch 186, Batch: 0, Loss: 0.636158/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635663
Train - Epoch 188, Batch: 0, Loss: 0.635230
Train - Epoch 189, Batch: 0, Loss: 0.636137
Train - Epoch 190, Batch: 0, Loss: 0.635645
Train - Epoch 191, Batch: 0, Loss: 0.635179
Train - Epoch 192, Batch: 0, Loss: 0.634415
Train - Epoch 193, Batch: 0, Loss: 0.634210
Train - Epoch 194, Batch: 0, Loss: 0.634544
Train - Epoch 195, Batch: 0, Loss: 0.633412
Train - Epoch 196, Batch: 0, Loss: 0.633203
Train - Epoch 197, Batch: 0, Loss: 0.633508
Train - Epoch 198, Batch: 0, Loss: 0.633108
Train - Epoch 199, Batch: 0, Loss: 0.633032
training_time:: 350.33054423332214
training time full:: 350.33061265945435
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926489
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.6554367542267
overhead:: 0
overhead2:: 0.404787540435791
overhead3:: 0
time_baseline:: 273.6557912826538
curr_diff: 0 tensor(0.0169, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0169, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926539
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624384)
RCV1 Test Avg. Accuracy:: 0.9217374103002809
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.022530555725097656
overhead3:: 0.12183070182800293
overhead4:: 32.29147458076477
overhead5:: 0
memory usage:: 26614124544
time_provenance:: 47.59286451339722
curr_diff: 0 tensor(1.5634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926539
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.01_10200
tensor(624383)
RCV1 Test Avg. Accuracy:: 0.9217359340654474
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693077
Train - Epoch 1, Batch: 0, Loss: 0.692710
Train - Epoch 2, Batch: 0, Loss: 0.692331
Train - Epoch 3, Batch: 0, Loss: 0.691971
Train - Epoch 4, Batch: 0, Loss: 0.691584
Train - Epoch 5, Batch: 0, Loss: 0.691199
Train - Epoch 6, Batch: 0, Loss: 0.690834
Train - Epoch 7, Batch: 0, Loss: 0.690462
Train - Epoch 8, Batch: 0, Loss: 0.690101
Train - Epoch 9, Batch: 0, Loss: 0.689730
Train - Epoch 10, Batch: 0, Loss: 0.689377
Train - Epoch 11, Batch: 0, Loss: 0.688922
Train - Epoch 12, Batch: 0, Loss: 0.688561
Train - Epoch 13, Batch: 0, Loss: 0.688287
Train - Epoch 14, Batch: 0, Loss: 0.687870
Train - Epoch 15, Batch: 0, Loss: 0.687574
Train - Epoch 16, Batch: 0, Loss: 0.687174
Train - Epoch 17, Batch: 0, Loss: 0.686836
Train - Epoch 18, Batch: 0, Loss: 0.686476
Train - Epoch 19, Batch: 0, Loss: 0.686159
Train - Epoch 20, Batch: 0, Loss: 0.685706
Train - Epoch 21, Batch: 0, Loss: 0.685370
Train - Epoch 22, Batch: 0, Loss: 0.685003
Train - Epoch 23, Batch: 0, Loss: 0.684635
Train - Epoch 24, Batch: 0, Loss: 0.684361
Train - Epoch 25, Batch: 0, Loss: 0.683936
Train - Epoch 26, Batch: 0, Loss: 0.683745
Train - Epoch 27, Batch: 0, Loss: 0.683323
Train - Epoch 28, Batch: 0, Loss: 0.682817
Train - Epoch 29, Batch: 0, Loss: 0.682582
Train - Epoch 30, Batch: 0, Loss: 0.682295
Train - Epoch 31, Batch: 0, Loss: 0.681878
Train - Epoch 32, Batch: 0, Loss: 0.681532
Train - Epoch 33, Batch: 0, Loss: 0.681161
Train - Epoch 34, Batch: 0, Loss: 0.680679
Train - Epoch 35, Batch: 0, Loss: 0.680505
Train - Epoch 36, Batch: 0, Loss: 0.680084
Train - Epoch 37, Batch: 0, Loss: 0.679849
Train - Epoch 38, Batch: 0, Loss: 0.679468
Train - Epoch 39, Batch: 0, Loss: 0.679026
Train - Epoch 40, Batch: 0, Loss: 0.678737
Train - Epoch 41, Batch: 0, Loss: 0.678257
Train - Epoch 42, Batch: 0, Loss: 0.678016
Train - Epoch 43, Batch: 0, Loss: 0.677638
Train - Epoch 44, Batch: 0, Loss: 0.677428
Train - Epoch 45, Batch: 0, Loss: 0.677067
Train - Epoch 46, Batch: 0, Loss: 0.676716
Train - Epoch 47, Batch: 0, Loss: 0.676435
Train - Epoch 48, Batch: 0, Loss: 0.676065
Train - Epoch 49, Batch: 0, Loss: 0.675636
Train - Epoch 50, Batch: 0, Loss: 0.675361
Train - Epoch 51, Batch: 0, Loss: 0.675010
Train - Epoch 52, Batch: 0, Loss: 0.674591
Train - Epoch 53, Batch: 0, Loss: 0.674322
Train - Epoch 54, Batch: 0, Loss: 0.673994
Train - Epoch 55, Batch: 0, Loss: 0.673655
Train - Epoch 56, Batch: 0, Loss: 0.673517
Train - Epoch 57, Batch: 0, Loss: 0.673060
Train - Epoch 58, Batch: 0, Loss: 0.672797
Train - Epoch 59, Batch: 0, Loss: 0.672444
Train - Epoch 60, Batch: 0, Loss: 0.672121
Train - Epoch 61, Batch: 0, Loss: 0.671750
Train - Epoch 62, Batch: 0, Loss: 0.671336
Train - Epoch 63, Batch: 0, Loss: 0.670933
Train - Epoch 64, Batch: 0, Loss: 0.670828
Train - Epoch 65, Batch: 0, Loss: 0.670592
Train - Epoch 66, Batch: 0, Loss: 0.670102
Train - Epoch 67, Batch: 0, Loss: 0.669901
Train - Epoch 68, Batch: 0, Loss: 0.669558
Train - Epoch 69, Batch: 0, Loss: 0.669237
Train - Epoch 70, Batch: 0, Loss: 0.669092
Train - Epoch 71, Batch: 0, Loss: 0.668474
Train - Epoch 72, Batch: 0, Loss: 0.667915
Train - Epoch 73, Batch: 0, Loss: 0.667923
Train - Epoch 74, Batch: 0, Loss: 0.667880
Train - Epoch 75, Batch: 0, Loss: 0.667508
Train - Epoch 76, Batch: 0, Loss: 0.666970
Train - Epoch 77, Batch: 0, Loss: 0.666465
Train - Epoch 78, Batch: 0, Loss: 0.666692
Train - Epoch 79, Batch: 0, Loss: 0.665952
Train - Epoch 80, Batch: 0, Loss: 0.665536
Train - Epoch 81, Batch: 0, Loss: 0.665363
Train - Epoch 82, Batch: 0, Loss: 0.665278
Train - Epoch 83, Batch: 0, Loss: 0.664960
Train - Epoch 84, Batch: 0, Loss: 0.664581
Train - Epoch 85, Batch: 0, Loss: 0.663951
Train - Epoch 86, Batch: 0, Loss: 0.663548
Train - Epoch 87, Batch: 0, Loss: 0.663424
Train - Epoch 88, Batch: 0, Loss: 0.663195
Train - Epoch 89, Batch: 0, Loss: 0.662843
Train - Epoch 90, Batch: 0, Loss: 0.662311
Train - Epoch 91, Batch: 0, Loss: 0.662210
Train - Epoch 92, Batch: 0, Loss: 0.661691
Train - Epoch 93, Batch: 0, Loss: 0.661529
Train - Epoch 94, Batch: 0, Loss: 0.661614
Train - Epoch 95, Batch: 0, Loss: 0.661233
Train - Epoch 96, Batch: 0, Loss: 0.660942
Train - Epoch 97, Batch: 0, Loss: 0.660117
Train - Epoch 98, Batch: 0, Loss: 0.660100
Train - Epoch 99, Batch: 0, Loss: 0.659674
Train - Epoch 100, Batch: 0, Loss: 0.659255
Train - Epoch 101, Batch: 0, Loss: 0.659294
Train - Epoch 102, Batch: 0, Loss: 0.658747
Train - Epoch 103, Batch: 0, Loss: 0.658663
Train - Epoch 104, Batch: 0, Loss: 0.658342
Train - Epoch 105, Batch: 0, Loss: 0.657881
Train - Epoch 106, Batch: 0, Loss: 0.657460
Train - Epoch 107, Batch: 0, Loss: 0.657290
Train - Epoch 108, Batch: 0, Loss: 0.657558
Train - Epoch 109, Batch: 0, Loss: 0.656970
Train - Epoch 110, Batch: 0, Loss: 0.656514
Train - Epoch 111, Batch: 0, Loss: 0.656303
Train - Epoch 112, Batch: 0, Loss: 0.655841
Train - Epoch 113, Batch: 0, Loss: 0.656090
Train - Epoch 114, Batch: 0, Loss: 0.655342
Train - Epoch 115, Batch: 0, Loss: 0.654976
Train - Epoch 116, Batch: 0, Loss: 0.655026
Train - Epoch 117, Batch: 0, Loss: 0.654516
Train - Epoch 118, Batch: 0, Loss: 0.654582
Train - Epoch 119, Batch: 0, Loss: 0.653817
Train - Epoch 120, Batch: 0, Loss: 0.653698
Train - Epoch 121, Batch: 0, Loss: 0.653648
Train - Epoch 122, Batch: 0, Loss: 0.653341
Train - Epoch 123, Batch: 0, Loss: 0.652734
Train - Epoch 124, Batch: 0, Loss: 0.652548
Train - Epoch 125, Batch: 0, Loss: 0.652427
Train - Epoch 126, Batch: 0, Loss: 0.652134
Train - Epoch 127, Batch: 0, Loss: 0.651744
Train - Epoch 128, Batch: 0, Loss: 0.651294
Train - Epoch 129, Batch: 0, Loss: 0.651306
Train - Epoch 130, Batch: 0, Loss: 0.650690
Train - Epoch 131, Batch: 0, Loss: 0.650775
Train - Epoch 132, Batch: 0, Loss: 0.650056
Train - Epoch 133, Batch: 0, Loss: 0.650130
Train - Epoch 134, Batch: 0, Loss: 0.649810
Train - Epoch 135, Batch: 0, Loss: 0.649583
Train - Epoch 136, Batch: 0, Loss: 0.649545
Train - Epoch 137, Batch: 0, Loss: 0.649265
Train - Epoch 138, Batch: 0, Loss: 0.648461
Train - Epoch 139, Batch: 0, Loss: 0.648357
Train - Epoch 140, Batch: 0, Loss: 0.647840
Train - Epoch 141, Batch: 0, Loss: 0.648007
Train - Epoch 142, Batch: 0, Loss: 0.647732
Train - Epoch 143, Batch: 0, Loss: 0.647297
Train - Epoch 144, Batch: 0, Loss: 0.646682
Train - Epoch 145, Batch: 0, Loss: 0.647030
Train - Epoch 146, Batch: 0, Loss: 0.646427
Train - Epoch 147, Batch: 0, Loss: 0.646222
Train - Epoch 148, Batch: 0, Loss: 0.645986
Train - Epoch 149, Batch: 0, Loss: 0.645881
Train - Epoch 150, Batch: 0, Loss: 0.645310
Train - Epoch 151, Batch: 0, Loss: 0.645112
Train - Epoch 152, Batch: 0, Loss: 0.644616
Train - Epoch 153, Batch: 0, Loss: 0.645062
Train - Epoch 154, Batch: 0, Loss: 0.643946
Train - Epoch 155, Batch: 0, Loss: 0.643790
Train - Epoch 156, Batch: 0, Loss: 0.643603
Train - Epoch 157, Batch: 0, Loss: 0.643393
Train - Epoch 158, Batch: 0, Loss: 0.643484
Train - Epoch 159, Batch: 0, Loss: 0.642410
Train - Epoch 160, Batch: 0, Loss: 0.642848
Train - Epoch 161, Batch: 0, Loss: 0.642542
Train - Epoch 162, Batch: 0, Loss: 0.642422
Train - Epoch 163, Batch: 0, Loss: 0.641713
Train - Epoch 164, Batch: 0, Loss: 0.641714
Train - Epoch 165, Batch: 0, Loss: 0.641674
Train - Epoch 166, Batch: 0, Loss: 0.640715
Train - Epoch 167, Batch: 0, Loss: 0.641034
Train - Epoch 168, Batch: 0, Loss: 0.640855
Train - Epoch 169, Batch: 0, Loss: 0.639862
Train - Epoch 170, Batch: 0, Loss: 0.639838
Train - Epoch 171, Batch: 0, Loss: 0.639842
Train - Epoch 172, Batch: 0, Loss: 0.639081
Train - Epoch 173, Batch: 0, Loss: 0.638977
Train - Epoch 174, Batch: 0, Loss: 0.638722
Train - Epoch 175, Batch: 0, Loss: 0.638747
Train - Epoch 176, Batch: 0, Loss: 0.638656
Train - Epoch 177, Batch: 0, Loss: 0.638368
Train - Epoch 178, Batch: 0, Loss: 0.638543
Train - Epoch 179, Batch: 0, Loss: 0.638553
Train - Epoch 180, Batch: 0, Loss: 0.637612
Train - Epoch 181, Batch: 0, Loss: 0.637534
Train - Epoch 182, Batch: 0, Loss: 0.637172
Train - Epoch 183, Batch: 0, Loss: 0.636699
Train - Epoch 184, Batch: 0, Loss: 0.636570
Train - Epoch 185, Batch: 0, Loss: 0.636440
Train - Epoch 186, Batch: 0, Loss: 0.635795/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635556
Train - Epoch 188, Batch: 0, Loss: 0.635576
Train - Epoch 189, Batch: 0, Loss: 0.635079
Train - Epoch 190, Batch: 0, Loss: 0.635561
Train - Epoch 191, Batch: 0, Loss: 0.634390
Train - Epoch 192, Batch: 0, Loss: 0.634203
Train - Epoch 193, Batch: 0, Loss: 0.635134
Train - Epoch 194, Batch: 0, Loss: 0.633680
Train - Epoch 195, Batch: 0, Loss: 0.633665
Train - Epoch 196, Batch: 0, Loss: 0.633766
Train - Epoch 197, Batch: 0, Loss: 0.633489
Train - Epoch 198, Batch: 0, Loss: 0.633397
Train - Epoch 199, Batch: 0, Loss: 0.632648
training_time:: 351.2551517486572
training time full:: 351.2552185058594
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926934
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.9926595687866
overhead:: 0
overhead2:: 0.3996601104736328
overhead3:: 0
time_baseline:: 273.99302673339844
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926835
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624620)
RCV1 Test Avg. Accuracy:: 0.9220858017209945
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.019962310791015625
overhead3:: 0.11545872688293457
overhead4:: 32.58943700790405
overhead5:: 0
memory usage:: 26646609920
time_provenance:: 47.86038613319397
curr_diff: 0 tensor(1.6102e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6102e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926835
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.01_10200
tensor(624620)
RCV1 Test Avg. Accuracy:: 0.9220858017209945
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693250
Train - Epoch 1, Batch: 0, Loss: 0.692867
Train - Epoch 2, Batch: 0, Loss: 0.692486
Train - Epoch 3, Batch: 0, Loss: 0.692122
Train - Epoch 4, Batch: 0, Loss: 0.691750
Train - Epoch 5, Batch: 0, Loss: 0.691399
Train - Epoch 6, Batch: 0, Loss: 0.691019
Train - Epoch 7, Batch: 0, Loss: 0.690674
Train - Epoch 8, Batch: 0, Loss: 0.690284
Train - Epoch 9, Batch: 0, Loss: 0.689874
Train - Epoch 10, Batch: 0, Loss: 0.689569
Train - Epoch 11, Batch: 0, Loss: 0.689208
Train - Epoch 12, Batch: 0, Loss: 0.688762
Train - Epoch 13, Batch: 0, Loss: 0.688484
Train - Epoch 14, Batch: 0, Loss: 0.688055
Train - Epoch 15, Batch: 0, Loss: 0.687753
Train - Epoch 16, Batch: 0, Loss: 0.687322
Train - Epoch 17, Batch: 0, Loss: 0.686956
Train - Epoch 18, Batch: 0, Loss: 0.686574
Train - Epoch 19, Batch: 0, Loss: 0.686190
Train - Epoch 20, Batch: 0, Loss: 0.685847
Train - Epoch 21, Batch: 0, Loss: 0.685461
Train - Epoch 22, Batch: 0, Loss: 0.685175
Train - Epoch 23, Batch: 0, Loss: 0.684922
Train - Epoch 24, Batch: 0, Loss: 0.684508
Train - Epoch 25, Batch: 0, Loss: 0.684057
Train - Epoch 26, Batch: 0, Loss: 0.683809
Train - Epoch 27, Batch: 0, Loss: 0.683445
Train - Epoch 28, Batch: 0, Loss: 0.683050
Train - Epoch 29, Batch: 0, Loss: 0.682660
Train - Epoch 30, Batch: 0, Loss: 0.682244
Train - Epoch 31, Batch: 0, Loss: 0.682024
Train - Epoch 32, Batch: 0, Loss: 0.681718
Train - Epoch 33, Batch: 0, Loss: 0.681245
Train - Epoch 34, Batch: 0, Loss: 0.680795
Train - Epoch 35, Batch: 0, Loss: 0.680751
Train - Epoch 36, Batch: 0, Loss: 0.680259
Train - Epoch 37, Batch: 0, Loss: 0.679991
Train - Epoch 38, Batch: 0, Loss: 0.679533
Train - Epoch 39, Batch: 0, Loss: 0.679301
Train - Epoch 40, Batch: 0, Loss: 0.678841
Train - Epoch 41, Batch: 0, Loss: 0.678413
Train - Epoch 42, Batch: 0, Loss: 0.678143
Train - Epoch 43, Batch: 0, Loss: 0.678016
Train - Epoch 44, Batch: 0, Loss: 0.677596
Train - Epoch 45, Batch: 0, Loss: 0.677140
Train - Epoch 46, Batch: 0, Loss: 0.676841
Train - Epoch 47, Batch: 0, Loss: 0.676569
Train - Epoch 48, Batch: 0, Loss: 0.676031
Train - Epoch 49, Batch: 0, Loss: 0.675878
Train - Epoch 50, Batch: 0, Loss: 0.675471
Train - Epoch 51, Batch: 0, Loss: 0.675299
Train - Epoch 52, Batch: 0, Loss: 0.674793
Train - Epoch 53, Batch: 0, Loss: 0.674596
Train - Epoch 54, Batch: 0, Loss: 0.674040
Train - Epoch 55, Batch: 0, Loss: 0.673942
Train - Epoch 56, Batch: 0, Loss: 0.673426
Train - Epoch 57, Batch: 0, Loss: 0.673237
Train - Epoch 58, Batch: 0, Loss: 0.672893
Train - Epoch 59, Batch: 0, Loss: 0.672393
Train - Epoch 60, Batch: 0, Loss: 0.672204
Train - Epoch 61, Batch: 0, Loss: 0.671931
Train - Epoch 62, Batch: 0, Loss: 0.671668
Train - Epoch 63, Batch: 0, Loss: 0.671154
Train - Epoch 64, Batch: 0, Loss: 0.670967
Train - Epoch 65, Batch: 0, Loss: 0.670634
Train - Epoch 66, Batch: 0, Loss: 0.670272
Train - Epoch 67, Batch: 0, Loss: 0.670309
Train - Epoch 68, Batch: 0, Loss: 0.669871
Train - Epoch 69, Batch: 0, Loss: 0.669349
Train - Epoch 70, Batch: 0, Loss: 0.668968
Train - Epoch 71, Batch: 0, Loss: 0.668703
Train - Epoch 72, Batch: 0, Loss: 0.668169
Train - Epoch 73, Batch: 0, Loss: 0.668357
Train - Epoch 74, Batch: 0, Loss: 0.667762
Train - Epoch 75, Batch: 0, Loss: 0.667335
Train - Epoch 76, Batch: 0, Loss: 0.667149
Train - Epoch 77, Batch: 0, Loss: 0.666526
Train - Epoch 78, Batch: 0, Loss: 0.666589
Train - Epoch 79, Batch: 0, Loss: 0.666253
Train - Epoch 80, Batch: 0, Loss: 0.665748
Train - Epoch 81, Batch: 0, Loss: 0.665370
Train - Epoch 82, Batch: 0, Loss: 0.665194
Train - Epoch 83, Batch: 0, Loss: 0.665016
Train - Epoch 84, Batch: 0, Loss: 0.664641
Train - Epoch 85, Batch: 0, Loss: 0.664039
Train - Epoch 86, Batch: 0, Loss: 0.663361
Train - Epoch 87, Batch: 0, Loss: 0.663362
Train - Epoch 88, Batch: 0, Loss: 0.663324
Train - Epoch 89, Batch: 0, Loss: 0.663241
Train - Epoch 90, Batch: 0, Loss: 0.662629
Train - Epoch 91, Batch: 0, Loss: 0.662072
Train - Epoch 92, Batch: 0, Loss: 0.662430
Train - Epoch 93, Batch: 0, Loss: 0.661665
Train - Epoch 94, Batch: 0, Loss: 0.661358
Train - Epoch 95, Batch: 0, Loss: 0.661265
Train - Epoch 96, Batch: 0, Loss: 0.660984
Train - Epoch 97, Batch: 0, Loss: 0.660291
Train - Epoch 98, Batch: 0, Loss: 0.660091
Train - Epoch 99, Batch: 0, Loss: 0.660110
Train - Epoch 100, Batch: 0, Loss: 0.659633
Train - Epoch 101, Batch: 0, Loss: 0.659693
Train - Epoch 102, Batch: 0, Loss: 0.659137
Train - Epoch 103, Batch: 0, Loss: 0.658553
Train - Epoch 104, Batch: 0, Loss: 0.658229
Train - Epoch 105, Batch: 0, Loss: 0.658407
Train - Epoch 106, Batch: 0, Loss: 0.657942
Train - Epoch 107, Batch: 0, Loss: 0.657656
Train - Epoch 108, Batch: 0, Loss: 0.657305
Train - Epoch 109, Batch: 0, Loss: 0.657045
Train - Epoch 110, Batch: 0, Loss: 0.656605
Train - Epoch 111, Batch: 0, Loss: 0.656336
Train - Epoch 112, Batch: 0, Loss: 0.655924
Train - Epoch 113, Batch: 0, Loss: 0.655812
Train - Epoch 114, Batch: 0, Loss: 0.655583
Train - Epoch 115, Batch: 0, Loss: 0.655137
Train - Epoch 116, Batch: 0, Loss: 0.654648
Train - Epoch 117, Batch: 0, Loss: 0.655256
Train - Epoch 118, Batch: 0, Loss: 0.654048
Train - Epoch 119, Batch: 0, Loss: 0.654111
Train - Epoch 120, Batch: 0, Loss: 0.653789
Train - Epoch 121, Batch: 0, Loss: 0.653718
Train - Epoch 122, Batch: 0, Loss: 0.653249
Train - Epoch 123, Batch: 0, Loss: 0.652610
Train - Epoch 124, Batch: 0, Loss: 0.652502
Train - Epoch 125, Batch: 0, Loss: 0.652421
Train - Epoch 126, Batch: 0, Loss: 0.652057
Train - Epoch 127, Batch: 0, Loss: 0.651563
Train - Epoch 128, Batch: 0, Loss: 0.651409
Train - Epoch 129, Batch: 0, Loss: 0.651197
Train - Epoch 130, Batch: 0, Loss: 0.650980
Train - Epoch 131, Batch: 0, Loss: 0.650485
Train - Epoch 132, Batch: 0, Loss: 0.649859
Train - Epoch 133, Batch: 0, Loss: 0.650039
Train - Epoch 134, Batch: 0, Loss: 0.649624
Train - Epoch 135, Batch: 0, Loss: 0.649681
Train - Epoch 136, Batch: 0, Loss: 0.649477
Train - Epoch 137, Batch: 0, Loss: 0.649009
Train - Epoch 138, Batch: 0, Loss: 0.648724
Train - Epoch 139, Batch: 0, Loss: 0.648281
Train - Epoch 140, Batch: 0, Loss: 0.648439
Train - Epoch 141, Batch: 0, Loss: 0.647872
Train - Epoch 142, Batch: 0, Loss: 0.647383
Train - Epoch 143, Batch: 0, Loss: 0.646715
Train - Epoch 144, Batch: 0, Loss: 0.647034
Train - Epoch 145, Batch: 0, Loss: 0.647202
Train - Epoch 146, Batch: 0, Loss: 0.646514
Train - Epoch 147, Batch: 0, Loss: 0.646514
Train - Epoch 148, Batch: 0, Loss: 0.645893
Train - Epoch 149, Batch: 0, Loss: 0.646019
Train - Epoch 150, Batch: 0, Loss: 0.645346
Train - Epoch 151, Batch: 0, Loss: 0.644946
Train - Epoch 152, Batch: 0, Loss: 0.645205
Train - Epoch 153, Batch: 0, Loss: 0.645025
Train - Epoch 154, Batch: 0, Loss: 0.644509
Train - Epoch 155, Batch: 0, Loss: 0.644668
Train - Epoch 156, Batch: 0, Loss: 0.643505
Train - Epoch 157, Batch: 0, Loss: 0.644067
Train - Epoch 158, Batch: 0, Loss: 0.643621
Train - Epoch 159, Batch: 0, Loss: 0.642974
Train - Epoch 160, Batch: 0, Loss: 0.643110
Train - Epoch 161, Batch: 0, Loss: 0.642697
Train - Epoch 162, Batch: 0, Loss: 0.641911
Train - Epoch 163, Batch: 0, Loss: 0.641601
Train - Epoch 164, Batch: 0, Loss: 0.641619
Train - Epoch 165, Batch: 0, Loss: 0.641692
Train - Epoch 166, Batch: 0, Loss: 0.641122
Train - Epoch 167, Batch: 0, Loss: 0.640060
Train - Epoch 168, Batch: 0, Loss: 0.640535
Train - Epoch 169, Batch: 0, Loss: 0.639948
Train - Epoch 170, Batch: 0, Loss: 0.640476
Train - Epoch 171, Batch: 0, Loss: 0.639654
Train - Epoch 172, Batch: 0, Loss: 0.640049
Train - Epoch 173, Batch: 0, Loss: 0.639058
Train - Epoch 174, Batch: 0, Loss: 0.639088
Train - Epoch 175, Batch: 0, Loss: 0.639081
Train - Epoch 176, Batch: 0, Loss: 0.638859
Train - Epoch 177, Batch: 0, Loss: 0.638552
Train - Epoch 178, Batch: 0, Loss: 0.638005
Train - Epoch 179, Batch: 0, Loss: 0.638394
Train - Epoch 180, Batch: 0, Loss: 0.638056
Train - Epoch 181, Batch: 0, Loss: 0.637803
Train - Epoch 182, Batch: 0, Loss: 0.637095
Train - Epoch 183, Batch: 0, Loss: 0.636908
Train - Epoch 184, Batch: 0, Loss: 0.636696
Train - Epoch 185, Batch: 0, Loss: 0.635701
Train - Epoch 186, Batch: 0, Loss: 0.635957/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635843
Train - Epoch 188, Batch: 0, Loss: 0.636331
Train - Epoch 189, Batch: 0, Loss: 0.635038
Train - Epoch 190, Batch: 0, Loss: 0.634872
Train - Epoch 191, Batch: 0, Loss: 0.634840
Train - Epoch 192, Batch: 0, Loss: 0.635289
Train - Epoch 193, Batch: 0, Loss: 0.634401
Train - Epoch 194, Batch: 0, Loss: 0.634212
Train - Epoch 195, Batch: 0, Loss: 0.633917
Train - Epoch 196, Batch: 0, Loss: 0.633151
Train - Epoch 197, Batch: 0, Loss: 0.633912
Train - Epoch 198, Batch: 0, Loss: 0.632909
Train - Epoch 199, Batch: 0, Loss: 0.632696
training_time:: 350.57584834098816
training time full:: 350.5759189128876
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000063, Accuracy: 0.926983
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.16489124298096
overhead:: 0
overhead2:: 0.4026808738708496
overhead3:: 0
time_baseline:: 273.1652901172638
curr_diff: 0 tensor(0.0169, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0169, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624742)
RCV1 Test Avg. Accuracy:: 0.9222659023706855
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.022483348846435547
overhead3:: 0.12224388122558594
overhead4:: 32.302249908447266
overhead5:: 0
memory usage:: 26620870656
time_provenance:: 47.58395195007324
curr_diff: 0 tensor(1.5830e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5830e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.01_10200
tensor(624742)
RCV1 Test Avg. Accuracy:: 0.9222659023706855
