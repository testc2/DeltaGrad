period::
init_iters::
varied deletion rate::
varied number of samples::
benchmark_rcv1_lr_varied_batch_size_period_add.sh: line 52: i: command not found
python3 generate_dataset_train_test.py Logistic_regression rcv1 16384 200 3
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  rcv1 1
torch.Size([20242, 47236])
tensor([19652])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.692924
Train - Epoch 1, Batch: 0, Loss: 0.692586
Train - Epoch 2, Batch: 0, Loss: 0.692177
Train - Epoch 3, Batch: 0, Loss: 0.691798
Train - Epoch 4, Batch: 0, Loss: 0.691457
Train - Epoch 5, Batch: 0, Loss: 0.691083
Train - Epoch 6, Batch: 0, Loss: 0.690682
Train - Epoch 7, Batch: 0, Loss: 0.690342
Train - Epoch 8, Batch: 0, Loss: 0.689965
Train - Epoch 9, Batch: 0, Loss: 0.689573
Train - Epoch 10, Batch: 0, Loss: 0.689244
Train - Epoch 11, Batch: 0, Loss: 0.688875
Train - Epoch 12, Batch: 0, Loss: 0.688482
Train - Epoch 13, Batch: 0, Loss: 0.688132
Train - Epoch 14, Batch: 0, Loss: 0.687779
Train - Epoch 15, Batch: 0, Loss: 0.687432
Train - Epoch 16, Batch: 0, Loss: 0.687040
Train - Epoch 17, Batch: 0, Loss: 0.686653
Train - Epoch 18, Batch: 0, Loss: 0.686350
Train - Epoch 19, Batch: 0, Loss: 0.686052
Train - Epoch 20, Batch: 0, Loss: 0.685642
Train - Epoch 21, Batch: 0, Loss: 0.685262
Train - Epoch 22, Batch: 0, Loss: 0.684829
Train - Epoch 23, Batch: 0, Loss: 0.684590
Train - Epoch 24, Batch: 0, Loss: 0.684265
Train - Epoch 25, Batch: 0, Loss: 0.683776
Train - Epoch 26, Batch: 0, Loss: 0.683449
Train - Epoch 27, Batch: 0, Loss: 0.683156
Train - Epoch 28, Batch: 0, Loss: 0.682717
Train - Epoch 29, Batch: 0, Loss: 0.682481
Train - Epoch 30, Batch: 0, Loss: 0.682060
Train - Epoch 31, Batch: 0, Loss: 0.681803
Train - Epoch 32, Batch: 0, Loss: 0.681434
Train - Epoch 33, Batch: 0, Loss: 0.680872
Train - Epoch 34, Batch: 0, Loss: 0.680713
Train - Epoch 35, Batch: 0, Loss: 0.680318
Train - Epoch 36, Batch: 0, Loss: 0.679946
Train - Epoch 37, Batch: 0, Loss: 0.679533
Train - Epoch 38, Batch: 0, Loss: 0.679353
Train - Epoch 39, Batch: 0, Loss: 0.678922
Train - Epoch 40, Batch: 0, Loss: 0.678698
Train - Epoch 41, Batch: 0, Loss: 0.678264
Train - Epoch 42, Batch: 0, Loss: 0.678081
Train - Epoch 43, Batch: 0, Loss: 0.677717
Train - Epoch 44, Batch: 0, Loss: 0.677500
Train - Epoch 45, Batch: 0, Loss: 0.676793
Train - Epoch 46, Batch: 0, Loss: 0.676478
Train - Epoch 47, Batch: 0, Loss: 0.676400
Train - Epoch 48, Batch: 0, Loss: 0.675872
Train - Epoch 49, Batch: 0, Loss: 0.675641
Train - Epoch 50, Batch: 0, Loss: 0.675310
Train - Epoch 51, Batch: 0, Loss: 0.675022
Train - Epoch 52, Batch: 0, Loss: 0.674465
Train - Epoch 53, Batch: 0, Loss: 0.674145
Train - Epoch 54, Batch: 0, Loss: 0.673843
Train - Epoch 55, Batch: 0, Loss: 0.673448
Train - Epoch 56, Batch: 0, Loss: 0.673095
Train - Epoch 57, Batch: 0, Loss: 0.673019
Train - Epoch 58, Batch: 0, Loss: 0.672754
Train - Epoch 59, Batch: 0, Loss: 0.672101
Train - Epoch 60, Batch: 0, Loss: 0.672110
Train - Epoch 61, Batch: 0, Loss: 0.671507
Train - Epoch 62, Batch: 0, Loss: 0.671083
Train - Epoch 63, Batch: 0, Loss: 0.671023
Train - Epoch 64, Batch: 0, Loss: 0.670416
Train - Epoch 65, Batch: 0, Loss: 0.670457
Train - Epoch 66, Batch: 0, Loss: 0.669973
Train - Epoch 67, Batch: 0, Loss: 0.669585
Train - Epoch 68, Batch: 0, Loss: 0.669474
Train - Epoch 69, Batch: 0, Loss: 0.669079
Train - Epoch 70, Batch: 0, Loss: 0.668845
Train - Epoch 71, Batch: 0, Loss: 0.668373
Train - Epoch 72, Batch: 0, Loss: 0.668225
Train - Epoch 73, Batch: 0, Loss: 0.667653
Train - Epoch 74, Batch: 0, Loss: 0.667047
Train - Epoch 75, Batch: 0, Loss: 0.667071
Train - Epoch 76, Batch: 0, Loss: 0.666607
Train - Epoch 77, Batch: 0, Loss: 0.666428
Train - Epoch 78, Batch: 0, Loss: 0.666031
Train - Epoch 79, Batch: 0, Loss: 0.665925
Train - Epoch 80, Batch: 0, Loss: 0.665652
Train - Epoch 81, Batch: 0, Loss: 0.665374
Train - Epoch 82, Batch: 0, Loss: 0.664918
Train - Epoch 83, Batch: 0, Loss: 0.664568
Train - Epoch 84, Batch: 0, Loss: 0.664272
Train - Epoch 85, Batch: 0, Loss: 0.663611
Train - Epoch 86, Batch: 0, Loss: 0.664035
Train - Epoch 87, Batch: 0, Loss: 0.663602
Train - Epoch 88, Batch: 0, Loss: 0.663007
Train - Epoch 89, Batch: 0, Loss: 0.662822
Train - Epoch 90, Batch: 0, Loss: 0.662347
Train - Epoch 91, Batch: 0, Loss: 0.662247
Train - Epoch 92, Batch: 0, Loss: 0.661462
Train - Epoch 93, Batch: 0, Loss: 0.661721
Train - Epoch 94, Batch: 0, Loss: 0.661251
Train - Epoch 95, Batch: 0, Loss: 0.660763
Train - Epoch 96, Batch: 0, Loss: 0.660664
Train - Epoch 97, Batch: 0, Loss: 0.660407
Train - Epoch 98, Batch: 0, Loss: 0.659742
Train - Epoch 99, Batch: 0, Loss: 0.659931
Train - Epoch 100, Batch: 0, Loss: 0.659598
Train - Epoch 101, Batch: 0, Loss: 0.659329
Train - Epoch 102, Batch: 0, Loss: 0.658890
Train - Epoch 103, Batch: 0, Loss: 0.658713
Train - Epoch 104, Batch: 0, Loss: 0.658251
Train - Epoch 105, Batch: 0, Loss: 0.657917
Train - Epoch 106, Batch: 0, Loss: 0.657851
Train - Epoch 107, Batch: 0, Loss: 0.657450
Train - Epoch 108, Batch: 0, Loss: 0.656881
Train - Epoch 109, Batch: 0, Loss: 0.656706
Train - Epoch 110, Batch: 0, Loss: 0.656382
Train - Epoch 111, Batch: 0, Loss: 0.656214
Train - Epoch 112, Batch: 0, Loss: 0.655867
Train - Epoch 113, Batch: 0, Loss: 0.655747
Train - Epoch 114, Batch: 0, Loss: 0.655468
Train - Epoch 115, Batch: 0, Loss: 0.655007
Train - Epoch 116, Batch: 0, Loss: 0.654858
Train - Epoch 117, Batch: 0, Loss: 0.654239
Train - Epoch 118, Batch: 0, Loss: 0.654215
Train - Epoch 119, Batch: 0, Loss: 0.653910
Train - Epoch 120, Batch: 0, Loss: 0.653554
Train - Epoch 121, Batch: 0, Loss: 0.653079
Train - Epoch 122, Batch: 0, Loss: 0.653004
Train - Epoch 123, Batch: 0, Loss: 0.652409
Train - Epoch 124, Batch: 0, Loss: 0.652221
Train - Epoch 125, Batch: 0, Loss: 0.651954
Train - Epoch 126, Batch: 0, Loss: 0.652117
Train - Epoch 127, Batch: 0, Loss: 0.651696
Train - Epoch 128, Batch: 0, Loss: 0.651263
Train - Epoch 129, Batch: 0, Loss: 0.651053
Train - Epoch 130, Batch: 0, Loss: 0.651023
Train - Epoch 131, Batch: 0, Loss: 0.650515
Train - Epoch 132, Batch: 0, Loss: 0.649906
Train - Epoch 133, Batch: 0, Loss: 0.649999
Train - Epoch 134, Batch: 0, Loss: 0.649629
Train - Epoch 135, Batch: 0, Loss: 0.649158
Train - Epoch 136, Batch: 0, Loss: 0.648819
Train - Epoch 137, Batch: 0, Loss: 0.648905
Train - Epoch 138, Batch: 0, Loss: 0.648938
Train - Epoch 139, Batch: 0, Loss: 0.648118
Train - Epoch 140, Batch: 0, Loss: 0.648136
Train - Epoch 141, Batch: 0, Loss: 0.647707
Train - Epoch 142, Batch: 0, Loss: 0.646707
Train - Epoch 143, Batch: 0, Loss: 0.647239
Train - Epoch 144, Batch: 0, Loss: 0.646793
Train - Epoch 145, Batch: 0, Loss: 0.646575
Train - Epoch 146, Batch: 0, Loss: 0.645741
Train - Epoch 147, Batch: 0, Loss: 0.645862
Train - Epoch 148, Batch: 0, Loss: 0.645379
Train - Epoch 149, Batch: 0, Loss: 0.645596
Train - Epoch 150, Batch: 0, Loss: 0.645532
Train - Epoch 151, Batch: 0, Loss: 0.644954
Train - Epoch 152, Batch: 0, Loss: 0.644800
Train - Epoch 153, Batch: 0, Loss: 0.644412
Train - Epoch 154, Batch: 0, Loss: 0.644202
Train - Epoch 155, Batch: 0, Loss: 0.643821
Train - Epoch 156, Batch: 0, Loss: 0.643625
Train - Epoch 157, Batch: 0, Loss: 0.643646
Train - Epoch 158, Batch: 0, Loss: 0.642908
Train - Epoch 159, Batch: 0, Loss: 0.643238
Train - Epoch 160, Batch: 0, Loss: 0.642488
Train - Epoch 161, Batch: 0, Loss: 0.642546
Train - Epoch 162, Batch: 0, Loss: 0.642056
Train - Epoch 163, Batch: 0, Loss: 0.641690
Train - Epoch 164, Batch: 0, Loss: 0.641694
Train - Epoch 165, Batch: 0, Loss: 0.641285
Train - Epoch 166, Batch: 0, Loss: 0.641079
Train - Epoch 167, Batch: 0, Loss: 0.640834
Train - Epoch 168, Batch: 0, Loss: 0.640373
Train - Epoch 169, Batch: 0, Loss: 0.640199
Train - Epoch 170, Batch: 0, Loss: 0.639406
Train - Epoch 171, Batch: 0, Loss: 0.639806
Train - Epoch 172, Batch: 0, Loss: 0.639607
Train - Epoch 173, Batch: 0, Loss: 0.639552
Train - Epoch 174, Batch: 0, Loss: 0.639201
Train - Epoch 175, Batch: 0, Loss: 0.638289
Train - Epoch 176, Batch: 0, Loss: 0.638263
Train - Epoch 177, Batch: 0, Loss: 0.638594
Train - Epoch 178, Batch: 0, Loss: 0.638081
Train - Epoch 179, Batch: 0, Loss: 0.637910
Train - Epoch 180, Batch: 0, Loss: 0.637753
Train - Epoch 181, Batch: 0, Loss: 0.637382
Train - Epoch 182, Batch: 0, Loss: 0.637144
Train - Epoch 183, Batch: 0, Loss: 0.636257
Train - Epoch 184, Batch: 0, Loss: 0.636747
Train - Epoch 185, Batch: 0, Loss: 0.636032
Train - Epoch 186, Batch: 0, Loss: 0.635309/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635271
Train - Epoch 188, Batch: 0, Loss: 0.635235
Train - Epoch 189, Batch: 0, Loss: 0.635298
Train - Epoch 190, Batch: 0, Loss: 0.635126
Train - Epoch 191, Batch: 0, Loss: 0.634893
Train - Epoch 192, Batch: 0, Loss: 0.634382
Train - Epoch 193, Batch: 0, Loss: 0.634476
Train - Epoch 194, Batch: 0, Loss: 0.633843
Train - Epoch 195, Batch: 0, Loss: 0.633843
Train - Epoch 196, Batch: 0, Loss: 0.633603
Train - Epoch 197, Batch: 0, Loss: 0.633875
Train - Epoch 198, Batch: 0, Loss: 0.632736
Train - Epoch 199, Batch: 0, Loss: 0.632568
training_time:: 350.26492404937744
training time full:: 350.2649703025818
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000062, Accuracy: 0.927082
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 1
training time is 298.4233989715576
overhead:: 0
overhead2:: 0
time_baseline:: 298.42411065101624
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000062, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624678)
RCV1 Test Avg. Accuracy:: 0.9221714233413395
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.013802051544189453
overhead3:: 0.1137075424194336
overhead4:: 35.098387718200684
overhead5:: 0
time_provenance:: 44.47851896286011
curr_diff: 0 tensor(5.1215e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1215e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000062, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.00005_10200
tensor(624678)
RCV1 Test Avg. Accuracy:: 0.9221714233413395
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693176
Train - Epoch 1, Batch: 0, Loss: 0.692807
Train - Epoch 2, Batch: 0, Loss: 0.692434
Train - Epoch 3, Batch: 0, Loss: 0.692032
Train - Epoch 4, Batch: 0, Loss: 0.691684
Train - Epoch 5, Batch: 0, Loss: 0.691319
Train - Epoch 6, Batch: 0, Loss: 0.690950
Train - Epoch 7, Batch: 0, Loss: 0.690539
Train - Epoch 8, Batch: 0, Loss: 0.690215
Train - Epoch 9, Batch: 0, Loss: 0.689847
Train - Epoch 10, Batch: 0, Loss: 0.689445
Train - Epoch 11, Batch: 0, Loss: 0.689156
Train - Epoch 12, Batch: 0, Loss: 0.688671
Train - Epoch 13, Batch: 0, Loss: 0.688387
Train - Epoch 14, Batch: 0, Loss: 0.688019
Train - Epoch 15, Batch: 0, Loss: 0.687666
Train - Epoch 16, Batch: 0, Loss: 0.687234
Train - Epoch 17, Batch: 0, Loss: 0.686870
Train - Epoch 18, Batch: 0, Loss: 0.686637
Train - Epoch 19, Batch: 0, Loss: 0.686207
Train - Epoch 20, Batch: 0, Loss: 0.685846
Train - Epoch 21, Batch: 0, Loss: 0.685535
Train - Epoch 22, Batch: 0, Loss: 0.685084
Train - Epoch 23, Batch: 0, Loss: 0.684694
Train - Epoch 24, Batch: 0, Loss: 0.684304
Train - Epoch 25, Batch: 0, Loss: 0.684077
Train - Epoch 26, Batch: 0, Loss: 0.683797
Train - Epoch 27, Batch: 0, Loss: 0.683372
Train - Epoch 28, Batch: 0, Loss: 0.683102
Train - Epoch 29, Batch: 0, Loss: 0.682760
Train - Epoch 30, Batch: 0, Loss: 0.682244
Train - Epoch 31, Batch: 0, Loss: 0.681867
Train - Epoch 32, Batch: 0, Loss: 0.681533
Train - Epoch 33, Batch: 0, Loss: 0.681241
Train - Epoch 34, Batch: 0, Loss: 0.680856
Train - Epoch 35, Batch: 0, Loss: 0.680544
Train - Epoch 36, Batch: 0, Loss: 0.680263
Train - Epoch 37, Batch: 0, Loss: 0.679868
Train - Epoch 38, Batch: 0, Loss: 0.679602
Train - Epoch 39, Batch: 0, Loss: 0.679239
Train - Epoch 40, Batch: 0, Loss: 0.678794
Train - Epoch 41, Batch: 0, Loss: 0.678635
Train - Epoch 42, Batch: 0, Loss: 0.678212
Train - Epoch 43, Batch: 0, Loss: 0.677864
Train - Epoch 44, Batch: 0, Loss: 0.677251
Train - Epoch 45, Batch: 0, Loss: 0.677255
Train - Epoch 46, Batch: 0, Loss: 0.676870
Train - Epoch 47, Batch: 0, Loss: 0.676470
Train - Epoch 48, Batch: 0, Loss: 0.676012
Train - Epoch 49, Batch: 0, Loss: 0.675948
Train - Epoch 50, Batch: 0, Loss: 0.675514
Train - Epoch 51, Batch: 0, Loss: 0.675008
Train - Epoch 52, Batch: 0, Loss: 0.674796
Train - Epoch 53, Batch: 0, Loss: 0.674577
Train - Epoch 54, Batch: 0, Loss: 0.674098
Train - Epoch 55, Batch: 0, Loss: 0.673758
Train - Epoch 56, Batch: 0, Loss: 0.673312
Train - Epoch 57, Batch: 0, Loss: 0.673023
Train - Epoch 58, Batch: 0, Loss: 0.672799
Train - Epoch 59, Batch: 0, Loss: 0.672419
Train - Epoch 60, Batch: 0, Loss: 0.672100
Train - Epoch 61, Batch: 0, Loss: 0.671764
Train - Epoch 62, Batch: 0, Loss: 0.671479
Train - Epoch 63, Batch: 0, Loss: 0.670940
Train - Epoch 64, Batch: 0, Loss: 0.671054
Train - Epoch 65, Batch: 0, Loss: 0.670439
Train - Epoch 66, Batch: 0, Loss: 0.670172
Train - Epoch 67, Batch: 0, Loss: 0.670057
Train - Epoch 68, Batch: 0, Loss: 0.669551
Train - Epoch 69, Batch: 0, Loss: 0.669164
Train - Epoch 70, Batch: 0, Loss: 0.668748
Train - Epoch 71, Batch: 0, Loss: 0.668667
Train - Epoch 72, Batch: 0, Loss: 0.668415
Train - Epoch 73, Batch: 0, Loss: 0.667996
Train - Epoch 74, Batch: 0, Loss: 0.667786
Train - Epoch 75, Batch: 0, Loss: 0.667410
Train - Epoch 76, Batch: 0, Loss: 0.667087
Train - Epoch 77, Batch: 0, Loss: 0.666743
Train - Epoch 78, Batch: 0, Loss: 0.666494
Train - Epoch 79, Batch: 0, Loss: 0.666113
Train - Epoch 80, Batch: 0, Loss: 0.665751
Train - Epoch 81, Batch: 0, Loss: 0.665305
Train - Epoch 82, Batch: 0, Loss: 0.665356
Train - Epoch 83, Batch: 0, Loss: 0.664630
Train - Epoch 84, Batch: 0, Loss: 0.664782
Train - Epoch 85, Batch: 0, Loss: 0.663882
Train - Epoch 86, Batch: 0, Loss: 0.663756
Train - Epoch 87, Batch: 0, Loss: 0.663551
Train - Epoch 88, Batch: 0, Loss: 0.663227
Train - Epoch 89, Batch: 0, Loss: 0.663129
Train - Epoch 90, Batch: 0, Loss: 0.663054
Train - Epoch 91, Batch: 0, Loss: 0.662813
Train - Epoch 92, Batch: 0, Loss: 0.662129
Train - Epoch 93, Batch: 0, Loss: 0.661750
Train - Epoch 94, Batch: 0, Loss: 0.661256
Train - Epoch 95, Batch: 0, Loss: 0.661024
Train - Epoch 96, Batch: 0, Loss: 0.660920
Train - Epoch 97, Batch: 0, Loss: 0.660433
Train - Epoch 98, Batch: 0, Loss: 0.660024
Train - Epoch 99, Batch: 0, Loss: 0.659644
Train - Epoch 100, Batch: 0, Loss: 0.659362
Train - Epoch 101, Batch: 0, Loss: 0.659801
Train - Epoch 102, Batch: 0, Loss: 0.659139
Train - Epoch 103, Batch: 0, Loss: 0.658865
Train - Epoch 104, Batch: 0, Loss: 0.658417
Train - Epoch 105, Batch: 0, Loss: 0.658390
Train - Epoch 106, Batch: 0, Loss: 0.658045
Train - Epoch 107, Batch: 0, Loss: 0.657539
Train - Epoch 108, Batch: 0, Loss: 0.657082
Train - Epoch 109, Batch: 0, Loss: 0.657021
Train - Epoch 110, Batch: 0, Loss: 0.657158
Train - Epoch 111, Batch: 0, Loss: 0.656428
Train - Epoch 112, Batch: 0, Loss: 0.656050
Train - Epoch 113, Batch: 0, Loss: 0.655736
Train - Epoch 114, Batch: 0, Loss: 0.655233
Train - Epoch 115, Batch: 0, Loss: 0.655340
Train - Epoch 116, Batch: 0, Loss: 0.654987
Train - Epoch 117, Batch: 0, Loss: 0.654913
Train - Epoch 118, Batch: 0, Loss: 0.654671
Train - Epoch 119, Batch: 0, Loss: 0.653876
Train - Epoch 120, Batch: 0, Loss: 0.653897
Train - Epoch 121, Batch: 0, Loss: 0.653477
Train - Epoch 122, Batch: 0, Loss: 0.653258
Train - Epoch 123, Batch: 0, Loss: 0.653139
Train - Epoch 124, Batch: 0, Loss: 0.652645
Train - Epoch 125, Batch: 0, Loss: 0.652741
Train - Epoch 126, Batch: 0, Loss: 0.652169
Train - Epoch 127, Batch: 0, Loss: 0.651821
Train - Epoch 128, Batch: 0, Loss: 0.651597
Train - Epoch 129, Batch: 0, Loss: 0.651564
Train - Epoch 130, Batch: 0, Loss: 0.650777
Train - Epoch 131, Batch: 0, Loss: 0.650842
Train - Epoch 132, Batch: 0, Loss: 0.650614
Train - Epoch 133, Batch: 0, Loss: 0.650378
Train - Epoch 134, Batch: 0, Loss: 0.650025
Train - Epoch 135, Batch: 0, Loss: 0.649673
Train - Epoch 136, Batch: 0, Loss: 0.649306
Train - Epoch 137, Batch: 0, Loss: 0.648742
Train - Epoch 138, Batch: 0, Loss: 0.648590
Train - Epoch 139, Batch: 0, Loss: 0.648470
Train - Epoch 140, Batch: 0, Loss: 0.648113
Train - Epoch 141, Batch: 0, Loss: 0.647959
Train - Epoch 142, Batch: 0, Loss: 0.647377
Train - Epoch 143, Batch: 0, Loss: 0.647453
Train - Epoch 144, Batch: 0, Loss: 0.646997
Train - Epoch 145, Batch: 0, Loss: 0.646681
Train - Epoch 146, Batch: 0, Loss: 0.646562
Train - Epoch 147, Batch: 0, Loss: 0.645996
Train - Epoch 148, Batch: 0, Loss: 0.645845
Train - Epoch 149, Batch: 0, Loss: 0.646300
Train - Epoch 150, Batch: 0, Loss: 0.645607
Train - Epoch 151, Batch: 0, Loss: 0.645751
Train - Epoch 152, Batch: 0, Loss: 0.644620
Train - Epoch 153, Batch: 0, Loss: 0.644472
Train - Epoch 154, Batch: 0, Loss: 0.644266
Train - Epoch 155, Batch: 0, Loss: 0.643732
Train - Epoch 156, Batch: 0, Loss: 0.644098
Train - Epoch 157, Batch: 0, Loss: 0.643683
Train - Epoch 158, Batch: 0, Loss: 0.643489
Train - Epoch 159, Batch: 0, Loss: 0.643453
Train - Epoch 160, Batch: 0, Loss: 0.642095
Train - Epoch 161, Batch: 0, Loss: 0.642452
Train - Epoch 162, Batch: 0, Loss: 0.642213
Train - Epoch 163, Batch: 0, Loss: 0.642012
Train - Epoch 164, Batch: 0, Loss: 0.642352
Train - Epoch 165, Batch: 0, Loss: 0.641270
Train - Epoch 166, Batch: 0, Loss: 0.641285
Train - Epoch 167, Batch: 0, Loss: 0.640840
Train - Epoch 168, Batch: 0, Loss: 0.640774
Train - Epoch 169, Batch: 0, Loss: 0.640262
Train - Epoch 170, Batch: 0, Loss: 0.640285
Train - Epoch 171, Batch: 0, Loss: 0.640475
Train - Epoch 172, Batch: 0, Loss: 0.639635
Train - Epoch 173, Batch: 0, Loss: 0.638608
Train - Epoch 174, Batch: 0, Loss: 0.639386
Train - Epoch 175, Batch: 0, Loss: 0.638587
Train - Epoch 176, Batch: 0, Loss: 0.639235
Train - Epoch 177, Batch: 0, Loss: 0.638452
Train - Epoch 178, Batch: 0, Loss: 0.638026
Train - Epoch 179, Batch: 0, Loss: 0.638520
Train - Epoch 180, Batch: 0, Loss: 0.637676
Train - Epoch 181, Batch: 0, Loss: 0.637446
Train - Epoch 182, Batch: 0, Loss: 0.637822
Train - Epoch 183, Batch: 0, Loss: 0.636730
Train - Epoch 184, Batch: 0, Loss: 0.636416
Train - Epoch 185, Batch: 0, Loss: 0.635919
Train - Epoch 186, Batch: 0, Loss: 0.635781/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635645
Train - Epoch 188, Batch: 0, Loss: 0.635860
Train - Epoch 189, Batch: 0, Loss: 0.635370
Train - Epoch 190, Batch: 0, Loss: 0.634963
Train - Epoch 191, Batch: 0, Loss: 0.634676
Train - Epoch 192, Batch: 0, Loss: 0.635068
Train - Epoch 193, Batch: 0, Loss: 0.635059
Train - Epoch 194, Batch: 0, Loss: 0.634503
Train - Epoch 195, Batch: 0, Loss: 0.634197
Train - Epoch 196, Batch: 0, Loss: 0.633213
Train - Epoch 197, Batch: 0, Loss: 0.633491
Train - Epoch 198, Batch: 0, Loss: 0.633043
Train - Epoch 199, Batch: 0, Loss: 0.632420
training_time:: 351.440016746521
training time full:: 351.440066576004
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926242
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 1
training time is 297.1832232475281
overhead:: 0
overhead2:: 0
time_baseline:: 297.18389081954956
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926242
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624607)
RCV1 Test Avg. Accuracy:: 0.9220666106681586
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.012459993362426758
overhead3:: 0.11298251152038574
overhead4:: 35.20612287521362
overhead5:: 0
time_provenance:: 44.53173089027405
curr_diff: 0 tensor(4.8320e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8320e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926242
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.00005_10200
tensor(624607)
RCV1 Test Avg. Accuracy:: 0.9220666106681586
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.692983
Train - Epoch 1, Batch: 0, Loss: 0.692612
Train - Epoch 2, Batch: 0, Loss: 0.692228
Train - Epoch 3, Batch: 0, Loss: 0.691861
Train - Epoch 4, Batch: 0, Loss: 0.691487
Train - Epoch 5, Batch: 0, Loss: 0.691112
Train - Epoch 6, Batch: 0, Loss: 0.690726
Train - Epoch 7, Batch: 0, Loss: 0.690366
Train - Epoch 8, Batch: 0, Loss: 0.690050
Train - Epoch 9, Batch: 0, Loss: 0.689642
Train - Epoch 10, Batch: 0, Loss: 0.689247
Train - Epoch 11, Batch: 0, Loss: 0.688926
Train - Epoch 12, Batch: 0, Loss: 0.688555
Train - Epoch 13, Batch: 0, Loss: 0.688144
Train - Epoch 14, Batch: 0, Loss: 0.687777
Train - Epoch 15, Batch: 0, Loss: 0.687469
Train - Epoch 16, Batch: 0, Loss: 0.687068
Train - Epoch 17, Batch: 0, Loss: 0.686701
Train - Epoch 18, Batch: 0, Loss: 0.686368
Train - Epoch 19, Batch: 0, Loss: 0.685951
Train - Epoch 20, Batch: 0, Loss: 0.685701
Train - Epoch 21, Batch: 0, Loss: 0.685319
Train - Epoch 22, Batch: 0, Loss: 0.684927
Train - Epoch 23, Batch: 0, Loss: 0.684564
Train - Epoch 24, Batch: 0, Loss: 0.684273
Train - Epoch 25, Batch: 0, Loss: 0.683963
Train - Epoch 26, Batch: 0, Loss: 0.683566
Train - Epoch 27, Batch: 0, Loss: 0.683199
Train - Epoch 28, Batch: 0, Loss: 0.682840
Train - Epoch 29, Batch: 0, Loss: 0.682489
Train - Epoch 30, Batch: 0, Loss: 0.682158
Train - Epoch 31, Batch: 0, Loss: 0.681734
Train - Epoch 32, Batch: 0, Loss: 0.681451
Train - Epoch 33, Batch: 0, Loss: 0.681036
Train - Epoch 34, Batch: 0, Loss: 0.680631
Train - Epoch 35, Batch: 0, Loss: 0.680461
Train - Epoch 36, Batch: 0, Loss: 0.680095
Train - Epoch 37, Batch: 0, Loss: 0.679687
Train - Epoch 38, Batch: 0, Loss: 0.679423
Train - Epoch 39, Batch: 0, Loss: 0.678893
Train - Epoch 40, Batch: 0, Loss: 0.678626
Train - Epoch 41, Batch: 0, Loss: 0.678273
Train - Epoch 42, Batch: 0, Loss: 0.678012
Train - Epoch 43, Batch: 0, Loss: 0.677670
Train - Epoch 44, Batch: 0, Loss: 0.677278
Train - Epoch 45, Batch: 0, Loss: 0.677030
Train - Epoch 46, Batch: 0, Loss: 0.676666
Train - Epoch 47, Batch: 0, Loss: 0.676269
Train - Epoch 48, Batch: 0, Loss: 0.676103
Train - Epoch 49, Batch: 0, Loss: 0.675678
Train - Epoch 50, Batch: 0, Loss: 0.675267
Train - Epoch 51, Batch: 0, Loss: 0.674880
Train - Epoch 52, Batch: 0, Loss: 0.674511
Train - Epoch 53, Batch: 0, Loss: 0.674274
Train - Epoch 54, Batch: 0, Loss: 0.674000
Train - Epoch 55, Batch: 0, Loss: 0.673667
Train - Epoch 56, Batch: 0, Loss: 0.673175
Train - Epoch 57, Batch: 0, Loss: 0.672992
Train - Epoch 58, Batch: 0, Loss: 0.672699
Train - Epoch 59, Batch: 0, Loss: 0.672169
Train - Epoch 60, Batch: 0, Loss: 0.671843
Train - Epoch 61, Batch: 0, Loss: 0.671915
Train - Epoch 62, Batch: 0, Loss: 0.671368
Train - Epoch 63, Batch: 0, Loss: 0.670887
Train - Epoch 64, Batch: 0, Loss: 0.670626
Train - Epoch 65, Batch: 0, Loss: 0.670315
Train - Epoch 66, Batch: 0, Loss: 0.670012
Train - Epoch 67, Batch: 0, Loss: 0.669645
Train - Epoch 68, Batch: 0, Loss: 0.669307
Train - Epoch 69, Batch: 0, Loss: 0.668905
Train - Epoch 70, Batch: 0, Loss: 0.668539
Train - Epoch 71, Batch: 0, Loss: 0.668359
Train - Epoch 72, Batch: 0, Loss: 0.668055
Train - Epoch 73, Batch: 0, Loss: 0.667830
Train - Epoch 74, Batch: 0, Loss: 0.667284
Train - Epoch 75, Batch: 0, Loss: 0.667247
Train - Epoch 76, Batch: 0, Loss: 0.666767
Train - Epoch 77, Batch: 0, Loss: 0.666437
Train - Epoch 78, Batch: 0, Loss: 0.666386
Train - Epoch 79, Batch: 0, Loss: 0.666129
Train - Epoch 80, Batch: 0, Loss: 0.665639
Train - Epoch 81, Batch: 0, Loss: 0.665253
Train - Epoch 82, Batch: 0, Loss: 0.665071
Train - Epoch 83, Batch: 0, Loss: 0.664362
Train - Epoch 84, Batch: 0, Loss: 0.664259
Train - Epoch 85, Batch: 0, Loss: 0.664205
Train - Epoch 86, Batch: 0, Loss: 0.663849
Train - Epoch 87, Batch: 0, Loss: 0.663080
Train - Epoch 88, Batch: 0, Loss: 0.662771
Train - Epoch 89, Batch: 0, Loss: 0.662868
Train - Epoch 90, Batch: 0, Loss: 0.662458
Train - Epoch 91, Batch: 0, Loss: 0.662109
Train - Epoch 92, Batch: 0, Loss: 0.662106
Train - Epoch 93, Batch: 0, Loss: 0.661782
Train - Epoch 94, Batch: 0, Loss: 0.661294
Train - Epoch 95, Batch: 0, Loss: 0.661120
Train - Epoch 96, Batch: 0, Loss: 0.660545
Train - Epoch 97, Batch: 0, Loss: 0.660211
Train - Epoch 98, Batch: 0, Loss: 0.659860
Train - Epoch 99, Batch: 0, Loss: 0.659776
Train - Epoch 100, Batch: 0, Loss: 0.659259
Train - Epoch 101, Batch: 0, Loss: 0.659042
Train - Epoch 102, Batch: 0, Loss: 0.659339
Train - Epoch 103, Batch: 0, Loss: 0.658262
Train - Epoch 104, Batch: 0, Loss: 0.658281
Train - Epoch 105, Batch: 0, Loss: 0.658100
Train - Epoch 106, Batch: 0, Loss: 0.657479
Train - Epoch 107, Batch: 0, Loss: 0.657474
Train - Epoch 108, Batch: 0, Loss: 0.656876
Train - Epoch 109, Batch: 0, Loss: 0.656984
Train - Epoch 110, Batch: 0, Loss: 0.656482
Train - Epoch 111, Batch: 0, Loss: 0.656452
Train - Epoch 112, Batch: 0, Loss: 0.655710
Train - Epoch 113, Batch: 0, Loss: 0.655775
Train - Epoch 114, Batch: 0, Loss: 0.655505
Train - Epoch 115, Batch: 0, Loss: 0.654914
Train - Epoch 116, Batch: 0, Loss: 0.654858
Train - Epoch 117, Batch: 0, Loss: 0.654273
Train - Epoch 118, Batch: 0, Loss: 0.653915
Train - Epoch 119, Batch: 0, Loss: 0.653926
Train - Epoch 120, Batch: 0, Loss: 0.653284
Train - Epoch 121, Batch: 0, Loss: 0.653232
Train - Epoch 122, Batch: 0, Loss: 0.652614
Train - Epoch 123, Batch: 0, Loss: 0.653043
Train - Epoch 124, Batch: 0, Loss: 0.652588
Train - Epoch 125, Batch: 0, Loss: 0.652228
Train - Epoch 126, Batch: 0, Loss: 0.652228
Train - Epoch 127, Batch: 0, Loss: 0.651748
Train - Epoch 128, Batch: 0, Loss: 0.651117
Train - Epoch 129, Batch: 0, Loss: 0.651279
Train - Epoch 130, Batch: 0, Loss: 0.650844
Train - Epoch 131, Batch: 0, Loss: 0.650588
Train - Epoch 132, Batch: 0, Loss: 0.650184
Train - Epoch 133, Batch: 0, Loss: 0.650008
Train - Epoch 134, Batch: 0, Loss: 0.649561
Train - Epoch 135, Batch: 0, Loss: 0.648968
Train - Epoch 136, Batch: 0, Loss: 0.649488
Train - Epoch 137, Batch: 0, Loss: 0.648801
Train - Epoch 138, Batch: 0, Loss: 0.648992
Train - Epoch 139, Batch: 0, Loss: 0.648350
Train - Epoch 140, Batch: 0, Loss: 0.647885
Train - Epoch 141, Batch: 0, Loss: 0.647365
Train - Epoch 142, Batch: 0, Loss: 0.647129
Train - Epoch 143, Batch: 0, Loss: 0.647482
Train - Epoch 144, Batch: 0, Loss: 0.646686
Train - Epoch 145, Batch: 0, Loss: 0.646526
Train - Epoch 146, Batch: 0, Loss: 0.646519
Train - Epoch 147, Batch: 0, Loss: 0.645863
Train - Epoch 148, Batch: 0, Loss: 0.645654
Train - Epoch 149, Batch: 0, Loss: 0.645519
Train - Epoch 150, Batch: 0, Loss: 0.645672
Train - Epoch 151, Batch: 0, Loss: 0.645176
Train - Epoch 152, Batch: 0, Loss: 0.644824
Train - Epoch 153, Batch: 0, Loss: 0.644023
Train - Epoch 154, Batch: 0, Loss: 0.644254
Train - Epoch 155, Batch: 0, Loss: 0.644004
Train - Epoch 156, Batch: 0, Loss: 0.643560
Train - Epoch 157, Batch: 0, Loss: 0.643907
Train - Epoch 158, Batch: 0, Loss: 0.643401
Train - Epoch 159, Batch: 0, Loss: 0.642380
Train - Epoch 160, Batch: 0, Loss: 0.642664
Train - Epoch 161, Batch: 0, Loss: 0.642517
Train - Epoch 162, Batch: 0, Loss: 0.641632
Train - Epoch 163, Batch: 0, Loss: 0.641691
Train - Epoch 164, Batch: 0, Loss: 0.641802
Train - Epoch 165, Batch: 0, Loss: 0.641246
Train - Epoch 166, Batch: 0, Loss: 0.641349
Train - Epoch 167, Batch: 0, Loss: 0.641042
Train - Epoch 168, Batch: 0, Loss: 0.640272
Train - Epoch 169, Batch: 0, Loss: 0.640099
Train - Epoch 170, Batch: 0, Loss: 0.640211
Train - Epoch 171, Batch: 0, Loss: 0.639484
Train - Epoch 172, Batch: 0, Loss: 0.639430
Train - Epoch 173, Batch: 0, Loss: 0.639433
Train - Epoch 174, Batch: 0, Loss: 0.639480
Train - Epoch 175, Batch: 0, Loss: 0.639186
Train - Epoch 176, Batch: 0, Loss: 0.638306
Train - Epoch 177, Batch: 0, Loss: 0.638391
Train - Epoch 178, Batch: 0, Loss: 0.637440
Train - Epoch 179, Batch: 0, Loss: 0.637112
Train - Epoch 180, Batch: 0, Loss: 0.637743
Train - Epoch 181, Batch: 0, Loss: 0.636771
Train - Epoch 182, Batch: 0, Loss: 0.637046
Train - Epoch 183, Batch: 0, Loss: 0.637240
Train - Epoch 184, Batch: 0, Loss: 0.636602
Train - Epoch 185, Batch: 0, Loss: 0.636403
Train - Epoch 186, Batch: 0, Loss: 0.636008/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635411
Train - Epoch 188, Batch: 0, Loss: 0.635505
Train - Epoch 189, Batch: 0, Loss: 0.635479
Train - Epoch 190, Batch: 0, Loss: 0.635146
Train - Epoch 191, Batch: 0, Loss: 0.634793
Train - Epoch 192, Batch: 0, Loss: 0.634790
Train - Epoch 193, Batch: 0, Loss: 0.634493
Train - Epoch 194, Batch: 0, Loss: 0.633687
Train - Epoch 195, Batch: 0, Loss: 0.633922
Train - Epoch 196, Batch: 0, Loss: 0.633816
Train - Epoch 197, Batch: 0, Loss: 0.633731
Train - Epoch 198, Batch: 0, Loss: 0.632841
Train - Epoch 199, Batch: 0, Loss: 0.632497
training_time:: 351.79618191719055
training time full:: 351.7962248325348
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926885
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 1
training time is 297.59901666641235
overhead:: 0
overhead2:: 0
time_baseline:: 297.59968090057373
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926885
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624838)
RCV1 Test Avg. Accuracy:: 0.9224076209147046
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1
max_epoch:: 200
overhead:: 0
overhead2:: 0.010623931884765625
overhead3:: 0.11190342903137207
overhead4:: 35.3054358959198
overhead5:: 0
time_provenance:: 44.63637328147888
curr_diff: 0 tensor(5.0990e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0990e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926885
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.00005_10200
tensor(624838)
RCV1 Test Avg. Accuracy:: 0.9224076209147046
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  rcv1 0
tensor([  155, 19652])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.692961
Train - Epoch 1, Batch: 0, Loss: 0.692609
Train - Epoch 2, Batch: 0, Loss: 0.692234
Train - Epoch 3, Batch: 0, Loss: 0.691839
Train - Epoch 4, Batch: 0, Loss: 0.691484
Train - Epoch 5, Batch: 0, Loss: 0.691107
Train - Epoch 6, Batch: 0, Loss: 0.690757
Train - Epoch 7, Batch: 0, Loss: 0.690397
Train - Epoch 8, Batch: 0, Loss: 0.689999
Train - Epoch 9, Batch: 0, Loss: 0.689649
Train - Epoch 10, Batch: 0, Loss: 0.689267
Train - Epoch 11, Batch: 0, Loss: 0.688978
Train - Epoch 12, Batch: 0, Loss: 0.688537
Train - Epoch 13, Batch: 0, Loss: 0.688161
Train - Epoch 14, Batch: 0, Loss: 0.687889
Train - Epoch 15, Batch: 0, Loss: 0.687445
Train - Epoch 16, Batch: 0, Loss: 0.687057
Train - Epoch 17, Batch: 0, Loss: 0.686752
Train - Epoch 18, Batch: 0, Loss: 0.686395
Train - Epoch 19, Batch: 0, Loss: 0.686044
Train - Epoch 20, Batch: 0, Loss: 0.685706
Train - Epoch 21, Batch: 0, Loss: 0.685281
Train - Epoch 22, Batch: 0, Loss: 0.684899
Train - Epoch 23, Batch: 0, Loss: 0.684532
Train - Epoch 24, Batch: 0, Loss: 0.684174
Train - Epoch 25, Batch: 0, Loss: 0.683977
Train - Epoch 26, Batch: 0, Loss: 0.683536
Train - Epoch 27, Batch: 0, Loss: 0.683192
Train - Epoch 28, Batch: 0, Loss: 0.682748
Train - Epoch 29, Batch: 0, Loss: 0.682548
Train - Epoch 30, Batch: 0, Loss: 0.682035
Train - Epoch 31, Batch: 0, Loss: 0.681923
Train - Epoch 32, Batch: 0, Loss: 0.681550
Train - Epoch 33, Batch: 0, Loss: 0.681121
Train - Epoch 34, Batch: 0, Loss: 0.680753
Train - Epoch 35, Batch: 0, Loss: 0.680252
Train - Epoch 36, Batch: 0, Loss: 0.679920
Train - Epoch 37, Batch: 0, Loss: 0.679658
Train - Epoch 38, Batch: 0, Loss: 0.679370
Train - Epoch 39, Batch: 0, Loss: 0.679143
Train - Epoch 40, Batch: 0, Loss: 0.678803
Train - Epoch 41, Batch: 0, Loss: 0.678368
Train - Epoch 42, Batch: 0, Loss: 0.677977
Train - Epoch 43, Batch: 0, Loss: 0.677760
Train - Epoch 44, Batch: 0, Loss: 0.677522
Train - Epoch 45, Batch: 0, Loss: 0.677069
Train - Epoch 46, Batch: 0, Loss: 0.676605
Train - Epoch 47, Batch: 0, Loss: 0.676268
Train - Epoch 48, Batch: 0, Loss: 0.676050
Train - Epoch 49, Batch: 0, Loss: 0.675822
Train - Epoch 50, Batch: 0, Loss: 0.675539
Train - Epoch 51, Batch: 0, Loss: 0.675009
Train - Epoch 52, Batch: 0, Loss: 0.674769
Train - Epoch 53, Batch: 0, Loss: 0.674309
Train - Epoch 54, Batch: 0, Loss: 0.673940
Train - Epoch 55, Batch: 0, Loss: 0.673649
Train - Epoch 56, Batch: 0, Loss: 0.673333
Train - Epoch 57, Batch: 0, Loss: 0.673086
Train - Epoch 58, Batch: 0, Loss: 0.672874
Train - Epoch 59, Batch: 0, Loss: 0.672359
Train - Epoch 60, Batch: 0, Loss: 0.672062
Train - Epoch 61, Batch: 0, Loss: 0.671804
Train - Epoch 62, Batch: 0, Loss: 0.671331
Train - Epoch 63, Batch: 0, Loss: 0.671255
Train - Epoch 64, Batch: 0, Loss: 0.670436
Train - Epoch 65, Batch: 0, Loss: 0.670451
Train - Epoch 66, Batch: 0, Loss: 0.670252
Train - Epoch 67, Batch: 0, Loss: 0.669596
Train - Epoch 68, Batch: 0, Loss: 0.669302
Train - Epoch 69, Batch: 0, Loss: 0.669203
Train - Epoch 70, Batch: 0, Loss: 0.668728
Train - Epoch 71, Batch: 0, Loss: 0.668453
Train - Epoch 72, Batch: 0, Loss: 0.668265
Train - Epoch 73, Batch: 0, Loss: 0.667896
Train - Epoch 74, Batch: 0, Loss: 0.667879
Train - Epoch 75, Batch: 0, Loss: 0.667177
Train - Epoch 76, Batch: 0, Loss: 0.667079
Train - Epoch 77, Batch: 0, Loss: 0.666557
Train - Epoch 78, Batch: 0, Loss: 0.666056
Train - Epoch 79, Batch: 0, Loss: 0.666316
Train - Epoch 80, Batch: 0, Loss: 0.665711
Train - Epoch 81, Batch: 0, Loss: 0.665272
Train - Epoch 82, Batch: 0, Loss: 0.665209
Train - Epoch 83, Batch: 0, Loss: 0.664763
Train - Epoch 84, Batch: 0, Loss: 0.664192
Train - Epoch 85, Batch: 0, Loss: 0.664209
Train - Epoch 86, Batch: 0, Loss: 0.663805
Train - Epoch 87, Batch: 0, Loss: 0.663462
Train - Epoch 88, Batch: 0, Loss: 0.663102
Train - Epoch 89, Batch: 0, Loss: 0.662833
Train - Epoch 90, Batch: 0, Loss: 0.662332
Train - Epoch 91, Batch: 0, Loss: 0.662031
Train - Epoch 92, Batch: 0, Loss: 0.661819
Train - Epoch 93, Batch: 0, Loss: 0.661984
Train - Epoch 94, Batch: 0, Loss: 0.661327
Train - Epoch 95, Batch: 0, Loss: 0.660754
Train - Epoch 96, Batch: 0, Loss: 0.660531
Train - Epoch 97, Batch: 0, Loss: 0.660096
Train - Epoch 98, Batch: 0, Loss: 0.660056
Train - Epoch 99, Batch: 0, Loss: 0.659606
Train - Epoch 100, Batch: 0, Loss: 0.659591
Train - Epoch 101, Batch: 0, Loss: 0.659417
Train - Epoch 102, Batch: 0, Loss: 0.659002
Train - Epoch 103, Batch: 0, Loss: 0.658158
Train - Epoch 104, Batch: 0, Loss: 0.658580
Train - Epoch 105, Batch: 0, Loss: 0.658355
Train - Epoch 106, Batch: 0, Loss: 0.657739
Train - Epoch 107, Batch: 0, Loss: 0.657477
Train - Epoch 108, Batch: 0, Loss: 0.656983
Train - Epoch 109, Batch: 0, Loss: 0.656387
Train - Epoch 110, Batch: 0, Loss: 0.656725
Train - Epoch 111, Batch: 0, Loss: 0.656212
Train - Epoch 112, Batch: 0, Loss: 0.655580
Train - Epoch 113, Batch: 0, Loss: 0.655439
Train - Epoch 114, Batch: 0, Loss: 0.655181
Train - Epoch 115, Batch: 0, Loss: 0.655243
Train - Epoch 116, Batch: 0, Loss: 0.654897
Train - Epoch 117, Batch: 0, Loss: 0.654533
Train - Epoch 118, Batch: 0, Loss: 0.654337
Train - Epoch 119, Batch: 0, Loss: 0.653901
Train - Epoch 120, Batch: 0, Loss: 0.653457
Train - Epoch 121, Batch: 0, Loss: 0.653481
Train - Epoch 122, Batch: 0, Loss: 0.653008
Train - Epoch 123, Batch: 0, Loss: 0.652649
Train - Epoch 124, Batch: 0, Loss: 0.652310
Train - Epoch 125, Batch: 0, Loss: 0.652210
Train - Epoch 126, Batch: 0, Loss: 0.652475
Train - Epoch 127, Batch: 0, Loss: 0.651358
Train - Epoch 128, Batch: 0, Loss: 0.651559
Train - Epoch 129, Batch: 0, Loss: 0.651430
Train - Epoch 130, Batch: 0, Loss: 0.651066
Train - Epoch 131, Batch: 0, Loss: 0.650496
Train - Epoch 132, Batch: 0, Loss: 0.650118
Train - Epoch 133, Batch: 0, Loss: 0.649811
Train - Epoch 134, Batch: 0, Loss: 0.649612
Train - Epoch 135, Batch: 0, Loss: 0.649393
Train - Epoch 136, Batch: 0, Loss: 0.648925
Train - Epoch 137, Batch: 0, Loss: 0.648753
Train - Epoch 138, Batch: 0, Loss: 0.648691
Train - Epoch 139, Batch: 0, Loss: 0.648716
Train - Epoch 140, Batch: 0, Loss: 0.648285
Train - Epoch 141, Batch: 0, Loss: 0.647779
Train - Epoch 142, Batch: 0, Loss: 0.647141
Train - Epoch 143, Batch: 0, Loss: 0.647138
Train - Epoch 144, Batch: 0, Loss: 0.646887
Train - Epoch 145, Batch: 0, Loss: 0.646219
Train - Epoch 146, Batch: 0, Loss: 0.646250
Train - Epoch 147, Batch: 0, Loss: 0.646199
Train - Epoch 148, Batch: 0, Loss: 0.646055
Train - Epoch 149, Batch: 0, Loss: 0.645726
Train - Epoch 150, Batch: 0, Loss: 0.645130
Train - Epoch 151, Batch: 0, Loss: 0.644780
Train - Epoch 152, Batch: 0, Loss: 0.644483
Train - Epoch 153, Batch: 0, Loss: 0.644952
Train - Epoch 154, Batch: 0, Loss: 0.644208
Train - Epoch 155, Batch: 0, Loss: 0.644026
Train - Epoch 156, Batch: 0, Loss: 0.643548
Train - Epoch 157, Batch: 0, Loss: 0.643756
Train - Epoch 158, Batch: 0, Loss: 0.643080
Train - Epoch 159, Batch: 0, Loss: 0.642702
Train - Epoch 160, Batch: 0, Loss: 0.642387
Train - Epoch 161, Batch: 0, Loss: 0.642186
Train - Epoch 162, Batch: 0, Loss: 0.641538
Train - Epoch 163, Batch: 0, Loss: 0.641796
Train - Epoch 164, Batch: 0, Loss: 0.641765
Train - Epoch 165, Batch: 0, Loss: 0.641046
Train - Epoch 166, Batch: 0, Loss: 0.640931
Train - Epoch 167, Batch: 0, Loss: 0.641024
Train - Epoch 168, Batch: 0, Loss: 0.641055
Train - Epoch 169, Batch: 0, Loss: 0.640413
Train - Epoch 170, Batch: 0, Loss: 0.640770
Train - Epoch 171, Batch: 0, Loss: 0.639448
Train - Epoch 172, Batch: 0, Loss: 0.639771
Train - Epoch 173, Batch: 0, Loss: 0.639165
Train - Epoch 174, Batch: 0, Loss: 0.639024
Train - Epoch 175, Batch: 0, Loss: 0.639094
Train - Epoch 176, Batch: 0, Loss: 0.638634
Train - Epoch 177, Batch: 0, Loss: 0.638229
Train - Epoch 178, Batch: 0, Loss: 0.638148
Train - Epoch 179, Batch: 0, Loss: 0.637956
Train - Epoch 180, Batch: 0, Loss: 0.637351
Train - Epoch 181, Batch: 0, Loss: 0.637307
Train - Epoch 182, Batch: 0, Loss: 0.637181
Train - Epoch 183, Batch: 0, Loss: 0.636488
Train - Epoch 184, Batch: 0, Loss: 0.636574
Train - Epoch 185, Batch: 0, Loss: 0.636604
Train - Epoch 186, Batch: 0, Loss: 0.636456/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635829
Train - Epoch 188, Batch: 0, Loss: 0.635447
Train - Epoch 189, Batch: 0, Loss: 0.634925
Train - Epoch 190, Batch: 0, Loss: 0.634647
Train - Epoch 191, Batch: 0, Loss: 0.634370
Train - Epoch 192, Batch: 0, Loss: 0.634687
Train - Epoch 193, Batch: 0, Loss: 0.633913
Train - Epoch 194, Batch: 0, Loss: 0.634329
Train - Epoch 195, Batch: 0, Loss: 0.633135
Train - Epoch 196, Batch: 0, Loss: 0.633431
Train - Epoch 197, Batch: 0, Loss: 0.632985
Train - Epoch 198, Batch: 0, Loss: 0.633247
Train - Epoch 199, Batch: 0, Loss: 0.633337
training_time:: 351.58638978004456
training time full:: 351.5864324569702
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000063, Accuracy: 0.928021
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 2
training time is 298.1508071422577
overhead:: 0
overhead2:: 0
time_baseline:: 298.15149879455566
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.928021
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624931)
RCV1 Test Avg. Accuracy:: 0.9225449107542232
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.014986276626586914
overhead3:: 0.11320734024047852
overhead4:: 35.25233697891235
overhead5:: 0
time_provenance:: 45.314351081848145
curr_diff: 0 tensor(8.6927e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6927e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.928021
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.0001_10200
tensor(624931)
RCV1 Test Avg. Accuracy:: 0.9225449107542232
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693082
Train - Epoch 1, Batch: 0, Loss: 0.692703
Train - Epoch 2, Batch: 0, Loss: 0.692316
Train - Epoch 3, Batch: 0, Loss: 0.691951
Train - Epoch 4, Batch: 0, Loss: 0.691574
Train - Epoch 5, Batch: 0, Loss: 0.691239
Train - Epoch 6, Batch: 0, Loss: 0.690830
Train - Epoch 7, Batch: 0, Loss: 0.690480
Train - Epoch 8, Batch: 0, Loss: 0.690133
Train - Epoch 9, Batch: 0, Loss: 0.689731
Train - Epoch 10, Batch: 0, Loss: 0.689348
Train - Epoch 11, Batch: 0, Loss: 0.688970
Train - Epoch 12, Batch: 0, Loss: 0.688641
Train - Epoch 13, Batch: 0, Loss: 0.688375
Train - Epoch 14, Batch: 0, Loss: 0.687939
Train - Epoch 15, Batch: 0, Loss: 0.687546
Train - Epoch 16, Batch: 0, Loss: 0.687136
Train - Epoch 17, Batch: 0, Loss: 0.686876
Train - Epoch 18, Batch: 0, Loss: 0.686394
Train - Epoch 19, Batch: 0, Loss: 0.686122
Train - Epoch 20, Batch: 0, Loss: 0.685806
Train - Epoch 21, Batch: 0, Loss: 0.685388
Train - Epoch 22, Batch: 0, Loss: 0.684968
Train - Epoch 23, Batch: 0, Loss: 0.684641
Train - Epoch 24, Batch: 0, Loss: 0.684377
Train - Epoch 25, Batch: 0, Loss: 0.683978
Train - Epoch 26, Batch: 0, Loss: 0.683619
Train - Epoch 27, Batch: 0, Loss: 0.683182
Train - Epoch 28, Batch: 0, Loss: 0.682901
Train - Epoch 29, Batch: 0, Loss: 0.682545
Train - Epoch 30, Batch: 0, Loss: 0.682080
Train - Epoch 31, Batch: 0, Loss: 0.681826
Train - Epoch 32, Batch: 0, Loss: 0.681622
Train - Epoch 33, Batch: 0, Loss: 0.681312
Train - Epoch 34, Batch: 0, Loss: 0.680698
Train - Epoch 35, Batch: 0, Loss: 0.680387
Train - Epoch 36, Batch: 0, Loss: 0.680072
Train - Epoch 37, Batch: 0, Loss: 0.679868
Train - Epoch 38, Batch: 0, Loss: 0.679560
Train - Epoch 39, Batch: 0, Loss: 0.679048
Train - Epoch 40, Batch: 0, Loss: 0.678851
Train - Epoch 41, Batch: 0, Loss: 0.678418
Train - Epoch 42, Batch: 0, Loss: 0.678180
Train - Epoch 43, Batch: 0, Loss: 0.677809
Train - Epoch 44, Batch: 0, Loss: 0.677468
Train - Epoch 45, Batch: 0, Loss: 0.676875
Train - Epoch 46, Batch: 0, Loss: 0.676772
Train - Epoch 47, Batch: 0, Loss: 0.676303
Train - Epoch 48, Batch: 0, Loss: 0.676035
Train - Epoch 49, Batch: 0, Loss: 0.675704
Train - Epoch 50, Batch: 0, Loss: 0.675479
Train - Epoch 51, Batch: 0, Loss: 0.675205
Train - Epoch 52, Batch: 0, Loss: 0.674743
Train - Epoch 53, Batch: 0, Loss: 0.674486
Train - Epoch 54, Batch: 0, Loss: 0.673839
Train - Epoch 55, Batch: 0, Loss: 0.673756
Train - Epoch 56, Batch: 0, Loss: 0.673344
Train - Epoch 57, Batch: 0, Loss: 0.672940
Train - Epoch 58, Batch: 0, Loss: 0.672501
Train - Epoch 59, Batch: 0, Loss: 0.672295
Train - Epoch 60, Batch: 0, Loss: 0.672151
Train - Epoch 61, Batch: 0, Loss: 0.671674
Train - Epoch 62, Batch: 0, Loss: 0.671454
Train - Epoch 63, Batch: 0, Loss: 0.671190
Train - Epoch 64, Batch: 0, Loss: 0.670883
Train - Epoch 65, Batch: 0, Loss: 0.670395
Train - Epoch 66, Batch: 0, Loss: 0.669961
Train - Epoch 67, Batch: 0, Loss: 0.669603
Train - Epoch 68, Batch: 0, Loss: 0.669670
Train - Epoch 69, Batch: 0, Loss: 0.669321
Train - Epoch 70, Batch: 0, Loss: 0.668714
Train - Epoch 71, Batch: 0, Loss: 0.668711
Train - Epoch 72, Batch: 0, Loss: 0.668333
Train - Epoch 73, Batch: 0, Loss: 0.668049
Train - Epoch 74, Batch: 0, Loss: 0.667616
Train - Epoch 75, Batch: 0, Loss: 0.667272
Train - Epoch 76, Batch: 0, Loss: 0.666980
Train - Epoch 77, Batch: 0, Loss: 0.666533
Train - Epoch 78, Batch: 0, Loss: 0.666270
Train - Epoch 79, Batch: 0, Loss: 0.665762
Train - Epoch 80, Batch: 0, Loss: 0.665543
Train - Epoch 81, Batch: 0, Loss: 0.665392
Train - Epoch 82, Batch: 0, Loss: 0.664744
Train - Epoch 83, Batch: 0, Loss: 0.664769
Train - Epoch 84, Batch: 0, Loss: 0.664729
Train - Epoch 85, Batch: 0, Loss: 0.664316
Train - Epoch 86, Batch: 0, Loss: 0.663842
Train - Epoch 87, Batch: 0, Loss: 0.663571
Train - Epoch 88, Batch: 0, Loss: 0.663120
Train - Epoch 89, Batch: 0, Loss: 0.662623
Train - Epoch 90, Batch: 0, Loss: 0.662572
Train - Epoch 91, Batch: 0, Loss: 0.662221
Train - Epoch 92, Batch: 0, Loss: 0.661810
Train - Epoch 93, Batch: 0, Loss: 0.662228
Train - Epoch 94, Batch: 0, Loss: 0.660905
Train - Epoch 95, Batch: 0, Loss: 0.660911
Train - Epoch 96, Batch: 0, Loss: 0.660451
Train - Epoch 97, Batch: 0, Loss: 0.660286
Train - Epoch 98, Batch: 0, Loss: 0.660145
Train - Epoch 99, Batch: 0, Loss: 0.659508
Train - Epoch 100, Batch: 0, Loss: 0.659377
Train - Epoch 101, Batch: 0, Loss: 0.659374
Train - Epoch 102, Batch: 0, Loss: 0.659097
Train - Epoch 103, Batch: 0, Loss: 0.658493
Train - Epoch 104, Batch: 0, Loss: 0.657993
Train - Epoch 105, Batch: 0, Loss: 0.657920
Train - Epoch 106, Batch: 0, Loss: 0.657944
Train - Epoch 107, Batch: 0, Loss: 0.657623
Train - Epoch 108, Batch: 0, Loss: 0.657083
Train - Epoch 109, Batch: 0, Loss: 0.656588
Train - Epoch 110, Batch: 0, Loss: 0.656689
Train - Epoch 111, Batch: 0, Loss: 0.656414
Train - Epoch 112, Batch: 0, Loss: 0.655833
Train - Epoch 113, Batch: 0, Loss: 0.655131
Train - Epoch 114, Batch: 0, Loss: 0.655394
Train - Epoch 115, Batch: 0, Loss: 0.655192
Train - Epoch 116, Batch: 0, Loss: 0.654892
Train - Epoch 117, Batch: 0, Loss: 0.654721
Train - Epoch 118, Batch: 0, Loss: 0.653997
Train - Epoch 119, Batch: 0, Loss: 0.654192
Train - Epoch 120, Batch: 0, Loss: 0.654288
Train - Epoch 121, Batch: 0, Loss: 0.653388
Train - Epoch 122, Batch: 0, Loss: 0.653032
Train - Epoch 123, Batch: 0, Loss: 0.652284
Train - Epoch 124, Batch: 0, Loss: 0.652490
Train - Epoch 125, Batch: 0, Loss: 0.652203
Train - Epoch 126, Batch: 0, Loss: 0.652088
Train - Epoch 127, Batch: 0, Loss: 0.651677
Train - Epoch 128, Batch: 0, Loss: 0.651580
Train - Epoch 129, Batch: 0, Loss: 0.651053
Train - Epoch 130, Batch: 0, Loss: 0.650924
Train - Epoch 131, Batch: 0, Loss: 0.650654
Train - Epoch 132, Batch: 0, Loss: 0.650189
Train - Epoch 133, Batch: 0, Loss: 0.649949
Train - Epoch 134, Batch: 0, Loss: 0.649956
Train - Epoch 135, Batch: 0, Loss: 0.649283
Train - Epoch 136, Batch: 0, Loss: 0.648691
Train - Epoch 137, Batch: 0, Loss: 0.648498
Train - Epoch 138, Batch: 0, Loss: 0.648532
Train - Epoch 139, Batch: 0, Loss: 0.648373
Train - Epoch 140, Batch: 0, Loss: 0.647733
Train - Epoch 141, Batch: 0, Loss: 0.647709
Train - Epoch 142, Batch: 0, Loss: 0.647606
Train - Epoch 143, Batch: 0, Loss: 0.647097
Train - Epoch 144, Batch: 0, Loss: 0.647389
Train - Epoch 145, Batch: 0, Loss: 0.646575
Train - Epoch 146, Batch: 0, Loss: 0.646811
Train - Epoch 147, Batch: 0, Loss: 0.646003
Train - Epoch 148, Batch: 0, Loss: 0.645411
Train - Epoch 149, Batch: 0, Loss: 0.645394
Train - Epoch 150, Batch: 0, Loss: 0.645477
Train - Epoch 151, Batch: 0, Loss: 0.645543
Train - Epoch 152, Batch: 0, Loss: 0.644638
Train - Epoch 153, Batch: 0, Loss: 0.644242
Train - Epoch 154, Batch: 0, Loss: 0.643895
Train - Epoch 155, Batch: 0, Loss: 0.644061
Train - Epoch 156, Batch: 0, Loss: 0.643538
Train - Epoch 157, Batch: 0, Loss: 0.643159
Train - Epoch 158, Batch: 0, Loss: 0.643255
Train - Epoch 159, Batch: 0, Loss: 0.642665
Train - Epoch 160, Batch: 0, Loss: 0.642770
Train - Epoch 161, Batch: 0, Loss: 0.642190
Train - Epoch 162, Batch: 0, Loss: 0.642209
Train - Epoch 163, Batch: 0, Loss: 0.642058
Train - Epoch 164, Batch: 0, Loss: 0.641758
Train - Epoch 165, Batch: 0, Loss: 0.641012
Train - Epoch 166, Batch: 0, Loss: 0.641453
Train - Epoch 167, Batch: 0, Loss: 0.640868
Train - Epoch 168, Batch: 0, Loss: 0.640610
Train - Epoch 169, Batch: 0, Loss: 0.639625
Train - Epoch 170, Batch: 0, Loss: 0.640144
Train - Epoch 171, Batch: 0, Loss: 0.640157
Train - Epoch 172, Batch: 0, Loss: 0.639880
Train - Epoch 173, Batch: 0, Loss: 0.639514
Train - Epoch 174, Batch: 0, Loss: 0.639423
Train - Epoch 175, Batch: 0, Loss: 0.638085
Train - Epoch 176, Batch: 0, Loss: 0.638914
Train - Epoch 177, Batch: 0, Loss: 0.638589
Train - Epoch 178, Batch: 0, Loss: 0.638399
Train - Epoch 179, Batch: 0, Loss: 0.638199
Train - Epoch 180, Batch: 0, Loss: 0.637263
Train - Epoch 181, Batch: 0, Loss: 0.637010
Train - Epoch 182, Batch: 0, Loss: 0.637115
Train - Epoch 183, Batch: 0, Loss: 0.637037
Train - Epoch 184, Batch: 0, Loss: 0.636298
Train - Epoch 185, Batch: 0, Loss: 0.635994
Train - Epoch 186, Batch: 0, Loss: 0.635806/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.636224
Train - Epoch 188, Batch: 0, Loss: 0.635587
Train - Epoch 189, Batch: 0, Loss: 0.635487
Train - Epoch 190, Batch: 0, Loss: 0.635052
Train - Epoch 191, Batch: 0, Loss: 0.634388
Train - Epoch 192, Batch: 0, Loss: 0.634159
Train - Epoch 193, Batch: 0, Loss: 0.634099
Train - Epoch 194, Batch: 0, Loss: 0.633578
Train - Epoch 195, Batch: 0, Loss: 0.633963
Train - Epoch 196, Batch: 0, Loss: 0.633233
Train - Epoch 197, Batch: 0, Loss: 0.633370
Train - Epoch 198, Batch: 0, Loss: 0.633168
Train - Epoch 199, Batch: 0, Loss: 0.632901
training_time:: 351.79310274124146
training time full:: 351.79314947128296
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927280
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 2
training time is 297.8678138256073
overhead:: 0
overhead2:: 0
time_baseline:: 297.86848068237305
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927428
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624896)
RCV1 Test Avg. Accuracy:: 0.9224932425350495
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.01690387725830078
overhead3:: 0.11369633674621582
overhead4:: 35.243433237075806
overhead5:: 0
time_provenance:: 44.92057824134827
curr_diff: 0 tensor(9.0196e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0196e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927428
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.0001_10200
tensor(624896)
RCV1 Test Avg. Accuracy:: 0.9224932425350495
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693229
Train - Epoch 1, Batch: 0, Loss: 0.692811
Train - Epoch 2, Batch: 0, Loss: 0.692470
Train - Epoch 3, Batch: 0, Loss: 0.692082
Train - Epoch 4, Batch: 0, Loss: 0.691704
Train - Epoch 5, Batch: 0, Loss: 0.691363
Train - Epoch 6, Batch: 0, Loss: 0.690953
Train - Epoch 7, Batch: 0, Loss: 0.690608
Train - Epoch 8, Batch: 0, Loss: 0.690235
Train - Epoch 9, Batch: 0, Loss: 0.689827
Train - Epoch 10, Batch: 0, Loss: 0.689560
Train - Epoch 11, Batch: 0, Loss: 0.689128
Train - Epoch 12, Batch: 0, Loss: 0.688800
Train - Epoch 13, Batch: 0, Loss: 0.688418
Train - Epoch 14, Batch: 0, Loss: 0.688088
Train - Epoch 15, Batch: 0, Loss: 0.687710
Train - Epoch 16, Batch: 0, Loss: 0.687268
Train - Epoch 17, Batch: 0, Loss: 0.686896
Train - Epoch 18, Batch: 0, Loss: 0.686554
Train - Epoch 19, Batch: 0, Loss: 0.686201
Train - Epoch 20, Batch: 0, Loss: 0.685893
Train - Epoch 21, Batch: 0, Loss: 0.685548
Train - Epoch 22, Batch: 0, Loss: 0.685120
Train - Epoch 23, Batch: 0, Loss: 0.684784
Train - Epoch 24, Batch: 0, Loss: 0.684483
Train - Epoch 25, Batch: 0, Loss: 0.684064
Train - Epoch 26, Batch: 0, Loss: 0.683712
Train - Epoch 27, Batch: 0, Loss: 0.683386
Train - Epoch 28, Batch: 0, Loss: 0.683126
Train - Epoch 29, Batch: 0, Loss: 0.682626
Train - Epoch 30, Batch: 0, Loss: 0.682238
Train - Epoch 31, Batch: 0, Loss: 0.682066
Train - Epoch 32, Batch: 0, Loss: 0.681711
Train - Epoch 33, Batch: 0, Loss: 0.681282
Train - Epoch 34, Batch: 0, Loss: 0.680857
Train - Epoch 35, Batch: 0, Loss: 0.680540
Train - Epoch 36, Batch: 0, Loss: 0.680256
Train - Epoch 37, Batch: 0, Loss: 0.679838
Train - Epoch 38, Batch: 0, Loss: 0.679489
Train - Epoch 39, Batch: 0, Loss: 0.679142
Train - Epoch 40, Batch: 0, Loss: 0.678900
Train - Epoch 41, Batch: 0, Loss: 0.678465
Train - Epoch 42, Batch: 0, Loss: 0.678212
Train - Epoch 43, Batch: 0, Loss: 0.677873
Train - Epoch 44, Batch: 0, Loss: 0.677636
Train - Epoch 45, Batch: 0, Loss: 0.677167
Train - Epoch 46, Batch: 0, Loss: 0.676901
Train - Epoch 47, Batch: 0, Loss: 0.676415
Train - Epoch 48, Batch: 0, Loss: 0.676215
Train - Epoch 49, Batch: 0, Loss: 0.675803
Train - Epoch 50, Batch: 0, Loss: 0.675532
Train - Epoch 51, Batch: 0, Loss: 0.675134
Train - Epoch 52, Batch: 0, Loss: 0.674849
Train - Epoch 53, Batch: 0, Loss: 0.674476
Train - Epoch 54, Batch: 0, Loss: 0.674137
Train - Epoch 55, Batch: 0, Loss: 0.673863
Train - Epoch 56, Batch: 0, Loss: 0.673600
Train - Epoch 57, Batch: 0, Loss: 0.673183
Train - Epoch 58, Batch: 0, Loss: 0.672965
Train - Epoch 59, Batch: 0, Loss: 0.672469
Train - Epoch 60, Batch: 0, Loss: 0.672143
Train - Epoch 61, Batch: 0, Loss: 0.671956
Train - Epoch 62, Batch: 0, Loss: 0.671375
Train - Epoch 63, Batch: 0, Loss: 0.671215
Train - Epoch 64, Batch: 0, Loss: 0.670887
Train - Epoch 65, Batch: 0, Loss: 0.670453
Train - Epoch 66, Batch: 0, Loss: 0.670173
Train - Epoch 67, Batch: 0, Loss: 0.669976
Train - Epoch 68, Batch: 0, Loss: 0.669481
Train - Epoch 69, Batch: 0, Loss: 0.669245
Train - Epoch 70, Batch: 0, Loss: 0.669075
Train - Epoch 71, Batch: 0, Loss: 0.668775
Train - Epoch 72, Batch: 0, Loss: 0.668032
Train - Epoch 73, Batch: 0, Loss: 0.668178
Train - Epoch 74, Batch: 0, Loss: 0.667618
Train - Epoch 75, Batch: 0, Loss: 0.667296
Train - Epoch 76, Batch: 0, Loss: 0.666780
Train - Epoch 77, Batch: 0, Loss: 0.666801
Train - Epoch 78, Batch: 0, Loss: 0.666226
Train - Epoch 79, Batch: 0, Loss: 0.666144
Train - Epoch 80, Batch: 0, Loss: 0.665786
Train - Epoch 81, Batch: 0, Loss: 0.665461
Train - Epoch 82, Batch: 0, Loss: 0.664946
Train - Epoch 83, Batch: 0, Loss: 0.664855
Train - Epoch 84, Batch: 0, Loss: 0.664427
Train - Epoch 85, Batch: 0, Loss: 0.664270
Train - Epoch 86, Batch: 0, Loss: 0.663500
Train - Epoch 87, Batch: 0, Loss: 0.663347
Train - Epoch 88, Batch: 0, Loss: 0.663381
Train - Epoch 89, Batch: 0, Loss: 0.662947
Train - Epoch 90, Batch: 0, Loss: 0.663131
Train - Epoch 91, Batch: 0, Loss: 0.662346
Train - Epoch 92, Batch: 0, Loss: 0.662048
Train - Epoch 93, Batch: 0, Loss: 0.661780
Train - Epoch 94, Batch: 0, Loss: 0.661512
Train - Epoch 95, Batch: 0, Loss: 0.661042
Train - Epoch 96, Batch: 0, Loss: 0.661082
Train - Epoch 97, Batch: 0, Loss: 0.660405
Train - Epoch 98, Batch: 0, Loss: 0.660089
Train - Epoch 99, Batch: 0, Loss: 0.659983
Train - Epoch 100, Batch: 0, Loss: 0.659800
Train - Epoch 101, Batch: 0, Loss: 0.659204
Train - Epoch 102, Batch: 0, Loss: 0.659021
Train - Epoch 103, Batch: 0, Loss: 0.659052
Train - Epoch 104, Batch: 0, Loss: 0.658678
Train - Epoch 105, Batch: 0, Loss: 0.658281
Train - Epoch 106, Batch: 0, Loss: 0.658060
Train - Epoch 107, Batch: 0, Loss: 0.657472
Train - Epoch 108, Batch: 0, Loss: 0.657190
Train - Epoch 109, Batch: 0, Loss: 0.656623
Train - Epoch 110, Batch: 0, Loss: 0.656693
Train - Epoch 111, Batch: 0, Loss: 0.656584
Train - Epoch 112, Batch: 0, Loss: 0.656036
Train - Epoch 113, Batch: 0, Loss: 0.656114
Train - Epoch 114, Batch: 0, Loss: 0.655385
Train - Epoch 115, Batch: 0, Loss: 0.655198
Train - Epoch 116, Batch: 0, Loss: 0.655061
Train - Epoch 117, Batch: 0, Loss: 0.654551
Train - Epoch 118, Batch: 0, Loss: 0.653972
Train - Epoch 119, Batch: 0, Loss: 0.654002
Train - Epoch 120, Batch: 0, Loss: 0.653962
Train - Epoch 121, Batch: 0, Loss: 0.653459
Train - Epoch 122, Batch: 0, Loss: 0.652801
Train - Epoch 123, Batch: 0, Loss: 0.653119
Train - Epoch 124, Batch: 0, Loss: 0.652232
Train - Epoch 125, Batch: 0, Loss: 0.652526
Train - Epoch 126, Batch: 0, Loss: 0.651976
Train - Epoch 127, Batch: 0, Loss: 0.651486
Train - Epoch 128, Batch: 0, Loss: 0.651462
Train - Epoch 129, Batch: 0, Loss: 0.651442
Train - Epoch 130, Batch: 0, Loss: 0.651149
Train - Epoch 131, Batch: 0, Loss: 0.650480
Train - Epoch 132, Batch: 0, Loss: 0.650090
Train - Epoch 133, Batch: 0, Loss: 0.650467
Train - Epoch 134, Batch: 0, Loss: 0.650039
Train - Epoch 135, Batch: 0, Loss: 0.649713
Train - Epoch 136, Batch: 0, Loss: 0.649249
Train - Epoch 137, Batch: 0, Loss: 0.649013
Train - Epoch 138, Batch: 0, Loss: 0.648370
Train - Epoch 139, Batch: 0, Loss: 0.648363
Train - Epoch 140, Batch: 0, Loss: 0.648029
Train - Epoch 141, Batch: 0, Loss: 0.647837
Train - Epoch 142, Batch: 0, Loss: 0.647582
Train - Epoch 143, Batch: 0, Loss: 0.647267
Train - Epoch 144, Batch: 0, Loss: 0.647104
Train - Epoch 145, Batch: 0, Loss: 0.646577
Train - Epoch 146, Batch: 0, Loss: 0.646135
Train - Epoch 147, Batch: 0, Loss: 0.646412
Train - Epoch 148, Batch: 0, Loss: 0.646117
Train - Epoch 149, Batch: 0, Loss: 0.645610
Train - Epoch 150, Batch: 0, Loss: 0.645447
Train - Epoch 151, Batch: 0, Loss: 0.645120
Train - Epoch 152, Batch: 0, Loss: 0.645255
Train - Epoch 153, Batch: 0, Loss: 0.645019
Train - Epoch 154, Batch: 0, Loss: 0.644418
Train - Epoch 155, Batch: 0, Loss: 0.644241
Train - Epoch 156, Batch: 0, Loss: 0.643734
Train - Epoch 157, Batch: 0, Loss: 0.643259
Train - Epoch 158, Batch: 0, Loss: 0.643779
Train - Epoch 159, Batch: 0, Loss: 0.643140
Train - Epoch 160, Batch: 0, Loss: 0.642496
Train - Epoch 161, Batch: 0, Loss: 0.642408
Train - Epoch 162, Batch: 0, Loss: 0.642446
Train - Epoch 163, Batch: 0, Loss: 0.642174
Train - Epoch 164, Batch: 0, Loss: 0.641862
Train - Epoch 165, Batch: 0, Loss: 0.641539
Train - Epoch 166, Batch: 0, Loss: 0.641513
Train - Epoch 167, Batch: 0, Loss: 0.641199
Train - Epoch 168, Batch: 0, Loss: 0.640141
Train - Epoch 169, Batch: 0, Loss: 0.640805
Train - Epoch 170, Batch: 0, Loss: 0.640936
Train - Epoch 171, Batch: 0, Loss: 0.639862
Train - Epoch 172, Batch: 0, Loss: 0.639946
Train - Epoch 173, Batch: 0, Loss: 0.639280
Train - Epoch 174, Batch: 0, Loss: 0.638854
Train - Epoch 175, Batch: 0, Loss: 0.639065
Train - Epoch 176, Batch: 0, Loss: 0.638787
Train - Epoch 177, Batch: 0, Loss: 0.638564
Train - Epoch 178, Batch: 0, Loss: 0.638232
Train - Epoch 179, Batch: 0, Loss: 0.638814
Train - Epoch 180, Batch: 0, Loss: 0.638587
Train - Epoch 181, Batch: 0, Loss: 0.637322
Train - Epoch 182, Batch: 0, Loss: 0.637366
Train - Epoch 183, Batch: 0, Loss: 0.637328
Train - Epoch 184, Batch: 0, Loss: 0.636694
Train - Epoch 185, Batch: 0, Loss: 0.636759
Train - Epoch 186, Batch: 0, Loss: 0.635840/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635972
Train - Epoch 188, Batch: 0, Loss: 0.635639
Train - Epoch 189, Batch: 0, Loss: 0.635769
Train - Epoch 190, Batch: 0, Loss: 0.635158
Train - Epoch 191, Batch: 0, Loss: 0.635011
Train - Epoch 192, Batch: 0, Loss: 0.634582
Train - Epoch 193, Batch: 0, Loss: 0.634518
Train - Epoch 194, Batch: 0, Loss: 0.634098
Train - Epoch 195, Batch: 0, Loss: 0.633857
Train - Epoch 196, Batch: 0, Loss: 0.634135
Train - Epoch 197, Batch: 0, Loss: 0.633461
Train - Epoch 198, Batch: 0, Loss: 0.632741
Train - Epoch 199, Batch: 0, Loss: 0.632803
training_time:: 350.6304612159729
training time full:: 350.63050413131714
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926885
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 2
training time is 298.17761993408203
overhead:: 0
overhead2:: 0
time_baseline:: 298.1783571243286
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926885
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624737)
RCV1 Test Avg. Accuracy:: 0.9222585211965179
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2
max_epoch:: 200
overhead:: 0
overhead2:: 0.01644730567932129
overhead3:: 0.1113739013671875
overhead4:: 35.14032745361328
overhead5:: 0
time_provenance:: 45.40199685096741
curr_diff: 0 tensor(7.8931e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8931e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926885
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.0001_10200
tensor(624737)
RCV1 Test Avg. Accuracy:: 0.9222585211965179
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  rcv1 0
tensor([13216, 19652,   100, 17192,  2985, 17325, 18989, 14577, 10773,   155])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693275
Train - Epoch 1, Batch: 0, Loss: 0.692918
Train - Epoch 2, Batch: 0, Loss: 0.692549
Train - Epoch 3, Batch: 0, Loss: 0.692179
Train - Epoch 4, Batch: 0, Loss: 0.691799
Train - Epoch 5, Batch: 0, Loss: 0.691425
Train - Epoch 6, Batch: 0, Loss: 0.691082
Train - Epoch 7, Batch: 0, Loss: 0.690681
Train - Epoch 8, Batch: 0, Loss: 0.690270
Train - Epoch 9, Batch: 0, Loss: 0.689936
Train - Epoch 10, Batch: 0, Loss: 0.689579
Train - Epoch 11, Batch: 0, Loss: 0.689216
Train - Epoch 12, Batch: 0, Loss: 0.688833
Train - Epoch 13, Batch: 0, Loss: 0.688494
Train - Epoch 14, Batch: 0, Loss: 0.688119
Train - Epoch 15, Batch: 0, Loss: 0.687741
Train - Epoch 16, Batch: 0, Loss: 0.687315
Train - Epoch 17, Batch: 0, Loss: 0.687052
Train - Epoch 18, Batch: 0, Loss: 0.686611
Train - Epoch 19, Batch: 0, Loss: 0.686362
Train - Epoch 20, Batch: 0, Loss: 0.685924
Train - Epoch 21, Batch: 0, Loss: 0.685642
Train - Epoch 22, Batch: 0, Loss: 0.685256
Train - Epoch 23, Batch: 0, Loss: 0.684807
Train - Epoch 24, Batch: 0, Loss: 0.684501
Train - Epoch 25, Batch: 0, Loss: 0.684188
Train - Epoch 26, Batch: 0, Loss: 0.683877
Train - Epoch 27, Batch: 0, Loss: 0.683475
Train - Epoch 28, Batch: 0, Loss: 0.682947
Train - Epoch 29, Batch: 0, Loss: 0.682866
Train - Epoch 30, Batch: 0, Loss: 0.682494
Train - Epoch 31, Batch: 0, Loss: 0.682010
Train - Epoch 32, Batch: 0, Loss: 0.681721
Train - Epoch 33, Batch: 0, Loss: 0.681364
Train - Epoch 34, Batch: 0, Loss: 0.680916
Train - Epoch 35, Batch: 0, Loss: 0.680643
Train - Epoch 36, Batch: 0, Loss: 0.680269
Train - Epoch 37, Batch: 0, Loss: 0.679874
Train - Epoch 38, Batch: 0, Loss: 0.679588
Train - Epoch 39, Batch: 0, Loss: 0.679360
Train - Epoch 40, Batch: 0, Loss: 0.678942
Train - Epoch 41, Batch: 0, Loss: 0.678667
Train - Epoch 42, Batch: 0, Loss: 0.678308
Train - Epoch 43, Batch: 0, Loss: 0.677889
Train - Epoch 44, Batch: 0, Loss: 0.677527
Train - Epoch 45, Batch: 0, Loss: 0.677188
Train - Epoch 46, Batch: 0, Loss: 0.676809
Train - Epoch 47, Batch: 0, Loss: 0.676574
Train - Epoch 48, Batch: 0, Loss: 0.676278
Train - Epoch 49, Batch: 0, Loss: 0.675890
Train - Epoch 50, Batch: 0, Loss: 0.675545
Train - Epoch 51, Batch: 0, Loss: 0.675235
Train - Epoch 52, Batch: 0, Loss: 0.674882
Train - Epoch 53, Batch: 0, Loss: 0.674440
Train - Epoch 54, Batch: 0, Loss: 0.674140
Train - Epoch 55, Batch: 0, Loss: 0.673843
Train - Epoch 56, Batch: 0, Loss: 0.673683
Train - Epoch 57, Batch: 0, Loss: 0.673297
Train - Epoch 58, Batch: 0, Loss: 0.673034
Train - Epoch 59, Batch: 0, Loss: 0.672803
Train - Epoch 60, Batch: 0, Loss: 0.672270
Train - Epoch 61, Batch: 0, Loss: 0.671718
Train - Epoch 62, Batch: 0, Loss: 0.671522
Train - Epoch 63, Batch: 0, Loss: 0.671283
Train - Epoch 64, Batch: 0, Loss: 0.670814
Train - Epoch 65, Batch: 0, Loss: 0.670809
Train - Epoch 66, Batch: 0, Loss: 0.670447
Train - Epoch 67, Batch: 0, Loss: 0.670187
Train - Epoch 68, Batch: 0, Loss: 0.669519
Train - Epoch 69, Batch: 0, Loss: 0.669286
Train - Epoch 70, Batch: 0, Loss: 0.668912
Train - Epoch 71, Batch: 0, Loss: 0.668733
Train - Epoch 72, Batch: 0, Loss: 0.668420
Train - Epoch 73, Batch: 0, Loss: 0.668024
Train - Epoch 74, Batch: 0, Loss: 0.667319
Train - Epoch 75, Batch: 0, Loss: 0.667387
Train - Epoch 76, Batch: 0, Loss: 0.667005
Train - Epoch 77, Batch: 0, Loss: 0.666858
Train - Epoch 78, Batch: 0, Loss: 0.666685
Train - Epoch 79, Batch: 0, Loss: 0.666253
Train - Epoch 80, Batch: 0, Loss: 0.665975
Train - Epoch 81, Batch: 0, Loss: 0.665284
Train - Epoch 82, Batch: 0, Loss: 0.665273
Train - Epoch 83, Batch: 0, Loss: 0.664731
Train - Epoch 84, Batch: 0, Loss: 0.664807
Train - Epoch 85, Batch: 0, Loss: 0.664477
Train - Epoch 86, Batch: 0, Loss: 0.663858
Train - Epoch 87, Batch: 0, Loss: 0.663557
Train - Epoch 88, Batch: 0, Loss: 0.663226
Train - Epoch 89, Batch: 0, Loss: 0.663025
Train - Epoch 90, Batch: 0, Loss: 0.662913
Train - Epoch 91, Batch: 0, Loss: 0.662548
Train - Epoch 92, Batch: 0, Loss: 0.662279
Train - Epoch 93, Batch: 0, Loss: 0.661932
Train - Epoch 94, Batch: 0, Loss: 0.661444
Train - Epoch 95, Batch: 0, Loss: 0.661060
Train - Epoch 96, Batch: 0, Loss: 0.661048
Train - Epoch 97, Batch: 0, Loss: 0.660591
Train - Epoch 98, Batch: 0, Loss: 0.660210
Train - Epoch 99, Batch: 0, Loss: 0.660024
Train - Epoch 100, Batch: 0, Loss: 0.659798
Train - Epoch 101, Batch: 0, Loss: 0.659602
Train - Epoch 102, Batch: 0, Loss: 0.658946
Train - Epoch 103, Batch: 0, Loss: 0.659036
Train - Epoch 104, Batch: 0, Loss: 0.658821
Train - Epoch 105, Batch: 0, Loss: 0.658170
Train - Epoch 106, Batch: 0, Loss: 0.657775
Train - Epoch 107, Batch: 0, Loss: 0.657460
Train - Epoch 108, Batch: 0, Loss: 0.657339
Train - Epoch 109, Batch: 0, Loss: 0.657014
Train - Epoch 110, Batch: 0, Loss: 0.656606
Train - Epoch 111, Batch: 0, Loss: 0.656002
Train - Epoch 112, Batch: 0, Loss: 0.656516
Train - Epoch 113, Batch: 0, Loss: 0.655573
Train - Epoch 114, Batch: 0, Loss: 0.655704
Train - Epoch 115, Batch: 0, Loss: 0.654931
Train - Epoch 116, Batch: 0, Loss: 0.654661
Train - Epoch 117, Batch: 0, Loss: 0.655124
Train - Epoch 118, Batch: 0, Loss: 0.654570
Train - Epoch 119, Batch: 0, Loss: 0.654197
Train - Epoch 120, Batch: 0, Loss: 0.654076
Train - Epoch 121, Batch: 0, Loss: 0.653504
Train - Epoch 122, Batch: 0, Loss: 0.653119
Train - Epoch 123, Batch: 0, Loss: 0.653041
Train - Epoch 124, Batch: 0, Loss: 0.652376
Train - Epoch 125, Batch: 0, Loss: 0.652199
Train - Epoch 126, Batch: 0, Loss: 0.652004
Train - Epoch 127, Batch: 0, Loss: 0.651965
Train - Epoch 128, Batch: 0, Loss: 0.651712
Train - Epoch 129, Batch: 0, Loss: 0.651025
Train - Epoch 130, Batch: 0, Loss: 0.651459
Train - Epoch 131, Batch: 0, Loss: 0.650780
Train - Epoch 132, Batch: 0, Loss: 0.650843
Train - Epoch 133, Batch: 0, Loss: 0.649870
Train - Epoch 134, Batch: 0, Loss: 0.649454
Train - Epoch 135, Batch: 0, Loss: 0.649432
Train - Epoch 136, Batch: 0, Loss: 0.649485
Train - Epoch 137, Batch: 0, Loss: 0.648734
Train - Epoch 138, Batch: 0, Loss: 0.648496
Train - Epoch 139, Batch: 0, Loss: 0.648494
Train - Epoch 140, Batch: 0, Loss: 0.648404
Train - Epoch 141, Batch: 0, Loss: 0.648275
Train - Epoch 142, Batch: 0, Loss: 0.647202
Train - Epoch 143, Batch: 0, Loss: 0.647489
Train - Epoch 144, Batch: 0, Loss: 0.647347
Train - Epoch 145, Batch: 0, Loss: 0.646867
Train - Epoch 146, Batch: 0, Loss: 0.647031
Train - Epoch 147, Batch: 0, Loss: 0.646376
Train - Epoch 148, Batch: 0, Loss: 0.645987
Train - Epoch 149, Batch: 0, Loss: 0.645511
Train - Epoch 150, Batch: 0, Loss: 0.645790
Train - Epoch 151, Batch: 0, Loss: 0.645006
Train - Epoch 152, Batch: 0, Loss: 0.644903
Train - Epoch 153, Batch: 0, Loss: 0.644911
Train - Epoch 154, Batch: 0, Loss: 0.644253
Train - Epoch 155, Batch: 0, Loss: 0.644136
Train - Epoch 156, Batch: 0, Loss: 0.643482
Train - Epoch 157, Batch: 0, Loss: 0.643311
Train - Epoch 158, Batch: 0, Loss: 0.643606
Train - Epoch 159, Batch: 0, Loss: 0.642777
Train - Epoch 160, Batch: 0, Loss: 0.643072
Train - Epoch 161, Batch: 0, Loss: 0.643202
Train - Epoch 162, Batch: 0, Loss: 0.642544
Train - Epoch 163, Batch: 0, Loss: 0.642003
Train - Epoch 164, Batch: 0, Loss: 0.641630
Train - Epoch 165, Batch: 0, Loss: 0.640780
Train - Epoch 166, Batch: 0, Loss: 0.641674
Train - Epoch 167, Batch: 0, Loss: 0.641211
Train - Epoch 168, Batch: 0, Loss: 0.641023
Train - Epoch 169, Batch: 0, Loss: 0.641043
Train - Epoch 170, Batch: 0, Loss: 0.639965
Train - Epoch 171, Batch: 0, Loss: 0.639930
Train - Epoch 172, Batch: 0, Loss: 0.639871
Train - Epoch 173, Batch: 0, Loss: 0.638954
Train - Epoch 174, Batch: 0, Loss: 0.638908
Train - Epoch 175, Batch: 0, Loss: 0.638658
Train - Epoch 176, Batch: 0, Loss: 0.638887
Train - Epoch 177, Batch: 0, Loss: 0.638804
Train - Epoch 178, Batch: 0, Loss: 0.638240
Train - Epoch 179, Batch: 0, Loss: 0.637937
Train - Epoch 180, Batch: 0, Loss: 0.637606
Train - Epoch 181, Batch: 0, Loss: 0.637480
Train - Epoch 182, Batch: 0, Loss: 0.637276
Train - Epoch 183, Batch: 0, Loss: 0.636933
Train - Epoch 184, Batch: 0, Loss: 0.636679
Train - Epoch 185, Batch: 0, Loss: 0.636330
Train - Epoch 186, Batch: 0, Loss: 0.636814/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.636035
Train - Epoch 188, Batch: 0, Loss: 0.635257
Train - Epoch 189, Batch: 0, Loss: 0.635404
Train - Epoch 190, Batch: 0, Loss: 0.634857
Train - Epoch 191, Batch: 0, Loss: 0.635424
Train - Epoch 192, Batch: 0, Loss: 0.635028
Train - Epoch 193, Batch: 0, Loss: 0.634808
Train - Epoch 194, Batch: 0, Loss: 0.634687
Train - Epoch 195, Batch: 0, Loss: 0.634069
Train - Epoch 196, Batch: 0, Loss: 0.633316
Train - Epoch 197, Batch: 0, Loss: 0.633792
Train - Epoch 198, Batch: 0, Loss: 0.633403
Train - Epoch 199, Batch: 0, Loss: 0.632999
training_time:: 351.65988540649414
training time full:: 351.6599326133728
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927823
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 10
training time is 298.7257330417633
overhead:: 0
overhead2:: 0
time_baseline:: 298.7264516353607
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927823
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624849)
RCV1 Test Avg. Accuracy:: 0.9224238594978735
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.02322220802307129
overhead3:: 0.11185169219970703
overhead4:: 35.068368911743164
overhead5:: 0
time_provenance:: 45.906697034835815
curr_diff: 0 tensor(2.1157e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1157e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927823
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.0005_10200
tensor(624848)
RCV1 Test Avg. Accuracy:: 0.9224223832630399
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693038
Train - Epoch 1, Batch: 0, Loss: 0.692676
Train - Epoch 2, Batch: 0, Loss: 0.692272
Train - Epoch 3, Batch: 0, Loss: 0.691919
Train - Epoch 4, Batch: 0, Loss: 0.691558
Train - Epoch 5, Batch: 0, Loss: 0.691231
Train - Epoch 6, Batch: 0, Loss: 0.690815
Train - Epoch 7, Batch: 0, Loss: 0.690434
Train - Epoch 8, Batch: 0, Loss: 0.690080
Train - Epoch 9, Batch: 0, Loss: 0.689694
Train - Epoch 10, Batch: 0, Loss: 0.689309
Train - Epoch 11, Batch: 0, Loss: 0.688975
Train - Epoch 12, Batch: 0, Loss: 0.688602
Train - Epoch 13, Batch: 0, Loss: 0.688258
Train - Epoch 14, Batch: 0, Loss: 0.687894
Train - Epoch 15, Batch: 0, Loss: 0.687494
Train - Epoch 16, Batch: 0, Loss: 0.687213
Train - Epoch 17, Batch: 0, Loss: 0.686783
Train - Epoch 18, Batch: 0, Loss: 0.686363
Train - Epoch 19, Batch: 0, Loss: 0.686065
Train - Epoch 20, Batch: 0, Loss: 0.685753
Train - Epoch 21, Batch: 0, Loss: 0.685329
Train - Epoch 22, Batch: 0, Loss: 0.685000
Train - Epoch 23, Batch: 0, Loss: 0.684636
Train - Epoch 24, Batch: 0, Loss: 0.684302
Train - Epoch 25, Batch: 0, Loss: 0.683889
Train - Epoch 26, Batch: 0, Loss: 0.683513
Train - Epoch 27, Batch: 0, Loss: 0.683259
Train - Epoch 28, Batch: 0, Loss: 0.682919
Train - Epoch 29, Batch: 0, Loss: 0.682554
Train - Epoch 30, Batch: 0, Loss: 0.682218
Train - Epoch 31, Batch: 0, Loss: 0.681905
Train - Epoch 32, Batch: 0, Loss: 0.681509
Train - Epoch 33, Batch: 0, Loss: 0.681162
Train - Epoch 34, Batch: 0, Loss: 0.680836
Train - Epoch 35, Batch: 0, Loss: 0.680471
Train - Epoch 36, Batch: 0, Loss: 0.680029
Train - Epoch 37, Batch: 0, Loss: 0.679658
Train - Epoch 38, Batch: 0, Loss: 0.679357
Train - Epoch 39, Batch: 0, Loss: 0.679091
Train - Epoch 40, Batch: 0, Loss: 0.678711
Train - Epoch 41, Batch: 0, Loss: 0.678288
Train - Epoch 42, Batch: 0, Loss: 0.678042
Train - Epoch 43, Batch: 0, Loss: 0.677626
Train - Epoch 44, Batch: 0, Loss: 0.677462
Train - Epoch 45, Batch: 0, Loss: 0.676940
Train - Epoch 46, Batch: 0, Loss: 0.676658
Train - Epoch 47, Batch: 0, Loss: 0.676361
Train - Epoch 48, Batch: 0, Loss: 0.676090
Train - Epoch 49, Batch: 0, Loss: 0.675505
Train - Epoch 50, Batch: 0, Loss: 0.675435
Train - Epoch 51, Batch: 0, Loss: 0.675135
Train - Epoch 52, Batch: 0, Loss: 0.674667
Train - Epoch 53, Batch: 0, Loss: 0.674236
Train - Epoch 54, Batch: 0, Loss: 0.673969
Train - Epoch 55, Batch: 0, Loss: 0.673636
Train - Epoch 56, Batch: 0, Loss: 0.673324
Train - Epoch 57, Batch: 0, Loss: 0.673020
Train - Epoch 58, Batch: 0, Loss: 0.672725
Train - Epoch 59, Batch: 0, Loss: 0.672716
Train - Epoch 60, Batch: 0, Loss: 0.672183
Train - Epoch 61, Batch: 0, Loss: 0.671605
Train - Epoch 62, Batch: 0, Loss: 0.671405
Train - Epoch 63, Batch: 0, Loss: 0.671072
Train - Epoch 64, Batch: 0, Loss: 0.670671
Train - Epoch 65, Batch: 0, Loss: 0.670276
Train - Epoch 66, Batch: 0, Loss: 0.670119
Train - Epoch 67, Batch: 0, Loss: 0.669853
Train - Epoch 68, Batch: 0, Loss: 0.669756
Train - Epoch 69, Batch: 0, Loss: 0.669103
Train - Epoch 70, Batch: 0, Loss: 0.668718
Train - Epoch 71, Batch: 0, Loss: 0.668503
Train - Epoch 72, Batch: 0, Loss: 0.668190
Train - Epoch 73, Batch: 0, Loss: 0.667835
Train - Epoch 74, Batch: 0, Loss: 0.667762
Train - Epoch 75, Batch: 0, Loss: 0.667364
Train - Epoch 76, Batch: 0, Loss: 0.666858
Train - Epoch 77, Batch: 0, Loss: 0.666472
Train - Epoch 78, Batch: 0, Loss: 0.666297
Train - Epoch 79, Batch: 0, Loss: 0.665747
Train - Epoch 80, Batch: 0, Loss: 0.665432
Train - Epoch 81, Batch: 0, Loss: 0.665307
Train - Epoch 82, Batch: 0, Loss: 0.665073
Train - Epoch 83, Batch: 0, Loss: 0.664756
Train - Epoch 84, Batch: 0, Loss: 0.664235
Train - Epoch 85, Batch: 0, Loss: 0.664226
Train - Epoch 86, Batch: 0, Loss: 0.663491
Train - Epoch 87, Batch: 0, Loss: 0.663281
Train - Epoch 88, Batch: 0, Loss: 0.663282
Train - Epoch 89, Batch: 0, Loss: 0.662717
Train - Epoch 90, Batch: 0, Loss: 0.662258
Train - Epoch 91, Batch: 0, Loss: 0.662161
Train - Epoch 92, Batch: 0, Loss: 0.661878
Train - Epoch 93, Batch: 0, Loss: 0.661489
Train - Epoch 94, Batch: 0, Loss: 0.661174
Train - Epoch 95, Batch: 0, Loss: 0.660998
Train - Epoch 96, Batch: 0, Loss: 0.660687
Train - Epoch 97, Batch: 0, Loss: 0.660442
Train - Epoch 98, Batch: 0, Loss: 0.660041
Train - Epoch 99, Batch: 0, Loss: 0.660087
Train - Epoch 100, Batch: 0, Loss: 0.659342
Train - Epoch 101, Batch: 0, Loss: 0.659457
Train - Epoch 102, Batch: 0, Loss: 0.658780
Train - Epoch 103, Batch: 0, Loss: 0.658614
Train - Epoch 104, Batch: 0, Loss: 0.658488
Train - Epoch 105, Batch: 0, Loss: 0.657959
Train - Epoch 106, Batch: 0, Loss: 0.657750
Train - Epoch 107, Batch: 0, Loss: 0.657461
Train - Epoch 108, Batch: 0, Loss: 0.657321
Train - Epoch 109, Batch: 0, Loss: 0.657065
Train - Epoch 110, Batch: 0, Loss: 0.656270
Train - Epoch 111, Batch: 0, Loss: 0.656185
Train - Epoch 112, Batch: 0, Loss: 0.655966
Train - Epoch 113, Batch: 0, Loss: 0.655519
Train - Epoch 114, Batch: 0, Loss: 0.655384
Train - Epoch 115, Batch: 0, Loss: 0.655039
Train - Epoch 116, Batch: 0, Loss: 0.654695
Train - Epoch 117, Batch: 0, Loss: 0.654269
Train - Epoch 118, Batch: 0, Loss: 0.654344
Train - Epoch 119, Batch: 0, Loss: 0.653916
Train - Epoch 120, Batch: 0, Loss: 0.654431
Train - Epoch 121, Batch: 0, Loss: 0.653554
Train - Epoch 122, Batch: 0, Loss: 0.653333
Train - Epoch 123, Batch: 0, Loss: 0.652679
Train - Epoch 124, Batch: 0, Loss: 0.652344
Train - Epoch 125, Batch: 0, Loss: 0.652393
Train - Epoch 126, Batch: 0, Loss: 0.652126
Train - Epoch 127, Batch: 0, Loss: 0.651870
Train - Epoch 128, Batch: 0, Loss: 0.651025
Train - Epoch 129, Batch: 0, Loss: 0.650839
Train - Epoch 130, Batch: 0, Loss: 0.650962
Train - Epoch 131, Batch: 0, Loss: 0.650683
Train - Epoch 132, Batch: 0, Loss: 0.650098
Train - Epoch 133, Batch: 0, Loss: 0.650239
Train - Epoch 134, Batch: 0, Loss: 0.649746
Train - Epoch 135, Batch: 0, Loss: 0.649035
Train - Epoch 136, Batch: 0, Loss: 0.649100
Train - Epoch 137, Batch: 0, Loss: 0.648975
Train - Epoch 138, Batch: 0, Loss: 0.648903
Train - Epoch 139, Batch: 0, Loss: 0.648087
Train - Epoch 140, Batch: 0, Loss: 0.648068
Train - Epoch 141, Batch: 0, Loss: 0.647461
Train - Epoch 142, Batch: 0, Loss: 0.647657
Train - Epoch 143, Batch: 0, Loss: 0.647168
Train - Epoch 144, Batch: 0, Loss: 0.647699
Train - Epoch 145, Batch: 0, Loss: 0.646730
Train - Epoch 146, Batch: 0, Loss: 0.646011
Train - Epoch 147, Batch: 0, Loss: 0.646196
Train - Epoch 148, Batch: 0, Loss: 0.645707
Train - Epoch 149, Batch: 0, Loss: 0.645301
Train - Epoch 150, Batch: 0, Loss: 0.645488
Train - Epoch 151, Batch: 0, Loss: 0.645460
Train - Epoch 152, Batch: 0, Loss: 0.644800
Train - Epoch 153, Batch: 0, Loss: 0.644592
Train - Epoch 154, Batch: 0, Loss: 0.644524
Train - Epoch 155, Batch: 0, Loss: 0.644132
Train - Epoch 156, Batch: 0, Loss: 0.643868
Train - Epoch 157, Batch: 0, Loss: 0.643472
Train - Epoch 158, Batch: 0, Loss: 0.642978
Train - Epoch 159, Batch: 0, Loss: 0.643215
Train - Epoch 160, Batch: 0, Loss: 0.642851
Train - Epoch 161, Batch: 0, Loss: 0.643135
Train - Epoch 162, Batch: 0, Loss: 0.642367
Train - Epoch 163, Batch: 0, Loss: 0.641935
Train - Epoch 164, Batch: 0, Loss: 0.641439
Train - Epoch 165, Batch: 0, Loss: 0.641893
Train - Epoch 166, Batch: 0, Loss: 0.640644
Train - Epoch 167, Batch: 0, Loss: 0.640763
Train - Epoch 168, Batch: 0, Loss: 0.640774
Train - Epoch 169, Batch: 0, Loss: 0.640258
Train - Epoch 170, Batch: 0, Loss: 0.639748
Train - Epoch 171, Batch: 0, Loss: 0.640681
Train - Epoch 172, Batch: 0, Loss: 0.639572
Train - Epoch 173, Batch: 0, Loss: 0.638871
Train - Epoch 174, Batch: 0, Loss: 0.638848
Train - Epoch 175, Batch: 0, Loss: 0.639122
Train - Epoch 176, Batch: 0, Loss: 0.638539
Train - Epoch 177, Batch: 0, Loss: 0.638216
Train - Epoch 178, Batch: 0, Loss: 0.638234
Train - Epoch 179, Batch: 0, Loss: 0.638264
Train - Epoch 180, Batch: 0, Loss: 0.637603
Train - Epoch 181, Batch: 0, Loss: 0.637734
Train - Epoch 182, Batch: 0, Loss: 0.637413
Train - Epoch 183, Batch: 0, Loss: 0.636963
Train - Epoch 184, Batch: 0, Loss: 0.635970
Train - Epoch 185, Batch: 0, Loss: 0.636344
Train - Epoch 186, Batch: 0, Loss: 0.636016/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635745
Train - Epoch 188, Batch: 0, Loss: 0.635192
Train - Epoch 189, Batch: 0, Loss: 0.635865
Train - Epoch 190, Batch: 0, Loss: 0.635894
Train - Epoch 191, Batch: 0, Loss: 0.634922
Train - Epoch 192, Batch: 0, Loss: 0.634831
Train - Epoch 193, Batch: 0, Loss: 0.634230
Train - Epoch 194, Batch: 0, Loss: 0.633888
Train - Epoch 195, Batch: 0, Loss: 0.633928
Train - Epoch 196, Batch: 0, Loss: 0.633487
Train - Epoch 197, Batch: 0, Loss: 0.632931
Train - Epoch 198, Batch: 0, Loss: 0.632860
Train - Epoch 199, Batch: 0, Loss: 0.633029
training_time:: 350.83993101119995
training time full:: 350.8399770259857
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926045
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 10
training time is 299.3907198905945
overhead:: 0
overhead2:: 0
time_baseline:: 299.3914020061493
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926144
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624833)
RCV1 Test Avg. Accuracy:: 0.922400239740537
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.02224278450012207
overhead3:: 0.10948538780212402
overhead4:: 35.24386382102966
overhead5:: 0
time_provenance:: 45.940382957458496
curr_diff: 0 tensor(1.8776e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8776e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926144
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.0005_10200
tensor(624833)
RCV1 Test Avg. Accuracy:: 0.922400239740537
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.692979
Train - Epoch 1, Batch: 0, Loss: 0.692609
Train - Epoch 2, Batch: 0, Loss: 0.692258
Train - Epoch 3, Batch: 0, Loss: 0.691882
Train - Epoch 4, Batch: 0, Loss: 0.691496
Train - Epoch 5, Batch: 0, Loss: 0.691128
Train - Epoch 6, Batch: 0, Loss: 0.690740
Train - Epoch 7, Batch: 0, Loss: 0.690396
Train - Epoch 8, Batch: 0, Loss: 0.689994
Train - Epoch 9, Batch: 0, Loss: 0.689649
Train - Epoch 10, Batch: 0, Loss: 0.689310
Train - Epoch 11, Batch: 0, Loss: 0.688892
Train - Epoch 12, Batch: 0, Loss: 0.688555
Train - Epoch 13, Batch: 0, Loss: 0.688185
Train - Epoch 14, Batch: 0, Loss: 0.687816
Train - Epoch 15, Batch: 0, Loss: 0.687459
Train - Epoch 16, Batch: 0, Loss: 0.687115
Train - Epoch 17, Batch: 0, Loss: 0.686735
Train - Epoch 18, Batch: 0, Loss: 0.686421
Train - Epoch 19, Batch: 0, Loss: 0.686080
Train - Epoch 20, Batch: 0, Loss: 0.685634
Train - Epoch 21, Batch: 0, Loss: 0.685312
Train - Epoch 22, Batch: 0, Loss: 0.684922
Train - Epoch 23, Batch: 0, Loss: 0.684604
Train - Epoch 24, Batch: 0, Loss: 0.684265
Train - Epoch 25, Batch: 0, Loss: 0.683904
Train - Epoch 26, Batch: 0, Loss: 0.683448
Train - Epoch 27, Batch: 0, Loss: 0.683183
Train - Epoch 28, Batch: 0, Loss: 0.682765
Train - Epoch 29, Batch: 0, Loss: 0.682504
Train - Epoch 30, Batch: 0, Loss: 0.682137
Train - Epoch 31, Batch: 0, Loss: 0.681858
Train - Epoch 32, Batch: 0, Loss: 0.681413
Train - Epoch 33, Batch: 0, Loss: 0.681054
Train - Epoch 34, Batch: 0, Loss: 0.680648
Train - Epoch 35, Batch: 0, Loss: 0.680543
Train - Epoch 36, Batch: 0, Loss: 0.679985
Train - Epoch 37, Batch: 0, Loss: 0.679668
Train - Epoch 38, Batch: 0, Loss: 0.679373
Train - Epoch 39, Batch: 0, Loss: 0.679012
Train - Epoch 40, Batch: 0, Loss: 0.678869
Train - Epoch 41, Batch: 0, Loss: 0.678131
Train - Epoch 42, Batch: 0, Loss: 0.677962
Train - Epoch 43, Batch: 0, Loss: 0.677599
Train - Epoch 44, Batch: 0, Loss: 0.677273
Train - Epoch 45, Batch: 0, Loss: 0.676942
Train - Epoch 46, Batch: 0, Loss: 0.676635
Train - Epoch 47, Batch: 0, Loss: 0.676387
Train - Epoch 48, Batch: 0, Loss: 0.675882
Train - Epoch 49, Batch: 0, Loss: 0.675906
Train - Epoch 50, Batch: 0, Loss: 0.675327
Train - Epoch 51, Batch: 0, Loss: 0.675044
Train - Epoch 52, Batch: 0, Loss: 0.674782
Train - Epoch 53, Batch: 0, Loss: 0.674223
Train - Epoch 54, Batch: 0, Loss: 0.674117
Train - Epoch 55, Batch: 0, Loss: 0.673613
Train - Epoch 56, Batch: 0, Loss: 0.673360
Train - Epoch 57, Batch: 0, Loss: 0.672795
Train - Epoch 58, Batch: 0, Loss: 0.672629
Train - Epoch 59, Batch: 0, Loss: 0.672182
Train - Epoch 60, Batch: 0, Loss: 0.672062
Train - Epoch 61, Batch: 0, Loss: 0.671717
Train - Epoch 62, Batch: 0, Loss: 0.671265
Train - Epoch 63, Batch: 0, Loss: 0.670850
Train - Epoch 64, Batch: 0, Loss: 0.670566
Train - Epoch 65, Batch: 0, Loss: 0.670418
Train - Epoch 66, Batch: 0, Loss: 0.670038
Train - Epoch 67, Batch: 0, Loss: 0.669745
Train - Epoch 68, Batch: 0, Loss: 0.669333
Train - Epoch 69, Batch: 0, Loss: 0.668961
Train - Epoch 70, Batch: 0, Loss: 0.668980
Train - Epoch 71, Batch: 0, Loss: 0.668338
Train - Epoch 72, Batch: 0, Loss: 0.667918
Train - Epoch 73, Batch: 0, Loss: 0.667737
Train - Epoch 74, Batch: 0, Loss: 0.667312
Train - Epoch 75, Batch: 0, Loss: 0.666923
Train - Epoch 76, Batch: 0, Loss: 0.667029
Train - Epoch 77, Batch: 0, Loss: 0.666684
Train - Epoch 78, Batch: 0, Loss: 0.665978
Train - Epoch 79, Batch: 0, Loss: 0.665972
Train - Epoch 80, Batch: 0, Loss: 0.665617
Train - Epoch 81, Batch: 0, Loss: 0.665333
Train - Epoch 82, Batch: 0, Loss: 0.664849
Train - Epoch 83, Batch: 0, Loss: 0.664793
Train - Epoch 84, Batch: 0, Loss: 0.664621
Train - Epoch 85, Batch: 0, Loss: 0.664113
Train - Epoch 86, Batch: 0, Loss: 0.663802
Train - Epoch 87, Batch: 0, Loss: 0.663243
Train - Epoch 88, Batch: 0, Loss: 0.662976
Train - Epoch 89, Batch: 0, Loss: 0.662610
Train - Epoch 90, Batch: 0, Loss: 0.662300
Train - Epoch 91, Batch: 0, Loss: 0.662334
Train - Epoch 92, Batch: 0, Loss: 0.661577
Train - Epoch 93, Batch: 0, Loss: 0.661350
Train - Epoch 94, Batch: 0, Loss: 0.660941
Train - Epoch 95, Batch: 0, Loss: 0.661217
Train - Epoch 96, Batch: 0, Loss: 0.660277
Train - Epoch 97, Batch: 0, Loss: 0.659950
Train - Epoch 98, Batch: 0, Loss: 0.659965
Train - Epoch 99, Batch: 0, Loss: 0.659674
Train - Epoch 100, Batch: 0, Loss: 0.659294
Train - Epoch 101, Batch: 0, Loss: 0.659310
Train - Epoch 102, Batch: 0, Loss: 0.658768
Train - Epoch 103, Batch: 0, Loss: 0.658554
Train - Epoch 104, Batch: 0, Loss: 0.658130
Train - Epoch 105, Batch: 0, Loss: 0.658062
Train - Epoch 106, Batch: 0, Loss: 0.657669
Train - Epoch 107, Batch: 0, Loss: 0.657545
Train - Epoch 108, Batch: 0, Loss: 0.657120
Train - Epoch 109, Batch: 0, Loss: 0.656727
Train - Epoch 110, Batch: 0, Loss: 0.656457
Train - Epoch 111, Batch: 0, Loss: 0.656006
Train - Epoch 112, Batch: 0, Loss: 0.656135
Train - Epoch 113, Batch: 0, Loss: 0.655721
Train - Epoch 114, Batch: 0, Loss: 0.655408
Train - Epoch 115, Batch: 0, Loss: 0.655048
Train - Epoch 116, Batch: 0, Loss: 0.654599
Train - Epoch 117, Batch: 0, Loss: 0.654740
Train - Epoch 118, Batch: 0, Loss: 0.654047
Train - Epoch 119, Batch: 0, Loss: 0.653618
Train - Epoch 120, Batch: 0, Loss: 0.653628
Train - Epoch 121, Batch: 0, Loss: 0.653518
Train - Epoch 122, Batch: 0, Loss: 0.652928
Train - Epoch 123, Batch: 0, Loss: 0.652778
Train - Epoch 124, Batch: 0, Loss: 0.652587
Train - Epoch 125, Batch: 0, Loss: 0.651890
Train - Epoch 126, Batch: 0, Loss: 0.651893
Train - Epoch 127, Batch: 0, Loss: 0.651719
Train - Epoch 128, Batch: 0, Loss: 0.651458
Train - Epoch 129, Batch: 0, Loss: 0.651125
Train - Epoch 130, Batch: 0, Loss: 0.650570
Train - Epoch 131, Batch: 0, Loss: 0.650522
Train - Epoch 132, Batch: 0, Loss: 0.650378
Train - Epoch 133, Batch: 0, Loss: 0.650254
Train - Epoch 134, Batch: 0, Loss: 0.649913
Train - Epoch 135, Batch: 0, Loss: 0.649222
Train - Epoch 136, Batch: 0, Loss: 0.648908
Train - Epoch 137, Batch: 0, Loss: 0.648850
Train - Epoch 138, Batch: 0, Loss: 0.648236
Train - Epoch 139, Batch: 0, Loss: 0.648043
Train - Epoch 140, Batch: 0, Loss: 0.648278
Train - Epoch 141, Batch: 0, Loss: 0.647896
Train - Epoch 142, Batch: 0, Loss: 0.647156
Train - Epoch 143, Batch: 0, Loss: 0.647347
Train - Epoch 144, Batch: 0, Loss: 0.646419
Train - Epoch 145, Batch: 0, Loss: 0.647054
Train - Epoch 146, Batch: 0, Loss: 0.646252
Train - Epoch 147, Batch: 0, Loss: 0.646389
Train - Epoch 148, Batch: 0, Loss: 0.645427
Train - Epoch 149, Batch: 0, Loss: 0.646086
Train - Epoch 150, Batch: 0, Loss: 0.645425
Train - Epoch 151, Batch: 0, Loss: 0.645033
Train - Epoch 152, Batch: 0, Loss: 0.644864
Train - Epoch 153, Batch: 0, Loss: 0.644814
Train - Epoch 154, Batch: 0, Loss: 0.644364
Train - Epoch 155, Batch: 0, Loss: 0.644299
Train - Epoch 156, Batch: 0, Loss: 0.643546
Train - Epoch 157, Batch: 0, Loss: 0.643671
Train - Epoch 158, Batch: 0, Loss: 0.643171
Train - Epoch 159, Batch: 0, Loss: 0.642928
Train - Epoch 160, Batch: 0, Loss: 0.642704
Train - Epoch 161, Batch: 0, Loss: 0.642300
Train - Epoch 162, Batch: 0, Loss: 0.642373
Train - Epoch 163, Batch: 0, Loss: 0.641743
Train - Epoch 164, Batch: 0, Loss: 0.641665
Train - Epoch 165, Batch: 0, Loss: 0.641628
Train - Epoch 166, Batch: 0, Loss: 0.641372
Train - Epoch 167, Batch: 0, Loss: 0.640697
Train - Epoch 168, Batch: 0, Loss: 0.640929
Train - Epoch 169, Batch: 0, Loss: 0.640525
Train - Epoch 170, Batch: 0, Loss: 0.640131
Train - Epoch 171, Batch: 0, Loss: 0.639932
Train - Epoch 172, Batch: 0, Loss: 0.639318
Train - Epoch 173, Batch: 0, Loss: 0.639333
Train - Epoch 174, Batch: 0, Loss: 0.639190
Train - Epoch 175, Batch: 0, Loss: 0.638558
Train - Epoch 176, Batch: 0, Loss: 0.638517
Train - Epoch 177, Batch: 0, Loss: 0.637737
Train - Epoch 178, Batch: 0, Loss: 0.637819
Train - Epoch 179, Batch: 0, Loss: 0.637796
Train - Epoch 180, Batch: 0, Loss: 0.638242
Train - Epoch 181, Batch: 0, Loss: 0.636908
Train - Epoch 182, Batch: 0, Loss: 0.636924
Train - Epoch 183, Batch: 0, Loss: 0.637138
Train - Epoch 184, Batch: 0, Loss: 0.636424
Train - Epoch 185, Batch: 0, Loss: 0.635896
Train - Epoch 186, Batch: 0, Loss: 0.635615/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635314
Train - Epoch 188, Batch: 0, Loss: 0.635643
Train - Epoch 189, Batch: 0, Loss: 0.635132
Train - Epoch 190, Batch: 0, Loss: 0.634849
Train - Epoch 191, Batch: 0, Loss: 0.634488
Train - Epoch 192, Batch: 0, Loss: 0.634136
Train - Epoch 193, Batch: 0, Loss: 0.634531
Train - Epoch 194, Batch: 0, Loss: 0.633990
Train - Epoch 195, Batch: 0, Loss: 0.633844
Train - Epoch 196, Batch: 0, Loss: 0.633256
Train - Epoch 197, Batch: 0, Loss: 0.633622
Train - Epoch 198, Batch: 0, Loss: 0.632853
Train - Epoch 199, Batch: 0, Loss: 0.633186
training_time:: 351.6509919166565
training time full:: 351.65103554725647
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927329
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 10
training time is 298.43669390678406
overhead:: 0
overhead2:: 0
time_baseline:: 298.43737840652466
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(625097)
RCV1 Test Avg. Accuracy:: 0.9227899657365896
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 10
max_epoch:: 200
overhead:: 0
overhead2:: 0.02372431755065918
overhead3:: 0.11197137832641602
overhead4:: 35.11158204078674
overhead5:: 0
time_provenance:: 45.889816999435425
curr_diff: 0 tensor(1.8574e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8574e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0039, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0039, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.0005_10200
tensor(625097)
RCV1 Test Avg. Accuracy:: 0.9227899657365896
deletion rate:: 0.001
python3 generate_rand_ids 0.001  rcv1 0
tensor([13216, 11745,  1825,  9178,   100, 19652,  9380, 17192,  2985, 17771,
        17325, 18989,  2458, 14577, 15217, 17043, 10773, 13498,   155,  6365])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693176
Train - Epoch 1, Batch: 0, Loss: 0.692829
Train - Epoch 2, Batch: 0, Loss: 0.692444
Train - Epoch 3, Batch: 0, Loss: 0.692040
Train - Epoch 4, Batch: 0, Loss: 0.691671
Train - Epoch 5, Batch: 0, Loss: 0.691309
Train - Epoch 6, Batch: 0, Loss: 0.690945
Train - Epoch 7, Batch: 0, Loss: 0.690595
Train - Epoch 8, Batch: 0, Loss: 0.690203
Train - Epoch 9, Batch: 0, Loss: 0.689856
Train - Epoch 10, Batch: 0, Loss: 0.689512
Train - Epoch 11, Batch: 0, Loss: 0.689085
Train - Epoch 12, Batch: 0, Loss: 0.688774
Train - Epoch 13, Batch: 0, Loss: 0.688350
Train - Epoch 14, Batch: 0, Loss: 0.687959
Train - Epoch 15, Batch: 0, Loss: 0.687661
Train - Epoch 16, Batch: 0, Loss: 0.687309
Train - Epoch 17, Batch: 0, Loss: 0.686914
Train - Epoch 18, Batch: 0, Loss: 0.686654
Train - Epoch 19, Batch: 0, Loss: 0.686306
Train - Epoch 20, Batch: 0, Loss: 0.685848
Train - Epoch 21, Batch: 0, Loss: 0.685438
Train - Epoch 22, Batch: 0, Loss: 0.685190
Train - Epoch 23, Batch: 0, Loss: 0.684758
Train - Epoch 24, Batch: 0, Loss: 0.684454
Train - Epoch 25, Batch: 0, Loss: 0.684101
Train - Epoch 26, Batch: 0, Loss: 0.683640
Train - Epoch 27, Batch: 0, Loss: 0.683268
Train - Epoch 28, Batch: 0, Loss: 0.683128
Train - Epoch 29, Batch: 0, Loss: 0.682680
Train - Epoch 30, Batch: 0, Loss: 0.682298
Train - Epoch 31, Batch: 0, Loss: 0.681918
Train - Epoch 32, Batch: 0, Loss: 0.681581
Train - Epoch 33, Batch: 0, Loss: 0.681299
Train - Epoch 34, Batch: 0, Loss: 0.681022
Train - Epoch 35, Batch: 0, Loss: 0.680499
Train - Epoch 36, Batch: 0, Loss: 0.680247
Train - Epoch 37, Batch: 0, Loss: 0.679915
Train - Epoch 38, Batch: 0, Loss: 0.679546
Train - Epoch 39, Batch: 0, Loss: 0.679254
Train - Epoch 40, Batch: 0, Loss: 0.678745
Train - Epoch 41, Batch: 0, Loss: 0.678620
Train - Epoch 42, Batch: 0, Loss: 0.678093
Train - Epoch 43, Batch: 0, Loss: 0.677719
Train - Epoch 44, Batch: 0, Loss: 0.677486
Train - Epoch 45, Batch: 0, Loss: 0.677227
Train - Epoch 46, Batch: 0, Loss: 0.676729
Train - Epoch 47, Batch: 0, Loss: 0.676502
Train - Epoch 48, Batch: 0, Loss: 0.676088
Train - Epoch 49, Batch: 0, Loss: 0.675708
Train - Epoch 50, Batch: 0, Loss: 0.675397
Train - Epoch 51, Batch: 0, Loss: 0.674904
Train - Epoch 52, Batch: 0, Loss: 0.674824
Train - Epoch 53, Batch: 0, Loss: 0.674376
Train - Epoch 54, Batch: 0, Loss: 0.674026
Train - Epoch 55, Batch: 0, Loss: 0.673809
Train - Epoch 56, Batch: 0, Loss: 0.673530
Train - Epoch 57, Batch: 0, Loss: 0.673071
Train - Epoch 58, Batch: 0, Loss: 0.672817
Train - Epoch 59, Batch: 0, Loss: 0.672583
Train - Epoch 60, Batch: 0, Loss: 0.672122
Train - Epoch 61, Batch: 0, Loss: 0.672015
Train - Epoch 62, Batch: 0, Loss: 0.671272
Train - Epoch 63, Batch: 0, Loss: 0.671121
Train - Epoch 64, Batch: 0, Loss: 0.671070
Train - Epoch 65, Batch: 0, Loss: 0.670489
Train - Epoch 66, Batch: 0, Loss: 0.670062
Train - Epoch 67, Batch: 0, Loss: 0.670054
Train - Epoch 68, Batch: 0, Loss: 0.669517
Train - Epoch 69, Batch: 0, Loss: 0.669328
Train - Epoch 70, Batch: 0, Loss: 0.669160
Train - Epoch 71, Batch: 0, Loss: 0.668591
Train - Epoch 72, Batch: 0, Loss: 0.668313
Train - Epoch 73, Batch: 0, Loss: 0.668056
Train - Epoch 74, Batch: 0, Loss: 0.667698
Train - Epoch 75, Batch: 0, Loss: 0.667599
Train - Epoch 76, Batch: 0, Loss: 0.667112
Train - Epoch 77, Batch: 0, Loss: 0.666634
Train - Epoch 78, Batch: 0, Loss: 0.666416
Train - Epoch 79, Batch: 0, Loss: 0.666268
Train - Epoch 80, Batch: 0, Loss: 0.665541
Train - Epoch 81, Batch: 0, Loss: 0.665245
Train - Epoch 82, Batch: 0, Loss: 0.665232
Train - Epoch 83, Batch: 0, Loss: 0.665032
Train - Epoch 84, Batch: 0, Loss: 0.664525
Train - Epoch 85, Batch: 0, Loss: 0.664098
Train - Epoch 86, Batch: 0, Loss: 0.663687
Train - Epoch 87, Batch: 0, Loss: 0.663492
Train - Epoch 88, Batch: 0, Loss: 0.663301
Train - Epoch 89, Batch: 0, Loss: 0.662971
Train - Epoch 90, Batch: 0, Loss: 0.662688
Train - Epoch 91, Batch: 0, Loss: 0.662257
Train - Epoch 92, Batch: 0, Loss: 0.662418
Train - Epoch 93, Batch: 0, Loss: 0.661335
Train - Epoch 94, Batch: 0, Loss: 0.661339
Train - Epoch 95, Batch: 0, Loss: 0.661385
Train - Epoch 96, Batch: 0, Loss: 0.660648
Train - Epoch 97, Batch: 0, Loss: 0.660338
Train - Epoch 98, Batch: 0, Loss: 0.660023
Train - Epoch 99, Batch: 0, Loss: 0.659775
Train - Epoch 100, Batch: 0, Loss: 0.659544
Train - Epoch 101, Batch: 0, Loss: 0.659292
Train - Epoch 102, Batch: 0, Loss: 0.658887
Train - Epoch 103, Batch: 0, Loss: 0.658719
Train - Epoch 104, Batch: 0, Loss: 0.658388
Train - Epoch 105, Batch: 0, Loss: 0.658261
Train - Epoch 106, Batch: 0, Loss: 0.657966
Train - Epoch 107, Batch: 0, Loss: 0.657530
Train - Epoch 108, Batch: 0, Loss: 0.657111
Train - Epoch 109, Batch: 0, Loss: 0.656914
Train - Epoch 110, Batch: 0, Loss: 0.656604
Train - Epoch 111, Batch: 0, Loss: 0.656427
Train - Epoch 112, Batch: 0, Loss: 0.656239
Train - Epoch 113, Batch: 0, Loss: 0.656207
Train - Epoch 114, Batch: 0, Loss: 0.655184
Train - Epoch 115, Batch: 0, Loss: 0.655114
Train - Epoch 116, Batch: 0, Loss: 0.655018
Train - Epoch 117, Batch: 0, Loss: 0.654530
Train - Epoch 118, Batch: 0, Loss: 0.653920
Train - Epoch 119, Batch: 0, Loss: 0.654206
Train - Epoch 120, Batch: 0, Loss: 0.653633
Train - Epoch 121, Batch: 0, Loss: 0.653511
Train - Epoch 122, Batch: 0, Loss: 0.653326
Train - Epoch 123, Batch: 0, Loss: 0.653169
Train - Epoch 124, Batch: 0, Loss: 0.652783
Train - Epoch 125, Batch: 0, Loss: 0.652312
Train - Epoch 126, Batch: 0, Loss: 0.652018
Train - Epoch 127, Batch: 0, Loss: 0.651498
Train - Epoch 128, Batch: 0, Loss: 0.651865
Train - Epoch 129, Batch: 0, Loss: 0.651156
Train - Epoch 130, Batch: 0, Loss: 0.650672
Train - Epoch 131, Batch: 0, Loss: 0.650218
Train - Epoch 132, Batch: 0, Loss: 0.650334
Train - Epoch 133, Batch: 0, Loss: 0.650031
Train - Epoch 134, Batch: 0, Loss: 0.649903
Train - Epoch 135, Batch: 0, Loss: 0.649612
Train - Epoch 136, Batch: 0, Loss: 0.648920
Train - Epoch 137, Batch: 0, Loss: 0.649235
Train - Epoch 138, Batch: 0, Loss: 0.648914
Train - Epoch 139, Batch: 0, Loss: 0.648277
Train - Epoch 140, Batch: 0, Loss: 0.648283
Train - Epoch 141, Batch: 0, Loss: 0.648041
Train - Epoch 142, Batch: 0, Loss: 0.647789
Train - Epoch 143, Batch: 0, Loss: 0.647114
Train - Epoch 144, Batch: 0, Loss: 0.647261
Train - Epoch 145, Batch: 0, Loss: 0.646352
Train - Epoch 146, Batch: 0, Loss: 0.646636
Train - Epoch 147, Batch: 0, Loss: 0.646138
Train - Epoch 148, Batch: 0, Loss: 0.645716
Train - Epoch 149, Batch: 0, Loss: 0.646172
Train - Epoch 150, Batch: 0, Loss: 0.645372
Train - Epoch 151, Batch: 0, Loss: 0.645487
Train - Epoch 152, Batch: 0, Loss: 0.644322
Train - Epoch 153, Batch: 0, Loss: 0.644667
Train - Epoch 154, Batch: 0, Loss: 0.644121
Train - Epoch 155, Batch: 0, Loss: 0.643943
Train - Epoch 156, Batch: 0, Loss: 0.644277
Train - Epoch 157, Batch: 0, Loss: 0.643078
Train - Epoch 158, Batch: 0, Loss: 0.643425
Train - Epoch 159, Batch: 0, Loss: 0.643100
Train - Epoch 160, Batch: 0, Loss: 0.642631
Train - Epoch 161, Batch: 0, Loss: 0.642244
Train - Epoch 162, Batch: 0, Loss: 0.641613
Train - Epoch 163, Batch: 0, Loss: 0.642497
Train - Epoch 164, Batch: 0, Loss: 0.641500
Train - Epoch 165, Batch: 0, Loss: 0.640930
Train - Epoch 166, Batch: 0, Loss: 0.641216
Train - Epoch 167, Batch: 0, Loss: 0.640869
Train - Epoch 168, Batch: 0, Loss: 0.640404
Train - Epoch 169, Batch: 0, Loss: 0.640672
Train - Epoch 170, Batch: 0, Loss: 0.640141
Train - Epoch 171, Batch: 0, Loss: 0.639668
Train - Epoch 172, Batch: 0, Loss: 0.639907
Train - Epoch 173, Batch: 0, Loss: 0.639516
Train - Epoch 174, Batch: 0, Loss: 0.639072
Train - Epoch 175, Batch: 0, Loss: 0.638941
Train - Epoch 176, Batch: 0, Loss: 0.638455
Train - Epoch 177, Batch: 0, Loss: 0.638380
Train - Epoch 178, Batch: 0, Loss: 0.637893
Train - Epoch 179, Batch: 0, Loss: 0.637925
Train - Epoch 180, Batch: 0, Loss: 0.637468
Train - Epoch 181, Batch: 0, Loss: 0.636642
Train - Epoch 182, Batch: 0, Loss: 0.637326
Train - Epoch 183, Batch: 0, Loss: 0.636567
Train - Epoch 184, Batch: 0, Loss: 0.636879
Train - Epoch 185, Batch: 0, Loss: 0.636924
Train - Epoch 186, Batch: 0, Loss: 0.636131/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635750
Train - Epoch 188, Batch: 0, Loss: 0.635903
Train - Epoch 189, Batch: 0, Loss: 0.635980
Train - Epoch 190, Batch: 0, Loss: 0.635319
Train - Epoch 191, Batch: 0, Loss: 0.634726
Train - Epoch 192, Batch: 0, Loss: 0.634346
Train - Epoch 193, Batch: 0, Loss: 0.634331
Train - Epoch 194, Batch: 0, Loss: 0.634104
Train - Epoch 195, Batch: 0, Loss: 0.633483
Train - Epoch 196, Batch: 0, Loss: 0.632510
Train - Epoch 197, Batch: 0, Loss: 0.633471
Train - Epoch 198, Batch: 0, Loss: 0.633552
Train - Epoch 199, Batch: 0, Loss: 0.632743
training_time:: 350.64476680755615
training time full:: 350.64481234550476
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927181
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 20
training time is 299.96363258361816
overhead:: 0
overhead2:: 0
time_baseline:: 299.96433305740356
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927181
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624760)
RCV1 Test Avg. Accuracy:: 0.9222924745976891
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.02163410186767578
overhead3:: 0.10772919654846191
overhead4:: 35.30266833305359
overhead5:: 0
time_provenance:: 46.002798318862915
curr_diff: 0 tensor(2.5417e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5417e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927181
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.001_10200
tensor(624760)
RCV1 Test Avg. Accuracy:: 0.9222924745976891
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693151
Train - Epoch 1, Batch: 0, Loss: 0.692762
Train - Epoch 2, Batch: 0, Loss: 0.692397
Train - Epoch 3, Batch: 0, Loss: 0.692011
Train - Epoch 4, Batch: 0, Loss: 0.691647
Train - Epoch 5, Batch: 0, Loss: 0.691273
Train - Epoch 6, Batch: 0, Loss: 0.690917
Train - Epoch 7, Batch: 0, Loss: 0.690566
Train - Epoch 8, Batch: 0, Loss: 0.690158
Train - Epoch 9, Batch: 0, Loss: 0.689814
Train - Epoch 10, Batch: 0, Loss: 0.689445
Train - Epoch 11, Batch: 0, Loss: 0.689056
Train - Epoch 12, Batch: 0, Loss: 0.688655
Train - Epoch 13, Batch: 0, Loss: 0.688294
Train - Epoch 14, Batch: 0, Loss: 0.687979
Train - Epoch 15, Batch: 0, Loss: 0.687606
Train - Epoch 16, Batch: 0, Loss: 0.687260
Train - Epoch 17, Batch: 0, Loss: 0.686916
Train - Epoch 18, Batch: 0, Loss: 0.686545
Train - Epoch 19, Batch: 0, Loss: 0.686149
Train - Epoch 20, Batch: 0, Loss: 0.685820
Train - Epoch 21, Batch: 0, Loss: 0.685518
Train - Epoch 22, Batch: 0, Loss: 0.685107
Train - Epoch 23, Batch: 0, Loss: 0.684759
Train - Epoch 24, Batch: 0, Loss: 0.684386
Train - Epoch 25, Batch: 0, Loss: 0.684000
Train - Epoch 26, Batch: 0, Loss: 0.683747
Train - Epoch 27, Batch: 0, Loss: 0.683249
Train - Epoch 28, Batch: 0, Loss: 0.682985
Train - Epoch 29, Batch: 0, Loss: 0.682622
Train - Epoch 30, Batch: 0, Loss: 0.682398
Train - Epoch 31, Batch: 0, Loss: 0.681901
Train - Epoch 32, Batch: 0, Loss: 0.681598
Train - Epoch 33, Batch: 0, Loss: 0.681133
Train - Epoch 34, Batch: 0, Loss: 0.680854
Train - Epoch 35, Batch: 0, Loss: 0.680592
Train - Epoch 36, Batch: 0, Loss: 0.680324
Train - Epoch 37, Batch: 0, Loss: 0.679850
Train - Epoch 38, Batch: 0, Loss: 0.679597
Train - Epoch 39, Batch: 0, Loss: 0.679106
Train - Epoch 40, Batch: 0, Loss: 0.678713
Train - Epoch 41, Batch: 0, Loss: 0.678519
Train - Epoch 42, Batch: 0, Loss: 0.678092
Train - Epoch 43, Batch: 0, Loss: 0.677711
Train - Epoch 44, Batch: 0, Loss: 0.677345
Train - Epoch 45, Batch: 0, Loss: 0.677099
Train - Epoch 46, Batch: 0, Loss: 0.676714
Train - Epoch 47, Batch: 0, Loss: 0.676393
Train - Epoch 48, Batch: 0, Loss: 0.676217
Train - Epoch 49, Batch: 0, Loss: 0.675816
Train - Epoch 50, Batch: 0, Loss: 0.675346
Train - Epoch 51, Batch: 0, Loss: 0.675194
Train - Epoch 52, Batch: 0, Loss: 0.674738
Train - Epoch 53, Batch: 0, Loss: 0.674351
Train - Epoch 54, Batch: 0, Loss: 0.674223
Train - Epoch 55, Batch: 0, Loss: 0.673678
Train - Epoch 56, Batch: 0, Loss: 0.673485
Train - Epoch 57, Batch: 0, Loss: 0.673409
Train - Epoch 58, Batch: 0, Loss: 0.672716
Train - Epoch 59, Batch: 0, Loss: 0.672443
Train - Epoch 60, Batch: 0, Loss: 0.672035
Train - Epoch 61, Batch: 0, Loss: 0.671554
Train - Epoch 62, Batch: 0, Loss: 0.671599
Train - Epoch 63, Batch: 0, Loss: 0.671143
Train - Epoch 64, Batch: 0, Loss: 0.670941
Train - Epoch 65, Batch: 0, Loss: 0.670233
Train - Epoch 66, Batch: 0, Loss: 0.670179
Train - Epoch 67, Batch: 0, Loss: 0.670059
Train - Epoch 68, Batch: 0, Loss: 0.669495
Train - Epoch 69, Batch: 0, Loss: 0.669380
Train - Epoch 70, Batch: 0, Loss: 0.668741
Train - Epoch 71, Batch: 0, Loss: 0.668419
Train - Epoch 72, Batch: 0, Loss: 0.668211
Train - Epoch 73, Batch: 0, Loss: 0.667989
Train - Epoch 74, Batch: 0, Loss: 0.667594
Train - Epoch 75, Batch: 0, Loss: 0.667596
Train - Epoch 76, Batch: 0, Loss: 0.667029
Train - Epoch 77, Batch: 0, Loss: 0.666680
Train - Epoch 78, Batch: 0, Loss: 0.666402
Train - Epoch 79, Batch: 0, Loss: 0.666054
Train - Epoch 80, Batch: 0, Loss: 0.665832
Train - Epoch 81, Batch: 0, Loss: 0.665434
Train - Epoch 82, Batch: 0, Loss: 0.665176
Train - Epoch 83, Batch: 0, Loss: 0.664636
Train - Epoch 84, Batch: 0, Loss: 0.664252
Train - Epoch 85, Batch: 0, Loss: 0.664251
Train - Epoch 86, Batch: 0, Loss: 0.663902
Train - Epoch 87, Batch: 0, Loss: 0.663797
Train - Epoch 88, Batch: 0, Loss: 0.662941
Train - Epoch 89, Batch: 0, Loss: 0.662854
Train - Epoch 90, Batch: 0, Loss: 0.662730
Train - Epoch 91, Batch: 0, Loss: 0.662611
Train - Epoch 92, Batch: 0, Loss: 0.662015
Train - Epoch 93, Batch: 0, Loss: 0.661863
Train - Epoch 94, Batch: 0, Loss: 0.661678
Train - Epoch 95, Batch: 0, Loss: 0.661254
Train - Epoch 96, Batch: 0, Loss: 0.660720
Train - Epoch 97, Batch: 0, Loss: 0.660654
Train - Epoch 98, Batch: 0, Loss: 0.660462
Train - Epoch 99, Batch: 0, Loss: 0.659915
Train - Epoch 100, Batch: 0, Loss: 0.659718
Train - Epoch 101, Batch: 0, Loss: 0.659399
Train - Epoch 102, Batch: 0, Loss: 0.658830
Train - Epoch 103, Batch: 0, Loss: 0.658950
Train - Epoch 104, Batch: 0, Loss: 0.658852
Train - Epoch 105, Batch: 0, Loss: 0.658277
Train - Epoch 106, Batch: 0, Loss: 0.657541
Train - Epoch 107, Batch: 0, Loss: 0.657084
Train - Epoch 108, Batch: 0, Loss: 0.657741
Train - Epoch 109, Batch: 0, Loss: 0.656709
Train - Epoch 110, Batch: 0, Loss: 0.656596
Train - Epoch 111, Batch: 0, Loss: 0.656342
Train - Epoch 112, Batch: 0, Loss: 0.655965
Train - Epoch 113, Batch: 0, Loss: 0.655719
Train - Epoch 114, Batch: 0, Loss: 0.655233
Train - Epoch 115, Batch: 0, Loss: 0.655451
Train - Epoch 116, Batch: 0, Loss: 0.654852
Train - Epoch 117, Batch: 0, Loss: 0.654928
Train - Epoch 118, Batch: 0, Loss: 0.653945
Train - Epoch 119, Batch: 0, Loss: 0.653978
Train - Epoch 120, Batch: 0, Loss: 0.653526
Train - Epoch 121, Batch: 0, Loss: 0.653448
Train - Epoch 122, Batch: 0, Loss: 0.653297
Train - Epoch 123, Batch: 0, Loss: 0.653043
Train - Epoch 124, Batch: 0, Loss: 0.653023
Train - Epoch 125, Batch: 0, Loss: 0.652520
Train - Epoch 126, Batch: 0, Loss: 0.652382
Train - Epoch 127, Batch: 0, Loss: 0.651916
Train - Epoch 128, Batch: 0, Loss: 0.651215
Train - Epoch 129, Batch: 0, Loss: 0.651221
Train - Epoch 130, Batch: 0, Loss: 0.650974
Train - Epoch 131, Batch: 0, Loss: 0.650449
Train - Epoch 132, Batch: 0, Loss: 0.650227
Train - Epoch 133, Batch: 0, Loss: 0.650033
Train - Epoch 134, Batch: 0, Loss: 0.649995
Train - Epoch 135, Batch: 0, Loss: 0.649727
Train - Epoch 136, Batch: 0, Loss: 0.649108
Train - Epoch 137, Batch: 0, Loss: 0.648503
Train - Epoch 138, Batch: 0, Loss: 0.648608
Train - Epoch 139, Batch: 0, Loss: 0.648502
Train - Epoch 140, Batch: 0, Loss: 0.648117
Train - Epoch 141, Batch: 0, Loss: 0.647590
Train - Epoch 142, Batch: 0, Loss: 0.646899
Train - Epoch 143, Batch: 0, Loss: 0.647145
Train - Epoch 144, Batch: 0, Loss: 0.646991
Train - Epoch 145, Batch: 0, Loss: 0.646477
Train - Epoch 146, Batch: 0, Loss: 0.646621
Train - Epoch 147, Batch: 0, Loss: 0.645916
Train - Epoch 148, Batch: 0, Loss: 0.645966
Train - Epoch 149, Batch: 0, Loss: 0.645493
Train - Epoch 150, Batch: 0, Loss: 0.645698
Train - Epoch 151, Batch: 0, Loss: 0.645228
Train - Epoch 152, Batch: 0, Loss: 0.645141
Train - Epoch 153, Batch: 0, Loss: 0.644480
Train - Epoch 154, Batch: 0, Loss: 0.644003
Train - Epoch 155, Batch: 0, Loss: 0.644230
Train - Epoch 156, Batch: 0, Loss: 0.643726
Train - Epoch 157, Batch: 0, Loss: 0.643486
Train - Epoch 158, Batch: 0, Loss: 0.643350
Train - Epoch 159, Batch: 0, Loss: 0.643257
Train - Epoch 160, Batch: 0, Loss: 0.642288
Train - Epoch 161, Batch: 0, Loss: 0.642473
Train - Epoch 162, Batch: 0, Loss: 0.642789
Train - Epoch 163, Batch: 0, Loss: 0.641867
Train - Epoch 164, Batch: 0, Loss: 0.641605
Train - Epoch 165, Batch: 0, Loss: 0.641414
Train - Epoch 166, Batch: 0, Loss: 0.640775
Train - Epoch 167, Batch: 0, Loss: 0.640908
Train - Epoch 168, Batch: 0, Loss: 0.640854
Train - Epoch 169, Batch: 0, Loss: 0.640436
Train - Epoch 170, Batch: 0, Loss: 0.640118
Train - Epoch 171, Batch: 0, Loss: 0.639554
Train - Epoch 172, Batch: 0, Loss: 0.640031
Train - Epoch 173, Batch: 0, Loss: 0.639427
Train - Epoch 174, Batch: 0, Loss: 0.639168
Train - Epoch 175, Batch: 0, Loss: 0.638532
Train - Epoch 176, Batch: 0, Loss: 0.638286
Train - Epoch 177, Batch: 0, Loss: 0.638590
Train - Epoch 178, Batch: 0, Loss: 0.637957
Train - Epoch 179, Batch: 0, Loss: 0.637662
Train - Epoch 180, Batch: 0, Loss: 0.637319
Train - Epoch 181, Batch: 0, Loss: 0.637397
Train - Epoch 182, Batch: 0, Loss: 0.637361
Train - Epoch 183, Batch: 0, Loss: 0.636487
Train - Epoch 184, Batch: 0, Loss: 0.636917
Train - Epoch 185, Batch: 0, Loss: 0.636266
Train - Epoch 186, Batch: 0, Loss: 0.635690/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635706
Train - Epoch 188, Batch: 0, Loss: 0.635525
Train - Epoch 189, Batch: 0, Loss: 0.635745
Train - Epoch 190, Batch: 0, Loss: 0.635308
Train - Epoch 191, Batch: 0, Loss: 0.634874
Train - Epoch 192, Batch: 0, Loss: 0.634508
Train - Epoch 193, Batch: 0, Loss: 0.634032
Train - Epoch 194, Batch: 0, Loss: 0.633884
Train - Epoch 195, Batch: 0, Loss: 0.634010
Train - Epoch 196, Batch: 0, Loss: 0.633751
Train - Epoch 197, Batch: 0, Loss: 0.633564
Train - Epoch 198, Batch: 0, Loss: 0.633313
Train - Epoch 199, Batch: 0, Loss: 0.632534
training_time:: 350.1027617454529
training time full:: 350.10281109809875
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927132
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 20
training time is 299.9477126598358
overhead:: 0
overhead2:: 0
time_baseline:: 299.9483759403229
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624933)
RCV1 Test Avg. Accuracy:: 0.9225478632238902
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.023440122604370117
overhead3:: 0.11193227767944336
overhead4:: 35.19156575202942
overhead5:: 0
time_provenance:: 46.0388503074646
curr_diff: 0 tensor(2.5932e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5932e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927280
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.001_10200
tensor(624933)
RCV1 Test Avg. Accuracy:: 0.9225478632238902
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693220
Train - Epoch 1, Batch: 0, Loss: 0.692852
Train - Epoch 2, Batch: 0, Loss: 0.692451
Train - Epoch 3, Batch: 0, Loss: 0.692080
Train - Epoch 4, Batch: 0, Loss: 0.691735
Train - Epoch 5, Batch: 0, Loss: 0.691362
Train - Epoch 6, Batch: 0, Loss: 0.690984
Train - Epoch 7, Batch: 0, Loss: 0.690600
Train - Epoch 8, Batch: 0, Loss: 0.690248
Train - Epoch 9, Batch: 0, Loss: 0.689872
Train - Epoch 10, Batch: 0, Loss: 0.689540
Train - Epoch 11, Batch: 0, Loss: 0.689127
Train - Epoch 12, Batch: 0, Loss: 0.688788
Train - Epoch 13, Batch: 0, Loss: 0.688446
Train - Epoch 14, Batch: 0, Loss: 0.688078
Train - Epoch 15, Batch: 0, Loss: 0.687628
Train - Epoch 16, Batch: 0, Loss: 0.687354
Train - Epoch 17, Batch: 0, Loss: 0.686935
Train - Epoch 18, Batch: 0, Loss: 0.686545
Train - Epoch 19, Batch: 0, Loss: 0.686237
Train - Epoch 20, Batch: 0, Loss: 0.685942
Train - Epoch 21, Batch: 0, Loss: 0.685497
Train - Epoch 22, Batch: 0, Loss: 0.685226
Train - Epoch 23, Batch: 0, Loss: 0.684843
Train - Epoch 24, Batch: 0, Loss: 0.684425
Train - Epoch 25, Batch: 0, Loss: 0.684143
Train - Epoch 26, Batch: 0, Loss: 0.683656
Train - Epoch 27, Batch: 0, Loss: 0.683456
Train - Epoch 28, Batch: 0, Loss: 0.683050
Train - Epoch 29, Batch: 0, Loss: 0.682698
Train - Epoch 30, Batch: 0, Loss: 0.682302
Train - Epoch 31, Batch: 0, Loss: 0.682167
Train - Epoch 32, Batch: 0, Loss: 0.681615
Train - Epoch 33, Batch: 0, Loss: 0.681244
Train - Epoch 34, Batch: 0, Loss: 0.681081
Train - Epoch 35, Batch: 0, Loss: 0.680463
Train - Epoch 36, Batch: 0, Loss: 0.680292
Train - Epoch 37, Batch: 0, Loss: 0.679863
Train - Epoch 38, Batch: 0, Loss: 0.679652
Train - Epoch 39, Batch: 0, Loss: 0.679205
Train - Epoch 40, Batch: 0, Loss: 0.678892
Train - Epoch 41, Batch: 0, Loss: 0.678504
Train - Epoch 42, Batch: 0, Loss: 0.678248
Train - Epoch 43, Batch: 0, Loss: 0.677868
Train - Epoch 44, Batch: 0, Loss: 0.677571
Train - Epoch 45, Batch: 0, Loss: 0.677095
Train - Epoch 46, Batch: 0, Loss: 0.676752
Train - Epoch 47, Batch: 0, Loss: 0.676468
Train - Epoch 48, Batch: 0, Loss: 0.676006
Train - Epoch 49, Batch: 0, Loss: 0.675766
Train - Epoch 50, Batch: 0, Loss: 0.675696
Train - Epoch 51, Batch: 0, Loss: 0.675137
Train - Epoch 52, Batch: 0, Loss: 0.674670
Train - Epoch 53, Batch: 0, Loss: 0.674406
Train - Epoch 54, Batch: 0, Loss: 0.674195
Train - Epoch 55, Batch: 0, Loss: 0.673913
Train - Epoch 56, Batch: 0, Loss: 0.673644
Train - Epoch 57, Batch: 0, Loss: 0.673324
Train - Epoch 58, Batch: 0, Loss: 0.672677
Train - Epoch 59, Batch: 0, Loss: 0.672388
Train - Epoch 60, Batch: 0, Loss: 0.672189
Train - Epoch 61, Batch: 0, Loss: 0.672014
Train - Epoch 62, Batch: 0, Loss: 0.671423
Train - Epoch 63, Batch: 0, Loss: 0.671151
Train - Epoch 64, Batch: 0, Loss: 0.670820
Train - Epoch 65, Batch: 0, Loss: 0.670631
Train - Epoch 66, Batch: 0, Loss: 0.670038
Train - Epoch 67, Batch: 0, Loss: 0.669945
Train - Epoch 68, Batch: 0, Loss: 0.669687
Train - Epoch 69, Batch: 0, Loss: 0.669301
Train - Epoch 70, Batch: 0, Loss: 0.668973
Train - Epoch 71, Batch: 0, Loss: 0.668667
Train - Epoch 72, Batch: 0, Loss: 0.668315
Train - Epoch 73, Batch: 0, Loss: 0.667977
Train - Epoch 74, Batch: 0, Loss: 0.667557
Train - Epoch 75, Batch: 0, Loss: 0.667361
Train - Epoch 76, Batch: 0, Loss: 0.666991
Train - Epoch 77, Batch: 0, Loss: 0.666802
Train - Epoch 78, Batch: 0, Loss: 0.666563
Train - Epoch 79, Batch: 0, Loss: 0.666142
Train - Epoch 80, Batch: 0, Loss: 0.665545
Train - Epoch 81, Batch: 0, Loss: 0.665158
Train - Epoch 82, Batch: 0, Loss: 0.664774
Train - Epoch 83, Batch: 0, Loss: 0.664954
Train - Epoch 84, Batch: 0, Loss: 0.664555
Train - Epoch 85, Batch: 0, Loss: 0.663992
Train - Epoch 86, Batch: 0, Loss: 0.664010
Train - Epoch 87, Batch: 0, Loss: 0.663642
Train - Epoch 88, Batch: 0, Loss: 0.663239
Train - Epoch 89, Batch: 0, Loss: 0.662795
Train - Epoch 90, Batch: 0, Loss: 0.662568
Train - Epoch 91, Batch: 0, Loss: 0.662268
Train - Epoch 92, Batch: 0, Loss: 0.661914
Train - Epoch 93, Batch: 0, Loss: 0.661993
Train - Epoch 94, Batch: 0, Loss: 0.661729
Train - Epoch 95, Batch: 0, Loss: 0.661039
Train - Epoch 96, Batch: 0, Loss: 0.660865
Train - Epoch 97, Batch: 0, Loss: 0.660383
Train - Epoch 98, Batch: 0, Loss: 0.659908
Train - Epoch 99, Batch: 0, Loss: 0.659707
Train - Epoch 100, Batch: 0, Loss: 0.659982
Train - Epoch 101, Batch: 0, Loss: 0.659484
Train - Epoch 102, Batch: 0, Loss: 0.659102
Train - Epoch 103, Batch: 0, Loss: 0.658695
Train - Epoch 104, Batch: 0, Loss: 0.658676
Train - Epoch 105, Batch: 0, Loss: 0.658276
Train - Epoch 106, Batch: 0, Loss: 0.658007
Train - Epoch 107, Batch: 0, Loss: 0.657385
Train - Epoch 108, Batch: 0, Loss: 0.657221
Train - Epoch 109, Batch: 0, Loss: 0.656585
Train - Epoch 110, Batch: 0, Loss: 0.656899
Train - Epoch 111, Batch: 0, Loss: 0.656253
Train - Epoch 112, Batch: 0, Loss: 0.656198
Train - Epoch 113, Batch: 0, Loss: 0.655801
Train - Epoch 114, Batch: 0, Loss: 0.655662
Train - Epoch 115, Batch: 0, Loss: 0.655104
Train - Epoch 116, Batch: 0, Loss: 0.655229
Train - Epoch 117, Batch: 0, Loss: 0.655009
Train - Epoch 118, Batch: 0, Loss: 0.654047
Train - Epoch 119, Batch: 0, Loss: 0.653733
Train - Epoch 120, Batch: 0, Loss: 0.653600
Train - Epoch 121, Batch: 0, Loss: 0.653290
Train - Epoch 122, Batch: 0, Loss: 0.653789
Train - Epoch 123, Batch: 0, Loss: 0.652464
Train - Epoch 124, Batch: 0, Loss: 0.652134
Train - Epoch 125, Batch: 0, Loss: 0.652477
Train - Epoch 126, Batch: 0, Loss: 0.651865
Train - Epoch 127, Batch: 0, Loss: 0.652248
Train - Epoch 128, Batch: 0, Loss: 0.651319
Train - Epoch 129, Batch: 0, Loss: 0.650998
Train - Epoch 130, Batch: 0, Loss: 0.651019
Train - Epoch 131, Batch: 0, Loss: 0.650442
Train - Epoch 132, Batch: 0, Loss: 0.650638
Train - Epoch 133, Batch: 0, Loss: 0.650576
Train - Epoch 134, Batch: 0, Loss: 0.649686
Train - Epoch 135, Batch: 0, Loss: 0.649629
Train - Epoch 136, Batch: 0, Loss: 0.649123
Train - Epoch 137, Batch: 0, Loss: 0.648716
Train - Epoch 138, Batch: 0, Loss: 0.648864
Train - Epoch 139, Batch: 0, Loss: 0.648173
Train - Epoch 140, Batch: 0, Loss: 0.648644
Train - Epoch 141, Batch: 0, Loss: 0.647796
Train - Epoch 142, Batch: 0, Loss: 0.648174
Train - Epoch 143, Batch: 0, Loss: 0.647316
Train - Epoch 144, Batch: 0, Loss: 0.646910
Train - Epoch 145, Batch: 0, Loss: 0.647189
Train - Epoch 146, Batch: 0, Loss: 0.646484
Train - Epoch 147, Batch: 0, Loss: 0.646385
Train - Epoch 148, Batch: 0, Loss: 0.645734
Train - Epoch 149, Batch: 0, Loss: 0.645801
Train - Epoch 150, Batch: 0, Loss: 0.645288
Train - Epoch 151, Batch: 0, Loss: 0.645001
Train - Epoch 152, Batch: 0, Loss: 0.644997
Train - Epoch 153, Batch: 0, Loss: 0.644700
Train - Epoch 154, Batch: 0, Loss: 0.644450
Train - Epoch 155, Batch: 0, Loss: 0.644606
Train - Epoch 156, Batch: 0, Loss: 0.643643
Train - Epoch 157, Batch: 0, Loss: 0.643742
Train - Epoch 158, Batch: 0, Loss: 0.643731
Train - Epoch 159, Batch: 0, Loss: 0.643276
Train - Epoch 160, Batch: 0, Loss: 0.642788
Train - Epoch 161, Batch: 0, Loss: 0.642601
Train - Epoch 162, Batch: 0, Loss: 0.642460
Train - Epoch 163, Batch: 0, Loss: 0.642047
Train - Epoch 164, Batch: 0, Loss: 0.641515
Train - Epoch 165, Batch: 0, Loss: 0.641469
Train - Epoch 166, Batch: 0, Loss: 0.641500
Train - Epoch 167, Batch: 0, Loss: 0.641143
Train - Epoch 168, Batch: 0, Loss: 0.640899
Train - Epoch 169, Batch: 0, Loss: 0.640709
Train - Epoch 170, Batch: 0, Loss: 0.639879
Train - Epoch 171, Batch: 0, Loss: 0.639590
Train - Epoch 172, Batch: 0, Loss: 0.639762
Train - Epoch 173, Batch: 0, Loss: 0.639537
Train - Epoch 174, Batch: 0, Loss: 0.639045
Train - Epoch 175, Batch: 0, Loss: 0.638809
Train - Epoch 176, Batch: 0, Loss: 0.639021
Train - Epoch 177, Batch: 0, Loss: 0.638703
Train - Epoch 178, Batch: 0, Loss: 0.638150
Train - Epoch 179, Batch: 0, Loss: 0.637826
Train - Epoch 180, Batch: 0, Loss: 0.637376
Train - Epoch 181, Batch: 0, Loss: 0.637902
Train - Epoch 182, Batch: 0, Loss: 0.637182
Train - Epoch 183, Batch: 0, Loss: 0.636377
Train - Epoch 184, Batch: 0, Loss: 0.636597
Train - Epoch 185, Batch: 0, Loss: 0.636115
Train - Epoch 186, Batch: 0, Loss: 0.636465/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635579
Train - Epoch 188, Batch: 0, Loss: 0.636523
Train - Epoch 189, Batch: 0, Loss: 0.635155
Train - Epoch 190, Batch: 0, Loss: 0.635230
Train - Epoch 191, Batch: 0, Loss: 0.634833
Train - Epoch 192, Batch: 0, Loss: 0.634535
Train - Epoch 193, Batch: 0, Loss: 0.634709
Train - Epoch 194, Batch: 0, Loss: 0.633828
Train - Epoch 195, Batch: 0, Loss: 0.633787
Train - Epoch 196, Batch: 0, Loss: 0.634003
Train - Epoch 197, Batch: 0, Loss: 0.633455
Train - Epoch 198, Batch: 0, Loss: 0.633537
Train - Epoch 199, Batch: 0, Loss: 0.632472
training_time:: 351.17884588241577
training time full:: 351.17889070510864
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926242
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 20
training time is 300.73542761802673
overhead:: 0
overhead2:: 0
time_baseline:: 300.73608565330505
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926489
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624304)
RCV1 Test Avg. Accuracy:: 0.9216193115135983
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 20
max_epoch:: 200
overhead:: 0
overhead2:: 0.02112603187561035
overhead3:: 0.10831522941589355
overhead4:: 35.3253276348114
overhead5:: 0
time_provenance:: 46.0271360874176
curr_diff: 0 tensor(2.6122e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6122e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926489
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.001_10200
tensor(624304)
RCV1 Test Avg. Accuracy:: 0.9216193115135983
deletion rate:: 0.01
python3 generate_rand_ids 0.01  rcv1 0
tensor([12802,  3075,  2573, 12303,  6674,  6162, 17938, 10773, 17946, 18460,
         3101, 12828, 17437,  2592, 12833,   545, 15902,  8741,  9768,  4648,
        18989,  2093, 15918, 13869, 15406,  9777,  3638,  8759,    55, 13370,
         4671, 10303,  9286,  9806, 13902,  4688,  6736, 14417,  6227, 12375,
        18009, 16478, 11870,  2656,  3680,  3170,  2659,   100, 18531,  4710,
        11883, 14956,   623, 12401,   627, 19572, 18554,  9348,  8327,  3210,
        17043, 14997, 11414,  6295,   155, 19107,  9380, 13475,  5285,  6822,
         8881,  6323,  6836,  1206, 14521, 13498, 17083,  6333, 18622, 16577,
         6850, 18626, 19652,  4811, 11468, 19149, 11471,   720,  6365, 20192,
        13538, 10978, 16108, 14577, 11509, 16118, 10489,  4347, 16650, 11022,
        14607,  9998,  1809,  2320, 16145, 16147,  1825,  7969, 14115, 11557,
        17192, 17706,  5931, 13102, 14127,  3378, 11578,  6460,   829,   317,
        17217,  1860, 17222,  4936, 17226, 13132, 15186,  5971, 17749,  5463,
         9561, 19805, 10078,  1889, 12130, 10597,  7016,  9065, 17771, 14189,
         2414, 15727, 15217, 17781, 19319, 10108, 17791,  7551, 17796, 10629,
          901,  9608, 15241, 11146, 10642, 14227,  9619,  8597,  1430, 16791,
         8088,  8089,  2458, 15772,  3998, 13216,  3489,  9123, 17829,  3495,
         3496,  2985, 13225,  9130,  6055, 17325, 12204,  6065, 10674,  1971,
         9652,  6585,  3517, 19390,  6088, 15305,  6601, 15308,  1484,  6606,
         1493,  9178, 12763, 11745,  3043,  7659,  5619, 11251,  6132, 10741,
        10233, 11259])
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693123
Train - Epoch 1, Batch: 0, Loss: 0.692740
Train - Epoch 2, Batch: 0, Loss: 0.692363
Train - Epoch 3, Batch: 0, Loss: 0.692015
Train - Epoch 4, Batch: 0, Loss: 0.691612
Train - Epoch 5, Batch: 0, Loss: 0.691230
Train - Epoch 6, Batch: 0, Loss: 0.690862
Train - Epoch 7, Batch: 0, Loss: 0.690545
Train - Epoch 8, Batch: 0, Loss: 0.690127
Train - Epoch 9, Batch: 0, Loss: 0.689774
Train - Epoch 10, Batch: 0, Loss: 0.689423
Train - Epoch 11, Batch: 0, Loss: 0.689027
Train - Epoch 12, Batch: 0, Loss: 0.688721
Train - Epoch 13, Batch: 0, Loss: 0.688320
Train - Epoch 14, Batch: 0, Loss: 0.687937
Train - Epoch 15, Batch: 0, Loss: 0.687654
Train - Epoch 16, Batch: 0, Loss: 0.687218
Train - Epoch 17, Batch: 0, Loss: 0.686816
Train - Epoch 18, Batch: 0, Loss: 0.686530
Train - Epoch 19, Batch: 0, Loss: 0.686203
Train - Epoch 20, Batch: 0, Loss: 0.685731
Train - Epoch 21, Batch: 0, Loss: 0.685365
Train - Epoch 22, Batch: 0, Loss: 0.684964
Train - Epoch 23, Batch: 0, Loss: 0.684723
Train - Epoch 24, Batch: 0, Loss: 0.684370
Train - Epoch 25, Batch: 0, Loss: 0.684061
Train - Epoch 26, Batch: 0, Loss: 0.683611
Train - Epoch 27, Batch: 0, Loss: 0.683323
Train - Epoch 28, Batch: 0, Loss: 0.682878
Train - Epoch 29, Batch: 0, Loss: 0.682583
Train - Epoch 30, Batch: 0, Loss: 0.682282
Train - Epoch 31, Batch: 0, Loss: 0.681771
Train - Epoch 32, Batch: 0, Loss: 0.681519
Train - Epoch 33, Batch: 0, Loss: 0.681357
Train - Epoch 34, Batch: 0, Loss: 0.680820
Train - Epoch 35, Batch: 0, Loss: 0.680473
Train - Epoch 36, Batch: 0, Loss: 0.680148
Train - Epoch 37, Batch: 0, Loss: 0.679784
Train - Epoch 38, Batch: 0, Loss: 0.679574
Train - Epoch 39, Batch: 0, Loss: 0.679187
Train - Epoch 40, Batch: 0, Loss: 0.678882
Train - Epoch 41, Batch: 0, Loss: 0.678383
Train - Epoch 42, Batch: 0, Loss: 0.678191
Train - Epoch 43, Batch: 0, Loss: 0.677858
Train - Epoch 44, Batch: 0, Loss: 0.677401
Train - Epoch 45, Batch: 0, Loss: 0.677237
Train - Epoch 46, Batch: 0, Loss: 0.676771
Train - Epoch 47, Batch: 0, Loss: 0.676332
Train - Epoch 48, Batch: 0, Loss: 0.676020
Train - Epoch 49, Batch: 0, Loss: 0.675677
Train - Epoch 50, Batch: 0, Loss: 0.675495
Train - Epoch 51, Batch: 0, Loss: 0.674882
Train - Epoch 52, Batch: 0, Loss: 0.674745
Train - Epoch 53, Batch: 0, Loss: 0.674643
Train - Epoch 54, Batch: 0, Loss: 0.674073
Train - Epoch 55, Batch: 0, Loss: 0.673572
Train - Epoch 56, Batch: 0, Loss: 0.673499
Train - Epoch 57, Batch: 0, Loss: 0.673120
Train - Epoch 58, Batch: 0, Loss: 0.672888
Train - Epoch 59, Batch: 0, Loss: 0.672579
Train - Epoch 60, Batch: 0, Loss: 0.672117
Train - Epoch 61, Batch: 0, Loss: 0.671833
Train - Epoch 62, Batch: 0, Loss: 0.671594
Train - Epoch 63, Batch: 0, Loss: 0.671175
Train - Epoch 64, Batch: 0, Loss: 0.670765
Train - Epoch 65, Batch: 0, Loss: 0.670645
Train - Epoch 66, Batch: 0, Loss: 0.670244
Train - Epoch 67, Batch: 0, Loss: 0.669940
Train - Epoch 68, Batch: 0, Loss: 0.669473
Train - Epoch 69, Batch: 0, Loss: 0.669049
Train - Epoch 70, Batch: 0, Loss: 0.669036
Train - Epoch 71, Batch: 0, Loss: 0.668411
Train - Epoch 72, Batch: 0, Loss: 0.668232
Train - Epoch 73, Batch: 0, Loss: 0.667954
Train - Epoch 74, Batch: 0, Loss: 0.667597
Train - Epoch 75, Batch: 0, Loss: 0.667338
Train - Epoch 76, Batch: 0, Loss: 0.666833
Train - Epoch 77, Batch: 0, Loss: 0.666750
Train - Epoch 78, Batch: 0, Loss: 0.666177
Train - Epoch 79, Batch: 0, Loss: 0.665844
Train - Epoch 80, Batch: 0, Loss: 0.665914
Train - Epoch 81, Batch: 0, Loss: 0.665171
Train - Epoch 82, Batch: 0, Loss: 0.665000
Train - Epoch 83, Batch: 0, Loss: 0.664221
Train - Epoch 84, Batch: 0, Loss: 0.664439
Train - Epoch 85, Batch: 0, Loss: 0.664051
Train - Epoch 86, Batch: 0, Loss: 0.663663
Train - Epoch 87, Batch: 0, Loss: 0.663786
Train - Epoch 88, Batch: 0, Loss: 0.663247
Train - Epoch 89, Batch: 0, Loss: 0.662626
Train - Epoch 90, Batch: 0, Loss: 0.662766
Train - Epoch 91, Batch: 0, Loss: 0.662768
Train - Epoch 92, Batch: 0, Loss: 0.661932
Train - Epoch 93, Batch: 0, Loss: 0.661427
Train - Epoch 94, Batch: 0, Loss: 0.661589
Train - Epoch 95, Batch: 0, Loss: 0.661315
Train - Epoch 96, Batch: 0, Loss: 0.660607
Train - Epoch 97, Batch: 0, Loss: 0.660596
Train - Epoch 98, Batch: 0, Loss: 0.660137
Train - Epoch 99, Batch: 0, Loss: 0.659838
Train - Epoch 100, Batch: 0, Loss: 0.660275
Train - Epoch 101, Batch: 0, Loss: 0.659437
Train - Epoch 102, Batch: 0, Loss: 0.658826
Train - Epoch 103, Batch: 0, Loss: 0.658751
Train - Epoch 104, Batch: 0, Loss: 0.658407
Train - Epoch 105, Batch: 0, Loss: 0.658137
Train - Epoch 106, Batch: 0, Loss: 0.657623
Train - Epoch 107, Batch: 0, Loss: 0.657818
Train - Epoch 108, Batch: 0, Loss: 0.657202
Train - Epoch 109, Batch: 0, Loss: 0.656825
Train - Epoch 110, Batch: 0, Loss: 0.656784
Train - Epoch 111, Batch: 0, Loss: 0.656131
Train - Epoch 112, Batch: 0, Loss: 0.655745
Train - Epoch 113, Batch: 0, Loss: 0.655615
Train - Epoch 114, Batch: 0, Loss: 0.655281
Train - Epoch 115, Batch: 0, Loss: 0.655443
Train - Epoch 116, Batch: 0, Loss: 0.655045
Train - Epoch 117, Batch: 0, Loss: 0.654255
Train - Epoch 118, Batch: 0, Loss: 0.654424
Train - Epoch 119, Batch: 0, Loss: 0.654037
Train - Epoch 120, Batch: 0, Loss: 0.653776
Train - Epoch 121, Batch: 0, Loss: 0.653475
Train - Epoch 122, Batch: 0, Loss: 0.652883
Train - Epoch 123, Batch: 0, Loss: 0.652569
Train - Epoch 124, Batch: 0, Loss: 0.652272
Train - Epoch 125, Batch: 0, Loss: 0.652612
Train - Epoch 126, Batch: 0, Loss: 0.652467
Train - Epoch 127, Batch: 0, Loss: 0.651770
Train - Epoch 128, Batch: 0, Loss: 0.651425
Train - Epoch 129, Batch: 0, Loss: 0.651215
Train - Epoch 130, Batch: 0, Loss: 0.651087
Train - Epoch 131, Batch: 0, Loss: 0.650647
Train - Epoch 132, Batch: 0, Loss: 0.650355
Train - Epoch 133, Batch: 0, Loss: 0.650026
Train - Epoch 134, Batch: 0, Loss: 0.649427
Train - Epoch 135, Batch: 0, Loss: 0.649386
Train - Epoch 136, Batch: 0, Loss: 0.648882
Train - Epoch 137, Batch: 0, Loss: 0.649094
Train - Epoch 138, Batch: 0, Loss: 0.648527
Train - Epoch 139, Batch: 0, Loss: 0.648388
Train - Epoch 140, Batch: 0, Loss: 0.648375
Train - Epoch 141, Batch: 0, Loss: 0.648033
Train - Epoch 142, Batch: 0, Loss: 0.648005
Train - Epoch 143, Batch: 0, Loss: 0.647569
Train - Epoch 144, Batch: 0, Loss: 0.647195
Train - Epoch 145, Batch: 0, Loss: 0.646351
Train - Epoch 146, Batch: 0, Loss: 0.646521
Train - Epoch 147, Batch: 0, Loss: 0.646098
Train - Epoch 148, Batch: 0, Loss: 0.645814
Train - Epoch 149, Batch: 0, Loss: 0.646570
Train - Epoch 150, Batch: 0, Loss: 0.645365
Train - Epoch 151, Batch: 0, Loss: 0.645584
Train - Epoch 152, Batch: 0, Loss: 0.645519
Train - Epoch 153, Batch: 0, Loss: 0.644714
Train - Epoch 154, Batch: 0, Loss: 0.644769
Train - Epoch 155, Batch: 0, Loss: 0.643577
Train - Epoch 156, Batch: 0, Loss: 0.643557
Train - Epoch 157, Batch: 0, Loss: 0.644063
Train - Epoch 158, Batch: 0, Loss: 0.643582
Train - Epoch 159, Batch: 0, Loss: 0.643073
Train - Epoch 160, Batch: 0, Loss: 0.642845
Train - Epoch 161, Batch: 0, Loss: 0.642665
Train - Epoch 162, Batch: 0, Loss: 0.642100
Train - Epoch 163, Batch: 0, Loss: 0.641957
Train - Epoch 164, Batch: 0, Loss: 0.641645
Train - Epoch 165, Batch: 0, Loss: 0.641758
Train - Epoch 166, Batch: 0, Loss: 0.640841
Train - Epoch 167, Batch: 0, Loss: 0.641052
Train - Epoch 168, Batch: 0, Loss: 0.640907
Train - Epoch 169, Batch: 0, Loss: 0.640840
Train - Epoch 170, Batch: 0, Loss: 0.640034
Train - Epoch 171, Batch: 0, Loss: 0.639691
Train - Epoch 172, Batch: 0, Loss: 0.639850
Train - Epoch 173, Batch: 0, Loss: 0.639208
Train - Epoch 174, Batch: 0, Loss: 0.639073
Train - Epoch 175, Batch: 0, Loss: 0.639493
Train - Epoch 176, Batch: 0, Loss: 0.639000
Train - Epoch 177, Batch: 0, Loss: 0.638413
Train - Epoch 178, Batch: 0, Loss: 0.638500
Train - Epoch 179, Batch: 0, Loss: 0.637436
Train - Epoch 180, Batch: 0, Loss: 0.637895
Train - Epoch 181, Batch: 0, Loss: 0.637231
Train - Epoch 182, Batch: 0, Loss: 0.637167
Train - Epoch 183, Batch: 0, Loss: 0.636764
Train - Epoch 184, Batch: 0, Loss: 0.636771
Train - Epoch 185, Batch: 0, Loss: 0.636069
Train - Epoch 186, Batch: 0, Loss: 0.635936/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635943
Train - Epoch 188, Batch: 0, Loss: 0.636129
Train - Epoch 189, Batch: 0, Loss: 0.635116
Train - Epoch 190, Batch: 0, Loss: 0.635412
Train - Epoch 191, Batch: 0, Loss: 0.634934
Train - Epoch 192, Batch: 0, Loss: 0.634373
Train - Epoch 193, Batch: 0, Loss: 0.634757
Train - Epoch 194, Batch: 0, Loss: 0.633849
Train - Epoch 195, Batch: 0, Loss: 0.634044
Train - Epoch 196, Batch: 0, Loss: 0.633535
Train - Epoch 197, Batch: 0, Loss: 0.633379
Train - Epoch 198, Batch: 0, Loss: 0.633307
Train - Epoch 199, Batch: 0, Loss: 0.632592
training_time:: 349.86118626594543
training time full:: 349.861230134964
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000063, Accuracy: 0.927626
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 202
training time is 302.97349882125854
overhead:: 0
overhead2:: 0
time_baseline:: 302.97419023513794
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927527
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624954)
RCV1 Test Avg. Accuracy:: 0.9225788641553944
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.021062374114990234
overhead3:: 0.10671758651733398
overhead4:: 34.75805616378784
overhead5:: 0
time_provenance:: 48.42181849479675
curr_diff: 0 tensor(1.5342e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5342e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927527
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_0_0.01_10200
tensor(624955)
RCV1 Test Avg. Accuracy:: 0.922580340390228
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693010
Train - Epoch 1, Batch: 0, Loss: 0.692593
Train - Epoch 2, Batch: 0, Loss: 0.692247
Train - Epoch 3, Batch: 0, Loss: 0.691852
Train - Epoch 4, Batch: 0, Loss: 0.691511
Train - Epoch 5, Batch: 0, Loss: 0.691130
Train - Epoch 6, Batch: 0, Loss: 0.690748
Train - Epoch 7, Batch: 0, Loss: 0.690381
Train - Epoch 8, Batch: 0, Loss: 0.690037
Train - Epoch 9, Batch: 0, Loss: 0.689653
Train - Epoch 10, Batch: 0, Loss: 0.689271
Train - Epoch 11, Batch: 0, Loss: 0.688943
Train - Epoch 12, Batch: 0, Loss: 0.688545
Train - Epoch 13, Batch: 0, Loss: 0.688130
Train - Epoch 14, Batch: 0, Loss: 0.687801
Train - Epoch 15, Batch: 0, Loss: 0.687427
Train - Epoch 16, Batch: 0, Loss: 0.687077
Train - Epoch 17, Batch: 0, Loss: 0.686748
Train - Epoch 18, Batch: 0, Loss: 0.686312
Train - Epoch 19, Batch: 0, Loss: 0.685910
Train - Epoch 20, Batch: 0, Loss: 0.685695
Train - Epoch 21, Batch: 0, Loss: 0.685328
Train - Epoch 22, Batch: 0, Loss: 0.684871
Train - Epoch 23, Batch: 0, Loss: 0.684586
Train - Epoch 24, Batch: 0, Loss: 0.684205
Train - Epoch 25, Batch: 0, Loss: 0.683860
Train - Epoch 26, Batch: 0, Loss: 0.683468
Train - Epoch 27, Batch: 0, Loss: 0.683106
Train - Epoch 28, Batch: 0, Loss: 0.682840
Train - Epoch 29, Batch: 0, Loss: 0.682509
Train - Epoch 30, Batch: 0, Loss: 0.682073
Train - Epoch 31, Batch: 0, Loss: 0.681849
Train - Epoch 32, Batch: 0, Loss: 0.681401
Train - Epoch 33, Batch: 0, Loss: 0.681003
Train - Epoch 34, Batch: 0, Loss: 0.680756
Train - Epoch 35, Batch: 0, Loss: 0.680444
Train - Epoch 36, Batch: 0, Loss: 0.679961
Train - Epoch 37, Batch: 0, Loss: 0.679623
Train - Epoch 38, Batch: 0, Loss: 0.679436
Train - Epoch 39, Batch: 0, Loss: 0.679008
Train - Epoch 40, Batch: 0, Loss: 0.678806
Train - Epoch 41, Batch: 0, Loss: 0.678374
Train - Epoch 42, Batch: 0, Loss: 0.677979
Train - Epoch 43, Batch: 0, Loss: 0.677718
Train - Epoch 44, Batch: 0, Loss: 0.677150
Train - Epoch 45, Batch: 0, Loss: 0.677109
Train - Epoch 46, Batch: 0, Loss: 0.676599
Train - Epoch 47, Batch: 0, Loss: 0.676249
Train - Epoch 48, Batch: 0, Loss: 0.676025
Train - Epoch 49, Batch: 0, Loss: 0.675546
Train - Epoch 50, Batch: 0, Loss: 0.675272
Train - Epoch 51, Batch: 0, Loss: 0.674720
Train - Epoch 52, Batch: 0, Loss: 0.674538
Train - Epoch 53, Batch: 0, Loss: 0.674389
Train - Epoch 54, Batch: 0, Loss: 0.673946
Train - Epoch 55, Batch: 0, Loss: 0.673712
Train - Epoch 56, Batch: 0, Loss: 0.673407
Train - Epoch 57, Batch: 0, Loss: 0.672914
Train - Epoch 58, Batch: 0, Loss: 0.672617
Train - Epoch 59, Batch: 0, Loss: 0.672386
Train - Epoch 60, Batch: 0, Loss: 0.671992
Train - Epoch 61, Batch: 0, Loss: 0.671734
Train - Epoch 62, Batch: 0, Loss: 0.671443
Train - Epoch 63, Batch: 0, Loss: 0.670976
Train - Epoch 64, Batch: 0, Loss: 0.670671
Train - Epoch 65, Batch: 0, Loss: 0.670428
Train - Epoch 66, Batch: 0, Loss: 0.669881
Train - Epoch 67, Batch: 0, Loss: 0.669718
Train - Epoch 68, Batch: 0, Loss: 0.669320
Train - Epoch 69, Batch: 0, Loss: 0.669153
Train - Epoch 70, Batch: 0, Loss: 0.668402
Train - Epoch 71, Batch: 0, Loss: 0.668315
Train - Epoch 72, Batch: 0, Loss: 0.667989
Train - Epoch 73, Batch: 0, Loss: 0.667730
Train - Epoch 74, Batch: 0, Loss: 0.667475
Train - Epoch 75, Batch: 0, Loss: 0.667050
Train - Epoch 76, Batch: 0, Loss: 0.666976
Train - Epoch 77, Batch: 0, Loss: 0.666711
Train - Epoch 78, Batch: 0, Loss: 0.665976
Train - Epoch 79, Batch: 0, Loss: 0.666080
Train - Epoch 80, Batch: 0, Loss: 0.665565
Train - Epoch 81, Batch: 0, Loss: 0.665233
Train - Epoch 82, Batch: 0, Loss: 0.664943
Train - Epoch 83, Batch: 0, Loss: 0.664404
Train - Epoch 84, Batch: 0, Loss: 0.664637
Train - Epoch 85, Batch: 0, Loss: 0.663956
Train - Epoch 86, Batch: 0, Loss: 0.663605
Train - Epoch 87, Batch: 0, Loss: 0.663295
Train - Epoch 88, Batch: 0, Loss: 0.663334
Train - Epoch 89, Batch: 0, Loss: 0.662794
Train - Epoch 90, Batch: 0, Loss: 0.662306
Train - Epoch 91, Batch: 0, Loss: 0.662509
Train - Epoch 92, Batch: 0, Loss: 0.661765
Train - Epoch 93, Batch: 0, Loss: 0.661459
Train - Epoch 94, Batch: 0, Loss: 0.661320
Train - Epoch 95, Batch: 0, Loss: 0.660895
Train - Epoch 96, Batch: 0, Loss: 0.660952
Train - Epoch 97, Batch: 0, Loss: 0.660649
Train - Epoch 98, Batch: 0, Loss: 0.660327
Train - Epoch 99, Batch: 0, Loss: 0.659822
Train - Epoch 100, Batch: 0, Loss: 0.659353
Train - Epoch 101, Batch: 0, Loss: 0.659499
Train - Epoch 102, Batch: 0, Loss: 0.658946
Train - Epoch 103, Batch: 0, Loss: 0.658389
Train - Epoch 104, Batch: 0, Loss: 0.658478
Train - Epoch 105, Batch: 0, Loss: 0.657948
Train - Epoch 106, Batch: 0, Loss: 0.657609
Train - Epoch 107, Batch: 0, Loss: 0.657278
Train - Epoch 108, Batch: 0, Loss: 0.656864
Train - Epoch 109, Batch: 0, Loss: 0.657031
Train - Epoch 110, Batch: 0, Loss: 0.656572
Train - Epoch 111, Batch: 0, Loss: 0.656415
Train - Epoch 112, Batch: 0, Loss: 0.656212
Train - Epoch 113, Batch: 0, Loss: 0.655501
Train - Epoch 114, Batch: 0, Loss: 0.655209
Train - Epoch 115, Batch: 0, Loss: 0.654664
Train - Epoch 116, Batch: 0, Loss: 0.654217
Train - Epoch 117, Batch: 0, Loss: 0.654631
Train - Epoch 118, Batch: 0, Loss: 0.654201
Train - Epoch 119, Batch: 0, Loss: 0.653521
Train - Epoch 120, Batch: 0, Loss: 0.653774
Train - Epoch 121, Batch: 0, Loss: 0.653152
Train - Epoch 122, Batch: 0, Loss: 0.653197
Train - Epoch 123, Batch: 0, Loss: 0.652952
Train - Epoch 124, Batch: 0, Loss: 0.652166
Train - Epoch 125, Batch: 0, Loss: 0.652264
Train - Epoch 126, Batch: 0, Loss: 0.651971
Train - Epoch 127, Batch: 0, Loss: 0.651613
Train - Epoch 128, Batch: 0, Loss: 0.651491
Train - Epoch 129, Batch: 0, Loss: 0.651184
Train - Epoch 130, Batch: 0, Loss: 0.650803
Train - Epoch 131, Batch: 0, Loss: 0.650430
Train - Epoch 132, Batch: 0, Loss: 0.650170
Train - Epoch 133, Batch: 0, Loss: 0.650280
Train - Epoch 134, Batch: 0, Loss: 0.649853
Train - Epoch 135, Batch: 0, Loss: 0.649546
Train - Epoch 136, Batch: 0, Loss: 0.649241
Train - Epoch 137, Batch: 0, Loss: 0.648685
Train - Epoch 138, Batch: 0, Loss: 0.648006
Train - Epoch 139, Batch: 0, Loss: 0.648186
Train - Epoch 140, Batch: 0, Loss: 0.648264
Train - Epoch 141, Batch: 0, Loss: 0.647615
Train - Epoch 142, Batch: 0, Loss: 0.647449
Train - Epoch 143, Batch: 0, Loss: 0.647810
Train - Epoch 144, Batch: 0, Loss: 0.647318
Train - Epoch 145, Batch: 0, Loss: 0.646506
Train - Epoch 146, Batch: 0, Loss: 0.646898
Train - Epoch 147, Batch: 0, Loss: 0.646036
Train - Epoch 148, Batch: 0, Loss: 0.645823
Train - Epoch 149, Batch: 0, Loss: 0.645309
Train - Epoch 150, Batch: 0, Loss: 0.645265
Train - Epoch 151, Batch: 0, Loss: 0.644745
Train - Epoch 152, Batch: 0, Loss: 0.644626
Train - Epoch 153, Batch: 0, Loss: 0.644601
Train - Epoch 154, Batch: 0, Loss: 0.644621
Train - Epoch 155, Batch: 0, Loss: 0.643606
Train - Epoch 156, Batch: 0, Loss: 0.643956
Train - Epoch 157, Batch: 0, Loss: 0.643655
Train - Epoch 158, Batch: 0, Loss: 0.643503
Train - Epoch 159, Batch: 0, Loss: 0.642285
Train - Epoch 160, Batch: 0, Loss: 0.642506
Train - Epoch 161, Batch: 0, Loss: 0.642872
Train - Epoch 162, Batch: 0, Loss: 0.642072
Train - Epoch 163, Batch: 0, Loss: 0.641956
Train - Epoch 164, Batch: 0, Loss: 0.641365
Train - Epoch 165, Batch: 0, Loss: 0.641514
Train - Epoch 166, Batch: 0, Loss: 0.640967
Train - Epoch 167, Batch: 0, Loss: 0.640874
Train - Epoch 168, Batch: 0, Loss: 0.641381
Train - Epoch 169, Batch: 0, Loss: 0.640431
Train - Epoch 170, Batch: 0, Loss: 0.640254
Train - Epoch 171, Batch: 0, Loss: 0.640288
Train - Epoch 172, Batch: 0, Loss: 0.639638
Train - Epoch 173, Batch: 0, Loss: 0.638983
Train - Epoch 174, Batch: 0, Loss: 0.638954
Train - Epoch 175, Batch: 0, Loss: 0.638869
Train - Epoch 176, Batch: 0, Loss: 0.638211
Train - Epoch 177, Batch: 0, Loss: 0.638997
Train - Epoch 178, Batch: 0, Loss: 0.637435
Train - Epoch 179, Batch: 0, Loss: 0.636890
Train - Epoch 180, Batch: 0, Loss: 0.637542
Train - Epoch 181, Batch: 0, Loss: 0.637335
Train - Epoch 182, Batch: 0, Loss: 0.637169
Train - Epoch 183, Batch: 0, Loss: 0.636267
Train - Epoch 184, Batch: 0, Loss: 0.637076
Train - Epoch 185, Batch: 0, Loss: 0.635539
Train - Epoch 186, Batch: 0, Loss: 0.636448/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635763
Train - Epoch 188, Batch: 0, Loss: 0.635416
Train - Epoch 189, Batch: 0, Loss: 0.634993
Train - Epoch 190, Batch: 0, Loss: 0.635443
Train - Epoch 191, Batch: 0, Loss: 0.634922
Train - Epoch 192, Batch: 0, Loss: 0.634647
Train - Epoch 193, Batch: 0, Loss: 0.634430
Train - Epoch 194, Batch: 0, Loss: 0.634226
Train - Epoch 195, Batch: 0, Loss: 0.633025
Train - Epoch 196, Batch: 0, Loss: 0.633784
Train - Epoch 197, Batch: 0, Loss: 0.633272
Train - Epoch 198, Batch: 0, Loss: 0.632797
Train - Epoch 199, Batch: 0, Loss: 0.632728
training_time:: 350.84961581230164
training time full:: 350.84966254234314
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926736
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 202
training time is 303.5070822238922
overhead:: 0
overhead2:: 0
time_baseline:: 303.50776410102844
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624940)
RCV1 Test Avg. Accuracy:: 0.9225581968677249
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.022584915161132812
overhead3:: 0.11025714874267578
overhead4:: 34.81333041191101
overhead5:: 0
time_provenance:: 48.521141052246094
curr_diff: 0 tensor(1.5420e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5420e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.927082
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_1_0.01_10200
tensor(624939)
RCV1 Test Avg. Accuracy:: 0.9225567206328914
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693310
Train - Epoch 1, Batch: 0, Loss: 0.692964
Train - Epoch 2, Batch: 0, Loss: 0.692587
Train - Epoch 3, Batch: 0, Loss: 0.692242
Train - Epoch 4, Batch: 0, Loss: 0.691844
Train - Epoch 5, Batch: 0, Loss: 0.691446
Train - Epoch 6, Batch: 0, Loss: 0.691088
Train - Epoch 7, Batch: 0, Loss: 0.690730
Train - Epoch 8, Batch: 0, Loss: 0.690368
Train - Epoch 9, Batch: 0, Loss: 0.689966
Train - Epoch 10, Batch: 0, Loss: 0.689632
Train - Epoch 11, Batch: 0, Loss: 0.689250
Train - Epoch 12, Batch: 0, Loss: 0.688945
Train - Epoch 13, Batch: 0, Loss: 0.688535
Train - Epoch 14, Batch: 0, Loss: 0.688157
Train - Epoch 15, Batch: 0, Loss: 0.687824
Train - Epoch 16, Batch: 0, Loss: 0.687463
Train - Epoch 17, Batch: 0, Loss: 0.687054
Train - Epoch 18, Batch: 0, Loss: 0.686735
Train - Epoch 19, Batch: 0, Loss: 0.686310
Train - Epoch 20, Batch: 0, Loss: 0.685980
Train - Epoch 21, Batch: 0, Loss: 0.685634
Train - Epoch 22, Batch: 0, Loss: 0.685315
Train - Epoch 23, Batch: 0, Loss: 0.684894
Train - Epoch 24, Batch: 0, Loss: 0.684590
Train - Epoch 25, Batch: 0, Loss: 0.684164
Train - Epoch 26, Batch: 0, Loss: 0.683739
Train - Epoch 27, Batch: 0, Loss: 0.683571
Train - Epoch 28, Batch: 0, Loss: 0.683199
Train - Epoch 29, Batch: 0, Loss: 0.682851
Train - Epoch 30, Batch: 0, Loss: 0.682509
Train - Epoch 31, Batch: 0, Loss: 0.682062
Train - Epoch 32, Batch: 0, Loss: 0.681856
Train - Epoch 33, Batch: 0, Loss: 0.681369
Train - Epoch 34, Batch: 0, Loss: 0.681064
Train - Epoch 35, Batch: 0, Loss: 0.680641
Train - Epoch 36, Batch: 0, Loss: 0.680362
Train - Epoch 37, Batch: 0, Loss: 0.680069
Train - Epoch 38, Batch: 0, Loss: 0.679638
Train - Epoch 39, Batch: 0, Loss: 0.679482
Train - Epoch 40, Batch: 0, Loss: 0.678906
Train - Epoch 41, Batch: 0, Loss: 0.678538
Train - Epoch 42, Batch: 0, Loss: 0.678274
Train - Epoch 43, Batch: 0, Loss: 0.677916
Train - Epoch 44, Batch: 0, Loss: 0.677626
Train - Epoch 45, Batch: 0, Loss: 0.677310
Train - Epoch 46, Batch: 0, Loss: 0.676925
Train - Epoch 47, Batch: 0, Loss: 0.676653
Train - Epoch 48, Batch: 0, Loss: 0.676336
Train - Epoch 49, Batch: 0, Loss: 0.675819
Train - Epoch 50, Batch: 0, Loss: 0.675684
Train - Epoch 51, Batch: 0, Loss: 0.675071
Train - Epoch 52, Batch: 0, Loss: 0.674767
Train - Epoch 53, Batch: 0, Loss: 0.674416
Train - Epoch 54, Batch: 0, Loss: 0.674438
Train - Epoch 55, Batch: 0, Loss: 0.673831
Train - Epoch 56, Batch: 0, Loss: 0.673496
Train - Epoch 57, Batch: 0, Loss: 0.673384
Train - Epoch 58, Batch: 0, Loss: 0.672850
Train - Epoch 59, Batch: 0, Loss: 0.672559
Train - Epoch 60, Batch: 0, Loss: 0.672141
Train - Epoch 61, Batch: 0, Loss: 0.671943
Train - Epoch 62, Batch: 0, Loss: 0.671685
Train - Epoch 63, Batch: 0, Loss: 0.671301
Train - Epoch 64, Batch: 0, Loss: 0.671104
Train - Epoch 65, Batch: 0, Loss: 0.670500
Train - Epoch 66, Batch: 0, Loss: 0.670140
Train - Epoch 67, Batch: 0, Loss: 0.670007
Train - Epoch 68, Batch: 0, Loss: 0.669786
Train - Epoch 69, Batch: 0, Loss: 0.669372
Train - Epoch 70, Batch: 0, Loss: 0.668913
Train - Epoch 71, Batch: 0, Loss: 0.668761
Train - Epoch 72, Batch: 0, Loss: 0.668385
Train - Epoch 73, Batch: 0, Loss: 0.668234
Train - Epoch 74, Batch: 0, Loss: 0.667881
Train - Epoch 75, Batch: 0, Loss: 0.667666
Train - Epoch 76, Batch: 0, Loss: 0.667028
Train - Epoch 77, Batch: 0, Loss: 0.666774
Train - Epoch 78, Batch: 0, Loss: 0.666472
Train - Epoch 79, Batch: 0, Loss: 0.666252
Train - Epoch 80, Batch: 0, Loss: 0.665920
Train - Epoch 81, Batch: 0, Loss: 0.665555
Train - Epoch 82, Batch: 0, Loss: 0.665238
Train - Epoch 83, Batch: 0, Loss: 0.664827
Train - Epoch 84, Batch: 0, Loss: 0.664735
Train - Epoch 85, Batch: 0, Loss: 0.664457
Train - Epoch 86, Batch: 0, Loss: 0.664046
Train - Epoch 87, Batch: 0, Loss: 0.663507
Train - Epoch 88, Batch: 0, Loss: 0.663795
Train - Epoch 89, Batch: 0, Loss: 0.662971
Train - Epoch 90, Batch: 0, Loss: 0.663104
Train - Epoch 91, Batch: 0, Loss: 0.662309
Train - Epoch 92, Batch: 0, Loss: 0.662067
Train - Epoch 93, Batch: 0, Loss: 0.661775
Train - Epoch 94, Batch: 0, Loss: 0.661402
Train - Epoch 95, Batch: 0, Loss: 0.661182
Train - Epoch 96, Batch: 0, Loss: 0.661049
Train - Epoch 97, Batch: 0, Loss: 0.660695
Train - Epoch 98, Batch: 0, Loss: 0.660324
Train - Epoch 99, Batch: 0, Loss: 0.659935
Train - Epoch 100, Batch: 0, Loss: 0.660087
Train - Epoch 101, Batch: 0, Loss: 0.659611
Train - Epoch 102, Batch: 0, Loss: 0.659167
Train - Epoch 103, Batch: 0, Loss: 0.659126
Train - Epoch 104, Batch: 0, Loss: 0.658952
Train - Epoch 105, Batch: 0, Loss: 0.657848
Train - Epoch 106, Batch: 0, Loss: 0.657996
Train - Epoch 107, Batch: 0, Loss: 0.657673
Train - Epoch 108, Batch: 0, Loss: 0.657290
Train - Epoch 109, Batch: 0, Loss: 0.657129
Train - Epoch 110, Batch: 0, Loss: 0.657243
Train - Epoch 111, Batch: 0, Loss: 0.656517
Train - Epoch 112, Batch: 0, Loss: 0.656206
Train - Epoch 113, Batch: 0, Loss: 0.655475
Train - Epoch 114, Batch: 0, Loss: 0.655422
Train - Epoch 115, Batch: 0, Loss: 0.655579
Train - Epoch 116, Batch: 0, Loss: 0.654762
Train - Epoch 117, Batch: 0, Loss: 0.654702
Train - Epoch 118, Batch: 0, Loss: 0.654759
Train - Epoch 119, Batch: 0, Loss: 0.654415
Train - Epoch 120, Batch: 0, Loss: 0.653849
Train - Epoch 121, Batch: 0, Loss: 0.653783
Train - Epoch 122, Batch: 0, Loss: 0.653437
Train - Epoch 123, Batch: 0, Loss: 0.653240
Train - Epoch 124, Batch: 0, Loss: 0.653045
Train - Epoch 125, Batch: 0, Loss: 0.652856
Train - Epoch 126, Batch: 0, Loss: 0.652310
Train - Epoch 127, Batch: 0, Loss: 0.652156
Train - Epoch 128, Batch: 0, Loss: 0.651359
Train - Epoch 129, Batch: 0, Loss: 0.651691
Train - Epoch 130, Batch: 0, Loss: 0.651001
Train - Epoch 131, Batch: 0, Loss: 0.650665
Train - Epoch 132, Batch: 0, Loss: 0.650218
Train - Epoch 133, Batch: 0, Loss: 0.650343
Train - Epoch 134, Batch: 0, Loss: 0.650112
Train - Epoch 135, Batch: 0, Loss: 0.649421
Train - Epoch 136, Batch: 0, Loss: 0.648979
Train - Epoch 137, Batch: 0, Loss: 0.649388
Train - Epoch 138, Batch: 0, Loss: 0.649248
Train - Epoch 139, Batch: 0, Loss: 0.648777
Train - Epoch 140, Batch: 0, Loss: 0.648437
Train - Epoch 141, Batch: 0, Loss: 0.648126
Train - Epoch 142, Batch: 0, Loss: 0.647420
Train - Epoch 143, Batch: 0, Loss: 0.647421
Train - Epoch 144, Batch: 0, Loss: 0.647381
Train - Epoch 145, Batch: 0, Loss: 0.646811
Train - Epoch 146, Batch: 0, Loss: 0.647015
Train - Epoch 147, Batch: 0, Loss: 0.646507
Train - Epoch 148, Batch: 0, Loss: 0.646401
Train - Epoch 149, Batch: 0, Loss: 0.645829
Train - Epoch 150, Batch: 0, Loss: 0.645445
Train - Epoch 151, Batch: 0, Loss: 0.645163
Train - Epoch 152, Batch: 0, Loss: 0.645074
Train - Epoch 153, Batch: 0, Loss: 0.644353
Train - Epoch 154, Batch: 0, Loss: 0.644337
Train - Epoch 155, Batch: 0, Loss: 0.644232
Train - Epoch 156, Batch: 0, Loss: 0.644546
Train - Epoch 157, Batch: 0, Loss: 0.643563
Train - Epoch 158, Batch: 0, Loss: 0.643163
Train - Epoch 159, Batch: 0, Loss: 0.642652
Train - Epoch 160, Batch: 0, Loss: 0.642918
Train - Epoch 161, Batch: 0, Loss: 0.642516
Train - Epoch 162, Batch: 0, Loss: 0.642119
Train - Epoch 163, Batch: 0, Loss: 0.641990
Train - Epoch 164, Batch: 0, Loss: 0.641867
Train - Epoch 165, Batch: 0, Loss: 0.641810
Train - Epoch 166, Batch: 0, Loss: 0.642044
Train - Epoch 167, Batch: 0, Loss: 0.641290
Train - Epoch 168, Batch: 0, Loss: 0.641212
Train - Epoch 169, Batch: 0, Loss: 0.640156
Train - Epoch 170, Batch: 0, Loss: 0.641094
Train - Epoch 171, Batch: 0, Loss: 0.639678
Train - Epoch 172, Batch: 0, Loss: 0.639770
Train - Epoch 173, Batch: 0, Loss: 0.639067
Train - Epoch 174, Batch: 0, Loss: 0.639724
Train - Epoch 175, Batch: 0, Loss: 0.639129
Train - Epoch 176, Batch: 0, Loss: 0.638891
Train - Epoch 177, Batch: 0, Loss: 0.638529
Train - Epoch 178, Batch: 0, Loss: 0.638216
Train - Epoch 179, Batch: 0, Loss: 0.637906
Train - Epoch 180, Batch: 0, Loss: 0.637812
Train - Epoch 181, Batch: 0, Loss: 0.637739
Train - Epoch 182, Batch: 0, Loss: 0.637057
Train - Epoch 183, Batch: 0, Loss: 0.637261
Train - Epoch 184, Batch: 0, Loss: 0.636985
Train - Epoch 185, Batch: 0, Loss: 0.636485
Train - Epoch 186, Batch: 0, Loss: 0.636100/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.636691
Train - Epoch 188, Batch: 0, Loss: 0.635624
Train - Epoch 189, Batch: 0, Loss: 0.634884
Train - Epoch 190, Batch: 0, Loss: 0.634790
Train - Epoch 191, Batch: 0, Loss: 0.634799
Train - Epoch 192, Batch: 0, Loss: 0.634763
Train - Epoch 193, Batch: 0, Loss: 0.634762
Train - Epoch 194, Batch: 0, Loss: 0.634458
Train - Epoch 195, Batch: 0, Loss: 0.633985
Train - Epoch 196, Batch: 0, Loss: 0.633355
Train - Epoch 197, Batch: 0, Loss: 0.633463
Train - Epoch 198, Batch: 0, Loss: 0.633253
Train - Epoch 199, Batch: 0, Loss: 0.633161
training_time:: 351.52359914779663
training time full:: 351.5236518383026
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000063, Accuracy: 0.926934
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta data size:: 202
training time is 303.48898434638977
overhead:: 0
overhead2:: 0
time_baseline:: 303.48963809013367
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926983
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: model_base_line
tensor(624465)
RCV1 Test Avg. Accuracy:: 0.921856985321797
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.022678852081298828
overhead3:: 0.11102771759033203
overhead4:: 34.85869812965393
overhead5:: 0
time_provenance:: 48.55859446525574
curr_diff: 0 tensor(1.5480e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5480e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0168, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0168, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000063, Accuracy: 0.926983
cal_test_accuracy_rcv1.py:138: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = softmax_layer(torch.from_numpy(out))
Model_name:: incremental_provenance_10_10_2_0.01_10200
tensor(624465)
RCV1 Test Avg. Accuracy:: 0.921856985321797
